{
  "doc_id": "3be9ee5a5c39523452045c723581faaa",
  "source": "pytorch_docs",
  "title": "Dynamo Deep-Dive \u2014 PyTorch 2.9 documentation",
  "text": "\n## Dynamo Deep-Dive#\n\nCreated On: Apr 02, 2024 | Last Updated On: Aug 12, 2025\nTorchDynamo (or simply Dynamo) is the tracer withintorch.compile,\nand it is, more often than not, the one to blame for those insane\nbacktraces. However, we cannot blindly blame Dynamo for these errors. In\norder to provide the user with the flexibility it does, Dynamo is given\nthe arduous task of understanding any Python program. In particular,\nDynamo has to implement a good part of the Python programming language\ninternally!\ntorch.compile\nIn this post, we will go over the internal design of Dynamo from the\nground up. We will discuss the functionality it provides, and how it is\nimplemented. By the end of this post, you will have a better\nunderstanding of what went wrong when youtorch.compileda PyTorch\nprogram and the compilation errored out, or succeeded but the speed-up\nwas not what you expected.\ntorch.compiled\n\n## A Gentle Introduction to Dynamo#\n\nBefore getting our hands dirty with all the implementation details,\nlet\u2019s start by discussing what it is that Dynamo does.\nDynamo is a tracer. This means, given and function and inputs to it, it\nexecutes the function and records a linear sequence of instructions\n(without control flow) into a graph. For example, consider the following\nprogram:\n\n```python\nimport torch\n\n@torch.compile\ndef mse(x, y):\n    z = (x - y) ** 2\n    return z.sum()\n\nx = torch.randn(200)\ny = torch.randn(200)\nmse(x, y)\n\n```\n\nIf we save this program into the fileexample.pyand we run\nexample.py\n\n```python\nTORCH_LOGS=graph_code python example.py\n\n```\n\nwe see the output that Dynamo traced\n\n```python\ndef forward(l_x_: torch.Tensor, l_y_: torch.Tensor):\n    # File: example.py:5, code: z = (x - y) ** 2\n    sub = l_x_ - l_y_\n    z = sub ** 2\n    # File: example.py:6, code: return z.sum()\n    sum_1 = z.sum()\n    return (sum_1,)\n\n```\n\nWe call this agraph (or trace) of the function for the given\ninputs. This is represented via anFX\ngraph. We will simply think\nof an FX graph as a container that stores a list of function calls.\nThe first thing we should notice is that the graph is a linear sequence\nof PyTorch operations.1Dynamo records all the PyTorch operations\nand stores them sequentially. For example, it splitz=(x-y)**2into its two constituting operations,sub=l_x_-l_y_andz=sub**2.\nz=(x-y)**2\nsub=l_x_-l_y_\nz=sub**2\nWhen we say that the trace is linear, we mean that there is no branching\nor any control flow. To see this, consider\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, n):\n    y = x ** 2\n    if n >= 0:\n        return (n + 1) * y\n    else:\n        return y / n\n\nx = torch.randn(200)\nfn(x, 2)\n\n```\n\nwhich, when executed withTORCH_LOGS=graph_code, returns\nTORCH_LOGS=graph_code\n\n```python\ndef forward(l_x_: torch.Tensor):\n    # File: example.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n    # File: example.py:7, code: return (n + 1) * y\n    mul = 3 * y\n    return (mul,)\n\n```\n\nWe see that Dynamo completely removed theifstatement from the\ntrace and just recorded the operations that were executed with the\ninputs.\nif\nAs such, it should be clear thatthe trace of a function depends on\nthe inputs. In particular, this means that the trace is not generated\nwhen we write@torch.compile, but when we execute the functionfn(x,2)with the actual arguments.\n@torch.compile\nfn(x,2)\nThe other interesting thing to note here is that Dynamo removed the\nsecond argument to the function. Instead, it treated it as a constant\nand recorded the result of the operationn+1in the graph. This is\nanother feature of Dynamo: Dynamo will treat as constant any non-tensor\nvalue\u2026 other than ints. Let\u2019s see now how are ints special.\nn+1\nThe last defining property of Dynamo is that it knows how to handle\ndynamic shapes. Symbolic shapes refer to Dynamo\u2019s ability of tracing\nshapes, and more generally, integers, rather than leaving them as\nconstants. This allows for avoiding recompilations and deploying generic\nmodels that work for any size in production. The main examples of places\nwhere dynamic shapes appear are the batch size, where we might train a\nmodel with a fixed batch size but then perform inference for an\narbitrary batch size, or the variable sequence length that one\nencounters when processing text or audio.\nWe can see this by executing a few more times the example above\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, n):\n    y = x ** 2\n    if n >= 0:\n        return (n + 1) * y\n    else:\n        return y / n\n\nx = torch.randn(200)\nfn(x, 2)\nfn(x, 3)\nfn(x, -2)\n\n```\n\nIn this case,TORCH_LOGS=graph_codegenerates two more graphs\nTORCH_LOGS=graph_code\n\n```python\n# Graph for n==2 omitted\n\ndef forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):\n    # File: a.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n\n    # File: a.py:7, code: return (n + 1) * y\n    add = l_n_ + 1\n    mul = add * y\n    return (mul,)\n\n```\n\n\n```python\ndef forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):\n    # File: a.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n\n    # File: a.py:9, code: return y / n\n    truediv = y / l_n_\n    return (truediv,)\n\n```\n\nDynamo detected that one integer changed its value after the first call\nand started tracing it. We see that these graphs are generic, and trace\nthe variablensymbolically via an object of typeSymInt.\nn\nSymInt\nIf after these calls we callfn(x,4), Dynamo would not recompile,\nbut rather reuse the graph that was already traced.\nfn(x,4)\nTo summarize: 1. Dynamo is a Python tracer 2. Given some inputs, it\nreturns an FX graph with the PyTorch functions that were executed 3. It\ncan also trace integers if it detects that they changed between calls 4.\nIt specializes any other value that is not a tensor or a scalar\nOf course, Dynamo does many more things, like figuring out when it needs\nto retrace, rewriting the bytecode of the function, implementing graph\nbreaks\u2026 To keep the introduction short, we will incrementally discuss\nall these in the sequel.\n\n## PEP 523: Adding a frame evaluation API to CPython#\n\nImagine now that we are given the task to implement Dynamo. Where would\nwe even start? Rather conveniently for us,PEP\n523was released with Python 3.6.\nThis PEPwas\ndesignedto\nallow third parties to create JIT compilers for Python. Let\u2019s see how.\nA note on CPython: CPython is internally implemented as astack\nmachine. A Python\nprogram is compiled intobytecodesthat then are\nexecuted by this interpreter. To learn more about these bytecodes, see\nthedis modulefrom the\nstandard library. See alsothe developer\ndocsfor an\nintroduction to CPython\u2019s interpreter. We will assume that the reader is\nfamiliar with the notion of a stack machine.\nPEP 523 exposes an API where a user can add a custom per-function\ninterpreter. Then, CPython will use this interpreter rather than its own\nto execute the function. In order to be able to execute the function, on\nentry, CPython provides the custom interpreter with things like - The\nbytecode of the function - The value of the arguments of the function\n(i.e., the local variables) and their names - The value of the global\nvariables and their names - The builtin functions likeabsorprint\nabs\nprint\nYou can see all the fieldshere.2\nIn summary, CPython provides the user\u2019s interpreter with all the\ninformation necessary to execute the function.3\nWith this API, we can implement a tracer by implementing an interpreter\nthat runs the code and records in a graph all the PyTorch operations\nthat occur during this execution. This is exactly what Dynamo does.\nDynamo uses this CPython API to parse all these objects and packs them\nintoa Python\nstructure.\nAfter it has done so\u2026 it goes back from C to python. Other than for this\npiece of code that communicates with CPython, Dynamo is fully\nimplemented in Python.\nIt should be clear that it is the decorator@torch.compile\u2019s job\nto install the necessary scaffolding that will pass the bytecode, the\nargs, global variables and so on to Dynamo when the function is called.\nAgain,@torch.compiledoes not actually compile anything.\n@torch.compile\n@torch.compile\n\n## Implementing CPython in Python#\n\nSo, we are back in the Python world. We have the bytecode of a function,\nand all the context necessary to execute it. In particular, we have\nlanded at_convert_frame_assert.\nThis is the function that the decoratortorch.compilereturns! We\nget to this function from_dynamo.optimize.\nThe decoratortorch.compileis just a nice API around_dynamo.optimize.\ntorch.compile\ntorch.compile\n_dynamo.optimize\nBefore getting into implementing a Python interpreter, we want to define\nanIR.\nIn particular, we want to wrap all the local and global variables in our\nown internal classes. This allows us to better track these objects and\ngroup together objects that can be treated in the same way to the eyes\nof Dynamo.\nThe parent class of the internal class structure isVariableTrackerand represents the different objects that Dynamo understands. For\nexample,ListVariable, represents alistobject, and keeps\ninternally alist of VariableTrackers.\nAnother example ofVariableTrackerisConstantVariable.\nConstantVariable wraps all theobjects considered constant by\nDynamo.\nWe also have special subclasses for objects that require special\nattention, likeTensorVariable.\nAll these internal classes are defined in thetorch/_dynamo/variablesfolder.\nVariableTracker\nListVariable\nlist\nVariableTracker\nPython objects are wrapped into their correspondingVariableTrackerclass inVariableBuilder._wrap.\nThis function is just a very long chain ofelifs that tries to\nrecursively pattern-match the Python inputs into the appropriate type ofVariableTracker.\nVariableTracker\nelif\nVariableTracker\nDebugging tip. When we get unexpected results from dynamo, it is\nsometimes caused by the builder. If the logic of the builder is wrong,\nsometimes Dynamo may wrap a variable in the incorrectVariableTrackertype, and this may cause issues later on. It is\nrather useful to have a look at theVariableTrackertypes that\nappear in the errors, and theVariableTrackermethod that throws the\nexception when you encounter a Dynamo error. In particular, sometimes we\nfind that an object is tracked as aUserDefinedObjectVariable(this\nis Dynamo\u2019s catch-all class), when it should have been tracked as\nsomething more specific. In these cases, theVariableBuilderlogic is often to blame.\nVariableTracker\nVariableTracker\nVariableTracker\nUserDefinedObjectVariable\nVariableBuilder\nDebugging tip. When running a program withTORCH_LOGS=dynamo,\none of the artifacts that are printed out is lines of the form\nTORCH_LOGS=dynamo\n\n```python\nTRACE LOAD_GLOBAL y [TorchInGraphFunctionVariable(<built-in method any>), TensorVariable()]\n\n```\n\nThis is the bytecode for the original program and the state of the stack\nat that point. This is very useful to find where an object was not\ntraced into the rightVariableTracker.\nVariableTracker\nOk, so we have an IR for our tracer, now wejustneed to reimplement\nCPython\u2019s stack machine. This is implemented byInstructorTranslatorBaseinsymbolic_convert.py.\nInstructionTranslatorBasehas about 200 methods, implementing almost\nall of Python bytecodes. As an example, we can see the implementation ofBUILD_LIST\nInstructionTranslatorBase\nBUILD_LIST\n\n```python\ndef BUILD_LIST(self, inst):\n    items = self.popn(inst.argval)\n    self.push(ListVariable(items, mutation_type=ValueMutationNew()))\n\n```\n\nThis is the bytecode generated by constructions likel=[2,3,4].\nIn this case, since there are three elements, the generated bytecode isBUILD_LIST3. This means that we pop the top3elements of the\nstack and push a new list object to the top of the stack formed by these\nthree elements.\nl=[2,3,4]\nBUILD_LIST3\n3\n\n## Generating the Output Graph#\n\nWith a way to symbolically execute Python code, we are set to extract\nthe PyTorch operations that happen during the symbolic execution of a\nprogram given some inputs. This is implemented in Dynamo via theOutputGraphobject. TheOutputGraphobject isbound to an\n`InstructionTranslator objectand it tracks all the data necessary to create the FX graph which will\nbe returned by Dynamo.\nOutputGraph\nAll the inputs and intermediary elements of the FX graph arefx.Nodes. In Dynamo,fx.Nodes are wrapped infx.Proxys.fx.Proxys are used to build the FX graph.\nIn particular, they record every PyTorch operation performed on them\ninto the graph. You can create a new operation to be added to\nthe graph by callingcreate_proxy.\nThen, we can add it to the graph through the functionwrap_fx_proxy.\nfx.Node\nfx.Node\nfx.Proxy\nfx.Proxy\nA graph stores operations on tensors\u2026 and operations on symbolic\nintegers. We will discuss symbolic integers later on, but first we will\ndiscuss how Dynamo addresses a rather important correctness issue.\n\n## Making Dynamo Sound: Guards#\n\nAt this point, we have a way to trace programs completely disregarding control flow.\nAnd for that, we have reimplemented all of CPython\u2026 If this sounds like a bit of an\noverkill, that is because it is.torch.jit.tracealready implements this without all this machinery, so what gives?\nThe issue withtorch.jit.trace, as it is warned in its docs, is that\nit just works if the traced program is not data dependent. In other\nwords, it will just work if the program itself is linear. This means\nwriting our program without using if-elses, for-while loops, exceptions.\nEven more, none of the libraries that we use can use any control flow!\nAll in all, not using control flow in a language as dynamic as Python\nis, in fact, a huge constraint.\ntorch.jit.trace\nJAX solves this problem by always retracing and caching the graph after\nretracing. Dynamo, on the other hand, uses guards to avoid retracing the\nwhole program every time.\nAguardis an assumption (a boolean expression on an input) made in\norder to specialize a frame for one set of example inputs. Reusing the\ngraph is only valid if these assumptions hold on the new inputs.\nFor example, any constant input to a function, like a string, installs a\nguard stating that that input should be of typestrand equal to the\nstring we passed. Running\nstr\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a * len(b)\n\nfn(torch.arange(10), \"Hello\")\n\n```\n\nwithTORCH_LOGS=guardsprints (among other guards)\nTORCH_LOGS=guards\n\n```python\n___check_type_id(L['b'], 94334122025024)\nL['b'] == 'Hello'\n\n```\n\nThis reads as \u201cthe local variablebshould have a specific type\n(strin this case, represented by the constant9433...) and\nits value should be'Hello'\u201d. If we then execute the function\nagain passing a different argument\nb\nstr\n9433...\n'Hello'\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a * len(b)\n\nfn(torch.arange(10), \"Hello\")\nfn(torch.arange(10), \"Hi\")\n\n```\n\nwe can see the guard that failed by runningTORCH_LOGS=recompiles\nTORCH_LOGS=recompiles\n\n```python\nRecompiling function fn in script.py:3\ntriggered by the following guard failure(s):\n     - L['b'] == 'Hello'\n\n```\n\nGuards are accumulated whilethe inputs to the function are wrapped in\nthe\nbuilderandduring the execution of the\nprogram.\nWe will show many more examples of guards in the next section, but first\nlet us discuss sources.\nAsourcetracks how to reconstruct a variable from the original\nlocal or global variables present when entering the current frame. In\nparticular, it tracks the original local and global objects and any of\nthe objects they contain. In\n\n```python\ndef foo(x: Tensor, y: List[Tensor]):\n    a = x * y[0]\n    return a * x\n\n```\n\nxandyhaveLocalSourceas their source, andy[0]hasGetItemSource,\nwhich stores aLocalSourceinside. On the other hand,awill not\nhave a source as it is an intermediate variable that only exists within\nthe fx graph.\nx\ny\ny[0]\nLocalSource\na\nAll these are defined intorch/_dynamo/source.py.\nWe can see the guard generated byGetItemSourcein the following\nexample:\nGetItemSource\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, l):\n    return x * len(l[0])\n\nfn(torch.randn(8), [\"Hi\", \"Hello\"])\n\n```\n\ngenerates the following guards\n\n```python\n___check_type_id(L['l'], 94439025877664)\nlen(L['l']) == 2\n___check_type_id(L['l'][0], 94439025840192)\nL['l'][0] == 'Hi'\n___check_type_id(L['l'][1], 94439025840192)\nL['l'][1] == 'Hello'\n\n```\n\nHere, we see the code generated byGetItemSource([0]and[1]) wrapping aLocalSource(L['l']).\nGetItemSource\n[0]\n[1]\nLocalSource\nL['l']\nAt this point, with sources and guards, we are able to implement a\ncaching system to avoid recompilation without having to retrace every\ntime. We will discuss a bit more in detail this caching system in the\nsequel.\nThe attentive reader will have noticed that this does not explain yet\nwhy we need to have such fine control over the Python interpreter as to\nhaving to reimplement it. The examples of guards that we have shown\ndepend on the input objects, so we could still compute these before\nexecuting the function. In other words, we could implement this guard\nsystem on top oftorch.jit.traceand get the same functionality with\nmuch less effort\u2026 Enter symbolic shapes.\ntorch.jit.trace\n\n## Symbolic Shapes#\n\nAnother point we discussed in the introduction is that Dynamo knows how\nto trace integers. In order to implement this, we use a symbolic classtorch.SymIntthat acts like anintbut it records all the operations performed on\nit in the output FX graph.4We already saw this class in the introduction\nwhen introducing symbolic integer tracing.\nint\nLet us now discuss the three properties that define symbolic shape\ntracing in Dynamo, and how to implement them.\n\n## Static by default#\n\nDynamo assumes that every integer, let that be an input or the shape of\na tensor, is static by default. In other words, no integers will be\ntraced on the first execution of a function. Then, only if it detects\nthat an integer or a shape changed value during the execution, it will\ntrace it and generate a graph generic on that variable.\nWe already saw this behavior in the introduction using integers. Let us\nnow look at an example using shapes of tensors.\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a.shape[0] * a * b\n\nfn(torch.randn(4, 3), torch.randn(4, 3))\nfn(torch.randn(8, 3), torch.randn(8, 3))\n\n```\n\nRunning this program withTORCH_LOGS=graph_codewe see that these\ntwo calls are traced as\nTORCH_LOGS=graph_code\n\n```python\ndef forward(self, l_a_: torch.Tensor, l_b_: torch.Tensor):\n    mul = 4 * l_a_\n    mul_1 = mul * l_b_\n    return (mul_1,)\n\ndef forward(self, s0: torch.SymInt, l_a_: torch.Tensor, l_b_: torch.Tensor):\n    size = l_a_.size()\n    getitem = size[0]\n    mul = getitem * l_a_\n    mul_1 = mul * l_b_\n    return (mul_1,)\n\n```\n\nIn the first graph the shape is traced as a constant, but once it\nchanges, it traces it symbolically using aSymInts. In general, a\nsimpler way to see the shapes of the intermediary values is by running\nthe program withTORCH_LOGS=graph_sizes\nSymInt\nTORCH_LOGS=graph_sizes\n\n```python\nTRACED GRAPH TENSOR SIZES\n===== __compiled_fn_1 =====\nl_a_: (s0, 3)\nl_a_ (concrete): (8, 3)\nl_b_: (s0, 3)\nl_b_ (concrete): (8, 3)\nmul: (s0, 3)\nmul (concrete): (8, 3)\nmul_1: (s0, 3)\nmul_1 (concrete): (8, 3)\n\n```\n\nwhere we can see that the first dimension of the two tensor args is\ndynamic, given that it is represented by thes0variable.\ns0\nWe can find how Dynamo implements this by runningTORCH_LOGS=guards\nTORCH_LOGS=guards\n\n```python\n# Guards first call\ncheck_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])\ncheck_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])\n\n# Guards second call\ncheck_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])\ncheck_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])\n\nL['b'].size()[0] == L['a'].size()[0]\n2 <= L['a'].size()[0]\n\n```\n\nWe see that on the first call, the guards check that the tensors have\nsome fixed sizes and strides. These guards fail in the second execution,\nso it retraces. Since it was anintguard that failed, in this\nsecond iteration it traces thisintsymbolically and it installs\nmore general guards on this more generic kernel.\nint\nint\nCompilation performance tip. If you know that a dimension will vary\nin size, you can mark it as dynamic by callingtorch._dynamo.mark_dynamicbefore callingtorch.compile. This will avoid the first compilation\nwith a static shape. There are other useful utility functions likemaybe_mark_dynamicormark_static. You can also have all\nintegers and shapes traced by callingtorch.compile(dynamic=True).\nThis is mostly useful for debugging purposes.\ntorch.compile\nmaybe_mark_dynamic\nmark_static\ntorch.compile(dynamic=True)\n\n## 0, 1 are always specialized#\n\nRegardless of whether we mark a dimension as dynamic, if we pass an input\nwhere that dimension is 0 or 1, Dynamo will trace it as non-dynamic and it\nwill generate a specific graph for it. This is the reason why in the example\nabove we find guards of the form2<=L['a'].size()[0].\n2<=L['a'].size()[0]\nThere are several reasons for this choice. There are two particularly\nimportant - A tensor is empty if and only if any of its dimensions is\nzero - A tensor can only be contiguous if one of the strides is one\nThis policy decision does NOT apply to plain Python ints; if we think a Python\nint should be compiled dynamically, we won\u2019t specialize them by default;\ninstead, whether or not it gets specialized depends on its usage.\n\n## Duck shaping#\n\nDynamo performs what we call \u201cduck shaping\u201d. If two dynamic integers\nhave the same value at trace time, we will assume that they are equal\nand guard on it. Effectively, this means that rather than having two\nsymbolss0,s1in the example above, we just unified them tos0and had the guardL['b'].size()[0]==L['a'].size()[0]. This\nenables performing fusions within the compiler while being able to\ngenerate kernels that are generic enough.\ns0\ns1\ns0\nL['b'].size()[0]==L['a'].size()[0]\n\n## Guards on symbolic ints#\n\nWe now understand how symbolic shapes are implemented at a high level\nand the properties they have. Now, why is that symbolic shapes forced us\nthrough the tricky route of getting control of the CPython interpreter?\nConsider the following example:\n\n```python\nimport torch\n\n@torch.compile(dynamic=True)\ndef fn(a):\n    if a.shape[0] * 2 < 16:\n        return a\n    else:\n        return a + 1\n\nfn(torch.randn(8))\n\n```\n\nThis code has a guard of the form2*L['a'].size()[0]>=16. This is\na non-trivial guard in terms of the inputs of the function, but it is\nregistered in the middle of the execution of the program. Even more so,\nwe cannot know this guard is needed until we see theifstatement\nconditional on aSymNodeVariableargument. Such conditions are\ninvisible totorch.jit.traceand require deep analysis of the python\ncode.\n2*L['a'].size()[0]>=16\nif\nSymNodeVariable\ntorch.jit.trace\nDebugging tipRunning this code withTORCH_LOGS=dynamotells us\nwhere this guard was added\nTORCH_LOGS=dynamo\n\n```python\neval 2*s0 >= 16 [guard added] at script.py:5 in fn (_dynamo/variables/tensor.py:812 in evaluate_expr)\n\n```\n\nPlacing a breakpoint there and looking at the backtrace is rather useful\nto understand where a guard came from.\n\n## Making Dynamo Complete: Graph Breaks#\n\nWith all the tools we have discussed, we have a tracer that can trace\nPyTorch operations on tensors and integers and has a caching system that\nknows when it can reuse a previously traced graph and when it needs to\nretrace. All this executing arbitrary Python code!\nThere is just one small issue with this. The statement \u201cexecuting\narbitrary Python code\u201d is perhaps a bit too general. Dynamo implements a\ngood part of Python, but does it implement the more complex parts, like\ncoroutines or async? Does it implement the whole Python standard\nlibrary? NumPy also has a Python API. Doestorch.compilealso\nunderstand NumPy? and Django?5\ntorch.compile\nPython\u2019s ecosystem is massive, and a good part of it is written in other\nmore performant languages like C++ or Rust, and it just exposes Python\nbindings. There is no hope in Dynamo tracing through Python objects that\nare implemented in C++. What can a tracer do when it finds an operation\nthat it does not understand?\nThe usual way machine learning tracers handle this issue is by informing\nthe user that the operation they choked on and giving up tracing\naltogether. This would pose a real usability issue in the case of\nPyTorch, where its users are used to the flexibility it gives them. As a\nreal-world example thedoctr_det_predictormodel uses NumPy and thecv2library topostprocess the model\u2019s\nresult.\ndoctr_det_predictor\ncv2\nHere is another place where having access to CPython is interesting.\nRather than erroring out, Dynamo can let CPython run that problematic\ncode! To do this, Dynamo generates at trace time one graph with all the\noperations before the problematic code, and one with all the operations\nafter.6Then, at runtime, it will delegate to CPython to execute the\nfirst graph, then the problematic code, and then the second graph. This\nprocess of stopping the tracing and generating multiple graphs is called\nagraph break.\nA small confession: I lied all throughout the introduction and the first\nsections. Dynamo does not generate one graph, butmultiple graphs!\nFor all practical purposes, starting retracing after a second graph can\nbe thought of as starting tracing a new function. The new graph after\nthe graph break will have its own guards, its new set of local\nvariables, and so on.\nTo discuss how to implement graph breaks, we need to first revisit how\nDynamo interacts with CPython. Using PEP 523, CPython allows a user to\nuse their own frame evaluation mechanism. What we had not discussed is\nthat CPython also exposes its own frame evaluation for others to use.\nDynamo leverages this to let the fast CPython interpreter run the\ncompiled code. For a function without graph breaks, the whole tracing /\nexecution process of a program that calls the function 2 times with the\nsame arguments looks like this:\nIn the first call to the function\nDynamo traces the function into an FX graph\nThe FX graph is compiled by the compiler (Inductor) into\nefficient low-level code\u2026 but that\u2019s a story for another day\nIt rewrites the bytecode of the function so that it simply calls\nthe compiled function\nIt gives CPython this new bytecode and asks it to run ithere\nIn the second call to the function\nIt checks the guards from the first call against the new argumentshere.\nSince they are the same arguments as before, they pass\nIt asks CPython to run the bytecode associated to those guardshere\nThis process on its own looks overly complicated. Why generate new\nbytecode and ask CPython to run it rather than simply creating a C++\nbinding to the compiled function and executing it? Well, this pattern\nallows us to implement graph breaks! The bytecode generated by a graph\nbreak has the following structure:\nBytecode that executes the first graph\nBytecode that leaves the stack as it would be if CPython would have\nexecuted the first graph. It also replays any modifications to local\nor global variables that would be visible at this point\nThe bytecode that made Dynamo graph break\nBytecode that executes the second graph\nLet us see this in a simple example\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a):\n    b = a + 2\n    print(\"Hi\")\n    return b + a\n\nfn(torch.randn(4))\n\n```\n\nRunning this withTORCH_LOGS=bytecodeshows us the initial bytecode\nand the modified bytecode\nTORCH_LOGS=bytecode\n\n```python\nMODIFIED BYTECODE fn script.py line 3\n 0 LOAD_GLOBAL              1 (__compiled_fn_0)\n 2 LOAD_FAST                0 (a)\n 4 CALL_FUNCTION            1\n 6 STORE_FAST               3 (graph_out_0)\n 8 LOAD_GLOBAL              0 (print)\n10 LOAD_CONST               2 ('Hi')\n12 LOAD_FAST                3 (graph_out_0)\n14 LOAD_CONST               3 (0)\n16 BINARY_SUBSCR\n18 STORE_FAST               1 (b)\n\n20 CALL_FUNCTION            1\n22 LOAD_GLOBAL              2 (__resume_at_14_1)\n24 ROT_TWO\n26 LOAD_FAST                0 (a)\n28 LOAD_FAST                1 (b)\n30 CALL_FUNCTION            3\n32 RETURN_VALUE\n\nMODIFIED BYTECODE resume_in_fn script.py line 6\n 0 LOAD_GLOBAL              1 (__compiled_fn_2)\n 2 LOAD_FAST                2 (b)\n 4 LOAD_FAST                1 (a)\n 6 CALL_FUNCTION            2\n 8 UNPACK_SEQUENCE          1\n10 RETURN_VALUE\n\n```\n\nWe can see that the modified bytecode is split into two functions,fn, the original function, and a function calledresume_in_fn.\nThis second function is a function created by Dynamo to implement the\nexecution of the program starting at the graph break. This is often\ncalled acontinuation\nfunction. This\ncontinuation function simply calls the second compiled function with the\nright arguments. The code for the initial function is rewritten\nimplementing the strategy that we described before\nfn\nresume_in_fn\nL0-4. Call the compiled function (a+2).\na+2\nL6. Store its result in a local variable calledgraph_out_0.graph_out_0is a tuple\ngraph_out_0\ngraph_out_0\nL8-18. Leave the stack as it would be at the point of the graph break\nL20. Execute the code that caused the graph break\nL22-32. Call the compiled continuation function (a+b)\na+b\nThe code generation of the stack in Dynamo is delegated toVariableTrackersubclasses. EveryVariableTrackerobject in\nDynamo has areconstructmethod that generates the necessary bytecode to create the python object\nit represents on the stack.\nVariableTracker\nVariableTracker\nDebugging tip. Graph breaks hamper performance, and as such, it is\nbest to avoid them. Running a program withTORCH_LOGS=graph_breaksis a great way to find how many graph breaks did our program hit. The\ninformation it returns is in terms ofVariableTrackerobjects, so\nthe debugging tips above are sometimes also helpful to figure out what\ncaused that graph break.\nTORCH_LOGS=graph_breaks\nVariableTracker\n\n## Conclusion#\n\nDynamo is a complex piece of software. Once you sign up to implement a\nCPython interpreter you know you are in for a ride. That being said, we\nhope that this post helps demystify it a bit.\nDynamo is (mostly) implemented in Python. We left plenty of links to the\npieces of the code that we discussed. We hope that reading those pieces\nof code and grepping for the places that call them, or putting\nbreakpoints on them and looking at the call stack helps understanding\nthe rest of the code base.\nOf course, the best way to learn how a piece of software works is by\nextending it. In this case, the best way is to have a look at theopen\ndynamo issues on\ngithub.\nMany of them require very minor changes in the code, once you find where\nyou need to make those changes.\n\n## Footnotes#\n\nBelow are additional details and references for concepts mentioned in this document.\nIn the literature, this is called a Directed Acyclical Graph (DAG).\nAll this binding code lives intorch/csrc/dynamo/eval_frame.c.\ntorch/csrc/dynamo/eval_frame.c\nIn CPython lingo, the set of all these objects are calleda\nframe.\nThere are alsoSymBoolandSymFloatclasses. The latter one\nis not used all that much at the time of this writing.\nSymBool\nSymFloat\nInterestingly enough, it does understand NumPy code! Have a look atthis blogpostandthe docs.\nNow, this is just possible because we reimplemented NumPy using\nPyTorch. Good luck implementing Django in PyTorch though\u2026\nAssuming there is just one piece of problematic code. If there are\nmore, Dynamo can split the code into as many graphs as it needs.",
  "url": "https://pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
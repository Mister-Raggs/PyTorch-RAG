{
  "doc_id": "e0c50fc5108f0a0560c9e4e4747f8363",
  "source": "pytorch_docs",
  "title": "Pipeline Parallelism \u2014 PyTorch 2.9 documentation",
  "text": "\n## Pipeline Parallelism#\n\nCreated On: Jun 16, 2025 | Last Updated On: Aug 13, 2025\nNote\ntorch.distributed.pipeliningis currently in alpha state and under\ndevelopment. API changes may be possible. It was migrated from thePiPPyproject.\ntorch.distributed.pipelining\n\n## Why Pipeline Parallel?#\n\nPipeline Parallelism is one of theprimitiveparallelism for deep learning.\nIt allows theexecutionof a model to be partitioned such that multiplemicro-batchescan execute different parts of the model code concurrently.\nPipeline parallelism can be an effective technique for:\nlarge-scale training\nbandwidth-limited clusters\nlarge model inference\nThe above scenarios share a commonality that the computation per device cannot\nhide the communication of conventional parallelism, for example, the weight\nall-gather of FSDP.\n\n## What istorch.distributed.pipelining?#\n\ntorch.distributed.pipelining\nWhile promising for scaling, pipelining is often difficult to implement because\nit needs topartition the executionof a model in addition to model weights.\nThe partitioning of execution often requires intrusive code changes to your\nmodel. Another aspect of complexity comes fromscheduling micro-batches in a\ndistributed environment, withdata flow dependencyconsidered.\nThepipeliningpackage provides a toolkit that does said thingsautomaticallywhich allows easy implementation of pipeline parallelism\nongeneralmodels.\npipelining\nIt consists of two parts: asplitting frontendand adistributed runtime.\nThe splitting frontend takes your model code as-is, splits it up into \u201cmodel\npartitions\u201d, and captures the data-flow relationship. The distributed runtime\nexecutes the pipeline stages on different devices in parallel, handling things\nlike micro-batch splitting, scheduling, communication, and gradient propagation,\netc.\nOverall, thepipeliningpackage provides the following features:\npipelining\nSplitting of model code based on simple specification.\nRich support for pipeline schedules, including GPipe, 1F1B,\nInterleaved 1F1B and Looped BFS, and providing the infrastructure for writing\ncustomized schedules.\nFirst-class support for cross-host pipeline parallelism, as this is where PP\nis typically used (over slower interconnects).\nComposability with other PyTorch parallel techniques such as data parallel\n(DDP, FSDP) or tensor parallel. TheTorchTitanproject demonstrates a \u201c3D parallel\u201d\napplication on the Llama model.\n\n## Step 1: buildPipelineStage#\n\nPipelineStage\nBefore we can use aPipelineSchedule, we need to createPipelineStageobjects that wrap the part of the model running in that stage. ThePipelineStageis responsible for allocating communication buffers and\ncreating send/recv ops to communicate with its peers. It manages intermediate\nbuffers e.g. for the outputs of forward that have not been consumed yet, and it\nprovides a utility for running the backwards for the stage model.\nPipelineSchedule\nPipelineStage\nPipelineStage\nAPipelineStageneeds to know the input and output shapes for the stage\nmodel, so that it can correctly allocate communication buffers. The shapes must\nbe static, e.g. at runtime the shapes can not change from step to step. A classPipeliningShapeErrorwill be raised if runtime shapes do not match the\nexpected shapes. When composing with other paralleisms or applying mixed\nprecision, these techniques must be taken into account so thePipelineStageknows the correct shape (and dtype) for the output of the stage module at\nruntime.\nPipelineStage\nPipeliningShapeError\nPipelineStage\nUsers may construct aPipelineStageinstance directly, by passing in annn.Modulerepresenting the portion of the model that should run on the\nstage. This may require changes to the original model code. See the example\ninOption 1: splitting a model manually.\nPipelineStage\nnn.Module\nAlternatively, the splitting frontend can use graph partitioning to split your\nmodel into a series ofnn.Moduleautomatically. This technique requires the\nmodel is traceable withtorch.Export. Composability of the resultingnn.Modulewith other parallelism techniques is experimental, and may require\nsome workarounds. Usage of this frontend may be more appealing if the user\ncannot easily change the model code. SeeOption 2: splitting a model automaticallyfor more\ninformation.\nnn.Module\ntorch.Export\nnn.Module\n\n## Step 2: usePipelineSchedulefor execution#\n\nPipelineSchedule\nWe can now attach thePipelineStageto a pipeline schedule, and run the\nschedule with input data. Here is a GPipe example:\nPipelineStage\n\n```python\nfrom torch.distributed.pipelining import ScheduleGPipe\n\n# Create a schedule\nschedule = ScheduleGPipe(stage, n_microbatches)\n\n# Input data (whole batch)\nx = torch.randn(batch_size, in_dim, device=device)\n\n# Run the pipeline with input `x`\n# `x` will be divided into microbatches automatically\nif rank == 0:\n    schedule.step(x)\nelse:\n    output = schedule.step()\n\n```\n\nNote that the above code needs to be launched for each worker, thus we use a\nlauncher service to launch multiple processes:\n\n```python\ntorchrun --nproc_per_node=2 example.py\n\n```\n\n\n## Options for Splitting a Model#\n\n\n## Option 1: splitting a model manually#\n\nTo directly construct aPipelineStage, the user is responsible for providing\na singlenn.Moduleinstance that owns the relevantnn.Parametersandnn.Buffers, and defines aforward()method that executes the operations\nrelevant for that stage. For example, a condensed version of the Transformer\nclass defined in Torchtitan shows a pattern of building an easily partitionable\nmodel.\nPipelineStage\nnn.Module\nnn.Parameters\nnn.Buffers\nforward()\n\n```python\nclass Transformer(nn.Module):\n    def __init__(self, model_args: ModelArgs):\n        super().__init__()\n\n        self.tok_embeddings = nn.Embedding(...)\n\n        # Using a ModuleDict lets us delete layers without affecting names,\n        # ensuring checkpoints will correctly save and load.\n        self.layers = torch.nn.ModuleDict()\n        for layer_id in range(model_args.n_layers):\n            self.layers[str(layer_id)] = TransformerBlock(...)\n\n        self.output = nn.Linear(...)\n\n    def forward(self, tokens: torch.Tensor):\n        # Handling layers being 'None' at runtime enables easy pipeline splitting\n        h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\n\n        for layer in self.layers.values():\n            h = layer(h, self.freqs_cis)\n\n        h = self.norm(h) if self.norm else h\n        output = self.output(h).float() if self.output else h\n        return output\n\n```\n\nA model defined in this manner can be easily configured per stage by first\ninitializing the whole model (using meta-device to avoid OOM errors), deleting\nundesired layers for that stage, and then creating a PipelineStage that wraps\nthe model. For example:\n\n```python\nwith torch.device(\"meta\"):\n    assert num_stages == 2, \"This is a simple 2-stage example\"\n\n    # we construct the entire model, then delete the parts we do not need for this stage\n    # in practice, this can be done using a helper function that automatically divides up layers across stages.\n    model = Transformer()\n\n    if stage_index == 0:\n        # prepare the first stage model\n        del model.layers[\"1\"]\n        model.norm = None\n        model.output = None\n\n    elif stage_index == 1:\n        # prepare the second stage model\n        model.tok_embeddings = None\n        del model.layers[\"0\"]\n\n    from torch.distributed.pipelining import PipelineStage\n    stage = PipelineStage(\n        model,\n        stage_index,\n        num_stages,\n        device,\n    )\n\n```\n\nWhen composing with other Data or Model parallelism techniques,output_argsmay also be required, if the output shape/dtype of the model chunk will be\naffected.\noutput_args\n\n## Option 2: splitting a model automatically#\n\nIf you have a full model and do not want to spend time on modifying it into a\nsequence of \u201cmodel partitions\u201d, thepipelineAPI is here to help.\nHere is a brief example:\npipeline\n\n```python\nclass Model(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.emb = torch.nn.Embedding(10, 3)\n        self.layers = torch.nn.ModuleList(\n            Layer() for _ in range(2)\n        )\n        self.lm = LMHead()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.emb(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.lm(x)\n        return x\n\n```\n\nIf we print the model, we can see multiple hierarchies, which makes it hard to split by hand:\n\n```python\nModel(\n  (emb): Embedding(10, 3)\n  (layers): ModuleList(\n    (0-1): 2 x Layer(\n      (lin): Linear(in_features=3, out_features=3, bias=True)\n    )\n  )\n  (lm): LMHead(\n    (proj): Linear(in_features=3, out_features=3, bias=True)\n  )\n)\n\n```\n\nLet us see how thepipelineAPI works:\npipeline\n\n```python\nfrom torch.distributed.pipelining import pipeline, SplitPoint\n\n# An example micro-batch input\nx = torch.LongTensor([1, 2, 4, 5])\n\npipe = pipeline(\n    module=mod,\n    mb_args=(x,),\n    split_spec={\n        \"layers.1\": SplitPoint.BEGINNING,\n    }\n)\n\n```\n\nThepipelineAPI splits your model given asplit_spec, whereSplitPoint.BEGINNINGstands for adding a split pointbeforeexecution of certain submodule in theforwardfunction, and\nsimilarly,SplitPoint.ENDfor split pointaftersuch.\npipeline\nsplit_spec\nSplitPoint.BEGINNING\nforward\nSplitPoint.END\nIf weprint(pipe), we can see:\nprint(pipe)\n\n```python\nGraphModule(\n  (submod_0): GraphModule(\n    (emb): InterpreterModule()\n    (layers): Module(\n      (0): InterpreterModule(\n        (lin): InterpreterModule()\n      )\n    )\n  )\n  (submod_1): GraphModule(\n    (layers): Module(\n      (1): InterpreterModule(\n        (lin): InterpreterModule()\n      )\n    )\n    (lm): InterpreterModule(\n      (proj): InterpreterModule()\n    )\n  )\n)\n\ndef forward(self, x):\n    submod_0 = self.submod_0(x);  x = None\n    submod_1 = self.submod_1(submod_0);  submod_0 = None\n    return (submod_1,)\n\n```\n\nThe \u201cmodel partitions\u201d are represented by submodules (submod_0,submod_1), each of which is reconstructed with original model operations, weights\nand hierarchies. In addition, a \u201croot-level\u201dforwardfunction is\nreconstructed to capture the data flow between those partitions. Such data flow\nwill be replayed by the pipeline runtime later, in a distributed fashion.\nsubmod_0\nsubmod_1\nforward\nThePipeobject provides a method for retrieving the \u201cmodel partitions\u201d:\nPipe\n\n```python\nstage_mod : nn.Module = pipe.get_stage_module(stage_idx)\n\n```\n\nThe returnedstage_modis ann.Module, with which you can create an\noptimizer, save or load checkpoints, or apply other parallelisms.\nstage_mod\nnn.Module\nPipealso allows you to create a distributed stage runtime on a device given\naProcessGroup:\nPipe\nProcessGroup\n\n```python\nstage = pipe.build_stage(stage_idx, device, group)\n\n```\n\nAlternatively, if you would like to build the stage runtime later after some\nmodification to thestage_mod, you can use a functional version of thebuild_stageAPI. For example:\nstage_mod\nbuild_stage\n\n```python\nfrom torch.distributed.pipelining import build_stage\nfrom torch.nn.parallel import DistributedDataParallel\n\ndp_mod = DistributedDataParallel(stage_mod)\ninfo = pipe.info()\nstage = build_stage(dp_mod, stage_idx, info, device, group)\n\n```\n\nNote\nThepipelinefrontend uses a tracer (torch.export) to capture your\nmodel into a single graph. If your model is not full-graph\u2019able, you can use\nour manual frontend below.\npipeline\ntorch.export\n\n## Hugging Face Examples#\n\nIn thePiPPyrepo where this package was\noriginal created, we kept examples based on unmodified Hugging Face models.\nSee theexamples/huggingfacedirectory.\nExamples include:\nGPT2\nLlama\n\n## Technical Deep Dive#\n\n\n## How does thepipelineAPI split a model?#\n\npipeline\nFirst, thepipelineAPI turns our model into a directed acyclic graph (DAG)\nby tracing the model. It traces the model usingtorch.export\u2013 a PyTorch 2\nfull-graph capturing tool.\npipeline\ntorch.export\nThen, it groups together theoperations and parametersneeded by a stage\ninto a reconstructed submodule:submod_0,submod_1, \u2026\nsubmod_0\nsubmod_1\nDifferent from conventional submodule access methods likeModule.children(),\nthepipelineAPI does not only cut the module structure of your model, but\nalso theforwardfunction of your model.\nModule.children()\npipeline\nThis is necessary because model structure likeModule.children()merely\ncaptures information duringModule.__init__(), and does not capture any\ninformation aboutModule.forward(). Said differently,Module.children()lacks information about the following aspects key to pipelininig:\nModule.children()\nModule.__init__()\nModule.forward()\nModule.children()\nExecution order of child modules inforward\nforward\nActivation flows between child modules\nWhether there are any functional operators between child modules (for example,reluoraddoperations will not be captured byModule.children()).\nrelu\nadd\nModule.children()\nThepipelineAPI, on the contrary, makes sure that theforwardbehavior\nis truly preserved. It also captures the activation flow between the partitions,\nhelping the distributed runtime to make correct send/receive calls without human\nintervention.\npipeline\nforward\nAnother flexibility of thepipelineAPI is that split points can be at\narbitrary levels within your model hierarchy. In the split partitions, the original model\nhierarchy related to that partition will be reconstructed at no cost to you.\nAt a result, fully-qualified names (FQNs) pointing to a submodule or parameter\nwould be still valid, and services that relies on FQNs (such as FSDP, TP or\ncheckpointing) can still run with your partitioned modules with almost zero code\nchange.\npipeline\n\n## Implementing Your Own Schedule#\n\nYou can implement your own pipeline schedule by extending one of the following two class:\nPipelineScheduleSingle\nPipelineScheduleSingle\nPipelineScheduleMulti\nPipelineScheduleMulti\nPipelineScheduleSingleis for schedules that assignsonly onestage per rank.PipelineScheduleMultiis for schedules that assigns multiple stages per rank.\nPipelineScheduleSingle\nPipelineScheduleMulti\nFor example,ScheduleGPipeandSchedule1F1Bare subclasses ofPipelineScheduleSingle.\nWhereas,ScheduleInterleaved1F1B,ScheduleLoopedBFS,ScheduleInterleavedZeroBubble, andScheduleZBVZeroBubbleare subclasses ofPipelineScheduleMulti.\nScheduleGPipe\nSchedule1F1B\nPipelineScheduleSingle\nScheduleInterleaved1F1B\nScheduleLoopedBFS\nScheduleInterleavedZeroBubble\nScheduleZBVZeroBubble\nPipelineScheduleMulti\n\n## Logging#\n\nYou can turn on additional logging using theTORCH_LOGSenvironment variable fromtorch._logging:\nTORCH_LOGS\nTORCH_LOGS=+ppwill displaylogging.DEBUGmessages and all levels above it.\nTORCH_LOGS=+pp\nlogging.DEBUG\nTORCH_LOGS=ppwill displaylogging.INFOmessages and above.\nTORCH_LOGS=pp\nlogging.INFO\nTORCH_LOGS=-ppwill displaylogging.WARNINGmessages and above.\nTORCH_LOGS=-pp\nlogging.WARNING\n\n## API Reference#\n\n\n## Model Split APIs#\n\nThe following set of APIs transform your model into a pipeline representation.\nEnum representing the points at which a split can occur in the execution of a submodule.\n:ivar BEGINNING: Represents adding a split pointbeforethe execution of a certain submodule in theforwardfunction.\n:ivar END: Represents adding a split pointafterthe execution of a certain submodule in theforwardfunction.\nSplit a module based on a specification.\nSeePipefor more details.\nmodule(Module) \u2013 The module to be split.\nmb_args(tuple[Any,...]) \u2013 Example positional inputs, in micro-batch form.\nmb_kwargs(Optional[dict[str,Any]]) \u2013 Example keyword inputs, in micro-batch form. (default:None)\nsplit_spec(Optional[dict[str,torch.distributed.pipelining._IR.SplitPoint]]) \u2013 A dictionary using submodule names as split marker. (default:None)\nsplit_policy(Optional[Callable[[GraphModule],GraphModule]]) \u2013 The policy to use for splitting the module. (default:None)\nA pipeline representation of classPipe.\npipe_split is a special operator that is used to mark the boundary between\nstages in a module. It is used to split the module into stages. It is a\nno-op if your annotated module is run eagerly.\nExample\n\n```python\n>>> def forward(self, x):\n>>>     x = torch.mm(x, self.mm_param)\n>>>     x = torch.relu(x)\n>>>     pipe_split()\n>>>     x = self.lin(x)\n>>>     return x\n\n```\n\nThe above example will be split into two stages.\n\n## Microbatch Utilities#\n\nClass used to specify chunking of inputs\nGiven a sequence of args and kwargs, split them into a number of chunks\naccording to  their respective chunking specs.\nargs(tuple[Any,...]) \u2013 Tuple of args\nkwargs(Optional[dict[str,Any]]) \u2013 Dict of kwargs\nchunks(int) \u2013 Number of chunks to split the args and kwargs into\nargs_chunk_spec(Optional[tuple[torch.distributed.pipelining.microbatch.TensorChunkSpec,...]]) \u2013 chunking specs for args, in same shape as args\nkwargs_chunk_spec(Optional[dict[str,torch.distributed.pipelining.microbatch.TensorChunkSpec]]) \u2013 chunking specs for kwargs, in same shape as kwargs\nList of sharded args\nkwargs_split: List of sharded kwargs\nargs_split\nGiven a list of chunks, merge them into a single value according to\nthe chunk spec.\nchunks(list[Any]) \u2013 list of chunks\nchunk_spec\u2013 Chunking spec for the chunks\nMerged value\nvalue\n\n## Pipeline Stages#\n\nA class representing a pipeline stage in a pipeline parallelism setup.\nPipelineStage assumes sequential partitioning of the model, i.e. the model is split into chunks where outputs from\none chunk feed into inputs of the next chunk, with no skip connections.\nPipelineStage performs runtime shape/dtype inference automatically by propagating the outputs from stage0 to\nstage1 and so forth, in linear order.  To bypass shape inference, pass theinput_argsandoutput_argsto each\nPipelineStage instance.\nsubmodule(nn.Module) \u2013 The PyTorch module wrapped by this stage.\nstage_index(int) \u2013 The ID of this stage.\nnum_stages(int) \u2013 The total number of stages.\ndevice(torch.device) \u2013 The device where this stage is located.\ninput_args(Union[torch.Tensor,Tuple[torch.tensor]],optional) \u2013 The input arguments for the submodule.\noutput_args(Union[torch.Tensor,Tuple[torch.tensor]],optional) \u2013 The output arguments for the submodule.\ngroup(dist.ProcessGroup,optional) \u2013 The process group for distributed training. If None, default group.\ndw_builder(Optional[Callable[[],Callable[...,None]]) \u2013 If provided, dw_builder will build a new dw_runner function\nthat will the W action (input weights) for F, I, W (Fwd, Input, Weight) zero bubble schedules.\nCreate a pipeline stage given a stage_module to be wrapped by this stage\nand pipeline information.\nstage_module(torch.nn.Module) \u2013 the module to be wrapped by this stage\nstage_index(int) \u2013 the index of this stage in the pipeline\npipe_info(PipeInfo) \u2013 information about the pipeline, can be retrieved bypipe.info()\ndevice(torch.device) \u2013 the device to be used by this stage\ngroup(Optional[dist.ProcessGroup]) \u2013 the process group to be used by this stage\na pipeline stage that can run withPipelineSchedules.\n_PipelineStage\n\n## Pipeline Schedules#\n\nThe GPipe schedule.\nWill go through all the microbatches in a fill-drain manner.\nThe 1F1B schedule.\nWill perform one forward and one backward on the microbatches in steady state.\nThe Interleaved 1F1B schedule.\nSeehttps://arxiv.org/pdf/2104.04473for details.\nWill perform one forward and one backward on the microbatches in steady\nstate and supports multiple stages per rank. When microbatches are ready for\nmultiple local stages, Interleaved 1F1B prioritizes the earlier microbatch\n(also called \u201cdepth first\u201d).\nThis schedule is mostly similar to the original paper.\nIt differs by being relaxing the requirement of num_microbatch % pp_size == 0.\nUsing the flex_pp schedule, we will have num_rounds = max(1, n_microbatches // pp_group_size) and\nit works as long as n_microbatches % num_rounds is 0. As a few examples, support\npp_group_size = 4, n_microbatches = 10. We will have num_rounds = 2 and n_microbatches % 2 is 0.\npp_group_size = 4, n_microbatches = 3. We will have num_rounds = 1 and n_microbatches % 1 is 0.\nBreadth-First Pipeline Parallelism.\nSeehttps://arxiv.org/abs/2211.05953for details.\nSimilar to Interleaved 1F1B, Looped BFS supports multiple stages per rank.\nWhat is different is that when microbatches are ready for multiple local\nstages, Loops BFS will prioritizes the earlier stage, running all available\nmicrobatches at once.\nThe Interleaved Zero Bubble schedule.\nSeehttps://arxiv.org/pdf/2401.10241for details.\nWill perform one forward and one backward on inputs for the microbatches in steady\nstate and supports multiple stages per rank. Uses the backward for weights to fill in\nthe pipeline bubble.\nIn particular this is implementing the ZB1P schedule in the paper.\nThe Zero Bubble schedule (ZBV variant).\nSeehttps://arxiv.org/pdf/2401.10241Section 6 for details.\nThis schedules requires exactly two stages per rank.\nThis schedule will perform one forward and one backward on inputs for the microbatches in steady\nstate and supports multiple stages per rank. Uses backward with respect to weights to fill in\nthe pipeline bubble.\nThis ZB-V schedule would have the \u201czero bubble\u201d property only if time forward == time backward input == time backward weights.\nIn practice, this is not likely true for real models so alternatively\na greedy scheduler could be implemented for unequal/unbalanced time.\nThe DualPipeV schedule. A more efficient schedule variant based on the\nDualPipe schedule introduced by DeepSeek inhttps://arxiv.org/pdf/2412.19437\nBased on the open sourced code fromdeepseek-ai/DualPipe\nBase class for single-stage schedules.\nImplements thestepmethod.\nDerived classes should implement_step_microbatches.\nGradients are scaled by num_microbatches depending on thescale_gradsargument, defaulting to True.  This setting\nshould match the configuration of your loss_fn, which may either average losses (scale_grads=True)\nor sum losses (scale_grads=False).\nRun one iteration of the pipeline schedule withwhole-batchinput.\nWill chunk the input into microbatches automatically, and go through the\nmicrobatches according to the schedule implementation.\nargs: positional arguments to the model (as in non-pipeline case).\nkwargs: keyword arguments to the model (as in non-pipeline case).\ntarget: target for the loss function.\nlosses: a list to store the losses for each microbatch.\nBase class for multi-stage schedules.\nImplements thestepmethod.\nGradients are scaled by num_microbatches depending on thescale_gradsargument, defaulting to True.  This setting\nshould match the configuration of your loss_fn, which may either average losses (scale_grads=True)\nor sum losses (scale_grads=False).\nRun one iteration of the pipeline schedule withwhole-batchinput.\nWill chunk the input into microbatches automatically, and go through the\nmicrobatches according to the schedule implementation.\nargs: positional arguments to the model (as in non-pipeline case).\nkwargs: keyword arguments to the model (as in non-pipeline case).\ntarget: target for the loss function.\nlosses: a list to store the losses for each microbatch.",
  "url": "https://pytorch.org/docs/stable/distributed.pipelining.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
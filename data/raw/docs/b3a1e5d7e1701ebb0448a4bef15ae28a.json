{
  "doc_id": "b3a1e5d7e1701ebb0448a4bef15ae28a",
  "source": "pytorch_docs",
  "title": "Multiprocessing best practices \u2014 PyTorch 2.9 documentation",
  "text": "\n## Multiprocessing best practices#\n\nCreated On: Jan 16, 2017 | Last Updated On: Jun 18, 2025\ntorch.multiprocessingis a drop in replacement for Python\u2019smultiprocessingmodule. It supports the exact same operations,\nbut extends it, so that all tensors sent through amultiprocessing.Queue, will have their data moved into shared\nmemory and will only send a handle to another process.\ntorch.multiprocessing\nmultiprocessing\nmultiprocessing.Queue\nNote\nWhen aTensoris sent to another process, theTensordata is shared. Iftorch.Tensor.gradis\nnotNone, it is also shared. After aTensorwithout\natorch.Tensor.gradfield is sent to the other process, it\ncreates a standard process-specific.gradTensorthat\nis not automatically shared across all processes, unlike how theTensor\u2019s data has been shared.\nTensor\nTensor\ntorch.Tensor.grad\nNone\nTensor\ntorch.Tensor.grad\n.grad\nTensor\nTensor\nThis allows to implement various training methods, like Hogwild, A3C, or any\nothers that require asynchronous operation.\n\n## Poison fork in multiprocessing#\n\nWhen using multiprocessing withaccelerators, a known issue called \u201cpoison fork\u201d may occur.\nThis happens when the accelerator\u2019s runtime is not fork safe and is initialized before a process forks, leading to\nruntime errors in child processes.\nAvoid initializing the accelerator in the main process before forking child processes.\nUse an alternative process start methods, such asspawnorforkserver, which ensures a clean initialization of each process.\nspawn\nforkserver\n\n## CUDA in multiprocessing#\n\nThe CUDA runtime has the limitation described inPoison fork in multiprocessingwhen using theforkstart method;\neither thespawnorforkserverstart method are required to use CUDA in subprocesses.\nfork\nspawn\nforkserver\nNote\nThe start method can be set via either creating a context withmultiprocessing.get_context(...)or directly usingmultiprocessing.set_start_method(...).\nmultiprocessing.get_context(...)\nmultiprocessing.set_start_method(...)\nUnlike CPU tensors, the sending process is required to keep the original tensor\nas long as the receiving process retains a copy of the tensor. It is implemented\nunder the hood but requires users to follow the best practices for the program\nto run correctly. For example, the sending process must stay alive as long as\nthe consumer process has references to the tensor, and the refcounting can not\nsave you if the consumer process exits abnormally via a fatal signal. Seethis section.\nSee also:Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel\n\n## Best practices and tips#\n\n\n## Avoiding and fighting deadlocks#\n\nThere are a lot of things that can go wrong when a new process is spawned, with\nthe most common cause of deadlocks being background threads. If there\u2019s any\nthread that holds a lock or imports a module, andforkis called, it\u2019s very\nlikely that the subprocess will be in a corrupted state and will deadlock or\nfail in a different way. Note that even if you don\u2019t, Python built in\nlibraries do - no need to look further thanmultiprocessing.multiprocessing.Queueis actually a very complex class, that\nspawns multiple threads used to serialize, send and receive objects, and they\ncan cause aforementioned problems too. If you find yourself in such situation\ntry using aSimpleQueue, that doesn\u2019t\nuse any additional threads.\nfork\nmultiprocessing\nmultiprocessing.Queue\nSimpleQueue\nWe\u2019re trying our best to make it easy for you and ensure these deadlocks don\u2019t\nhappen but some things are out of our control. If you have any issues you can\u2019t\ncope with for a while, try reaching out on forums, and we\u2019ll see if it\u2019s an\nissue we can fix.\n\n## Reuse buffers passed through a Queue#\n\nRemember that each time you put aTensorinto amultiprocessing.Queue, it has to be moved into shared memory.\nIf it\u2019s already shared, it is a no-op, otherwise it will incur an additional\nmemory copy that can slow down the whole process. Even if you have a pool of\nprocesses sending data to a single one, make it send the buffers back - this\nis nearly free and will let you avoid a copy when sending next batch.\nTensor\nmultiprocessing.Queue\n\n## Asynchronous multiprocess training (e.g. Hogwild)#\n\nUsingtorch.multiprocessing, it is possible to train a model\nasynchronously, with parameters either shared all the time, or being\nperiodically synchronized. In the first case, we recommend sending over the whole\nmodel object, while in the latter, we advise to only send thestate_dict().\ntorch.multiprocessing\nstate_dict()\nWe recommend usingmultiprocessing.Queuefor passing all kinds\nof PyTorch objects between processes. It is possible to e.g. inherit the tensors\nand storages already in shared memory, when using theforkstart method,\nhowever it is very bug prone and should be used with care, and only by advanced\nusers. Queues, even though they\u2019re sometimes a less elegant solution, will work\nproperly in all cases.\nmultiprocessing.Queue\nfork\nWarning\nYou should be careful about having global statements, that are not guarded\nwith anif__name__=='__main__'. If a different start method thanforkis used, they will be executed in all subprocesses.\nif__name__=='__main__'\nfork\nA concrete Hogwild implementation can be found in theexamples repository,\nbut to showcase the overall structure of the code, there\u2019s also a minimal\nexample below as well:\n\n```python\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n\n```\n\n\n## CPU in multiprocessing#\n\nInappropriate multiprocessing can lead to CPU oversubscription, causing\ndifferent processes to compete for CPU resources, resulting in low\nefficiency.\nThis tutorial will explain what CPU oversubscription is and how to\navoid it.\n\n## CPU oversubscription#\n\nCPU oversubscription is a technical term that refers to a situation\nwhere the total number of vCPUs allocated to a system exceeds the total\nnumber of vCPUs available on the hardware.\nThis leads to severe contention for CPU resources. In such cases, there\nis frequent switching between processes, which increases processes\nswitching overhead and decreases overall system efficiency.\nSee CPU oversubscription with the code examples in the Hogwild\nimplementation found in theexample\nrepository.\nWhen running the training example with the following command on CPU\nusing 4 processes:\n\n```python\npython main.py --num-processes 4\n\n```\n\nAssuming there are N vCPUs available on the machine, executing the above\ncommand will generate 4 subprocesses. Each subprocess will allocate N\nvCPUs for itself, resulting in a requirement of 4*N vCPUs. However, the\nmachine only has N vCPUs available. Consequently, the different\nprocesses will compete for resources, leading to frequent process\nswitching.\nThe following observations indicate the presence of CPU over\nsubscription:\nHigh CPU Utilization: By using thehtopcommand, you can observe\nthat the CPU utilization is consistently high, often reaching or\nexceeding its maximum capacity. This indicates that the demand for\nCPU resources exceeds the available physical cores, causing\ncontention and competition among processes for CPU time.\nhtop\nFrequent Context Switching with Low System Efficiency: In an\noversubscribed CPU scenario, processes compete for CPU time, and the\noperating system needs to rapidly switch between different processes\nto allocate resources fairly. This frequent context switching adds\noverhead and reduces the overall system efficiency.\n\n## Avoid CPU oversubscription#\n\nA good way to avoid CPU oversubscription is proper resource allocation.\nEnsure that the number of processes or threads running concurrently does\nnot exceed the available CPU resources.\nIn this case, a solution would be to specify the appropriate number of\nthreads in the subprocesses. This can be achieved by setting the number\nof threads for each process using thetorch.set_num_threads(int)function in subprocess.\ntorch.set_num_threads(int)\nAssuming there are N vCPUs on the machine and M processes will be\ngenerated, the maximumnum_threadsvalue used by each process would\nbefloor(N/M). To avoid CPU oversubscription in the mnist_hogwild\nexample, the following changes are needed for the filetrain.pyinexample\nrepository.\nnum_threads\nfloor(N/M)\ntrain.py\n\n```python\ndef train(rank, args, model, device, dataset, dataloader_kwargs):\n    torch.manual_seed(args.seed + rank)\n\n    #### define the num threads used in current sub-processes\n    torch.set_num_threads(floor(N/M))\n\n    train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)\n\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    for epoch in range(1, args.epochs + 1):\n        train_epoch(epoch, args, model, device, train_loader, optimizer)\n\n```\n\nSetnum_threadfor each process usingtorch.set_num_threads(floor(N/M)). where you replace N with the\nnumber of vCPUs available and M with the chosen number of processes. The\nappropriatenum_threadvalue will vary depending on the specific\ntask at hand. However, as a general guideline, the maximum value for thenum_threadshould befloor(N/M)to avoid CPU oversubscription.\nIn themnist_hogwildtraining example, after avoiding CPU over\nsubscription, you can achieve a 30x performance boost.\nnum_thread\ntorch.set_num_threads(floor(N/M))\nnum_thread\nnum_thread\nfloor(N/M)",
  "url": "https://pytorch.org/docs/stable/notes/multiprocessing.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
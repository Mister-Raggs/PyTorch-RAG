{
  "doc_id": "d2c493d5680a9f4df51ecc37989ebccf",
  "source": "pytorch_docs",
  "title": "Gradcheck mechanics \u2014 PyTorch 2.9 documentation",
  "text": "\n## Gradcheck mechanics#\n\nCreated On: Apr 27, 2021 | Last Updated On: Jun 18, 2025\nThis note presents an overview of how thegradcheck()andgradgradcheck()functions work.\ngradcheck()\ngradgradcheck()\nIt will cover both forward and backward mode AD for both real and complex-valued functions as well as higher-order derivatives.\nThis note also covers both the default behavior of gradcheck as well as the case wherefast_mode=Trueargument is passed (referred to as fast gradcheck below).\nfast_mode=True\nNotations and background information\nDefault backward mode gradcheck behavior\nReal-to-real functions\nComplex-to-real functions\nFunctions with complex outputs\nFast backward mode gradcheck\nFast gradcheck for real-to-real functions\nFast gradcheck for complex-to-real functions\nFast gradcheck for functions with complex outputs\nGradgradcheck implementation\n\n## Notations and background information#\n\nThroughout this note, we will use the following convention:\nxxx,yyy,aaa,bbb,vvv,uuu,urururanduiuiuiare real-valued vectors andzzzis a complex-valued vector that can be rewritten in terms of two real-valued vectors asz=a+ibz = a + i bz=a+ib.\nNNNandMMMare two integers that we will use for the dimension of the input and output space respectively.\nf:RN\u2192RMf: \\mathcal{R}^N \\to \\mathcal{R}^Mf:RN\u2192RMis our basic real-to-real function such thaty=f(x)y = f(x)y=f(x).\ng:CN\u2192RMg: \\mathcal{C}^N \\to \\mathcal{R}^Mg:CN\u2192RMis our basic complex-to-real function such thaty=g(z)y = g(z)y=g(z).\nFor the simple real-to-real case, we write asJfJ_fJf\u200bthe Jacobian matrix associated withfffof sizeM\u00d7NM \\times NM\u00d7N.\nThis matrix contains all the partial derivatives such that the entry at position(i,j)(i, j)(i,j)contains\u2202yi\u2202xj\\frac{\\partial y_i}{\\partial x_j}\u2202xj\u200b\u2202yi\u200b\u200b.\nBackward mode AD is then computing, for a given vectorvvvof sizeMMM, the quantityvTJfv^T J_fvTJf\u200b.\nForward mode AD on the other hand is computing, for a given vectoruuuof sizeNNN, the quantityJfuJ_f uJf\u200bu.\nFor functions that contain complex values, the story is a lot more complex. We only provide the gist here and the full description can be found atAutograd for Complex Numbers.\nThe constraints to satisfy complex differentiability (Cauchy-Riemann equations) are too restrictive for all real-valued loss functions, so we instead opted to use Wirtinger calculus.\nIn a basic setting of Wirtinger calculus, the chain rule requires access to both the Wirtinger derivative (calledWWWbelow) and the Conjugate Wirtinger derivative (calledCWCWCWbelow).\nBothWWWandCWCWCWneed to be propagated because in general, despite their name, one is not the complex conjugate of the other.\nTo avoid having to propagate both values, for backward mode AD, we always work under the assumption that the function whose derivative is being calculated is either a real-valued function or is part of a bigger real-valued function. This assumption means that all the intermediary gradients we compute during the backward pass are also associated with real-valued functions.\nIn practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers).\nUnder this assumption, usingWWWandCWCWCWdefinitions, we can show thatW=CW\u2217W = CW^*W=CW\u2217(we use\u2217*\u2217to denote complex conjugation here) and so only one of the two values actually need to be \u201cbackwarded through the graph\u201d as the other one can easily be recovered.\nTo simplify internal computations, PyTorch uses2\u2217CW2 * CW2\u2217CWas the value it backwards and returns when the user asks for gradients.\nSimilarly to the real case, when the output is actually inRM\\mathcal{R}^MRM, backward mode AD does not compute2\u2217CW2 * CW2\u2217CWbut onlyvT(2\u2217CW)v^T (2 * CW)vT(2\u2217CW)for a given vectorv\u2208RMv \\in \\mathcal{R}^Mv\u2208RM.\nFor forward mode AD, we use a similar logic, in this case, assuming that the function is part of a larger function whose input is inR\\mathcal{R}R. Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is inR\\mathcal{R}Rand in this case, usingWWWandCWCWCWdefinitions, we can show thatW=CWW = CWW=CWfor the intermediary functions.\nTo make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes2\u2217CW2 * CW2\u2217CW.\nSimilarly to the real case, when the input is actually inRN\\mathcal{R}^NRN, forward mode AD does not compute2\u2217CW2 * CW2\u2217CWbut only(2\u2217CW)u(2 * CW) u(2\u2217CW)ufor a given vectoru\u2208RNu \\in \\mathcal{R}^Nu\u2208RN.\n\n## Default backward mode gradcheck behavior#\n\n\n## Real-to-real functions#\n\nTo test a functionf:RN\u2192RM,x\u2192yf: \\mathcal{R}^N \\to \\mathcal{R}^M, x \\to yf:RN\u2192RM,x\u2192y, we reconstruct the full Jacobian matrixJfJ_fJf\u200bof sizeM\u00d7NM \\times NM\u00d7Nin two ways: analytically and numerically.\nThe analytical version uses our backward mode AD while the numerical version uses finite difference.\nThe two reconstructed Jacobian matrices are then compared elementwise for equality.\nIf we consider the elementary case of a one-dimensional function (N=M=1N = M = 1N=M=1), then we can use the basic finite difference formula fromthe wikipedia article. We use the \u201ccentral difference\u201d for better numerical properties:\nThis formula easily generalizes for multiple outputs (M>1M \\gt 1M>1) by having\u2202y\u2202x\\frac{\\partial y}{\\partial x}\u2202x\u2202y\u200bbe a column vector of sizeM\u00d71M \\times 1M\u00d71likef(x+eps)f(x + eps)f(x+eps).\nIn that case, the above formula can be reused as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namelyf(x+eps)f(x + eps)f(x+eps)andf(x\u2212eps)f(x - eps)f(x\u2212eps)).\nIt is more computationally expensive to handle the case with multiple inputs (N>1N \\gt 1N>1). In this scenario, we loop over all the inputs one after the other and apply theepsepsepsperturbation for each element ofxxxone after the other. This allows us to reconstruct theJfJ_fJf\u200bmatrix column by column.\nFor the analytical evaluation, we use the fact, as described above, that backward mode AD computesvTJfv^T J_fvTJf\u200b.\nFor functions with a single output, we simply usev=1v = 1v=1to recover the full Jacobian matrix with a single backward pass.\nFor functions with more than one output, we resort to a for-loop which iterates over the outputs where eachvvvis a one-hot vector corresponding to each output one after the other. This allows to reconstruct theJfJ_fJf\u200bmatrix row by row.\n\n## Complex-to-real functions#\n\nTo test a functiong:CN\u2192RM,z\u2192yg: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to yg:CN\u2192RM,z\u2192ywithz=a+ibz = a + i bz=a+ib, we reconstruct the (complex-valued) matrix that contains2\u2217CW2 * CW2\u2217CW.\nConsider the elementary case whereN=M=1N = M = 1N=M=1first. We know from (chapter 3 of)this research paperthat:\nNote that\u2202y\u2202a\\frac{\\partial y}{\\partial a}\u2202a\u2202y\u200band\u2202y\u2202b\\frac{\\partial y}{\\partial b}\u2202b\u2202y\u200b, in the above equation, areR\u2192R\\mathcal{R} \\to \\mathcal{R}R\u2192Rderivatives.\nTo evaluate these numerically, we use the method described above for the real-to-real case.\nThis allows us to compute theCWCWCWmatrix and then multiply it by222.\nNote that the code, as of time of writing, computes this value in a slightly convoluted way:\n\n```python\n# Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105\n# Notation changes in this code block:\n# s here is y above\n# x, y here are a, b above\n\nds_dx = compute_gradient(eps)\nds_dy = compute_gradient(eps * 1j)\n# conjugate wirtinger derivative\nconj_w_d = 0.5 * (ds_dx + ds_dy * 1j)\n# wirtinger derivative\nw_d = 0.5 * (ds_dx - ds_dy * 1j)\nd[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj()\n\n# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.\n\n```\n\nSince backward mode AD computes exactly twice theCWCWCWderivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs.\n\n## Functions with complex outputs#\n\nIn this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued.\nThis means that using autograd directly on this function is not well defined.\nTo solve this, we will replace the test of the functionh:PN\u2192CMh: \\mathcal{P}^N \\to \\mathcal{C}^Mh:PN\u2192CM(whereP\\mathcal{P}Pcan be eitherR\\mathcal{R}RorC\\mathcal{C}C), with two functions:hrhrhrandhihihisuch that:\nwhereq\u2208Pq \\in \\mathcal{P}q\u2208P.\nWe then do a basic gradcheck for bothhrhrhrandhihihiusing either the real-to-real or complex-to-real case described above, depending onP\\mathcal{P}P.\nNote that, the code, as of time of writing, does not create these functions explicitly but perform the chain rule with therealrealrealorimagimagimagfunctions manually by passing thegrad_out\\text{grad\\_out}grad_outarguments to the different functions.\nWhengrad_out=1\\text{grad\\_out} = 1grad_out=1, then we are consideringhrhrhr.\nWhengrad_out=1j\\text{grad\\_out} = 1jgrad_out=1j, then we are consideringhihihi.\n\n## Fast backward mode gradcheck#\n\nWhile the above formulation of gradcheck is great, both, to ensure correctness and debuggability, it is very slow because it reconstructs the full Jacobian matrices.\nThis section presents a way to perform gradcheck in a faster way without affecting its correctness.\nThe debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user.\nThe high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians.\n\n## Fast gradcheck for real-to-real functions#\n\nThe scalar quantity that we want to compute here isvTJfuv^T J_f uvTJf\u200bufor a given random vectorv\u2208RMv \\in \\mathcal{R}^Mv\u2208RMand a random unit norm vectoru\u2208RNu \\in \\mathcal{R}^Nu\u2208RN.\nFor the numerical evaluation, we can efficiently compute\nWe then perform the dot product between this vector andvvvto get the scalar value of interest.\nFor the analytical version, we can use backward mode AD to computevTJfv^T J_fvTJf\u200bdirectly. We then perform the dot product withuuuto get the expected value.\n\n## Fast gradcheck for complex-to-real functions#\n\nSimilar to the real-to-real case, we want to perform a reduction of the full matrix. But the2\u2217CW2 * CW2\u2217CWmatrix is complex-valued and so in this case, we will compare to complex scalars.\nDue to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value:\nwherev\u2208RMv \\in \\mathcal{R}^Mv\u2208RM,ur\u2208RNur \\in \\mathcal{R}^Nur\u2208RNandui\u2208RNui \\in \\mathcal{R}^Nui\u2208RN.\nWe first consider how to computessswith a numerical method. To do so, keeping in mind that we\u2019re consideringg:CN\u2192RM,z\u2192yg: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to yg:CN\u2192RM,z\u2192ywithz=a+ibz = a + i bz=a+ib, and thatCW=12\u2217(\u2202y\u2202a+i\u2202y\u2202b)CW = \\frac{1}{2} * (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})CW=21\u200b\u2217(\u2202a\u2202y\u200b+i\u2202b\u2202y\u200b),  we rewrite it as follows:\nIn this formula, we can see that\u2202y\u2202aur\\frac{\\partial y}{\\partial a} ur\u2202a\u2202y\u200burand\u2202y\u2202bui\\frac{\\partial y}{\\partial b} ui\u2202b\u2202y\u200buican be evaluated the same way as the fast version for the real-to-real case.\nOnce these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valuedvvvvector.\nFor the analytical case, things are simpler and we rewrite the formula as:\nWe can thus use the fact that the backward mode AD provides us with an efficient way to computevT(2\u2217CW)v^T (2 * CW)vT(2\u2217CW)and then perform a dot product of the real part withurururand the imaginary part withuiuiuibefore reconstructing the final complex scalarsss.\nAt this point, you might be wondering why we did not select a complexuuuand just performed the reduction2\u2217vTCWu\u20322 * v^T CW u'2\u2217vTCWu\u2032.\nTo dive into this, in this paragraph, we will use the complex version ofuuunotedu\u2032=ur\u2032+iui\u2032u' = ur' + i ui'u\u2032=ur\u2032+iui\u2032.\nUsing such complexu\u2032u'u\u2032, the problem is that when doing the numerical evaluation, we would need to compute:\nWhich would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above).\nSince this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above.\n\n## Fast gradcheck for functions with complex outputs#\n\nJust like in the slow case, we consider two real-valued functions and use the appropriate rule from above for each function.\n\n## Gradgradcheck implementation#\n\nPyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing.\nThis feature is implemented by considering the functionF:x,v\u2192vTJfF: x, v \\to v^T J_fF:x,v\u2192vTJf\u200band use the gradcheck defined above on this function.\nNote thatvvvin this case is just a random vector with the same type asf(x)f(x)f(x).\nThe fast version of gradgradcheck is implemented by using the fast version of gradcheck on that same functionFFF.",
  "url": "https://pytorch.org/docs/stable/notes/gradcheck.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
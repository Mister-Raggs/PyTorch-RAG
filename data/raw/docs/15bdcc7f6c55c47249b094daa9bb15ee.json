{
  "doc_id": "15bdcc7f6c55c47249b094daa9bb15ee",
  "source": "pytorch_docs",
  "title": "torchrun (Elastic Launch) \u2014 PyTorch 2.9 documentation",
  "text": "\n## torchrun (Elastic Launch)#\n\nCreated On: May 04, 2021 | Last Updated On: Aug 26, 2021\nModuletorch.distributed.run.\ntorch.distributed.run\ntorch.distributed.runis a module that spawns up multiple distributed\ntraining processes on each of the training nodes.\ntorch.distributed.run\ntorchrunis a pythonconsole scriptto the main moduletorch.distributed.rundeclared in theentry_pointsconfiguration insetup.py.\nIt is equivalent to invokingpython-mtorch.distributed.run.\ntorchrun\nentry_points\npython-mtorch.distributed.run\ntorchruncan be used for single-node distributed training, in which one or\nmore processes per node will be spawned. It can be used for either\nCPU training or GPU training. If it is used for GPU training,\neach distributed process will be operating on a single GPU. This can achieve\nwell-improved single-node training performance.torchruncan also be used in\nmulti-node distributed training, by spawning up multiple processes on each node\nfor well-improved multi-node distributed training performance as well.\nThis will especially be beneficial for systems with multiple Infiniband\ninterfaces that have direct-GPU support, since all of them can be utilized for\naggregated communication bandwidth.\ntorchrun\ntorchrun\nIn both cases of single-node distributed training or multi-node distributed\ntraining,torchrunwill launch the given number of processes per node\n(--nproc-per-node). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (nproc_per_node),\nand each process will be operating on a single GPU fromGPU 0 to\nGPU (nproc_per_node - 1).\ntorchrun\n--nproc-per-node\nnproc_per_node\nChanged in version 2.0.0:torchrunwill pass the--local-rank=<rank>argument to your script.\nFrom PyTorch 2.0.0 onwards, the dashed--local-rankis preferred over the\npreviously used underscored--local_rank.\ntorchrun\n--local-rank=<rank>\n--local-rank\n--local_rank\nFor backward compatibility, it may be necessary for users to handle both\ncases in their argument parsing code. This means including both\"--local-rank\"and\"--local_rank\"in the argument parser. If only\"--local_rank\"is\nprovided,torchrunwill trigger an error: \u201cerror: unrecognized arguments:\n\u2013local-rank=<rank>\u201d. For training code that only supports PyTorch 2.0.0+,\nincluding\"--local-rank\"should be sufficient.\n\"--local-rank\"\n\"--local_rank\"\n\"--local_rank\"\ntorchrun\n\"--local-rank\"\n\n```python\n>>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local-rank\", \"--local_rank\", type=int)\n>>> args = parser.parse_args()\n\n```\n\n\n## Usage#\n\n\n## Single-node multi-worker#\n\n\n```python\ntorchrun\n    --standalone\n    --nnodes=1\n    --nproc-per-node=$NUM_TRAINERS\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\nNote\n--nproc-per-nodemay be\"gpu\"(spawn one process per GPU),\"cpu\"(spawn one process per CPU),\"auto\"(equivalent to\"gpu\"if CUDA is available,\nelse equivalent to\"cpu\"),\nor an integer specifying the number of processes.\nSeetorch.distributed.run.determine_local_world_sizefor more details.\n--nproc-per-node\n\"gpu\"\n\"cpu\"\n\"auto\"\n\"gpu\"\n\"cpu\"\n\n## Stacked single-node multi-worker#\n\nTo run multiple instances (separate jobs) of single-node, multi-worker on the\nsame host, we need to make sure that each instance (job) is\nsetup on different ports to avoid port conflicts (or worse, two jobs being merged\nas a single job). To do this you have to run with--rdzv-backend=c10dand specify a different port by setting--rdzv-endpoint=localhost:$PORT_k.\nFor--nodes=1, its often convenient to lettorchrunpick a free random\nport automatically instead of manually assigning different ports for each run.\n--rdzv-backend=c10d\n--rdzv-endpoint=localhost:$PORT_k\n--nodes=1\ntorchrun\n\n```python\ntorchrun\n    --rdzv-backend=c10d\n    --rdzv-endpoint=localhost:0\n    --nnodes=1\n    --nproc-per-node=$NUM_TRAINERS\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\n\n## Fault tolerant (fixed sized number of workers, no elasticity, tolerates 3 failures)#\n\n\n```python\ntorchrun\n    --nnodes=$NUM_NODES\n    --nproc-per-node=$NUM_TRAINERS\n    --max-restarts=3\n    --rdzv-id=$JOB_ID\n    --rdzv-backend=c10d\n    --rdzv-endpoint=$HOST_NODE_ADDR\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\nHOST_NODE_ADDR, in form <host>[:<port>] (e.g. node1.example.com:29400), specifies the node and\nthe port on which the C10d rendezvous backend should be instantiated and hosted. It can be any\nnode in your training cluster, but ideally you should pick a node that has a high bandwidth.\nHOST_NODE_ADDR\nNote\nIf no port number is specifiedHOST_NODE_ADDRdefaults to 29400.\nHOST_NODE_ADDR\n\n## Elastic (min=1,max=4, tolerates up to 3 membership changes or failures)#\n\nmin=1\nmax=4\n\n```python\ntorchrun\n    --nnodes=1:4\n    --nproc-per-node=$NUM_TRAINERS\n    --max-restarts=3\n    --rdzv-id=$JOB_ID\n    --rdzv-backend=c10d\n    --rdzv-endpoint=$HOST_NODE_ADDR\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\nHOST_NODE_ADDR, in form <host>[:<port>] (e.g. node1.example.com:29400), specifies the node and\nthe port on which the C10d rendezvous backend should be instantiated and hosted. It can be any\nnode in your training cluster, but ideally you should pick a node that has a high bandwidth.\nHOST_NODE_ADDR\nNote\nIf no port number is specifiedHOST_NODE_ADDRdefaults to 29400.\nHOST_NODE_ADDR\n\n## Note on rendezvous backend#\n\nFor multi-node training you need to specify:\n--rdzv-id: A unique job id (shared by all nodes participating in the job)\n--rdzv-id\n--rdzv-backend: An implementation oftorch.distributed.elastic.rendezvous.RendezvousHandler\n--rdzv-backend\ntorch.distributed.elastic.rendezvous.RendezvousHandler\n--rdzv-endpoint: The endpoint where the rendezvous backend is running; usually in formhost:port.\n--rdzv-endpoint\nhost:port\nCurrentlyc10d(recommended),etcd-v2, andetcd(legacy)  rendezvous backends are\nsupported out of the box. To useetcd-v2oretcd, setup an etcd server with thev2api\nenabled (e.g.--enable-v2).\nc10d\netcd-v2\netcd\netcd-v2\netcd\nv2\n--enable-v2\nWarning\netcd-v2andetcdrendezvous use etcd API v2. You MUST enable the v2 API on the etcd\nserver. Our tests use etcd v3.4.3.\netcd-v2\netcd\nWarning\nFor etcd-based rendezvous we recommend usingetcd-v2overetcdwhich is functionally\nequivalent, but uses a revised implementation.etcdis in maintenance mode and will be\nremoved in a future version.\netcd-v2\netcd\netcd\n\n## Definitions#\n\nNode- A physical instance or a container; maps to the unit that the job manager works with.\nNode\nWorker- A worker in the context of distributed training.\nWorker\nWorkerGroup- The set of workers that execute the same function (e.g. trainers).\nWorkerGroup\nLocalWorkerGroup- A subset of the workers in the worker group running on the same node.\nLocalWorkerGroup\nRANK- The rank of the worker within a worker group.\nRANK\nWORLD_SIZE- The total number of workers in a worker group.\nWORLD_SIZE\nLOCAL_RANK- The rank of the worker within a local worker group.\nLOCAL_RANK\nLOCAL_WORLD_SIZE- The size of the local worker group.\nLOCAL_WORLD_SIZE\nrdzv_id- A user-defined id that uniquely identifies the worker group for a job. This id is\nused by each node to join as a member of a particular worker group.\nrdzv_id\nrdzv_backend- The backend of the rendezvous (e.g.c10d). This is typically a strongly\nconsistent key-value store.\nrdzv_backend\nc10d\nrdzv_endpoint- The rendezvous backend endpoint; usually in form<host>:<port>.\nrdzv_endpoint\n<host>:<port>\nANoderunsLOCAL_WORLD_SIZEworkers which comprise aLocalWorkerGroup. The union of\nallLocalWorkerGroupsin the nodes in the job comprise theWorkerGroup.\nNode\nLOCAL_WORLD_SIZE\nLocalWorkerGroup\nLocalWorkerGroups\nWorkerGroup\n\n## Environment Variables#\n\nThe following environment variables are made available to you in your script:\nLOCAL_RANK-  The local rank.\nLOCAL_RANK\nRANK-  The global rank.\nRANK\nGROUP_RANK- The rank of the worker group. A number between 0 andmax_nnodes. When\nrunning a single worker group per node, this is the rank of the node.\nGROUP_RANK\nmax_nnodes\nROLE_RANK-  The rank of the worker across all the workers that have the same role. The role\nof the worker is specified in theWorkerSpec.\nROLE_RANK\nWorkerSpec\nLOCAL_WORLD_SIZE- The local world size (e.g. number of workers running locally); equals to--nproc-per-nodespecified ontorchrun.\nLOCAL_WORLD_SIZE\n--nproc-per-node\ntorchrun\nWORLD_SIZE- The world size (total number of workers in the job).\nWORLD_SIZE\nROLE_WORLD_SIZE- The total number of workers that was launched with the same role specified\ninWorkerSpec.\nROLE_WORLD_SIZE\nWorkerSpec\nMASTER_ADDR- The FQDN of the host that is running worker with rank 0; used to initialize\nthe Torch Distributed backend.\nMASTER_ADDR\nMASTER_PORT- The port on theMASTER_ADDRthat can be used to host the C10d TCP store.\nMASTER_PORT\nMASTER_ADDR\nTORCHELASTIC_RESTART_COUNT- The number of worker group restarts so far.\nTORCHELASTIC_RESTART_COUNT\nTORCHELASTIC_MAX_RESTARTS- The configured maximum number of restarts.\nTORCHELASTIC_MAX_RESTARTS\nTORCHELASTIC_RUN_ID- Equal to the rendezvousrun_id(e.g. unique job id).\nTORCHELASTIC_RUN_ID\nrun_id\nPYTHON_EXEC- System executable override. If provided, the python user script will\nuse the value ofPYTHON_EXECas executable. Thesys.executableis used by default.\nPYTHON_EXEC\nPYTHON_EXEC\n\n## Deployment#\n\n(Not needed for the C10d backend) Start the rendezvous backend server and get the endpoint (to be\npassed as--rdzv-endpointtotorchrun)\n--rdzv-endpoint\ntorchrun\nSingle-node multi-worker: Starttorchrunon the host to start the agent process which\ncreates and monitors a local worker group.\ntorchrun\nMulti-node multi-worker: Starttorchrunwith the same arguments on all the nodes\nparticipating in training.\ntorchrun\nWhen using a job/cluster manager, the entry point command to the multi-node job should betorchrun.\ntorchrun\n\n## Failure Modes#\n\nWorker failure: For a training job withnworkers, ifk<=nworkers fail all workers\nare stopped and restarted up tomax_restarts.\nn\nk<=n\nmax_restarts\nAgent failure: An agent failure results in a local worker group failure. It is up to the job\nmanager to fail the entire job (gang semantics) or attempt to replace the node. Both behaviors\nare supported by the agent.\nNode failure: Same as agent failure.\n\n## Membership Changes#\n\nNode departure (scale-down): The agent is notified of the departure, all existing workers are\nstopped, a newWorkerGroupis formed, and all workers are started with a newRANKandWORLD_SIZE.\nWorkerGroup\nRANK\nWORLD_SIZE\nNode arrival (scale-up): The new node is admitted to the job, all existing workers are stopped,\na newWorkerGroupis formed, and all workers are started with a newRANKandWORLD_SIZE.\nWorkerGroup\nRANK\nWORLD_SIZE\n\n## Important Notices#\n\nThis utility and multi-process distributed (single-node or\nmulti-node) GPU training currently only achieves the best performance using\nthe NCCL distributed backend. Thus NCCL backend is the recommended backend to\nuse for GPU training.\nThe environment variables necessary to initialize a Torch process group are provided to you by\nthis module, no need for you to passRANKmanually.  To initialize a process group in your\ntraining script, simply run:\nRANK\n\n```python\n>>> import torch.distributed as dist\n>>> dist.init_process_group(backend=\"gloo|nccl\")\n\n```\n\nIn your training program, you can either use regular distributed functions\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\nhere is how to configure it.\ntorch.nn.parallel.DistributedDataParallel()\ntorch.nn.parallel.DistributedDataParallel()\n\n```python\nlocal_rank = int(os.environ[\"LOCAL_RANK\"])\nmodel = torch.nn.parallel.DistributedDataParallel(\n    model, device_ids=[local_rank], output_device=local_rank\n)\n\n```\n\nPlease ensure thatdevice_idsargument is set to be the only GPU device id\nthat your code will be operating on. This is generally the local rank of the\nprocess. In other words, thedevice_idsneeds to be[int(os.environ(\"LOCAL_RANK\"))],\nandoutput_deviceneeds to beint(os.environ(\"LOCAL_RANK\"))in order to use this\nutility\ndevice_ids\ndevice_ids\n[int(os.environ(\"LOCAL_RANK\"))]\noutput_device\nint(os.environ(\"LOCAL_RANK\"))\nOn failures or membership changes ALL surviving workers are killed immediately. Make sure to\ncheckpoint your progress. The frequency of checkpoints should depend on your job\u2019s tolerance\nfor lost work.\nThis module only supports homogeneousLOCAL_WORLD_SIZE. That is, it is assumed that all\nnodes run the same number of local workers (per role).\nLOCAL_WORLD_SIZE\nRANKis NOT stable. Between restarts, the local workers on a node can be assigned a\ndifferent range of ranks than before. NEVER hard code any assumptions about the stable-ness of\nranks or some correlation betweenRANKandLOCAL_RANK.\nRANK\nRANK\nLOCAL_RANK\nWhen using elasticity (min_size!=max_size) DO NOT hard code assumptions aboutWORLD_SIZEas the world size can change as nodes are allowed to leave and join.\nmin_size!=max_size\nWORLD_SIZE\nIt is recommended for your script to have the following structure:\n\n```python\ndef main():\n    load_checkpoint(checkpoint_path)\n    initialize()\n    train()\n\n\ndef train():\n    for batch in iter(dataset):\n        train_step(batch)\n\n        if should_checkpoint:\n            save_checkpoint(checkpoint_path)\n\n```\n\n(Recommended) On worker errors, this tool will summarize the details of the error\n(e.g. time, rank, host, pid, traceback, etc). On each node, the first error (by timestamp)\nis heuristically reported as the \u201cRoot Cause\u201d error. To get tracebacks as part of this\nerror summary print out, you must decorate your main entrypoint function in your\ntraining script as shown in the example below. If not decorated, then the summary\nwill not include the traceback of the exception and will only contain the exitcode.\nFor details on torchelastic error handling see:https://pytorch.org/docs/stable/elastic/errors.html\n\n```python\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\n\n@record\ndef main():\n    # do train\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n",
  "url": "https://pytorch.org/docs/stable/elastic/run.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
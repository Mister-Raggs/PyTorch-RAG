{
  "doc_id": "208a8119d358d05019b004ad0b671391",
  "source": "pytorch_docs",
  "title": "torch.profiler \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.profiler#\n\nCreated On: Dec 18, 2020 | Last Updated On: Jun 13, 2025\n\n## Overview#\n\nPyTorch Profiler is a tool that allows the collection of performance metrics during training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace.\nNote\nAn earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated.\ntorch.autograd\n\n## API Reference#\n\nLow-level profiler wrap the autograd profile\nactivities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA,torch.profiler.ProfilerActivity.XPU.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA\nor (when available) ProfilerActivity.XPU.\ntorch.profiler.ProfilerActivity.CPU\ntorch.profiler.ProfilerActivity.CUDA\ntorch.profiler.ProfilerActivity.XPU\nrecord_shapes(bool) \u2013 save information about operator\u2019s input shapes.\nprofile_memory(bool) \u2013 track tensor memory allocation/deallocation (seeexport_memory_timelinefor more details).\nexport_memory_timeline\nwith_stack(bool) \u2013 record source information (file and line number) for the ops.\nwith_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).\nwith_modules(bool) \u2013 record module hierarchy (including function names)\ncorresponding to the callstack of the op. e.g. If module A\u2019s forward call\u2019s\nmodule B\u2019s forward which contains an aten::add op,\nthen aten::add\u2019s module hierarchy is A.B\nNote that this support exist, at the moment, only for TorchScript models\nand not eager mode models.\nexperimental_config(_ExperimentalConfig) \u2013 A set of experimental options\nused by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.\nexecution_trace_observer(ExecutionTraceObserver) \u2013 A PyTorch Execution Trace Observer object.PyTorch Execution Tracesoffer a graph based\nrepresentation of AI/ML workloads and enable replay benchmarks, simulators, and emulators.\nWhen this argument is included the observer start() and stop() will be called for the\nsame time window as PyTorch profiler.\nacc_events(bool) \u2013 Enable the accumulation of FunctionEvents across multiple profiling cycles\nNote\nThis API is experimental and subject to change in the future.\nEnabling shape and stack tracing results in additional overhead.\nWhen record_shapes=True is specified, profiler will temporarily hold references to the tensors;\nthat may further prevent certain optimizations that depend on the reference count and introduce\nextra tensor copies.\nAdds a user defined metadata with a string key and a string value\ninto the trace file\nAdds a user defined metadata with a string key and a valid json value\ninto the trace file\nReturns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished\nExports the collected trace in Chrome JSON format. If kineto is enabled, only\nlast cycle in schedule is exported.\nExport memory event information from the profiler collected\ntree for a given device, and export a timeline plot. There are 3\nexportable files usingexport_memory_timeline, each controlled by thepath\u2019s suffix.\nexport_memory_timeline\npath\nFor an HTML compatible plot, use the suffix.html, and a memory timeline\nplot will be embedded as a PNG file in the HTML file.\n.html\nFor plot points consisting of[times,[sizesbycategory]], wheretimesare timestamps andsizesare memory usage for each category.\nThe memory timeline plot will be saved a JSON (.json) or gzipped JSON\n(.json.gz) depending on the suffix.\n[times,[sizesbycategory]]\ntimes\nsizes\n.json\n.json.gz\nFor raw memory points, use the suffix.raw.json.gz. Each raw memory\nevent will consist of(timestamp,action,numbytes,category), whereactionis one of[PREEXISTING,CREATE,INCREMENT_VERSION,DESTROY],\nandcategoryis one of the enums fromtorch.profiler._memory_profiler.Category.\n.raw.json.gz\n(timestamp,action,numbytes,category)\naction\n[PREEXISTING,CREATE,INCREMENT_VERSION,DESTROY]\ncategory\ntorch.profiler._memory_profiler.Category\nOutput: Memory timeline written as gzipped JSON, JSON, or HTML.\nSave stack traces to a file\npath(str) \u2013 save stacks file to this location;\nmetric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d\nAverages events, grouping them by operator name and (optionally) input shapes, stack\nand overload name.\nNote\nTo use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager.\nPreset a user defined metadata when the profiler is not started\nand added into the trace file later.\nMetadata is in the format of a string key and a valid json value\nToggle collection of activities on/off at any point of collection. Currently supports toggling Torch Ops\n(CPU) and CUDA activity supported in Kineto\nactivities(iterable) \u2013 list of activity groups to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA\ntorch.profiler.ProfilerActivity.CPU\ntorch.profiler.ProfilerActivity.CUDA\nExamples:\n\n```python\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ]\n) as p:\n    code_to_profile_0()\n    // turn off collection of all CUDA activity\n    p.toggle_collection_dynamic(False, [torch.profiler.ProfilerActivity.CUDA])\n    code_to_profile_1()\n    // turn on collection of all CUDA activity\n    p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CUDA])\n    code_to_profile_2()\nprint(p.key_averages().table(\n    sort_by=\"self_cuda_time_total\", row_limit=-1))\n\n```\n\nProfiler context manager.\nactivities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA,torch.profiler.ProfilerActivity.XPU.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA\nor (when available) ProfilerActivity.XPU.\ntorch.profiler.ProfilerActivity.CPU\ntorch.profiler.ProfilerActivity.CUDA\ntorch.profiler.ProfilerActivity.XPU\nschedule(Callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step.\nProfilerAction\non_trace_ready(Callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling.\nschedule\nProfilerAction.RECORD_AND_SAVE\nrecord_shapes(bool) \u2013 save information about operator\u2019s input shapes.\nprofile_memory(bool) \u2013 track tensor memory allocation/deallocation.\nwith_stack(bool) \u2013 record source information (file and line number) for the ops.\nwith_flops(bool) \u2013 use formula to estimate the FLOPs (floating point operations) of specific operators\n(matrix multiplication and 2D convolution).\nwith_modules(bool) \u2013 record module hierarchy (including function names)\ncorresponding to the callstack of the op. e.g. If module A\u2019s forward call\u2019s\nmodule B\u2019s forward which contains an aten::add op,\nthen aten::add\u2019s module hierarchy is A.B\nNote that this support exist, at the moment, only for TorchScript models\nand not eager mode models.\nexperimental_config(_ExperimentalConfig) \u2013 A set of experimental options\nused for Kineto library features. Note, backward compatibility is not guaranteed.\nexecution_trace_observer(ExecutionTraceObserver) \u2013 A PyTorch Execution Trace Observer object.PyTorch Execution Tracesoffer a graph based\nrepresentation of AI/ML workloads and enable replay benchmarks, simulators, and emulators.\nWhen this argument is included the observer start() and stop() will be called for the\nsame time window as PyTorch profiler. See the examples section below for a code sample.\nacc_events(bool) \u2013 Enable the accumulation of FunctionEvents across multiple profiling cycles\nuse_cuda(bool) \u2013Deprecated since version 1.8.1:useactivitiesinstead.\nDeprecated since version 1.8.1:useactivitiesinstead.\nactivities\nNote\nUseschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager.\nschedule()\nNote\nUsetensorboard_trace_handler()to generate result files for TensorBoard:\ntensorboard_trace_handler()\non_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)\non_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)\nAfter profiling, result files can be found in the specified directory. Use the command:\ntensorboard--logdirdir_name\ntensorboard--logdirdir_name\nto see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin\nNote\nEnabling shape and stack tracing results in additional overhead.\nWhen record_shapes=True is specified, profiler will temporarily hold references to the tensors;\nthat may further prevent certain optimizations that depend on the reference count and introduce\nextra tensor copies.\nExamples:\n\n```python\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ]\n) as p:\n    code_to_profile()\nprint(p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=-1))\n\n```\n\nUsing the profiler\u2019sschedule,on_trace_readyandstepfunctions:\nschedule\non_trace_ready\nstep\n\n```python\n# Non-default profiler schedule allows user to turn profiler on and off\n# on different iterations of the training loop;\n# trace_handler is called every time a new trace becomes available\ndef trace_handler(prof):\n    print(\n        prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=-1)\n    )\n    # prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n\n\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ],\n    # In this example with wait=1, warmup=1, active=2, repeat=1,\n    # profiler will skip the first step/iteration,\n    # start warming up on the second, record\n    # the third and the forth iterations,\n    # after which the trace will become available\n    # and on_trace_ready (when set) is called;\n    # the cycle repeats starting with the next step\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=2, repeat=1),\n    on_trace_ready=trace_handler,\n    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n    # used when outputting for tensorboard\n) as p:\n    for iter in range(N):\n        code_iteration_to_profile(iter)\n        # send a signal to the profiler that the next iteration has started\n        p.step()\n\n```\n\nThe following sample shows how to setup up an Execution Trace Observer (execution_trace_observer)\n\n```python\nwith torch.profiler.profile(\n    ...\n    execution_trace_observer=(\n        ExecutionTraceObserver().register_callback(\"./execution_trace.json\")\n    ),\n) as p:\n    for iter in range(N):\n        code_iteration_to_profile(iter)\n        p.step()\n\n```\n\nYou can also refer to test_execution_trace_with_kineto() in tests/profiler/test_profiler.py.\nNote: One can also pass any object satisfying the _ITraceObserver interface.\nReturns the current trace ID.\nSets a callback to be called when a new trace ID is generated.\nSignals the profiler that the next profiling step has started.\nProfiler actions that can be taken at the specified intervals\nMembers:\nCPU\nXPU\nMTIA\nCUDA\nHPU\nPrivateUse1\nReturns a callable that can be used as profilerscheduleargument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with therepeatparameter, the zero value means that\nthe cycles will continue until the profiling is finished.\nschedule\nskip_first\nwait\nwarmup\nactive\nwait\nrepeat\nTheskip_first_waitparameter controls whether the firstwaitstage should be skipped.\nThis can be useful if a user wants to wait longer thanskip_firstbetween cycles, but not\nfor the first profile. For example, ifskip_firstis 10 andwaitis 20, the first cycle will\nwait 10 + 20 = 30 steps before warmup ifskip_first_waitis zero, but will wait only 10\nsteps ifskip_first_waitis non-zero. All subsequent cycles will then wait 20 steps between the\nlast active and warmup.\nskip_first_wait\nwait\nskip_first\nskip_first\nwait\nskip_first_wait\nskip_first_wait\nCallable\nOutputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.\ndir_name\nworker_name\n\n## Intel Instrumentation and Tracing Technology APIs#\n\nCheck if ITT feature is available or not\nDescribe an instantaneous event that occurred at some point.\nmsg(str) \u2013 ASCII message to associate with the event.\nPushes a range onto a stack of nested range span.  Returns zero-based\ndepth of the range that is started.\nmsg(str) \u2013 ASCII message to associate with range\nPops a range off of a stack of nested range spans. Returns the\nzero-based depth of the range that is ended.",
  "url": "https://pytorch.org/docs/stable/profiler.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
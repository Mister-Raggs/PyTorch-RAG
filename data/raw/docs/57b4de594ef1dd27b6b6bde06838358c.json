{
  "doc_id": "57b4de594ef1dd27b6b6bde06838358c",
  "source": "pytorch_docs",
  "title": "Dynamo Overview \u2014 PyTorch 2.9 documentation",
  "text": "\n## Dynamo Overview#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jun 13, 2025\nBefore you read this section, readtorch.compiler.\nTorchDynamo (or simply Dynamo) is a Python-level Just-In-Time (JIT) compiler designed to make\nunmodified PyTorch programs faster. Dynamo hooks into the frame evaluation\nAPI in CPython (PEP 523) to\ndynamically modify Python bytecode right before it is executed. It\nrewrites Python bytecode to extract sequences of PyTorch\noperations into anFX Graphwhich is then compiled with a customizable backend.\nIt creates this FX Graph through bytecode analysis and is designed to\nmix Python execution with compiled backends to get the best of both\nworlds \u2014 usability and performance.\nDynamo makes it easy to experiment with different compiler\nbackends to make PyTorch code faster with a single line decoratortorch._dynamo.optimize()which is wrapped for convenience bytorch.compile()\ntorch._dynamo.optimize()\ntorch.compile()\nThe following diagram demonstrates how PyTorch works withtorch.compileand without it:\ntorch.compile\nTorchInductoris one of the backends\nsupported byDynamo GraphintoTritonfor GPUs orC++/OpenMPfor CPUs. We have atraining performance dashboardthat provides performance comparison for different training backends. You can read\nmore in theTorchInductor post on PyTorch\ndev-discuss.\nTorchInductor\nFor an in-depth overview, read the sections below, watch the deep-dive video,\nand check out the dev-discuss topics.\nDynamo deep-dive video\ndev-discuss topics\n\n## Dynamo Internals#\n\nAuthor:Jason AnselandKaichao You\nThis section will go over some of the Dynamo internals and will\ndemonstrate how Dynamo works under the hood.\n\n## What is a guard?#\n\nDynamo operates just-in-time and specializes graphs based on\ndynamic properties. Below is a basic example of how to use Dynamo.\nOne can decorate a function or a method usingtorchdynamo.optimizeto enable\nDynamo optimization:\ntorchdynamo.optimize\n\n```python\nfrom typing import List\nimport torch\nfrom torch import _dynamo as torchdynamo\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n\n@torchdynamo.optimize(my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n\n```\n\nFor example, the first graph above has the following\nguards:\n\n```python\nGUARDS:\nhasattr(L['a'], '_dynamo_dynamic_indices') == False\nhasattr(L['b'], '_dynamo_dynamic_indices') == False\nutils_device.CURRENT_DEVICE == None\n___skip_backend_check() or ___current_backend() == ___lookup_backend(140355900538256)\ncheck_tensor(L['a'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1])\ncheck_tensor(L['b'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1])\n\n```\n\nIf any of those guards fail, the graph will be recaptured and\nrecompiled. The interesting guard there ischeck_tensor, which\nchecks the followingtorch.Tensorproperties:\ncheck_tensor\ntorch.Tensor\nPython class of the tensor (tensor subclassing, etc)\ndtype\ndevice\nrequires_grad\ndispatch_key (with thread-local includes/excludes applied)\nndim\nsizes*\nstrides*\nThe full specialization mode allows the backend compiler to assume an\nentirely static graph. Unfortunately, most backends require this.\nOperators which return dynamic shapes will trigger a graph break when\nnot in dynamic shape mode.\n\n## What is Dynamo doing?#\n\nIf you want to understand better what Dynamo is doing, you can run your code with:\n\n```python\nTORCH_LOGS=\"+dynamo,guards,bytecode\"\n\n```\n\nIf you are not familiar with Python bytecode, you can add a decompiler hook\nto decompile the bytecode into human-readable source code. One available\ntool isdepyf. If you don\u2019t havedepyfalready installed, runpipinstalldepyf. Then, add the\nfollowing code to install decompilation hooks before you run any code.\ndepyf\npipinstalldepyf\n\n```python\nimport depyf\ndepyf.install()\n\n```\n\nThis code triggers useful (but spammy) printouts.\nFor example, the printouts for the first graph in thetoy_exampleare:\ntoy_example\n\n```python\n__compiled_fn_0 <eval_with_key>.1\nopcode         name     target                                                  args              kwargs\n-------------  -------  ------------------------------------------------------  ----------------  --------\nplaceholder    a        a                                                       ()                {}\nplaceholder    b        b                                                       ()                {}\ncall_function  abs_1    <built-in method abs of type object at 0x7f9ca082f8a0>  (a,)              {}\ncall_function  add      <built-in function add>                                 (abs_1, 1)        {}\ncall_function  truediv  <built-in function truediv>                             (a, add)          {}\ncall_method    sum_1    sum                                                     (b,)              {}\ncall_function  lt       <built-in function lt>                                  (sum_1, 0)        {}\noutput         output   output                                                  ((truediv, lt),)  {}\nORIGINAL BYTECODE toy_example example.py line 12\n 14           0 LOAD_FAST                0 (a)\n              2 LOAD_GLOBAL              0 (torch)\n              4 LOAD_METHOD              1 (abs)\n              6 LOAD_FAST                0 (a)\n              8 CALL_METHOD              1\n             10 LOAD_CONST               1 (1)\n             12 BINARY_ADD\n             14 BINARY_TRUE_DIVIDE\n             16 STORE_FAST               2 (x)\n 15          18 LOAD_FAST                1 (b)\n             20 LOAD_METHOD              2 (sum)\n             22 CALL_METHOD              0\n             24 LOAD_CONST               2 (0)\n             26 COMPARE_OP               0 (<)\n             28 POP_JUMP_IF_FALSE       19 (to 38)\n 16          30 LOAD_FAST                1 (b)\n             32 LOAD_CONST               3 (-1)\n             34 BINARY_MULTIPLY\n             36 STORE_FAST               1 (b)\n 17     >>   38 LOAD_FAST                2 (x)\n             40 LOAD_FAST                1 (b)\n             42 BINARY_MULTIPLY\n             44 RETURN_VALUE\nMODIFIED BYTECODE toy_example example.py line 12\n 12           0 LOAD_GLOBAL              3 (__compiled_fn_0)\n              2 LOAD_FAST                0 (a)\n              4 LOAD_FAST                1 (b)\n              6 CALL_FUNCTION            2\n              8 UNPACK_SEQUENCE          2\n             10 STORE_FAST               2 (x)\n             12 POP_JUMP_IF_FALSE       12 (to 24)\n             14 LOAD_GLOBAL              4 (__resume_at_30_1)\n             16 LOAD_FAST                1 (b)\n             18 LOAD_FAST                2 (x)\n             20 CALL_FUNCTION            2\n             22 RETURN_VALUE\n        >>   24 LOAD_GLOBAL              5 (__resume_at_38_2)\n             26 LOAD_FAST                1 (b)\n             28 LOAD_FAST                2 (x)\n             30 CALL_FUNCTION            2\n             32 RETURN_VALUE\npossible source code:\ndef toy_example(a, b):\n    __temp_1 = __compiled_fn_0(a, b)\n    x = __temp_1[0]\n    if __temp_1[1]:\n        return __resume_at_30_1(b, x)\n    return __resume_at_38_2(b, x)\nIf you find the decompiled code is wrong,please submit an issue at https://github.com/youkaichao/depyf/issues.\n\n```\n\nAt the top you can see the FX graph.\nNext, you see the original bytecode of the function, followed by the\nmodified bytecode generated by Dynamo, and the decompiled source\ncode for reference. Finally, you see the guards which we covered above.\nIn the modified bytecode,__compiled_fn_0is the return value ofmy_compiler()(the compiled graph).__resume_at_30_1and__resume_at_38_2are both generated continuation functions that pick\nup execution after a graph break (at bytecode offsets 30 and 38). Each\nof these functions take the form:\n__compiled_fn_0\nmy_compiler()\n__resume_at_30_1\n__resume_at_38_2\n\n```python\n__resume_at_<offset>:\n    ... restore stack state if needed ...\n    JUMP_ABSOLUTE <offset> into toy_example\n    ... original bytecode of toy_example ...\n\n```\n\nBy generating thisresume_atfunction, we force the remainder of the\nfunction to be executed in a new Python frame which recursively\ntriggers Dynamo to restart its capture once execution reaches that\npoint for the first time.\nresume_at\n\n## How to inspect artifacts generated by Dynamo?#\n\nTo inspect the artifacts generated by Dynamo, there is an APItorch._dynamo.eval_frame._debug_get_cache_entry_listthat retrieves compiled code and guards out of a function\u2019s__code__object. A compiled function can have several cache entries, and each cache entry consists a generated function to check guards, and atypes.CodeTypeobject to keep the code to be executed if the guarding conditions are satisfied.\ntorch._dynamo.eval_frame._debug_get_cache_entry_list\n__code__\ntypes.CodeType\n\n```python\nfrom torch._dynamo.eval_frame import _debug_get_cache_entry_list, innermost_fn\ncache_entries = _debug_get_cache_entry_list(innermost_fn(toy_example))\ncache_entry = cache_entries[0]\nguard, code = cache_entry.check_fn, cache_entry.code\n# the guard takes the local variables of an input frame, and tells whether a re-compilation should be triggered.\nimport dis\ndis.dis(guard)\ndis.dis(code)\n\n```\n\nIf you know Python bytecode, you can understand the above output.\nFor the guard function, there is no need to inspect the bytecode. We can directly access its guarding conditions:\n\n```python\nfor code_part in guard.code_parts:\n    print(code_part)\n\n```\n\nThe output is:\n\n```python\n___guarded_code.valid\n___check_global_state()\nhasattr(L['a'], '_dynamo_dynamic_indices') == False\nhasattr(L['b'], '_dynamo_dynamic_indices') == False\nutils_device.CURRENT_DEVICE == None\n___skip_backend_check() or ___current_backend() == ___lookup_backend(140215810860528)\n___check_tensors(L['a'], L['b'], tensor_check_names=tensor_check_names)\n\n```\n\nOnly when all the conditions are satisfied, the guard function returns true, and the compiled code is executed.\nFor the compiled code, we cannot directly access its source but have to decompile it.\n\n```python\nfrom depyf import decompile\nprint(decompile(code))\n\n```\n\nThe output is:\n\n```python\ndef toy_example(a, b):\n    __temp_1 = __compiled_fn_0(a, b)\n    x = __temp_1[0]\n    if __temp_1[1]:\n        return __resume_at_30_1(b, x)\n    return __resume_at_38_2(b, x)\n\n```\n\nSome names referenced in the code are:\nCompiled functions, stored in the global namespace of the module containing the original functiontoy_example. These include names like__compiled_fn_0/__resume_at_30_1/__resume_at_38_2.\ntoy_example\n__compiled_fn_0\n__resume_at_30_1\n__resume_at_38_2\nClosure variables used for checking guards. The names can be accessed fromguard.__code__.co_freevars, and the values are stored inguard.__closure__. These include names like___guarded_code/___is_grad_enabled/___are_deterministic_algorithms_enabled/___is_torch_function_enabled/utils_device/___check_tensors/tensor_check_names.\nguard.__code__.co_freevars\nguard.__closure__\n___guarded_code\n___is_grad_enabled\n___are_deterministic_algorithms_enabled\n___is_torch_function_enabled\nutils_device\n___check_tensors\ntensor_check_names\nArgumentLof theguardfunction. This is a dict mapping the name of arguments oftoy_exampleto its values. This is only available when the function is called, where the frame evaluation API comes into play. In short,Lis adictwith structure of{'a':value_a,'b':value_b}. Therefore, you can see the code usesL['a']to refer to the input variablea.\nL\nguard\ntoy_example\nL\ndict\n{'a':value_a,'b':value_b}\nL['a']\na\nThe graph break is shown in the code of compiledtoy_example, where we have to use Python interpreter to select the following graph to execute.\ntoy_example\nNote that we pass a simplemy_compilerfunction as the backend compiler, therefore the subgraph code__resume_at_38_2,__resume_at_30_1, and__compiled_fn_0remain Python code. This can also be inspected (please ignore the function name, and only use the function signature and function body code):\nmy_compiler\n__resume_at_38_2\n__resume_at_30_1\n__compiled_fn_0\n\n```python\nprint(\"source code of __compiled_fn_0:\")\nprint(innermost_fn(__compiled_fn_0).__self__.code)\nprint(\"=\" * 60)\nprint(\"source code of __resume_at_30_1:\")\nprint(decompile(__resume_at_30_1))\nprint(\"=\" * 60)\nprint(\"source code of __resume_at_38_2:\")\nprint(decompile(__resume_at_38_2))\n\n```\n\n\n```python\nsource code of __compiled_fn_0:\ndef forward(self, L_a_ : torch.Tensor, L_b_ : torch.Tensor):\n    l_a_ = L_a_\n    l_b_ = L_b_\n    abs_1 = torch.abs(l_a_)\n    add = abs_1 + 1;  abs_1 = None\n    truediv = l_a_ / add;  l_a_ = add = None\n    sum_1 = l_b_.sum();  l_b_ = None\n    lt = sum_1 < 0;  sum_1 = None\n    return (truediv, lt)\n# To see more debug info, please use ``graph_module.print_readable()``\n============================================================\nsource code of __resume_at_30_1:\ndef <resume in toy_example>(b, x):\n    b = b * -1\n    return x * b\n============================================================\nsource code of __resume_at_38_2:\ndef <resume in toy_example>(b, x):\n    return x * b\n\n```\n\nHowever, if we use other backends like the built-ininductor, the subgraph code will be compiled CUDA kernels for GPU or C++ code for CPU.\ninductor\nTo summarize, the compiled code is conceptually equivalent to the code below:\n\n```python\ndef compiled_example(a, b):\n    L = {'a': a, 'b': b}\n    for guard, code in get_cache_entries():\n        if guard(L):\n            return code(a, b)\n    recompile_and_add_another_cache_entry()\n\n```\n\nThe following diagram demonstrates howtorch.compiletransforms and optimizes user-written code: it first extracts computation graphs from the user-written function, and compiles these graphs into optimized functions, then assembles them into a new function, which is functionally equivalent to the user-written code but optimized to have a good computation speed.\ntorch.compile\nTo learn more about how all this is implemented internally, seeDynamo Deep-Dive.",
  "url": "https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "47f16377afa9e1121921c6bb6fa5ce58",
  "source": "pytorch_docs",
  "title": "Operator Registration \u2014 PyTorch 2.9 documentation",
  "text": "\n## Operator Registration#\n\nCreated On: Aug 27, 2025 | Last Updated On: Sep 02, 2025\nFor new accelerators, one of the most important and fundamental aspects of integration is supporting high-performance operators. To facilitate operator adaptation for users and accelerator developers, PyTorch provides multiple methods for developing and registering operators in bothPythonandC++. The following sections detail some of PyTorch\u2019s fundamental capabilities for operator registration.\nPython\nC++\nNote\nDispatchKeyis used to uniquely identify accelerator within PyTorch, such asCPU,CUDA,MPS, andPrivateUse1. In theory, all subsequent new accelerators will sharePrivateUse1, leveraging its built-in comprehensive scaffolding capabilities to complete the integration of new accelerators. Please refer toLet\u2019s talk about the PyTorch dispatcherif you are interested with dispatcher.\nDispatchKey\nCPU\nCUDA\nMPS\nPrivateUse1\nPrivateUse1\n\n## Operator Set#\n\nPyTorch currently has over 3500 built-in operators (including related operator variants). This represents a significant workload from any perspective, and supporting this massive number of operators in a short period of time is no easy task. Therefore, as the first step in developing new backend operators, our goal should be to focus on the essential operators. For other operators, we can first use the community\u2019s fallback mechanism to support the feature as the first priority. After that, we can gradually complete other operators to improve the performance of the new backend.\nThe required operator set is listed below, primarily consisting of low-level operators required by factory functions and fallback operators:\nOperator Name\nDispatch Key\nDescription\nempty.memory_format\nPrivateUse1\nCreate an uninitialized Tensor with the specified shape and memory layout (the stride is automatically calculated)\nempty_strided\nPrivateUse1\nCreate an uninitialized Tensor of the specified shape and stride (more degrees of freedom)\nas_strided\nPrivateUse1\nCreate a shared view of the input Tensor with new shape, stride, and offset (without allocating new memory)\nview\nPrivateUse1\nCreate a shared view of the input Tensor with new shape, but the original Tensor must be memory-contiguous\n_reshape_alias\nPrivateUse1\nCreates a shared view without safety checks(Internal version of reshape)\nresize_\nPrivateUse1\nModify the shape of the Tensor in place and reallocate memory if capacity is insufficient\n_copy_from\nPrivateUse1\nThe underlying core function of Tensor.copy_ is responsible for the actual cross-device data copying\n_copy_from_and_resize\nPrivateUse1\nCombineresize_and_copy_fromto resize first and then copy\nresize_\n_copy_from\n_local_scalar_dense\nPrivateUse1\nThe underlying implementation of.item(), extracting values from Tensor to CPU scalars\n.item()\nset_.source_Tensor\nPrivateUse1\nSet the current Tensor using the specified Tensor\nset_.source_Storage\nPrivateUse1\nSet the current Tensor using the specified Storage\nset_.source_Storage_storage_offset\nPrivateUse1\nSet the current Tensor using the specified Storage with the storage offset\nfallback\nPrivateUse1\nFallback to CPU\n\n## Basics#\n\nNow that we have defined the initial scope of operator support, we can begin developing operator adaptations. This section will explain these implementations inPythonandC++based on actual scenarios.\nPython\nC++\n\n## Step 1#\n\nThe operators mentioned aboveshare a common characteristic: They are built-in PyTorch operators with definednamespacesandSchemas, and these operators\u2019 built-in accelerators (CPU,CUDA, etc.) have been implemented. What we have to do next is to implement these operators for the new accelerators.\nnamespaces\nSchemas\nCPU\nCUDA\n\n```python\n 1at::Tensor empty_memory_format(\n 2    c10::IntArrayRef size,\n 3    std::optional<c10::ScalarType> dtype_opt,\n 4    std::optional<c10::Layout> layout_opt,\n 5    std::optional<c10::Device> device_opt,\n 6    std::optional<bool> pin_memory_opt,\n 7    std::optional<c10::MemoryFormat> memory_format_opt) {\n 8  const auto device = c10::device_or_default(device_opt);\n 9  const auto dtype = c10::dtype_or_default(dtype_opt);\n10  TORCH_CHECK(device.is_privateuseone());\n11  TORCH_CHECK(\n12      c10::layout_or_default(layout_opt) == c10::Layout::Strided,\n13      \"Non strided layout not supported\");\n14  TORCH_CHECK(\n15      !c10::pinned_memory_or_default(pin_memory_opt),\n16      \"Pin memory can only be on CPU\");\n17  const c10::DeviceGuard device_guard(device);\n18  constexpr c10::DispatchKeySet pu1_dks(c10::DispatchKey::PrivateUse1);\n19  auto allocator = at::GetAllocator(at::kPrivateUse1);\n20  return at::detail::empty_generic(\n21      size, allocator, pu1_dks, dtype, memory_format_opt);\n22}\n\n```\n\n\n```python\n 1at::Tensor wrapper_empty_memory_format(\n 2    c10::IntArrayRef size,\n 3    std::optional<c10::ScalarType> dtype_opt,\n 4    std::optional<c10::Layout> layout_opt,\n 5    std::optional<c10::Device> device_opt,\n 6    std::optional<bool> pin_memory_opt,\n 7    std::optional<c10::MemoryFormat> memory_format_opt) {\n 8  return at::native::openreg::empty_memory_format(\n 9      size,\n10      dtype_opt,\n11      layout_opt,\n12      device_opt,\n13      pin_memory_opt,\n14      memory_format_opt);\n15}\n\n```\n\nTaking theempty.memory_formatoperator as an example, we first need to query the operator\u2019sschemainformation innative_functions.yaml, which contains detailed signature information. Then, we can implement the operator based on the capabilities of the new accelerator.\nempty.memory_format\nschema\nnative_functions.yaml\n\n```python\n- func: empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor\ndispatch:\n    CPU: empty_cpu\n    CUDA: empty_cuda\n    ...\n\n```\n\n\n```python\n 1TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n 2  m.impl(\"empty.memory_format\", wrapper_empty_memory_format);\n 3  m.impl(\"empty_strided\", wrapper_empty_strided);\n 4  m.impl(\"as_strided\", wrapper_as_strided);\n 5  m.impl(\"resize_\", wrapper_resize_);\n 6  m.impl(\"_reshape_alias\", wrapper__reshape_alias);\n 7  m.impl(\"_copy_from\", wrapper__copy_from);\n 8  m.impl(\"_copy_from_and_resize\", wrapper__copy_from_and_resize);\n 9  m.impl(\"_local_scalar_dense\", wrapper__local_scalar_densor);\n10  m.impl(\"set_.source_Tensor\", wrapper_set_source_Tensor_);\n11  m.impl(\"set_.source_Storage\", wrapper_set_source_Storage_);\n12  m.impl(\n13      \"set_.source_Storage_storage_offset\",\n14      wrapper_set_source_Storage_storage_offsetset_);\n15  m.impl(\"view\", wrapper_view);\n16}\n\n```\n\nAfter completing thewrapper_empty_memory_format, we can registeraten::empty.memory_formatforPrivateUse1throughTORCH_LIBRARY_IMPL.\nwrapper_empty_memory_format\naten::empty.memory_format\nPrivateUse1\nTORCH_LIBRARY_IMPL\n\n## Step 2#\n\nBy followingStep 1, we can complete the development and registration of all operators exceptfallback. Next, to support operators related to operations (such as mathematical operations and convolution operations), we need to implement the registration of fallback semantics. This is a built-in capability provided by the PyTorch framework that can fallback some operations that are not supported by new accelerators to the CPU for execution. For new backends in development, this is an extremely effective way to ensure functionality at the expense of performance.\nfallback\n\n```python\n 1void cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {\n 2  static const std::unordered_set<c10::OperatorName> cpu_fallback_blocklist = {\n 3      c10::OperatorName(\"aten::abs\", \"\"),\n 4      c10::OperatorName(\"aten::abs\", \"out\"),\n 5  };\n 6\n 7  const auto& op_name = op.schema().operator_name();\n 8  if (cpu_fallback_blocklist.count(op_name)) {\n 9    TORCH_CHECK(\n10        false,\n11        \"Operator '\",\n12        op_name,\n13        \"' is not implemented for device openreg.\");\n14  } else {\n15    at::native::cpu_fallback(op, stack);\n16  }\n17}\n\n```\n\n\n```python\n1void wrapper_cpu_fallback(\n2    const c10::OperatorHandle& op,\n3    torch::jit::Stack* stack) {\n4  at::native::openreg::cpu_fallback(op, stack);\n5}\n\n```\n\n\n```python\n1TORCH_LIBRARY_IMPL(_, PrivateUse1, m) {\n2  m.fallback(\n3      torch::CppFunction::makeFromBoxedFunction<&wrapper_cpu_fallback>());\n4}\n\n```\n\nwrapper_cpu_fallbackwraps theat::native::cpu_fallbackmethod provided by PyTorch and is registered withPrivateUse1in PyTorch viaTORCH_LIBRARY_IMPL. Subsequent operations not supported by the new backend will automatically fall back to the CPU for execution, and the results will be passed back to the new backend after execution.\nwrapper_cpu_fallback\nat::native::cpu_fallback\nPrivateUse1\nTORCH_LIBRARY_IMPL\n\n## Advanced#\n\n\n## Selective Fallback#\n\nEnabling the fallback mechanism only for certain operators, while following PyTorch\u2019s default behavior for other operators (an error will be reported if the accelerator does not have a corresponding operator implementation), this is a very reasonable scenario as well.\n\n```python\n1void wrapper_cpu_fallback(\n2    const c10::OperatorHandle& op,\n3    torch::jit::Stack* stack) {\n4  at::native::openreg::cpu_fallback(op, stack);\n5}\n\n```\n\n\n```python\n1TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n2  m.impl(\n3      \"sub.Tensor\",\n4      torch::CppFunction::makeFromBoxedFunction<&wrapper_cpu_fallback>());\n5}\n\n```\n\nPer-operator fallbacks are very similar to global fallbacks, the only difference being the registration method: callingm.implregisters an implementation for a specific operator, whilem.fallbackregisters a default implementation for all operators.\nm.impl\nm.fallback\n\n```python\n 1void cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {\n 2  static const std::unordered_set<c10::OperatorName> cpu_fallback_blocklist = {\n 3      c10::OperatorName(\"aten::abs\", \"\"),\n 4      c10::OperatorName(\"aten::abs\", \"out\"),\n 5  };\n 6\n 7  const auto& op_name = op.schema().operator_name();\n 8  if (cpu_fallback_blocklist.count(op_name)) {\n 9    TORCH_CHECK(\n10        false,\n11        \"Operator '\",\n12        op_name,\n13        \"' is not implemented for device openreg.\");\n14  } else {\n15    at::native::cpu_fallback(op, stack);\n16  }\n17}\n\n```\n\nOf course, global fallbacks can also be combined with a blacklist of fallbacks, which is a common approach, especially when only a few operators do not support fallbacks.\n\n## PyTorch STUB#\n\nPyTorch also provides another approach for built-in operators:STUB. This method is essentially based on theStep1<step-one>approach, but adds secondary scheduling capabilities (for example, scheduling based on CPU characteristics).\nSTUB\nStep1<step-one>\nNote\nTheSTUBmethod currently supports only a limited set of operators. For new accelerator devices, the advantage of theSTUBmethod is that it significantly reduces the cost of development at the cost of a small performance overhead. PyTorch currently does not clearly list the set of operators that can be registered throughSTUB. Due to the large number of related operators, only the query method for the supported operator list is provided here.\nSTUB\nSTUB\nSTUB\n\n```python\npushd ${TORCH_ROOT}\n\nfind aten -type f -a -name \"*.h\" | xargs -I {} grep -wl \"^DECLARE_DISPATCH\" {}\n\npopd\n\n```\n\nDECLARE_DISPATCHis a macro used to explicitly declareSTUB. It is currently distributed in theatendirectory. Based on this macro, you can find all operators that can be integrated using theSTUBmethod.\nDECLARE_DISPATCH\nSTUB\naten\nSTUB\n\n```python\n...\naten/src/ATen/native/Activation.h\naten/src/ATen/native/FusedSGD.h\naten/src/ATen/native/nested/NestedTensorBinaryOps.h\naten/src/ATen/native/TensorCompare.h\naten/src/ATen/native/Sorting.h\n...\n\n```\n\n\n```python\nusing unary_fn = void(*)(TensorIteratorBase&);\n\nDECLARE_DISPATCH(unary_fn, abs_stub)\n\n```\n\nThe above listing contains the file that declares theSTUBoperator, where you can clearly see the STUB name and the associated function signature. Next, we will takeabs_stubas an example to briefly introduce the path to support operators throughSTUB.\nSTUB\nabs_stub\nSTUB\n\n```python\n 1void abs_kernel(at::TensorIteratorBase& iter) {\n 2  TORCH_CHECK(iter.ntensors() == 2, \"Abs kernel expects 2 tensors\");\n 3  TORCH_CHECK(\n 4      iter.common_dtype() == at::ScalarType::Float,\n 5      \"Abs kernel only supports float type\");\n 6\n 7  auto& output_tensor = iter.tensor(0);\n 8  auto& input_tensor = iter.tensor(1);\n 9\n10  TORCH_CHECK(\n11      input_tensor.sizes() == output_tensor.sizes(),\n12      \"Input and output tensor sizes must match.\");\n13\n14  auto abs_loop = [](float* out_ptr, const float* in_ptr, int64_t n) {\n15    for (int64_t i = 0; i < n; ++i) {\n16      out_ptr[i] = std::abs(in_ptr[i]);\n17    }\n18  };\n19\n20  MemoryGuard guard(input_tensor, output_tensor);\n21\n22  if (iter.is_contiguous()) {\n23    abs_loop(\n24        static_cast<float*>(iter.data_ptr(0)),\n25        static_cast<float*>(iter.data_ptr(1)),\n26        iter.numel());\n27  } else {\n28    TORCH_CHECK(\n29        input_tensor.is_contiguous(), \"Input tensor must be contiguous.\")\n30\n31    auto output = at::empty(\n32        input_tensor.sizes(),\n33        input_tensor.options().memory_format(\n34            input_tensor.suggest_memory_format()));\n35\n36    MemoryGuard guard(output);\n37\n38    abs_loop(\n39        static_cast<float*>(output.data_ptr()),\n40        static_cast<float*>(iter.data_ptr(1)),\n41        iter.numel());\n42\n43    output_tensor.copy_(output);\n44  }\n45}\n\n```\n\n\n```python\n1REGISTER_PRIVATEUSE1_DISPATCH(abs_stub, &wrapper_abs_stub);\n2REGISTER_PRIVATEUSE1_DISPATCH(\n3    quantize_tensor_per_tensor_affine_stub,\n4    &wrapper_quantize_tensor_per_tensor_affine_stub);\n5REGISTER_PRIVATEUSE1_DISPATCH(\n6    _fused_sdp_choice_stub,\n7    &wrapper__fused_sdp_choice);\n\n```\n\nFrom the signature, we can see that the input ofabs_stubisTensorIteratorBase, a powerful helper class provided by PyTorch that contains all input and output operators, as well as some other auxiliary methods. Based on it, we can develop theabs_kerneloperator and then callREGISTER_PRIVATEUSE1_DISPATCHto specifyabs_stubto complete the registration.\nabs_stub\nTensorIteratorBase\nabs_kernel\nREGISTER_PRIVATEUSE1_DISPATCH\nabs_stub\n\n## Custom Operators#\n\nIn addition to PyTorch\u2019s built-in operators, custom accelerator operators are also very common to improve performance in specific scenarios. These can be categorized into three main approaches:\nForward-only\nForward and backward: Separate registration\nForward and backward: Implemented usingtorch.autograd.Function\ntorch.autograd.Function\nNote\nThere are more details in PyTorch tutorials, so refer toPyTorch Custom Operatorsif you are interested.\nHere, we\u2019ll briefly introduce the implementation process of custom operators, focusing on the forward-only approach. The implementation can be summarized into the following three points:\nDefine Schema:\n\n```python\n1TORCH_LIBRARY(openreg, m) {\n2  m.def(\"custom_abs(Tensor input)-> Tensor\");\n3}\n\n```\n\nNamespace Name:openreg\nopenreg\nFunction Name:custom_abs\ncustom_abs\nInput Parameters:\nType:Tensor\nTensor\nName:input\ninput\nOutput Type:Tensor\nTensor\nRegister Operator&Autograd Fallback:\n\n```python\n1TORCH_LIBRARY_IMPL(openreg, PrivateUse1, m) {\n2  m.impl(\"custom_abs\", &wrapper_custom_abs);\n3}\n\n```\n\n\n```python\n1TORCH_LIBRARY_IMPL(_, AutogradPrivateUse1, m) {\n2  m.fallback(torch::autograd::autogradNotImplementedFallback());\n3}\n\n```\n\nUseTORCH_LIBRARY_IMPLto register thewrapper_custom_absimplementation for thecustom_absoperator inPrivateUse1. However, becauseAutogradis always enabled in PyTorch, PyTorch defaults to finding and executing the corresponding backward implementation even if only forward computation is required(will fallthrough in backward implementation). Therefore, we also need to register the corresponding implementation forAutogradPrivateUse1of thecustom_absoperator. Fortunately, PyTorch also provides a generalAutogradFallbackmechanism namedtorch::autograd::autogradNotImplementedFallback, if only forward computation is involved, it is equivalent to a fallthrough operation, selecting the next DispatchKey for computation; if backward computation is involved, an error is thrown.\nTORCH_LIBRARY_IMPL\nwrapper_custom_abs\ncustom_abs\nPrivateUse1\nAutograd\nAutogradPrivateUse1\ncustom_abs\nAutogradFallback\ntorch::autograd::autogradNotImplementedFallback\nRegister Metadata(optional, but required by the graph mode, etc.):\n\n```python\n1lib = torch.library.Library(\"openreg\", \"IMPL\", \"Meta\")  # noqa: TOR901\n2\n3\n4@torch.library.impl(lib, \"custom_abs\")\n5def custom_abs(self):\n6    return torch.empty_like(self)\n7\n8\n\n```\n\nPyTorch supports registeringMetain both C++ and Python. Since Python registration is simpler, Python is used as an example here. Similar to theTORCH_LIBRARY_IMPLfunction in C++, Python provides the more user-friendlytorch.library.impldecorator.\nMeta\nTORCH_LIBRARY_IMPL\ntorch.library.impl\n\n## Tools#\n\nOperator registration in PyTorch is complex, with diverse registration methods and numerous scenarios. Therefore, the PyTorch community has provided a number of tools to help developers quickly understand the underlying principles and assist in troubleshooting. Here we briefly introduce several commonly used tools:\n\n## Commands#\n\nPyTorch provides a set of commands prefixed withtorch._C._dispatch_around its Dispatch feature. You can query all related interfaces using the following command:\ntorch._C._dispatch_\n\n```python\npython -c 'import torch; print(\"\\n\".join([x for x in dir(torch._C) if x.startswith(\"_dispatch_\")]))'\n\n...\n_dispatch_dump\n_dispatch_dump_table\n_dispatch_has_kernel\n_dispatch_has_kernel_for_any_dispatch_key\n_dispatch_has_kernel_for_dispatch_key\n_dispatch_isTensorSubclassLike\n_dispatch_is_alias_key\n_dispatch_is_included_in_alias\n_dispatch_is_main_interpreter\n_dispatch_kernel_for_dispatch_key_is_fallthrough\n_dispatch_key_for_device\n_dispatch_key_name\n_dispatch_key_parse\n_dispatch_key_set\n...\n\n```\n\nHere are explanations for several commonly used commands:\ntorch._C._dispatch_key_set:\ntorch._C._dispatch_key_set\nDisplays the DispatchKey of the current Tensor, with priority increasing from left to right.\n\n```python\n>>> import torch\n>>> a = torch.randn(3,3,device=\"cuda\")\n>>> torch._C._dispatch_key_set(a)\n'DispatchKeySet(CUDA, ADInplaceOrView, AutogradCUDA, AutocastCUDA)'\n\n```\n\ntorch._C._dispatch_dump_table:\ntorch._C._dispatch_dump_table\nQueries the support status of a given operator across different Dispatch Keys, making it easy to locate the corresponding implementation code.\n\n```python\n>>> import torch\n>>> print(torch._C._dispatch_dump_table(\"aten::add.Tensor\"))\n>>> ...\n    CPU: registered at ./build/aten/src/ATen/RegisterCPU_0.cpp:1309 [kernel]\n    CUDA: registered at ./build/aten/src/ATen/RegisterCUDA_0.cpp:2420 [kernel]\n    HIP: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    MPS: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    IPU: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    XPU: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    HPU: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    VE: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    MTIA: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    MAIA: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    PrivateUse1: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    ...\n\n```\n\nYou can easily query the corresponding implementation of theaten::add.Tensoroperator on other platforms, so that you can track the entire operator calling process from the source code level.\naten::add.Tensor\n\n## Environment Variables#\n\nPyTorch also provides some dispatcher-related environment variables that can help with learning and quickly locating issues.\nTORCH_SHOW_DISPATCH_TRACE\nDisplays detailed internal dispatch key scheduling during PyTorch execution.\n\n```python\nexport TORCH_SHOW_DISPATCH_TRACE=1\n\n```\n\n\n```python\n>>> import torch\n>>> a = torch.randn(3,3)\n [call] op=[aten::randn], key=[BackendSelect]\n   [redispatch] op=[aten::randn], key=[CPU]\n     [call] op=[aten::empty.memory_format], key=[BackendSelect]\n       [redispatch] op=[aten::empty.memory_format], key=[CPU]\n     [call] op=[aten::normal_], key=[CPU]\n\n```\n\nYou can clearly see all the underlying operators called by Python-level operators within PyTorch: including the operator name, calling hierarchy, and correspondingDispatchKey.\nDispatchKey",
  "url": "https://pytorch.org/docs/stable/accelerator/operators.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
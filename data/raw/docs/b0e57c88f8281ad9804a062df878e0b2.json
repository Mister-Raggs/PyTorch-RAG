{
  "doc_id": "b0e57c88f8281ad9804a062df878e0b2",
  "source": "pytorch_docs",
  "title": "torch.Tensor \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.Tensor#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jun 27, 2025\nAtorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Please seetorch.dtypefor more details about dtype support.\ntorch.Tensor\n\n## Initializing and basic operations#\n\nA tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:\nlist\ntorch.tensor()\n\n```python\n>>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n\n```\n\nWarning\ntorch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor().\ntorch.tensor()\ndata\ndata\nrequires_grad\nrequires_grad_()\ndetach()\ntorch.as_tensor()\nA tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op:\ntorch.dtype\ntorch.device\n\n```python\n>>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n\n```\n\nFor more information about building Tensors, seeCreation Ops\nThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:\n\n```python\n>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n\n```\n\nUsetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value:\ntorch.Tensor.item()\n\n```python\n>>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5\n\n```\n\nFor more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops\nA tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.\nrequires_grad=True\ntorch.autograd\n\n```python\n>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n\n```\n\nEach tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it.\ntorch.Storage\nNote\nFor more information on tensor views, seeTensor Views.\nNote\nFor more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes.\ntorch.dtype\ntorch.device\ntorch.layout\ntorch.Tensor\nNote\nMethods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor.\ntorch.FloatTensor.abs_()\ntorch.FloatTensor.abs()\nNote\nTo change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor.\ntorch.device\ntorch.dtype\nto()\nWarning\nCurrent implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.\ntorch.Tensor\n\n## Tensor class reference#\n\nThere are a few main ways to create a tensor, depending on your use case.\nTo create a tensor with pre-existing data, usetorch.tensor().\ntorch.tensor()\nTo create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops).\ntorch.*\nTo create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops).\ntorch.*_like\nTo create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops.\ntensor.new_*\nThere is a legacy constructortorch.Tensorwhose use is discouraged.\nUsetorch.tensor()instead.\ntorch.Tensor\ntorch.tensor()\nThis constructor is deprecated, we recommend usingtorch.tensor()instead.\nWhat this constructor does depends on the type ofdata.\ntorch.tensor()\ndata\nIfdatais a Tensor, returns an alias to the original Tensor.  Unliketorch.tensor(), this tracks autograd and will propagate gradients to\nthe original Tensor.devicekwarg is not supported for thisdatatype.\ndata\ntorch.tensor()\ndevice\ndata\nIfdatais a sequence or nested sequence, create a tensor of the default\ndtype (typicallytorch.float32) whose data is the values in the\nsequences, performing coercions if necessary.  Notably, this differs fromtorch.tensor()in that this constructor will always construct a float\ntensor, even if the inputs are all integers.\ndata\ntorch.float32\ntorch.tensor()\nIfdatais atorch.Size, returns an empty tensor of that size.\ndata\ntorch.Size\nThis constructor does not support explicitly specifyingdtypeordeviceof\nthe returned tensor.  We recommend usingtorch.tensor()which provides this\nfunctionality.\ndtype\ndevice\ntorch.tensor()\ndata (array_like): The tensor to construct from.\ntorch.device\nDefault: if None, sametorch.deviceas this tensor.\ntorch.device\nReturns a view of this tensor with its dimensions reversed.\nIfnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0).\nn\nx\nx.T\nx.permute(n-1,n-2,...,0)\nWarning\nThe use ofTensor.T()on tensors of dimension other than 2 to reverse their shape\nis deprecated and it will throw an error in a future release. ConsidermTto transpose batches of matrices orx.permute(*torch.arange(x.ndim - 1, -1, -1))to reverse\nthe dimensions of a tensor.\nTensor.T()\nmT\nReturns a view of a matrix (2-D tensor) conjugated and transposed.\nx.His equivalent tox.transpose(0,1).conj()for complex matrices andx.transpose(0,1)for real matrices.\nx.H\nx.transpose(0,1).conj()\nx.transpose(0,1)\nSee also\nmH: An attribute that also works on batches of matrices.\nmH\nReturns a view of this tensor with the last two dimensions transposed.\nx.mTis equivalent tox.transpose(-2,-1).\nx.mT\nx.transpose(-2,-1)\nAccessing this property is equivalent to callingadjoint().\nadjoint()\nTensor.new_tensor\nTensor.new_tensor\nReturns a new Tensor withdataas the tensor data.\ndata\nTensor.new_full\nTensor.new_full\nReturns a Tensor of sizesizefilled withfill_value.\nsize\nfill_value\nTensor.new_empty\nTensor.new_empty\nReturns a Tensor of sizesizefilled with uninitialized data.\nsize\nTensor.new_ones\nTensor.new_ones\nReturns a Tensor of sizesizefilled with1.\nsize\n1\nTensor.new_zeros\nTensor.new_zeros\nReturns a Tensor of sizesizefilled with0.\nsize\n0\nTensor.is_cuda\nTensor.is_cuda\nIsTrueif the Tensor is stored on the GPU,Falseotherwise.\nTrue\nFalse\nTensor.is_quantized\nTensor.is_quantized\nIsTrueif the Tensor is quantized,Falseotherwise.\nTrue\nFalse\nTensor.is_meta\nTensor.is_meta\nIsTrueif the Tensor is a meta tensor,Falseotherwise.\nTrue\nFalse\nTensor.device\nTensor.device\nIs thetorch.devicewhere this Tensor is.\ntorch.device\nTensor.grad\nTensor.grad\nThis attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself.\nNone\nbackward()\nself\nTensor.ndim\nTensor.ndim\nAlias fordim()\ndim()\nTensor.real\nTensor.real\nReturns a new tensor containing real values of theselftensor for a complex-valued input tensor.\nself\nTensor.imag\nTensor.imag\nReturns a new tensor containing imaginary values of theselftensor.\nself\nTensor.nbytes\nTensor.nbytes\nReturns the number of bytes consumed by the \"view\" of elements of the Tensor if the Tensor does not use sparse storage layout.\nTensor.itemsize\nTensor.itemsize\nAlias forelement_size()\nelement_size()\nTensor.abs\nTensor.abs\nSeetorch.abs()\ntorch.abs()\nTensor.abs_\nTensor.abs_\nIn-place version ofabs()\nabs()\nTensor.absolute\nTensor.absolute\nAlias forabs()\nabs()\nTensor.absolute_\nTensor.absolute_\nIn-place version ofabsolute()Alias forabs_()\nabsolute()\nabs_()\nTensor.acos\nTensor.acos\nSeetorch.acos()\ntorch.acos()\nTensor.acos_\nTensor.acos_\nIn-place version ofacos()\nacos()\nTensor.arccos\nTensor.arccos\nSeetorch.arccos()\ntorch.arccos()\nTensor.arccos_\nTensor.arccos_\nIn-place version ofarccos()\narccos()\nTensor.add\nTensor.add\nAdd a scalar or tensor toselftensor.\nself\nTensor.add_\nTensor.add_\nIn-place version ofadd()\nadd()\nTensor.addbmm\nTensor.addbmm\nSeetorch.addbmm()\ntorch.addbmm()\nTensor.addbmm_\nTensor.addbmm_\nIn-place version ofaddbmm()\naddbmm()\nTensor.addcdiv\nTensor.addcdiv\nSeetorch.addcdiv()\ntorch.addcdiv()\nTensor.addcdiv_\nTensor.addcdiv_\nIn-place version ofaddcdiv()\naddcdiv()\nTensor.addcmul\nTensor.addcmul\nSeetorch.addcmul()\ntorch.addcmul()\nTensor.addcmul_\nTensor.addcmul_\nIn-place version ofaddcmul()\naddcmul()\nTensor.addmm\nTensor.addmm\nSeetorch.addmm()\ntorch.addmm()\nTensor.addmm_\nTensor.addmm_\nIn-place version ofaddmm()\naddmm()\nTensor.sspaddmm\nTensor.sspaddmm\nSeetorch.sspaddmm()\ntorch.sspaddmm()\nTensor.addmv\nTensor.addmv\nSeetorch.addmv()\ntorch.addmv()\nTensor.addmv_\nTensor.addmv_\nIn-place version ofaddmv()\naddmv()\nTensor.addr\nTensor.addr\nSeetorch.addr()\ntorch.addr()\nTensor.addr_\nTensor.addr_\nIn-place version ofaddr()\naddr()\nTensor.adjoint\nTensor.adjoint\nAlias foradjoint()\nadjoint()\nTensor.allclose\nTensor.allclose\nSeetorch.allclose()\ntorch.allclose()\nTensor.amax\nTensor.amax\nSeetorch.amax()\ntorch.amax()\nTensor.amin\nTensor.amin\nSeetorch.amin()\ntorch.amin()\nTensor.aminmax\nTensor.aminmax\nSeetorch.aminmax()\ntorch.aminmax()\nTensor.angle\nTensor.angle\nSeetorch.angle()\ntorch.angle()\nTensor.apply_\nTensor.apply_\nApplies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable.\ncallable\ncallable\nTensor.argmax\nTensor.argmax\nSeetorch.argmax()\ntorch.argmax()\nTensor.argmin\nTensor.argmin\nSeetorch.argmin()\ntorch.argmin()\nTensor.argsort\nTensor.argsort\nSeetorch.argsort()\ntorch.argsort()\nTensor.argwhere\nTensor.argwhere\nSeetorch.argwhere()\ntorch.argwhere()\nTensor.asin\nTensor.asin\nSeetorch.asin()\ntorch.asin()\nTensor.asin_\nTensor.asin_\nIn-place version ofasin()\nasin()\nTensor.arcsin\nTensor.arcsin\nSeetorch.arcsin()\ntorch.arcsin()\nTensor.arcsin_\nTensor.arcsin_\nIn-place version ofarcsin()\narcsin()\nTensor.as_strided\nTensor.as_strided\nSeetorch.as_strided()\ntorch.as_strided()\nTensor.atan\nTensor.atan\nSeetorch.atan()\ntorch.atan()\nTensor.atan_\nTensor.atan_\nIn-place version ofatan()\natan()\nTensor.arctan\nTensor.arctan\nSeetorch.arctan()\ntorch.arctan()\nTensor.arctan_\nTensor.arctan_\nIn-place version ofarctan()\narctan()\nTensor.atan2\nTensor.atan2\nSeetorch.atan2()\ntorch.atan2()\nTensor.atan2_\nTensor.atan2_\nIn-place version ofatan2()\natan2()\nTensor.arctan2\nTensor.arctan2\nSeetorch.arctan2()\ntorch.arctan2()\nTensor.arctan2_\nTensor.arctan2_\natan2_(other) -> Tensor\nTensor.all\nTensor.all\nSeetorch.all()\ntorch.all()\nTensor.any\nTensor.any\nSeetorch.any()\ntorch.any()\nTensor.backward\nTensor.backward\nComputes the gradient of current tensor wrt graph leaves.\nTensor.baddbmm\nTensor.baddbmm\nSeetorch.baddbmm()\ntorch.baddbmm()\nTensor.baddbmm_\nTensor.baddbmm_\nIn-place version ofbaddbmm()\nbaddbmm()\nTensor.bernoulli\nTensor.bernoulli\nReturns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]).\nTensor.bernoulli_\nTensor.bernoulli_\nFills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p).\nself\nTensor.bfloat16\nTensor.bfloat16\nself.bfloat16()is equivalent toself.to(torch.bfloat16).\nself.bfloat16()\nself.to(torch.bfloat16)\nTensor.bincount\nTensor.bincount\nSeetorch.bincount()\ntorch.bincount()\nTensor.bitwise_not\nTensor.bitwise_not\nSeetorch.bitwise_not()\ntorch.bitwise_not()\nTensor.bitwise_not_\nTensor.bitwise_not_\nIn-place version ofbitwise_not()\nbitwise_not()\nTensor.bitwise_and\nTensor.bitwise_and\nSeetorch.bitwise_and()\ntorch.bitwise_and()\nTensor.bitwise_and_\nTensor.bitwise_and_\nIn-place version ofbitwise_and()\nbitwise_and()\nTensor.bitwise_or\nTensor.bitwise_or\nSeetorch.bitwise_or()\ntorch.bitwise_or()\nTensor.bitwise_or_\nTensor.bitwise_or_\nIn-place version ofbitwise_or()\nbitwise_or()\nTensor.bitwise_xor\nTensor.bitwise_xor\nSeetorch.bitwise_xor()\ntorch.bitwise_xor()\nTensor.bitwise_xor_\nTensor.bitwise_xor_\nIn-place version ofbitwise_xor()\nbitwise_xor()\nTensor.bitwise_left_shift\nTensor.bitwise_left_shift\nSeetorch.bitwise_left_shift()\ntorch.bitwise_left_shift()\nTensor.bitwise_left_shift_\nTensor.bitwise_left_shift_\nIn-place version ofbitwise_left_shift()\nbitwise_left_shift()\nTensor.bitwise_right_shift\nTensor.bitwise_right_shift\nSeetorch.bitwise_right_shift()\ntorch.bitwise_right_shift()\nTensor.bitwise_right_shift_\nTensor.bitwise_right_shift_\nIn-place version ofbitwise_right_shift()\nbitwise_right_shift()\nTensor.bmm\nTensor.bmm\nSeetorch.bmm()\ntorch.bmm()\nTensor.bool\nTensor.bool\nself.bool()is equivalent toself.to(torch.bool).\nself.bool()\nself.to(torch.bool)\nTensor.byte\nTensor.byte\nself.byte()is equivalent toself.to(torch.uint8).\nself.byte()\nself.to(torch.uint8)\nTensor.broadcast_to\nTensor.broadcast_to\nSeetorch.broadcast_to().\ntorch.broadcast_to()\nTensor.cauchy_\nTensor.cauchy_\nFills the tensor with numbers drawn from the Cauchy distribution:\nTensor.ceil\nTensor.ceil\nSeetorch.ceil()\ntorch.ceil()\nTensor.ceil_\nTensor.ceil_\nIn-place version ofceil()\nceil()\nTensor.char\nTensor.char\nself.char()is equivalent toself.to(torch.int8).\nself.char()\nself.to(torch.int8)\nTensor.cholesky\nTensor.cholesky\nSeetorch.cholesky()\ntorch.cholesky()\nTensor.cholesky_inverse\nTensor.cholesky_inverse\nSeetorch.cholesky_inverse()\ntorch.cholesky_inverse()\nTensor.cholesky_solve\nTensor.cholesky_solve\nSeetorch.cholesky_solve()\ntorch.cholesky_solve()\nTensor.chunk\nTensor.chunk\nSeetorch.chunk()\ntorch.chunk()\nTensor.clamp\nTensor.clamp\nSeetorch.clamp()\ntorch.clamp()\nTensor.clamp_\nTensor.clamp_\nIn-place version ofclamp()\nclamp()\nTensor.clip\nTensor.clip\nAlias forclamp().\nclamp()\nTensor.clip_\nTensor.clip_\nAlias forclamp_().\nclamp_()\nTensor.clone\nTensor.clone\nSeetorch.clone()\ntorch.clone()\nTensor.contiguous\nTensor.contiguous\nReturns a contiguous in memory tensor containing the same data asselftensor.\nself\nTensor.copy_\nTensor.copy_\nCopies the elements fromsrcintoselftensor and returnsself.\nsrc\nself\nself\nTensor.conj\nTensor.conj\nSeetorch.conj()\ntorch.conj()\nTensor.conj_physical\nTensor.conj_physical\nSeetorch.conj_physical()\ntorch.conj_physical()\nTensor.conj_physical_\nTensor.conj_physical_\nIn-place version ofconj_physical()\nconj_physical()\nTensor.resolve_conj\nTensor.resolve_conj\nSeetorch.resolve_conj()\ntorch.resolve_conj()\nTensor.resolve_neg\nTensor.resolve_neg\nSeetorch.resolve_neg()\ntorch.resolve_neg()\nTensor.copysign\nTensor.copysign\nSeetorch.copysign()\ntorch.copysign()\nTensor.copysign_\nTensor.copysign_\nIn-place version ofcopysign()\ncopysign()\nTensor.cos\nTensor.cos\nSeetorch.cos()\ntorch.cos()\nTensor.cos_\nTensor.cos_\nIn-place version ofcos()\ncos()\nTensor.cosh\nTensor.cosh\nSeetorch.cosh()\ntorch.cosh()\nTensor.cosh_\nTensor.cosh_\nIn-place version ofcosh()\ncosh()\nTensor.corrcoef\nTensor.corrcoef\nSeetorch.corrcoef()\ntorch.corrcoef()\nTensor.count_nonzero\nTensor.count_nonzero\nSeetorch.count_nonzero()\ntorch.count_nonzero()\nTensor.cov\nTensor.cov\nSeetorch.cov()\ntorch.cov()\nTensor.acosh\nTensor.acosh\nSeetorch.acosh()\ntorch.acosh()\nTensor.acosh_\nTensor.acosh_\nIn-place version ofacosh()\nacosh()\nTensor.arccosh\nTensor.arccosh\nacosh() -> Tensor\nTensor.arccosh_\nTensor.arccosh_\nacosh_() -> Tensor\nTensor.cpu\nTensor.cpu\nReturns a copy of this object in CPU memory.\nTensor.cross\nTensor.cross\nSeetorch.cross()\ntorch.cross()\nTensor.cuda\nTensor.cuda\nReturns a copy of this object in CUDA memory.\nTensor.logcumsumexp\nTensor.logcumsumexp\nSeetorch.logcumsumexp()\ntorch.logcumsumexp()\nTensor.cummax\nTensor.cummax\nSeetorch.cummax()\ntorch.cummax()\nTensor.cummin\nTensor.cummin\nSeetorch.cummin()\ntorch.cummin()\nTensor.cumprod\nTensor.cumprod\nSeetorch.cumprod()\ntorch.cumprod()\nTensor.cumprod_\nTensor.cumprod_\nIn-place version ofcumprod()\ncumprod()\nTensor.cumsum\nTensor.cumsum\nSeetorch.cumsum()\ntorch.cumsum()\nTensor.cumsum_\nTensor.cumsum_\nIn-place version ofcumsum()\ncumsum()\nTensor.chalf\nTensor.chalf\nself.chalf()is equivalent toself.to(torch.complex32).\nself.chalf()\nself.to(torch.complex32)\nTensor.cfloat\nTensor.cfloat\nself.cfloat()is equivalent toself.to(torch.complex64).\nself.cfloat()\nself.to(torch.complex64)\nTensor.cdouble\nTensor.cdouble\nself.cdouble()is equivalent toself.to(torch.complex128).\nself.cdouble()\nself.to(torch.complex128)\nTensor.data_ptr\nTensor.data_ptr\nReturns the address of the first element ofselftensor.\nself\nTensor.deg2rad\nTensor.deg2rad\nSeetorch.deg2rad()\ntorch.deg2rad()\nTensor.dequantize\nTensor.dequantize\nGiven a quantized Tensor, dequantize it and return the dequantized float Tensor.\nTensor.det\nTensor.det\nSeetorch.det()\ntorch.det()\nTensor.dense_dim\nTensor.dense_dim\nReturn the number of dense dimensions in asparse tensorself.\nself\nTensor.detach\nTensor.detach\nReturns a new Tensor, detached from the current graph.\nTensor.detach_\nTensor.detach_\nDetaches the Tensor from the graph that created it, making it a leaf.\nTensor.diag\nTensor.diag\nSeetorch.diag()\ntorch.diag()\nTensor.diag_embed\nTensor.diag_embed\nSeetorch.diag_embed()\ntorch.diag_embed()\nTensor.diagflat\nTensor.diagflat\nSeetorch.diagflat()\ntorch.diagflat()\nTensor.diagonal\nTensor.diagonal\nSeetorch.diagonal()\ntorch.diagonal()\nTensor.diagonal_scatter\nTensor.diagonal_scatter\nSeetorch.diagonal_scatter()\ntorch.diagonal_scatter()\nTensor.fill_diagonal_\nTensor.fill_diagonal_\nFill the main diagonal of a tensor that has at least 2-dimensions.\nTensor.fmax\nTensor.fmax\nSeetorch.fmax()\ntorch.fmax()\nTensor.fmin\nTensor.fmin\nSeetorch.fmin()\ntorch.fmin()\nTensor.diff\nTensor.diff\nSeetorch.diff()\ntorch.diff()\nTensor.digamma\nTensor.digamma\nSeetorch.digamma()\ntorch.digamma()\nTensor.digamma_\nTensor.digamma_\nIn-place version ofdigamma()\ndigamma()\nTensor.dim\nTensor.dim\nReturns the number of dimensions ofselftensor.\nself\nTensor.dim_order\nTensor.dim_order\nReturns the uniquely determined tuple of int describing the dim order or physical layout ofself.\nself\nTensor.dist\nTensor.dist\nSeetorch.dist()\ntorch.dist()\nTensor.div\nTensor.div\nSeetorch.div()\ntorch.div()\nTensor.div_\nTensor.div_\nIn-place version ofdiv()\ndiv()\nTensor.divide\nTensor.divide\nSeetorch.divide()\ntorch.divide()\nTensor.divide_\nTensor.divide_\nIn-place version ofdivide()\ndivide()\nTensor.dot\nTensor.dot\nSeetorch.dot()\ntorch.dot()\nTensor.double\nTensor.double\nself.double()is equivalent toself.to(torch.float64).\nself.double()\nself.to(torch.float64)\nTensor.dsplit\nTensor.dsplit\nSeetorch.dsplit()\ntorch.dsplit()\nTensor.element_size\nTensor.element_size\nReturns the size in bytes of an individual element.\nTensor.eq\nTensor.eq\nSeetorch.eq()\ntorch.eq()\nTensor.eq_\nTensor.eq_\nIn-place version ofeq()\neq()\nTensor.equal\nTensor.equal\nSeetorch.equal()\ntorch.equal()\nTensor.erf\nTensor.erf\nSeetorch.erf()\ntorch.erf()\nTensor.erf_\nTensor.erf_\nIn-place version oferf()\nerf()\nTensor.erfc\nTensor.erfc\nSeetorch.erfc()\ntorch.erfc()\nTensor.erfc_\nTensor.erfc_\nIn-place version oferfc()\nerfc()\nTensor.erfinv\nTensor.erfinv\nSeetorch.erfinv()\ntorch.erfinv()\nTensor.erfinv_\nTensor.erfinv_\nIn-place version oferfinv()\nerfinv()\nTensor.exp\nTensor.exp\nSeetorch.exp()\ntorch.exp()\nTensor.exp_\nTensor.exp_\nIn-place version ofexp()\nexp()\nTensor.expm1\nTensor.expm1\nSeetorch.expm1()\ntorch.expm1()\nTensor.expm1_\nTensor.expm1_\nIn-place version ofexpm1()\nexpm1()\nTensor.expand\nTensor.expand\nReturns a new view of theselftensor with singleton dimensions expanded to a larger size.\nself\nTensor.expand_as\nTensor.expand_as\nExpand this tensor to the same size asother.\nother\nTensor.exponential_\nTensor.exponential_\nFillsselftensor with elements drawn from the PDF (probability density function):\nself\nTensor.fix\nTensor.fix\nSeetorch.fix().\ntorch.fix()\nTensor.fix_\nTensor.fix_\nIn-place version offix()\nfix()\nTensor.fill_\nTensor.fill_\nFillsselftensor with the specified value.\nself\nTensor.flatten\nTensor.flatten\nSeetorch.flatten()\ntorch.flatten()\nTensor.flip\nTensor.flip\nSeetorch.flip()\ntorch.flip()\nTensor.fliplr\nTensor.fliplr\nSeetorch.fliplr()\ntorch.fliplr()\nTensor.flipud\nTensor.flipud\nSeetorch.flipud()\ntorch.flipud()\nTensor.float\nTensor.float\nself.float()is equivalent toself.to(torch.float32).\nself.float()\nself.to(torch.float32)\nTensor.float_power\nTensor.float_power\nSeetorch.float_power()\ntorch.float_power()\nTensor.float_power_\nTensor.float_power_\nIn-place version offloat_power()\nfloat_power()\nTensor.floor\nTensor.floor\nSeetorch.floor()\ntorch.floor()\nTensor.floor_\nTensor.floor_\nIn-place version offloor()\nfloor()\nTensor.floor_divide\nTensor.floor_divide\nSeetorch.floor_divide()\ntorch.floor_divide()\nTensor.floor_divide_\nTensor.floor_divide_\nIn-place version offloor_divide()\nfloor_divide()\nTensor.fmod\nTensor.fmod\nSeetorch.fmod()\ntorch.fmod()\nTensor.fmod_\nTensor.fmod_\nIn-place version offmod()\nfmod()\nTensor.frac\nTensor.frac\nSeetorch.frac()\ntorch.frac()\nTensor.frac_\nTensor.frac_\nIn-place version offrac()\nfrac()\nTensor.frexp\nTensor.frexp\nSeetorch.frexp()\ntorch.frexp()\nTensor.gather\nTensor.gather\nSeetorch.gather()\ntorch.gather()\nTensor.gcd\nTensor.gcd\nSeetorch.gcd()\ntorch.gcd()\nTensor.gcd_\nTensor.gcd_\nIn-place version ofgcd()\ngcd()\nTensor.ge\nTensor.ge\nSeetorch.ge().\ntorch.ge()\nTensor.ge_\nTensor.ge_\nIn-place version ofge().\nge()\nTensor.greater_equal\nTensor.greater_equal\nSeetorch.greater_equal().\ntorch.greater_equal()\nTensor.greater_equal_\nTensor.greater_equal_\nIn-place version ofgreater_equal().\ngreater_equal()\nTensor.geometric_\nTensor.geometric_\nFillsselftensor with elements drawn from the geometric distribution:\nself\nTensor.geqrf\nTensor.geqrf\nSeetorch.geqrf()\ntorch.geqrf()\nTensor.ger\nTensor.ger\nSeetorch.ger()\ntorch.ger()\nTensor.get_device\nTensor.get_device\nFor CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.\nTensor.gt\nTensor.gt\nSeetorch.gt().\ntorch.gt()\nTensor.gt_\nTensor.gt_\nIn-place version ofgt().\ngt()\nTensor.greater\nTensor.greater\nSeetorch.greater().\ntorch.greater()\nTensor.greater_\nTensor.greater_\nIn-place version ofgreater().\ngreater()\nTensor.half\nTensor.half\nself.half()is equivalent toself.to(torch.float16).\nself.half()\nself.to(torch.float16)\nTensor.hardshrink\nTensor.hardshrink\nSeetorch.nn.functional.hardshrink()\ntorch.nn.functional.hardshrink()\nTensor.heaviside\nTensor.heaviside\nSeetorch.heaviside()\ntorch.heaviside()\nTensor.histc\nTensor.histc\nSeetorch.histc()\ntorch.histc()\nTensor.histogram\nTensor.histogram\nSeetorch.histogram()\ntorch.histogram()\nTensor.hsplit\nTensor.hsplit\nSeetorch.hsplit()\ntorch.hsplit()\nTensor.hypot\nTensor.hypot\nSeetorch.hypot()\ntorch.hypot()\nTensor.hypot_\nTensor.hypot_\nIn-place version ofhypot()\nhypot()\nTensor.i0\nTensor.i0\nSeetorch.i0()\ntorch.i0()\nTensor.i0_\nTensor.i0_\nIn-place version ofi0()\ni0()\nTensor.igamma\nTensor.igamma\nSeetorch.igamma()\ntorch.igamma()\nTensor.igamma_\nTensor.igamma_\nIn-place version ofigamma()\nigamma()\nTensor.igammac\nTensor.igammac\nSeetorch.igammac()\ntorch.igammac()\nTensor.igammac_\nTensor.igammac_\nIn-place version ofigammac()\nigammac()\nTensor.index_add_\nTensor.index_add_\nAccumulate the elements ofalphatimessourceinto theselftensor by adding to the indices in the order given inindex.\nalpha\nsource\nself\nindex\nTensor.index_add\nTensor.index_add\nOut-of-place version oftorch.Tensor.index_add_().\ntorch.Tensor.index_add_()\nTensor.index_copy_\nTensor.index_copy_\nCopies the elements oftensorinto theselftensor by selecting the indices in the order given inindex.\ntensor\nself\nindex\nTensor.index_copy\nTensor.index_copy\nOut-of-place version oftorch.Tensor.index_copy_().\ntorch.Tensor.index_copy_()\nTensor.index_fill_\nTensor.index_fill_\nFills the elements of theselftensor with valuevalueby selecting the indices in the order given inindex.\nself\nvalue\nindex\nTensor.index_fill\nTensor.index_fill\nOut-of-place version oftorch.Tensor.index_fill_().\ntorch.Tensor.index_fill_()\nTensor.index_put_\nTensor.index_put_\nPuts values from the tensorvaluesinto the tensorselfusing the indices specified inindices(which is a tuple of Tensors).\nvalues\nself\nindices\nTensor.index_put\nTensor.index_put\nOut-place version ofindex_put_().\nindex_put_()\nTensor.index_reduce_\nTensor.index_reduce_\nAccumulate the elements ofsourceinto theselftensor by accumulating to the indices in the order given inindexusing the reduction given by thereduceargument.\nsource\nself\nindex\nreduce\nTensor.index_reduce\nTensor.index_reduce\n\nTensor.index_select\nTensor.index_select\nSeetorch.index_select()\ntorch.index_select()\nTensor.indices\nTensor.indices\nReturn the indices tensor of asparse COO tensor.\nTensor.inner\nTensor.inner\nSeetorch.inner().\ntorch.inner()\nTensor.int\nTensor.int\nself.int()is equivalent toself.to(torch.int32).\nself.int()\nself.to(torch.int32)\nTensor.int_repr\nTensor.int_repr\nGiven a quantized Tensor,self.int_repr()returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.\nself.int_repr()\nTensor.inverse\nTensor.inverse\nSeetorch.inverse()\ntorch.inverse()\nTensor.isclose\nTensor.isclose\nSeetorch.isclose()\ntorch.isclose()\nTensor.isfinite\nTensor.isfinite\nSeetorch.isfinite()\ntorch.isfinite()\nTensor.isinf\nTensor.isinf\nSeetorch.isinf()\ntorch.isinf()\nTensor.isposinf\nTensor.isposinf\nSeetorch.isposinf()\ntorch.isposinf()\nTensor.isneginf\nTensor.isneginf\nSeetorch.isneginf()\ntorch.isneginf()\nTensor.isnan\nTensor.isnan\nSeetorch.isnan()\ntorch.isnan()\nTensor.is_contiguous\nTensor.is_contiguous\nReturns True ifselftensor is contiguous in memory in the order specified by memory format.\nself\nTensor.is_complex\nTensor.is_complex\nReturns True if the data type ofselfis a complex data type.\nself\nTensor.is_conj\nTensor.is_conj\nReturns True if the conjugate bit ofselfis set to true.\nself\nTensor.is_floating_point\nTensor.is_floating_point\nReturns True if the data type ofselfis a floating point data type.\nself\nTensor.is_inference\nTensor.is_inference\nSeetorch.is_inference()\ntorch.is_inference()\nTensor.is_leaf\nTensor.is_leaf\nAll Tensors that haverequires_gradwhich isFalsewill be leaf Tensors by convention.\nrequires_grad\nFalse\nTensor.is_pinned\nTensor.is_pinned\nReturns true if this tensor resides in pinned memory.\nTensor.is_set_to\nTensor.is_set_to\nReturns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).\nTensor.is_shared\nTensor.is_shared\nChecks if tensor is in shared memory.\nTensor.is_signed\nTensor.is_signed\nReturns True if the data type ofselfis a signed data type.\nself\nTensor.is_sparse\nTensor.is_sparse\nIsTrueif the Tensor uses sparse COO storage layout,Falseotherwise.\nTrue\nFalse\nTensor.istft\nTensor.istft\nSeetorch.istft()\ntorch.istft()\nTensor.isreal\nTensor.isreal\nSeetorch.isreal()\ntorch.isreal()\nTensor.item\nTensor.item\nReturns the value of this tensor as a standard Python number.\nTensor.kthvalue\nTensor.kthvalue\nSeetorch.kthvalue()\ntorch.kthvalue()\nTensor.lcm\nTensor.lcm\nSeetorch.lcm()\ntorch.lcm()\nTensor.lcm_\nTensor.lcm_\nIn-place version oflcm()\nlcm()\nTensor.ldexp\nTensor.ldexp\nSeetorch.ldexp()\ntorch.ldexp()\nTensor.ldexp_\nTensor.ldexp_\nIn-place version ofldexp()\nldexp()\nTensor.le\nTensor.le\nSeetorch.le().\ntorch.le()\nTensor.le_\nTensor.le_\nIn-place version ofle().\nle()\nTensor.less_equal\nTensor.less_equal\nSeetorch.less_equal().\ntorch.less_equal()\nTensor.less_equal_\nTensor.less_equal_\nIn-place version ofless_equal().\nless_equal()\nTensor.lerp\nTensor.lerp\nSeetorch.lerp()\ntorch.lerp()\nTensor.lerp_\nTensor.lerp_\nIn-place version oflerp()\nlerp()\nTensor.lgamma\nTensor.lgamma\nSeetorch.lgamma()\ntorch.lgamma()\nTensor.lgamma_\nTensor.lgamma_\nIn-place version oflgamma()\nlgamma()\nTensor.log\nTensor.log\nSeetorch.log()\ntorch.log()\nTensor.log_\nTensor.log_\nIn-place version oflog()\nlog()\nTensor.logdet\nTensor.logdet\nSeetorch.logdet()\ntorch.logdet()\nTensor.log10\nTensor.log10\nSeetorch.log10()\ntorch.log10()\nTensor.log10_\nTensor.log10_\nIn-place version oflog10()\nlog10()\nTensor.log1p\nTensor.log1p\nSeetorch.log1p()\ntorch.log1p()\nTensor.log1p_\nTensor.log1p_\nIn-place version oflog1p()\nlog1p()\nTensor.log2\nTensor.log2\nSeetorch.log2()\ntorch.log2()\nTensor.log2_\nTensor.log2_\nIn-place version oflog2()\nlog2()\nTensor.log_normal_\nTensor.log_normal_\nFillsselftensor with numbers samples from the log-normal distribution parameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3.\nself\nTensor.logaddexp\nTensor.logaddexp\nSeetorch.logaddexp()\ntorch.logaddexp()\nTensor.logaddexp2\nTensor.logaddexp2\nSeetorch.logaddexp2()\ntorch.logaddexp2()\nTensor.logsumexp\nTensor.logsumexp\nSeetorch.logsumexp()\ntorch.logsumexp()\nTensor.logical_and\nTensor.logical_and\nSeetorch.logical_and()\ntorch.logical_and()\nTensor.logical_and_\nTensor.logical_and_\nIn-place version oflogical_and()\nlogical_and()\nTensor.logical_not\nTensor.logical_not\nSeetorch.logical_not()\ntorch.logical_not()\nTensor.logical_not_\nTensor.logical_not_\nIn-place version oflogical_not()\nlogical_not()\nTensor.logical_or\nTensor.logical_or\nSeetorch.logical_or()\ntorch.logical_or()\nTensor.logical_or_\nTensor.logical_or_\nIn-place version oflogical_or()\nlogical_or()\nTensor.logical_xor\nTensor.logical_xor\nSeetorch.logical_xor()\ntorch.logical_xor()\nTensor.logical_xor_\nTensor.logical_xor_\nIn-place version oflogical_xor()\nlogical_xor()\nTensor.logit\nTensor.logit\nSeetorch.logit()\ntorch.logit()\nTensor.logit_\nTensor.logit_\nIn-place version oflogit()\nlogit()\nTensor.long\nTensor.long\nself.long()is equivalent toself.to(torch.int64).\nself.long()\nself.to(torch.int64)\nTensor.lt\nTensor.lt\nSeetorch.lt().\ntorch.lt()\nTensor.lt_\nTensor.lt_\nIn-place version oflt().\nlt()\nTensor.less\nTensor.less\nlt(other) -> Tensor\nTensor.less_\nTensor.less_\nIn-place version ofless().\nless()\nTensor.lu\nTensor.lu\nSeetorch.lu()\ntorch.lu()\nTensor.lu_solve\nTensor.lu_solve\nSeetorch.lu_solve()\ntorch.lu_solve()\nTensor.as_subclass\nTensor.as_subclass\nMakes aclsinstance with the same data pointer asself.\ncls\nself\nTensor.map_\nTensor.map_\nAppliescallablefor each element inselftensor and the giventensorand stores the results inselftensor.\ncallable\nself\ntensor\nself\nTensor.masked_scatter_\nTensor.masked_scatter_\nCopies elements fromsourceintoselftensor at positions where themaskis True.\nsource\nself\nmask\nTensor.masked_scatter\nTensor.masked_scatter\nOut-of-place version oftorch.Tensor.masked_scatter_()\ntorch.Tensor.masked_scatter_()\nTensor.masked_fill_\nTensor.masked_fill_\nFills elements ofselftensor withvaluewheremaskis True.\nself\nvalue\nmask\nTensor.masked_fill\nTensor.masked_fill\nOut-of-place version oftorch.Tensor.masked_fill_()\ntorch.Tensor.masked_fill_()\nTensor.masked_select\nTensor.masked_select\nSeetorch.masked_select()\ntorch.masked_select()\nTensor.matmul\nTensor.matmul\nSeetorch.matmul()\ntorch.matmul()\nTensor.matrix_power\nTensor.matrix_power\nNotematrix_power()is deprecated, usetorch.linalg.matrix_power()instead.\nNote\nmatrix_power()is deprecated, usetorch.linalg.matrix_power()instead.\nmatrix_power()\ntorch.linalg.matrix_power()\nTensor.matrix_exp\nTensor.matrix_exp\nSeetorch.matrix_exp()\ntorch.matrix_exp()\nTensor.max\nTensor.max\nSeetorch.max()\ntorch.max()\nTensor.maximum\nTensor.maximum\nSeetorch.maximum()\ntorch.maximum()\nTensor.mean\nTensor.mean\nSeetorch.mean()\ntorch.mean()\nTensor.module_load\nTensor.module_load\nDefines how to transformotherwhen loading it intoselfinload_state_dict().\nother\nself\nload_state_dict()\nTensor.nanmean\nTensor.nanmean\nSeetorch.nanmean()\ntorch.nanmean()\nTensor.median\nTensor.median\nSeetorch.median()\ntorch.median()\nTensor.nanmedian\nTensor.nanmedian\nSeetorch.nanmedian()\ntorch.nanmedian()\nTensor.min\nTensor.min\nSeetorch.min()\ntorch.min()\nTensor.minimum\nTensor.minimum\nSeetorch.minimum()\ntorch.minimum()\nTensor.mm\nTensor.mm\nSeetorch.mm()\ntorch.mm()\nTensor.smm\nTensor.smm\nSeetorch.smm()\ntorch.smm()\nTensor.mode\nTensor.mode\nSeetorch.mode()\ntorch.mode()\nTensor.movedim\nTensor.movedim\nSeetorch.movedim()\ntorch.movedim()\nTensor.moveaxis\nTensor.moveaxis\nSeetorch.moveaxis()\ntorch.moveaxis()\nTensor.msort\nTensor.msort\nSeetorch.msort()\ntorch.msort()\nTensor.mul\nTensor.mul\nSeetorch.mul().\ntorch.mul()\nTensor.mul_\nTensor.mul_\nIn-place version ofmul().\nmul()\nTensor.multiply\nTensor.multiply\nSeetorch.multiply().\ntorch.multiply()\nTensor.multiply_\nTensor.multiply_\nIn-place version ofmultiply().\nmultiply()\nTensor.multinomial\nTensor.multinomial\nSeetorch.multinomial()\ntorch.multinomial()\nTensor.mv\nTensor.mv\nSeetorch.mv()\ntorch.mv()\nTensor.mvlgamma\nTensor.mvlgamma\nSeetorch.mvlgamma()\ntorch.mvlgamma()\nTensor.mvlgamma_\nTensor.mvlgamma_\nIn-place version ofmvlgamma()\nmvlgamma()\nTensor.nansum\nTensor.nansum\nSeetorch.nansum()\ntorch.nansum()\nTensor.narrow\nTensor.narrow\nSeetorch.narrow().\ntorch.narrow()\nTensor.narrow_copy\nTensor.narrow_copy\nSeetorch.narrow_copy().\ntorch.narrow_copy()\nTensor.ndimension\nTensor.ndimension\nAlias fordim()\ndim()\nTensor.nan_to_num\nTensor.nan_to_num\nSeetorch.nan_to_num().\ntorch.nan_to_num()\nTensor.nan_to_num_\nTensor.nan_to_num_\nIn-place version ofnan_to_num().\nnan_to_num()\nTensor.ne\nTensor.ne\nSeetorch.ne().\ntorch.ne()\nTensor.ne_\nTensor.ne_\nIn-place version ofne().\nne()\nTensor.not_equal\nTensor.not_equal\nSeetorch.not_equal().\ntorch.not_equal()\nTensor.not_equal_\nTensor.not_equal_\nIn-place version ofnot_equal().\nnot_equal()\nTensor.neg\nTensor.neg\nSeetorch.neg()\ntorch.neg()\nTensor.neg_\nTensor.neg_\nIn-place version ofneg()\nneg()\nTensor.negative\nTensor.negative\nSeetorch.negative()\ntorch.negative()\nTensor.negative_\nTensor.negative_\nIn-place version ofnegative()\nnegative()\nTensor.nelement\nTensor.nelement\nAlias fornumel()\nnumel()\nTensor.nextafter\nTensor.nextafter\nSeetorch.nextafter()\ntorch.nextafter()\nTensor.nextafter_\nTensor.nextafter_\nIn-place version ofnextafter()\nnextafter()\nTensor.nonzero\nTensor.nonzero\nSeetorch.nonzero()\ntorch.nonzero()\nTensor.norm\nTensor.norm\nSeetorch.norm()\ntorch.norm()\nTensor.normal_\nTensor.normal_\nFillsselftensor with elements samples from the normal distribution parameterized bymeanandstd.\nself\nmean\nstd\nTensor.numel\nTensor.numel\nSeetorch.numel()\ntorch.numel()\nTensor.numpy\nTensor.numpy\nReturns the tensor as a NumPyndarray.\nndarray\nTensor.orgqr\nTensor.orgqr\nSeetorch.orgqr()\ntorch.orgqr()\nTensor.ormqr\nTensor.ormqr\nSeetorch.ormqr()\ntorch.ormqr()\nTensor.outer\nTensor.outer\nSeetorch.outer().\ntorch.outer()\nTensor.permute\nTensor.permute\nSeetorch.permute()\ntorch.permute()\nTensor.pin_memory\nTensor.pin_memory\nCopies the tensor to pinned memory, if it's not already pinned.\nTensor.pinverse\nTensor.pinverse\nSeetorch.pinverse()\ntorch.pinverse()\nTensor.polygamma\nTensor.polygamma\nSeetorch.polygamma()\ntorch.polygamma()\nTensor.polygamma_\nTensor.polygamma_\nIn-place version ofpolygamma()\npolygamma()\nTensor.positive\nTensor.positive\nSeetorch.positive()\ntorch.positive()\nTensor.pow\nTensor.pow\nSeetorch.pow()\ntorch.pow()\nTensor.pow_\nTensor.pow_\nIn-place version ofpow()\npow()\nTensor.prod\nTensor.prod\nSeetorch.prod()\ntorch.prod()\nTensor.put_\nTensor.put_\nCopies the elements fromsourceinto the positions specified byindex.\nsource\nindex\nTensor.qr\nTensor.qr\nSeetorch.qr()\ntorch.qr()\nTensor.qscheme\nTensor.qscheme\nReturns the quantization scheme of a given QTensor.\nTensor.quantile\nTensor.quantile\nSeetorch.quantile()\ntorch.quantile()\nTensor.nanquantile\nTensor.nanquantile\nSeetorch.nanquantile()\ntorch.nanquantile()\nTensor.q_scale\nTensor.q_scale\nGiven a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().\nTensor.q_zero_point\nTensor.q_zero_point\nGiven a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().\nTensor.q_per_channel_scales\nTensor.q_per_channel_scales\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.\nTensor.q_per_channel_zero_points\nTensor.q_per_channel_zero_points\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.\nTensor.q_per_channel_axis\nTensor.q_per_channel_axis\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.\nTensor.rad2deg\nTensor.rad2deg\nSeetorch.rad2deg()\ntorch.rad2deg()\nTensor.random_\nTensor.random_\nFillsselftensor with numbers sampled from the discrete uniform distribution over[from,to-1].\nself\n[from,to-1]\nTensor.ravel\nTensor.ravel\nseetorch.ravel()\ntorch.ravel()\nTensor.reciprocal\nTensor.reciprocal\nSeetorch.reciprocal()\ntorch.reciprocal()\nTensor.reciprocal_\nTensor.reciprocal_\nIn-place version ofreciprocal()\nreciprocal()\nTensor.record_stream\nTensor.record_stream\nMarks the tensor as having been used by this stream.\nTensor.register_hook\nTensor.register_hook\nRegisters a backward hook.\nTensor.register_post_accumulate_grad_hook\nTensor.register_post_accumulate_grad_hook\nRegisters a backward hook that runs after grad accumulation.\nTensor.remainder\nTensor.remainder\nSeetorch.remainder()\ntorch.remainder()\nTensor.remainder_\nTensor.remainder_\nIn-place version ofremainder()\nremainder()\nTensor.renorm\nTensor.renorm\nSeetorch.renorm()\ntorch.renorm()\nTensor.renorm_\nTensor.renorm_\nIn-place version ofrenorm()\nrenorm()\nTensor.repeat\nTensor.repeat\nRepeats this tensor along the specified dimensions.\nTensor.repeat_interleave\nTensor.repeat_interleave\nSeetorch.repeat_interleave().\ntorch.repeat_interleave()\nTensor.requires_grad\nTensor.requires_grad\nIsTrueif gradients need to be computed for this Tensor,Falseotherwise.\nTrue\nFalse\nTensor.requires_grad_\nTensor.requires_grad_\nChange if autograd should record operations on this tensor: sets this tensor'srequires_gradattribute in-place.\nrequires_grad\nTensor.reshape\nTensor.reshape\nReturns a tensor with the same data and number of elements asselfbut with the specified shape.\nself\nTensor.reshape_as\nTensor.reshape_as\nReturns this tensor as the same shape asother.\nother\nTensor.resize_\nTensor.resize_\nResizesselftensor to the specified size.\nself\nTensor.resize_as_\nTensor.resize_as_\nResizes theselftensor to be the same size as the specifiedtensor.\nself\ntensor\nTensor.retain_grad\nTensor.retain_grad\nEnables this Tensor to have theirgradpopulated duringbackward().\ngrad\nbackward()\nTensor.retains_grad\nTensor.retains_grad\nIsTrueif this Tensor is non-leaf and itsgradis enabled to be populated duringbackward(),Falseotherwise.\nTrue\ngrad\nbackward()\nFalse\nTensor.roll\nTensor.roll\nSeetorch.roll()\ntorch.roll()\nTensor.rot90\nTensor.rot90\nSeetorch.rot90()\ntorch.rot90()\nTensor.round\nTensor.round\nSeetorch.round()\ntorch.round()\nTensor.round_\nTensor.round_\nIn-place version ofround()\nround()\nTensor.rsqrt\nTensor.rsqrt\nSeetorch.rsqrt()\ntorch.rsqrt()\nTensor.rsqrt_\nTensor.rsqrt_\nIn-place version ofrsqrt()\nrsqrt()\nTensor.scatter\nTensor.scatter\nOut-of-place version oftorch.Tensor.scatter_()\ntorch.Tensor.scatter_()\nTensor.scatter_\nTensor.scatter_\nWrites all values from the tensorsrcintoselfat the indices specified in theindextensor.\nsrc\nself\nindex\nTensor.scatter_add_\nTensor.scatter_add_\nAdds all values from the tensorsrcintoselfat the indices specified in theindextensor in a similar fashion asscatter_().\nsrc\nself\nindex\nscatter_()\nTensor.scatter_add\nTensor.scatter_add\nOut-of-place version oftorch.Tensor.scatter_add_()\ntorch.Tensor.scatter_add_()\nTensor.scatter_reduce_\nTensor.scatter_reduce_\nReduces all values from thesrctensor to the indices specified in theindextensor in theselftensor using the applied reduction defined via thereduceargument (\"sum\",\"prod\",\"mean\",\"amax\",\"amin\").\nsrc\nindex\nself\nreduce\n\"sum\"\n\"prod\"\n\"mean\"\n\"amax\"\n\"amin\"\nTensor.scatter_reduce\nTensor.scatter_reduce\nOut-of-place version oftorch.Tensor.scatter_reduce_()\ntorch.Tensor.scatter_reduce_()\nTensor.select\nTensor.select\nSeetorch.select()\ntorch.select()\nTensor.select_scatter\nTensor.select_scatter\nSeetorch.select_scatter()\ntorch.select_scatter()\nTensor.set_\nTensor.set_\nSets the underlying storage, size, and strides.\nTensor.share_memory_\nTensor.share_memory_\nMoves the underlying storage to shared memory.\nTensor.short\nTensor.short\nself.short()is equivalent toself.to(torch.int16).\nself.short()\nself.to(torch.int16)\nTensor.sigmoid\nTensor.sigmoid\nSeetorch.sigmoid()\ntorch.sigmoid()\nTensor.sigmoid_\nTensor.sigmoid_\nIn-place version ofsigmoid()\nsigmoid()\nTensor.sign\nTensor.sign\nSeetorch.sign()\ntorch.sign()\nTensor.sign_\nTensor.sign_\nIn-place version ofsign()\nsign()\nTensor.signbit\nTensor.signbit\nSeetorch.signbit()\ntorch.signbit()\nTensor.sgn\nTensor.sgn\nSeetorch.sgn()\ntorch.sgn()\nTensor.sgn_\nTensor.sgn_\nIn-place version ofsgn()\nsgn()\nTensor.sin\nTensor.sin\nSeetorch.sin()\ntorch.sin()\nTensor.sin_\nTensor.sin_\nIn-place version ofsin()\nsin()\nTensor.sinc\nTensor.sinc\nSeetorch.sinc()\ntorch.sinc()\nTensor.sinc_\nTensor.sinc_\nIn-place version ofsinc()\nsinc()\nTensor.sinh\nTensor.sinh\nSeetorch.sinh()\ntorch.sinh()\nTensor.sinh_\nTensor.sinh_\nIn-place version ofsinh()\nsinh()\nTensor.asinh\nTensor.asinh\nSeetorch.asinh()\ntorch.asinh()\nTensor.asinh_\nTensor.asinh_\nIn-place version ofasinh()\nasinh()\nTensor.arcsinh\nTensor.arcsinh\nSeetorch.arcsinh()\ntorch.arcsinh()\nTensor.arcsinh_\nTensor.arcsinh_\nIn-place version ofarcsinh()\narcsinh()\nTensor.shape\nTensor.shape\nReturns the size of theselftensor.\nself\nTensor.size\nTensor.size\nReturns the size of theselftensor.\nself\nTensor.slogdet\nTensor.slogdet\nSeetorch.slogdet()\ntorch.slogdet()\nTensor.slice_scatter\nTensor.slice_scatter\nSeetorch.slice_scatter()\ntorch.slice_scatter()\nTensor.softmax\nTensor.softmax\nAlias fortorch.nn.functional.softmax().\ntorch.nn.functional.softmax()\nTensor.sort\nTensor.sort\nSeetorch.sort()\ntorch.sort()\nTensor.split\nTensor.split\nSeetorch.split()\ntorch.split()\nTensor.sparse_mask\nTensor.sparse_mask\nReturns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask.\nself\nmask\nTensor.sparse_dim\nTensor.sparse_dim\nReturn the number of sparse dimensions in asparse tensorself.\nself\nTensor.sqrt\nTensor.sqrt\nSeetorch.sqrt()\ntorch.sqrt()\nTensor.sqrt_\nTensor.sqrt_\nIn-place version ofsqrt()\nsqrt()\nTensor.square\nTensor.square\nSeetorch.square()\ntorch.square()\nTensor.square_\nTensor.square_\nIn-place version ofsquare()\nsquare()\nTensor.squeeze\nTensor.squeeze\nSeetorch.squeeze()\ntorch.squeeze()\nTensor.squeeze_\nTensor.squeeze_\nIn-place version ofsqueeze()\nsqueeze()\nTensor.std\nTensor.std\nSeetorch.std()\ntorch.std()\nTensor.stft\nTensor.stft\nSeetorch.stft()\ntorch.stft()\nTensor.storage\nTensor.storage\nReturns the underlyingTypedStorage.\nTypedStorage\nTensor.untyped_storage\nTensor.untyped_storage\nReturns the underlyingUntypedStorage.\nUntypedStorage\nTensor.storage_offset\nTensor.storage_offset\nReturnsselftensor's offset in the underlying storage in terms of number of storage elements (not bytes).\nself\nTensor.storage_type\nTensor.storage_type\nReturns the type of the underlying storage.\nTensor.stride\nTensor.stride\nReturns the stride ofselftensor.\nself\nTensor.sub\nTensor.sub\nSeetorch.sub().\ntorch.sub()\nTensor.sub_\nTensor.sub_\nIn-place version ofsub()\nsub()\nTensor.subtract\nTensor.subtract\nSeetorch.subtract().\ntorch.subtract()\nTensor.subtract_\nTensor.subtract_\nIn-place version ofsubtract().\nsubtract()\nTensor.sum\nTensor.sum\nSeetorch.sum()\ntorch.sum()\nTensor.sum_to_size\nTensor.sum_to_size\nSumthistensor tosize.\nthis\nsize\nTensor.svd\nTensor.svd\nSeetorch.svd()\ntorch.svd()\nTensor.swapaxes\nTensor.swapaxes\nSeetorch.swapaxes()\ntorch.swapaxes()\nTensor.swapdims\nTensor.swapdims\nSeetorch.swapdims()\ntorch.swapdims()\nTensor.t\nTensor.t\nSeetorch.t()\ntorch.t()\nTensor.t_\nTensor.t_\nIn-place version oft()\nt()\nTensor.tensor_split\nTensor.tensor_split\nSeetorch.tensor_split()\ntorch.tensor_split()\nTensor.tile\nTensor.tile\nSeetorch.tile()\ntorch.tile()\nTensor.to\nTensor.to\nPerforms Tensor dtype and/or device conversion.\nTensor.to_mkldnn\nTensor.to_mkldnn\nReturns a copy of the tensor intorch.mkldnnlayout.\ntorch.mkldnn\nTensor.take\nTensor.take\nSeetorch.take()\ntorch.take()\nTensor.take_along_dim\nTensor.take_along_dim\nSeetorch.take_along_dim()\ntorch.take_along_dim()\nTensor.tan\nTensor.tan\nSeetorch.tan()\ntorch.tan()\nTensor.tan_\nTensor.tan_\nIn-place version oftan()\ntan()\nTensor.tanh\nTensor.tanh\nSeetorch.tanh()\ntorch.tanh()\nTensor.tanh_\nTensor.tanh_\nIn-place version oftanh()\ntanh()\nTensor.atanh\nTensor.atanh\nSeetorch.atanh()\ntorch.atanh()\nTensor.atanh_\nTensor.atanh_\nIn-place version ofatanh()\natanh()\nTensor.arctanh\nTensor.arctanh\nSeetorch.arctanh()\ntorch.arctanh()\nTensor.arctanh_\nTensor.arctanh_\nIn-place version ofarctanh()\narctanh()\nTensor.tolist\nTensor.tolist\nReturns the tensor as a (nested) list.\nTensor.topk\nTensor.topk\nSeetorch.topk()\ntorch.topk()\nTensor.to_dense\nTensor.to_dense\nCreates a strided copy ofselfifselfis not a strided tensor, otherwise returnsself.\nself\nself\nself\nTensor.to_sparse\nTensor.to_sparse\nReturns a sparse copy of the tensor.\nTensor.to_sparse_csr\nTensor.to_sparse_csr\nConvert a tensor to compressed row storage format (CSR).\nTensor.to_sparse_csc\nTensor.to_sparse_csc\nConvert a tensor to compressed column storage (CSC) format.\nTensor.to_sparse_bsr\nTensor.to_sparse_bsr\nConvert a tensor to a block sparse row (BSR) storage format of given blocksize.\nTensor.to_sparse_bsc\nTensor.to_sparse_bsc\nConvert a tensor to a block sparse column (BSC) storage format of given blocksize.\nTensor.trace\nTensor.trace\nSeetorch.trace()\ntorch.trace()\nTensor.transpose\nTensor.transpose\nSeetorch.transpose()\ntorch.transpose()\nTensor.transpose_\nTensor.transpose_\nIn-place version oftranspose()\ntranspose()\nTensor.triangular_solve\nTensor.triangular_solve\nSeetorch.triangular_solve()\ntorch.triangular_solve()\nTensor.tril\nTensor.tril\nSeetorch.tril()\ntorch.tril()\nTensor.tril_\nTensor.tril_\nIn-place version oftril()\ntril()\nTensor.triu\nTensor.triu\nSeetorch.triu()\ntorch.triu()\nTensor.triu_\nTensor.triu_\nIn-place version oftriu()\ntriu()\nTensor.true_divide\nTensor.true_divide\nSeetorch.true_divide()\ntorch.true_divide()\nTensor.true_divide_\nTensor.true_divide_\nIn-place version oftrue_divide_()\ntrue_divide_()\nTensor.trunc\nTensor.trunc\nSeetorch.trunc()\ntorch.trunc()\nTensor.trunc_\nTensor.trunc_\nIn-place version oftrunc()\ntrunc()\nTensor.type\nTensor.type\nReturns the type ifdtypeis not provided, else casts this object to the specified type.\nTensor.type_as\nTensor.type_as\nReturns this tensor cast to the type of the given tensor.\nTensor.unbind\nTensor.unbind\nSeetorch.unbind()\ntorch.unbind()\nTensor.unflatten\nTensor.unflatten\nSeetorch.unflatten().\ntorch.unflatten()\nTensor.unfold\nTensor.unfold\nReturns a view of the original tensor which contains all slices of sizesizefromselftensor in the dimensiondimension.\nsize\nself\ndimension\nTensor.uniform_\nTensor.uniform_\nFillsselftensor with numbers sampled from the continuous uniform distribution:\nself\nTensor.unique\nTensor.unique\nReturns the unique elements of the input tensor.\nTensor.unique_consecutive\nTensor.unique_consecutive\nEliminates all but the first element from every consecutive group of equivalent elements.\nTensor.unsqueeze\nTensor.unsqueeze\nSeetorch.unsqueeze()\ntorch.unsqueeze()\nTensor.unsqueeze_\nTensor.unsqueeze_\nIn-place version ofunsqueeze()\nunsqueeze()\nTensor.values\nTensor.values\nReturn the values tensor of asparse COO tensor.\nTensor.var\nTensor.var\nSeetorch.var()\ntorch.var()\nTensor.vdot\nTensor.vdot\nSeetorch.vdot()\ntorch.vdot()\nTensor.view\nTensor.view\nReturns a new tensor with the same data as theselftensor but of a differentshape.\nself\nshape\nTensor.view_as\nTensor.view_as\nView this tensor as the same size asother.\nother\nTensor.vsplit\nTensor.vsplit\nSeetorch.vsplit()\ntorch.vsplit()\nTensor.where\nTensor.where\nself.where(condition,y)is equivalent totorch.where(condition,self,y).\nself.where(condition,y)\ntorch.where(condition,self,y)\nTensor.xlogy\nTensor.xlogy\nSeetorch.xlogy()\ntorch.xlogy()\nTensor.xlogy_\nTensor.xlogy_\nIn-place version ofxlogy()\nxlogy()\nTensor.xpu\nTensor.xpu\nReturns a copy of this object in XPU memory.\nTensor.zero_\nTensor.zero_\nFillsselftensor with zeros.\nself",
  "url": "https://pytorch.org/docs/stable/tensors.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
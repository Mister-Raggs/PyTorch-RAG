{
  "doc_id": "5ef6724221ff7de22e2665cfebab41c7",
  "source": "pytorch_docs",
  "title": "torch.optim \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.optim#\n\nCreated On: Jun 13, 2025 | Last Updated On: Aug 24, 2025\ntorch.optimis a package implementing various optimization algorithms.\ntorch.optim\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can also be easily integrated in the\nfuture.\n\n## How to use an optimizer#\n\nTo usetorch.optimyou have to construct an optimizer object that will hold\nthe current state and will update the parameters based on the computed gradients.\ntorch.optim\n\n## Constructing it#\n\nTo construct anOptimizeryou have to give it an iterable containing the\nparameters (all should beParameters) or named parameters\n(tuples of (str,Parameter)) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc.\nOptimizer\nParameter\nParameter\nExample:\n\n```python\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001)\n\n```\n\nNamed parameters example:\n\n```python\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([('layer0', var1), ('layer1', var2)], lr=0.0001)\n\n```\n\n\n## Per-parameter options#\n\nOptimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group.\nOptimizer\nVariable\ndict\nparams\nFor example, this is very useful when one wants to specify per-layer learning rates:\n\n```python\noptim.SGD([\n    {'params': model.base.parameters(), 'lr': 1e-2},\n    {'params': model.classifier.parameters()}\n], lr=1e-3, momentum=0.9)\n\noptim.SGD([\n    {'params': model.base.named_parameters(), 'lr': 1e-2},\n    {'params': model.classifier.named_parameters()}\n], lr=1e-3, momentum=0.9)\n\n```\n\nThis means thatmodel.base\u2019s parameters will use a learning rate of1e-2, whereasmodel.classifier\u2019s parameters will stick to the default learning rate of1e-3.\nFinally a momentum of0.9will be used for all parameters.\nmodel.base\n1e-2\nmodel.classifier\n1e-3\n0.9\nNote\nYou can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups.\nAlso consider the following example related to the distinct penalization of parameters.\nRemember thatparameters()returns an iterable that\ncontains all learnable parameters, including biases and other\nparameters that may prefer distinct penalization. To address this, one can specify\nindividual penalization weights for each parameter group:\nparameters()\n\n```python\nbias_params = [p for name, p in self.named_parameters() if 'bias' in name]\nothers = [p for name, p in self.named_parameters() if 'bias' not in name]\n\noptim.SGD([\n    {'params': others},\n    {'params': bias_params, 'weight_decay': 0}\n], weight_decay=1e-2, lr=1e-2)\n\n```\n\nIn this manner, bias terms are isolated from non-bias terms, and aweight_decayof0is set specifically for the bias terms, as to avoid any penalization for\nthis group.\nweight_decay\n0\n\n## Taking an optimization step#\n\nAll optimizers implement astep()method, that updates the\nparameters. It can be used in two ways:\nstep()\noptimizer.step()\nThis is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward().\nbackward()\nExample:\n\n```python\nfor input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n\n```\n\noptimizer.step(closure)\nSome optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it.\nExample:\n\n```python\nfor input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n\n```\n\n\n## Base class#\n\nBase class for all optimizers.\nWarning\nParameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.\nparams(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized.\ntorch.Tensor\ndict\ndefaults(dict[str,Any]) \u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them).\nOptimizer.add_param_group\nOptimizer.add_param_group\nAdd a param group to theOptimizersparam_groups.\nOptimizer\nOptimizer.load_state_dict\nOptimizer.load_state_dict\nLoad the optimizer state.\nOptimizer.register_load_state_dict_pre_hook\nOptimizer.register_load_state_dict_pre_hook\nRegister a load_state_dict pre-hook which will be called beforeload_state_dict()is called. It should have the following signature::.\nload_state_dict()\nOptimizer.register_load_state_dict_post_hook\nOptimizer.register_load_state_dict_post_hook\nRegister a load_state_dict post-hook which will be called afterload_state_dict()is called. It should have the following signature::.\nload_state_dict()\nOptimizer.state_dict\nOptimizer.state_dict\nReturn the state of the optimizer as adict.\ndict\nOptimizer.register_state_dict_pre_hook\nOptimizer.register_state_dict_pre_hook\nRegister a state dict pre-hook which will be called beforestate_dict()is called.\nstate_dict()\nOptimizer.register_state_dict_post_hook\nOptimizer.register_state_dict_post_hook\nRegister a state dict post-hook which will be called afterstate_dict()is called.\nstate_dict()\nOptimizer.step\nOptimizer.step\nPerform a single optimization step to update parameter.\nOptimizer.register_step_pre_hook\nOptimizer.register_step_pre_hook\nRegister an optimizer step pre hook which will be called before optimizer step.\nOptimizer.register_step_post_hook\nOptimizer.register_step_post_hook\nRegister an optimizer step post hook which will be called after optimizer step.\nOptimizer.zero_grad\nOptimizer.zero_grad\nReset the gradients of all optimizedtorch.Tensors.\ntorch.Tensor\n\n## Algorithms#\n\nAdadelta\n\nAdadelta\nImplements Adadelta algorithm.\nAdafactor\n\nAdafactor\nImplements Adafactor algorithm.\nAdagrad\n\nAdagrad\nImplements Adagrad algorithm.\nAdam\n\nAdam\nImplements Adam algorithm.\nAdamW\n\nAdamW\nImplements AdamW algorithm, where weight decay does not accumulate in the momentum nor variance.\nSparseAdam\n\nSparseAdam\nSparseAdam implements a masked version of the Adam algorithm suitable for sparse gradients.\nAdamax\n\nAdamax\nImplements Adamax algorithm (a variant of Adam based on infinity norm).\nASGD\n\nASGD\nImplements Averaged Stochastic Gradient Descent.\nLBFGS\n\nLBFGS\nImplements L-BFGS algorithm.\nMuon\n\nMuon\nImplements Muon algorithm.\nNAdam\n\nNAdam\nImplements NAdam algorithm.\nRAdam\n\nRAdam\nImplements RAdam algorithm.\nRMSprop\n\nRMSprop\nImplements RMSprop algorithm.\nRprop\n\nRprop\nImplements the resilient backpropagation algorithm.\nSGD\n\nSGD\nImplements stochastic gradient descent (optionally with momentum).\nMany of our algorithms have various implementations optimized for performance,\nreadability and/or generality, so we attempt to default to the generally fastest\nimplementation for the current device if no particular implementation has been\nspecified by the user.\nWe have 3 major categories of implementations: for-loop, foreach (multi-tensor), and\nfused. The most straightforward implementations are for-loops over the parameters with\nbig chunks of computation. For-looping is usually slower than our foreach\nimplementations, which combine parameters into a multi-tensor and run the big chunks\nof computation all at once, thereby saving many sequential kernel calls. A few of our\noptimizers have even faster fused implementations, which fuse the big chunks of\ncomputation into one kernel. We can think of foreach implementations as fusing\nhorizontally and fused implementations as fusing vertically on top of that.\nIn general, the performance ordering of the 3 implementations is fused > foreach > for-loop.\nSo when applicable, we default to foreach over for-loop. Applicable means the foreach\nimplementation is available, the user has not specified any implementation-specific kwargs\n(e.g., fused, foreach, differentiable), and all tensors are native. Note that while fused\nshould be even faster than foreach, the implementations are newer and we would like to give\nthem more bake-in time before flipping the switch everywhere. We summarize the stability status\nfor each implementation on the second table below, you are welcome to try them out though!\nBelow is a table showing the available and default implementations of each algorithm:\nAlgorithm\nDefault\nHas foreach?\nHas fused?\nAdadelta\n\nAdadelta\nforeach\nyes\nno\nAdafactor\n\nAdafactor\nfor-loop\nno\nno\nAdagrad\n\nAdagrad\nforeach\nyes\nyes (cpu only)\nAdam\n\nAdam\nforeach\nyes\nyes\nAdamW\n\nAdamW\nforeach\nyes\nyes\nSparseAdam\n\nSparseAdam\nfor-loop\nno\nno\nAdamax\n\nAdamax\nforeach\nyes\nno\nASGD\n\nASGD\nforeach\nyes\nno\nLBFGS\n\nLBFGS\nfor-loop\nno\nno\nMuon\n\nMuon\nfor-loop\nno\nno\nNAdam\n\nNAdam\nforeach\nyes\nno\nRAdam\n\nRAdam\nforeach\nyes\nno\nRMSprop\n\nRMSprop\nforeach\nyes\nno\nRprop\n\nRprop\nforeach\nyes\nno\nSGD\n\nSGD\nforeach\nyes\nyes\nBelow table is showing the stability status for fused implementations:\nAlgorithm\nCPU\nCUDA\nMPS\nAdadelta\n\nAdadelta\nunsupported\nunsupported\nunsupported\nAdafactor\n\nAdafactor\nunsupported\nunsupported\nunsupported\nAdagrad\n\nAdagrad\nbeta\nunsupported\nunsupported\nAdam\n\nAdam\nbeta\nstable\nbeta\nAdamW\n\nAdamW\nbeta\nstable\nbeta\nSparseAdam\n\nSparseAdam\nunsupported\nunsupported\nunsupported\nAdamax\n\nAdamax\nunsupported\nunsupported\nunsupported\nASGD\n\nASGD\nunsupported\nunsupported\nunsupported\nLBFGS\n\nLBFGS\nunsupported\nunsupported\nunsupported\nMuon\n\nMuon\nunsupported\nunsupported\nunsupported\nNAdam\n\nNAdam\nunsupported\nunsupported\nunsupported\nRAdam\n\nRAdam\nunsupported\nunsupported\nunsupported\nRMSprop\n\nRMSprop\nunsupported\nunsupported\nunsupported\nRprop\n\nRprop\nunsupported\nunsupported\nunsupported\nSGD\n\nSGD\nbeta\nbeta\nbeta\n\n## How to adjust learning rate#\n\ntorch.optim.lr_scheduler.LRSchedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements.\ntorch.optim.lr_scheduler.LRScheduler\ntorch.optim.lr_scheduler.ReduceLROnPlateau\nLearning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way:\nExample:\n\n```python\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = ExponentialLR(optimizer, gamma=0.9)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n\n```\n\nMost learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it.\nExample:\n\n```python\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler1.step()\n    scheduler2.step()\n\n```\n\nIn many places in the documentation, we will use the following template to refer to schedulers\nalgorithms.\n\n```python\n>>> scheduler = ...\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n\n```\n\nWarning\nPrior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time.\nscheduler.step()\noptimizer.step()\nscheduler.step()\nlr_scheduler.LRScheduler\nlr_scheduler.LRScheduler\nAdjusts the learning rate during optimization.\nlr_scheduler.LambdaLR\nlr_scheduler.LambdaLR\nSets the initial learning rate.\nlr_scheduler.MultiplicativeLR\nlr_scheduler.MultiplicativeLR\nMultiply the learning rate of each parameter group by the factor given in the specified function.\nlr_scheduler.StepLR\nlr_scheduler.StepLR\nDecays the learning rate of each parameter group by gamma every step_size epochs.\nlr_scheduler.MultiStepLR\nlr_scheduler.MultiStepLR\nDecays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.\nlr_scheduler.ConstantLR\nlr_scheduler.ConstantLR\nMultiply the learning rate of each parameter group by a small constant factor.\nlr_scheduler.LinearLR\nlr_scheduler.LinearLR\nDecays the learning rate of each parameter group by linearly changing small multiplicative factor.\nlr_scheduler.ExponentialLR\nlr_scheduler.ExponentialLR\nDecays the learning rate of each parameter group by gamma every epoch.\nlr_scheduler.PolynomialLR\nlr_scheduler.PolynomialLR\nDecays the learning rate of each parameter group using a polynomial function in the given total_iters.\nlr_scheduler.CosineAnnealingLR\nlr_scheduler.CosineAnnealingLR\nSet the learning rate of each parameter group using a cosine annealing schedule.\nlr_scheduler.ChainedScheduler\nlr_scheduler.ChainedScheduler\nChains a list of learning rate schedulers.\nlr_scheduler.SequentialLR\nlr_scheduler.SequentialLR\nContains a list of schedulers expected to be called sequentially during the optimization process.\nlr_scheduler.ReduceLROnPlateau\nlr_scheduler.ReduceLROnPlateau\nReduce learning rate when a metric has stopped improving.\nlr_scheduler.CyclicLR\nlr_scheduler.CyclicLR\nSets the learning rate of each parameter group according to cyclical learning rate policy (CLR).\nlr_scheduler.OneCycleLR\nlr_scheduler.OneCycleLR\nSets the learning rate of each parameter group according to the 1cycle learning rate policy.\nlr_scheduler.CosineAnnealingWarmRestarts\nlr_scheduler.CosineAnnealingWarmRestarts\nSet the learning rate of each parameter group using a cosine annealing schedule.\n\n## How to utilize named parameters to load optimizer state dict#\n\nThe functionload_state_dict()stores the optionalparam_namescontent from the\nloaded state dict if present. However, the process of loading the optimizer state is not affected,\nas the order of the parameters matters to maintain compatibility (in case of different ordering).\nTo utilize the loaded parameters names from the loaded state dict, a customregister_load_state_dict_pre_hookneeds to be implemented according to the desired behavior.\nload_state_dict()\nparam_names\nregister_load_state_dict_pre_hook\nThis can be useful, for instance, when the model architecture changes, but the weights and optimizer states need to\nremain unchanged. The following example demonstrates how to implement this customization.\nExample:\n\n```python\nclass OneLayerModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(3, 4)\n\n    def forward(self, x):\n        return self.fc(x)\n\nmodel = OneLayerModel()\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\n# training..\ntorch.save(optimizer.state_dict(), PATH)\n\n```\n\nLet\u2019s say thatmodelimplements an expert (MoE), and we want to duplicate it and resume training\nfor two experts, both initialized the same way as thefclayer. For the followingmodel2we create two layers identical tofcand resume training by loading the model weights and optimizer states frommodelinto bothfc1andfc2ofmodel2(and adjust them accordingly):\nmodel\nfc\nmodel2\nfc\nmodel\nfc1\nfc2\nmodel2\n\n```python\nclass TwoLayerModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(3, 4)\n        self.fc2 = nn.Linear(3, 4)\n\n    def forward(self, x):\n        return (self.fc1(x) + self.fc2(x)) / 2\n\nmodel2 = TwoLayerModel()\n# adapt and load model weights..\noptimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)\n\n```\n\nTo load the state dict foroptimizer2with the state dict of the previous optimizer such that bothfc1andfc2will be initialized with a copy offcoptimizer states\n(to resume training for each layer fromfc), we can use the following hook:\noptimizer2\nfc1\nfc2\nfc\nfc\n\n```python\ndef adapt_state_dict_ids(optimizer, state_dict):\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n    for k, v in state_dict['param_groups'][0].items():\n        if k not in ['params', 'param_names']:\n            adapted_state_dict['param_groups'][0][k] = v\n\n    lookup_dict = {\n        'fc1.weight': 'fc.weight',\n        'fc1.bias': 'fc.bias',\n        'fc2.weight': 'fc.weight',\n        'fc2.bias': 'fc.bias'\n    }\n    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}\n    for param_id, param_name in zip(\n            optimizer.state_dict()['param_groups'][0]['params'],\n            optimizer.state_dict()['param_groups'][0]['param_names']):\n        name_in_loaded = lookup_dict[param_name]\n        index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)\n        id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]\n        # Copy the state of the corresponding parameter\n        if id_in_loaded in state_dict['state']:\n            adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\noptimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)\noptimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict\n\n```\n\nThis ensures that the adapted state_dict with the correct states for the layers ofmodel2will be used\nduring model loading.\nNote that this code is designed specifically for this example (e.g., assuming a single parameter group),\nand other cases might require different adaptations.\nmodel2\nThe following example shows how to handle missing parameters in a loadedstatedictwhen the model structure changes.\nTheModel_bypassadds a newbypasslayer, which is not present in the originalModel1.\nTo resume training, a customadapt_state_dict_missing_paramhook is used to adapt the optimizer\u2019sstate_dict,\nensuring existing parameters are mapped correctly, while missing ones (like the bypass layer) remain unchanged\n(as initialized in this example).\nThis approach enables smooth loading and resuming of the optimizer state despite model changes.\nThe new bypass layer will be trained from scratch:\nstatedict\nModel_bypass\nbypass\nModel1\nadapt_state_dict_missing_param\nstate_dict\n\n```python\nclass Model1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(5, 5)\n\n    def forward(self, x):\n        return self.fc(x) + x\n\n\nmodel = Model1()\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\n# training..\ntorch.save(optimizer.state_dict(), PATH)\n\nclass Model_bypass(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(5, 5)\n        self.bypass = nn.Linear(5, 5, bias=False)\n        torch.nn.init.eye_(self.bypass.weight)\n\n    def forward(self, x):\n        return self.fc(x) + self.bypass(x)\n\nmodel2 = Model_bypass()\noptimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)\n\ndef adapt_state_dict_missing_param(optimizer, state_dict):\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n    for k, v in state_dict['param_groups'][0].items():\n        if k not in ['params', 'param_names']:\n            adapted_state_dict['param_groups'][0][k] = v\n\n    lookup_dict = {\n        'fc.weight': 'fc.weight',\n        'fc.bias': 'fc.bias',\n        'bypass.weight': None,\n    }\n\n    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}\n    for param_id, param_name in zip(\n            optimizer.state_dict()['param_groups'][0]['params'],\n            optimizer.state_dict()['param_groups'][0]['param_names']):\n        name_in_loaded = lookup_dict[param_name]\n        if name_in_loaded in state_dict['param_groups'][0]['param_names']:\n            index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)\n            id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]\n            # Copy the state of the corresponding parameter\n            if id_in_loaded in state_dict['state']:\n                adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\noptimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)\noptimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict\n\n```\n\nAs a third example, instead of loading a state according to the order of parameters (the default approach),\nthis hook can be used to load according to the parameters\u2019 names:\n\n```python\ndef names_matching(optimizer, state_dict):\n    assert len(state_dict['param_groups']) == len(optimizer.state_dict()['param_groups'])\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    for g_ind in range(len(state_dict['param_groups'])):\n        assert len(state_dict['param_groups'][g_ind]['params']) == len(\n            optimizer.state_dict()['param_groups'][g_ind]['params'])\n\n        for k, v in state_dict['param_groups'][g_ind].items():\n            if k not in ['params', 'param_names']:\n                adapted_state_dict['param_groups'][g_ind][k] = v\n\n        for param_id, param_name in zip(\n                optimizer.state_dict()['param_groups'][g_ind]['params'],\n                optimizer.state_dict()['param_groups'][g_ind]['param_names']):\n            index_in_loaded_list = state_dict['param_groups'][g_ind]['param_names'].index(param_name)\n            id_in_loaded = state_dict['param_groups'][g_ind]['params'][index_in_loaded_list]\n            # Copy the state of the corresponding parameter\n            if id_in_loaded in state_dict['state']:\n                adapted_state_dict['state'][param_id] = deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\n```\n\n\n## Weight Averaging (SWA and EMA)#\n\ntorch.optim.swa_utils.AveragedModelimplements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA),torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA/EMA batch\nnormalization statistics at the end of training.\ntorch.optim.swa_utils.AveragedModel\ntorch.optim.swa_utils.SWALR\ntorch.optim.swa_utils.update_bn()\nSWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization.\nEMA is a widely known technique to reduce the training time by reducing the number of weight updates needed.\nIt is a variation ofPolyak averaging, but using exponential weights instead of equal weights across iterations.\n\n## Constructing averaged models#\n\nTheAveragedModelclass serves to compute the weights of the SWA or EMA model.\nAveragedModel\nYou can create an SWA averaged model by running:\n\n```python\n>>> averaged_model = AveragedModel(model)\n\n```\n\nEMA models are constructed by specifying themulti_avg_fnargument as follows:\nmulti_avg_fn\n\n```python\n>>> decay = 0.999\n>>> averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))\n\n```\n\nDecay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided totorch.optim.swa_utils.get_ema_multi_avg_fn(), the default is 0.999. Decay value should be close to 1.0, as smaller values can cause optimization convergence issues.\ntorch.optim.swa_utils.get_ema_multi_avg_fn()\ntorch.optim.swa_utils.get_ema_multi_avg_fn()returns a function that applies the following EMA equation to the weights:\ntorch.optim.swa_utils.get_ema_multi_avg_fn()\nwhere alpha is the EMA decay.\nHere the modelmodelcan be an arbitrarytorch.nn.Moduleobject.averaged_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you should use theupdate_parameters()function after theoptimizer.step():\nmodel\ntorch.nn.Module\naveraged_model\nmodel\nupdate_parameters()\noptimizer.step()\n\n```python\n>>> averaged_model.update_parameters(model)\n\n```\n\nFor SWA and EMA, this call is usually done right after the optimizerstep(). In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training.\nstep()\n\n## Custom averaging strategies#\n\nBy default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnormulti_avg_fnparameters:\ntorch.optim.swa_utils.AveragedModel\navg_fn\nmulti_avg_fn\navg_fnallows defining a function operating on each parameter tuple (averaged parameter, model parameter) and should return the new averaged parameter.\navg_fn\nmulti_avg_fnallows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using thetorch._foreach*functions. This function must update the averaged parameters in-place.\nmulti_avg_fn\ntorch._foreach*\nIn the following exampleema_modelcomputes an exponential moving average using theavg_fnparameter:\nema_model\navg_fn\n\n```python\n>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.9 * averaged_model_parameter + 0.1 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)\n\n```\n\nIn the following exampleema_modelcomputes an exponential moving average using the more efficientmulti_avg_fnparameter:\nema_model\nmulti_avg_fn\n\n```python\n>>> ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9))\n\n```\n\n\n## SWA learning rate schedules#\n\nTypically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group:\nSWALR\n\n```python\n>>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\n\n```\n\nYou can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\".\nanneal_strategy=\"cos\"\n\n## Taking care of batch normalization#\n\nupdate_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training:\nupdate_bn()\nloader\n\n```python\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n\n```\n\nupdate_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model.\nupdate_bn()\nswa_model\nWarning\nupdate_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.\nupdate_bn()\nloader\nswa_model\nswa_model\nswa_model\n\n## Putting it all together: SWA#\n\nIn the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:\nswa_model\n\n```python\n>>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input)\n\n```\n\n\n## Putting it all together: EMA#\n\nIn the example below,ema_modelis the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999.\nWe train the model for a total of 300 epochs and start to collect EMA averages immediately.\nema_model\n\n```python\n>>> loader, optimizer, model, loss_fn = ...\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, \\\n>>>             multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>           ema_model.update_parameters(model)\n>>>\n>>> # Update bn statistics for the ema_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, ema_model)\n>>> # Use ema_model to make predictions on test data\n>>> preds = ema_model(test_input)\n\n```\n\nswa_utils.AveragedModel\nswa_utils.AveragedModel\nImplements averaged model for Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA).\nswa_utils.SWALR\nswa_utils.SWALR\nAnneals the learning rate in each parameter group to a fixed value.\nGet the function applying exponential moving average (EMA) across multiple params.\nUpdate BatchNorm running_mean, running_var buffers in the model.\nIt performs one pass over data inloaderto estimate the activation\nstatistics for BatchNorm layers in the model.\nloader(torch.utils.data.DataLoader) \u2013 dataset loader to compute the\nactivation statistics on. Each data batch should be either a\ntensor, or a list/tuple whose first element is a tensor\ncontaining data.\nmodel(torch.nn.Module) \u2013 model for which we seek to update BatchNorm\nstatistics.\ndevice(torch.device,optional) \u2013 If set, data will be transferred todevicebefore being passed intomodel.\ndevice\nmodel\nExample\n\n```python\n>>> loader, model = ...\n>>> torch.optim.swa_utils.update_bn(loader, model)\n\n```\n\nNote\nTheupdate_bnutility assumes that each data batch inloaderis either a tensor or a list or tuple of tensors; in the latter case it\nis assumed thatmodel.forward()should be called on the first\nelement of the list or tuple corresponding to the data batch.\nloader\nmodel.forward()",
  "url": "https://pytorch.org/docs/stable/optim.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
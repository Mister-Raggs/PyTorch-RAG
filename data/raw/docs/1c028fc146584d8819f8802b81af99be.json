{
  "doc_id": "1c028fc146584d8819f8802b81af99be",
  "source": "pytorch_docs",
  "title": "Frequently Asked Questions \u2014 PyTorch 2.9 documentation",
  "text": "\n## Frequently Asked Questions#\n\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\nAuthor:Mark Saroufim\n\n## Doestorch.compilesupport training?#\n\ntorch.compile\ntorch.compilesupports training, using AOTAutograd to capture backwards:\ntorch.compile\nThe.forward()graph andoptimizer.step()is captured by\nTorchDynamo\u2019s pythonevalframefrontend.\n.forward()\noptimizer.step()\nevalframe\nFor each segment of.forward()that torchdynamo captures, it uses\nAOTAutograd to generate a backward graph segment.\n.forward()\nEach pair of forward and backward graph are (optionally) min-cut\npartitioned to save the minimal state between forward and backward.\nThe forward and backward pairs are wrapped inautograd.functionmodules.\nautograd.function\nUser code calling.backward()still triggers eager\u2019s autograd engine,\nwhich runs eachcompiled backwardgraph as if it were one op, also running\nany non-compiled eager ops\u2019.backward()functions.\n.backward()\n.backward()\n\n## Do you support Distributed code?#\n\ntorch.compilesupportsDistributedDataParallel(DDP).\nSupport for other distributed training libraries is being considered.\ntorch.compile\nDistributedDataParallel\nThe main reason why Distributed code is challenging with dynamo is\nbecause AOTAutograd unrolls both the forward and backward pass and\nprovides 2 graphs for backends to optimize. This is a problem for\ndistributed code because we\u2019d like to ideally overlap communication\noperations with computations. Eager pytorch accomplishes this in\ndifferent ways for DDP/FSDP- using autograd hooks, module hooks, and\nmodifications/mutations of module states. In a naive application of\ndynamo, hooks that should run directly after an operation during\nbackwards may be delayed until after the entire compiled region of\nbackwards ops, due to how AOTAutograd compiled functions interact with\ndispatcher hooks.\nThe basic strategy for optimizing DDP with Dynamo is outlined indistributed.pywhere the main idea will be to graph break onDDP bucket\nboundaries.\nWhen each node in DDP needs to synchronize its weights with the other\nnodes it organizes its gradients and parameters into buckets which\nreduces communication times and allows a node to broadcast a fraction of\nits gradients to other waiting nodes.\nGraph breaks in distributed code mean you can expect dynamo and its\nbackends to optimize the compute overhead of a distributed program but\nnot its communication overhead. Graph-breaks may interfere with\ncompilation speedups, if the reduced graph-size robs the compiler of\nfusion opportunities. However, there are diminishing returns with\nincreasing graph size since most of the current compute optimizations\nare local fusions. So in practice this approach may be sufficient.\n\n## Do I still need to export whole graphs?#\n\nFor the vast majority of models you probably don\u2019t and you can usetorch.compile()as is but there are a few situations where\nfull graphs are necessary and you can can ensure a full graph by simply\nrunningtorch.compile(...,fullgraph=True). These situations include:\ntorch.compile()\ntorch.compile(...,fullgraph=True)\nLarge scale training runs, such as $250K+ that require pipeline parallelism\nand other advanced sharding strategies.\nInference optimizers likeTensorRTorAITemplatethat\nrely on fusing much more aggressively than training optimizers.\nMobile training or inference.\nFuture work will include tracing communication operations into graphs,\ncoordinating these operations with compute optimizations, and optimizing\nthe communication operations.\n\n## Why is my code crashing?#\n\nIf your code ran just fine withouttorch.compileand started to\ncrash with it is enabled, then the most important first step is figuring\nout which part of the stack your failure occurred. To troubleshoot that,\nfollow the steps below and only try the next step if the previous one\nsucceeded.\ntorch.compile\ntorch.compile(...,backend=\"eager\")which only runs TorchDynamo\nforward graph capture and then runs the captured graph with PyTorch.\nIf this fails then there\u2019s an issue with TorchDynamo.\ntorch.compile(...,backend=\"eager\")\ntorch.compile(...,backend=\"aot_eager\")which runs TorchDynamo to capture a forward graph, and then AOTAutograd\nto trace the backward graph without any additional backend compiler\nsteps. PyTorch eager will then be used to run the forward and backward\ngraphs. If this fails then there\u2019s an issue with AOTAutograd.\ntorch.compile(...,backend=\"aot_eager\")\ntorch.compile(...,backend=\"inductor\")which runs TorchDynamo to capture a\nforward graph, and then AOTAutograd to trace the backward graph with the\nTorchInductor compiler. If this fails then there\u2019s an issue with TorchInductor\ntorch.compile(...,backend=\"inductor\")\n\n## Why is compilation slow?#\n\nDynamo Compilation\u2013 TorchDynamo has a builtin stats function for\ncollecting and displaying the time spent in each compilation phase.\nThese stats can be accessed by callingtorch._dynamo.utils.compile_times()after executingtorch._dynamo. By default, this returns a string\nrepresentation of the compile times spent in each TorchDynamo function by name.\ntorch._dynamo.utils.compile_times()\ntorch._dynamo\nInductor Compilation\u2013 TorchInductor has a builtin stats and trace function\nfor displaying time spent in each compilation phase, output code, output\ngraph visualization and IR dump.envTORCH_COMPILE_DEBUG=1pythonrepro.py.\nThis is a debugging tool designed to make it easier to debug/understand the\ninternals of TorchInductor with an output that will look something likethisEach file in that debug trace can be enabled/disabled viatorch._inductor.config.trace.*. The profile and the diagram are both\ndisabled by default since they are expensive to generate. See theexample debug directory\noutputfor more examples.\nenvTORCH_COMPILE_DEBUG=1pythonrepro.py\ntorch._inductor.config.trace.*\nExcessive RecompilationWhen TorchDynamo compiles a function (or part of one), it makes certain\nassumptions about locals and globals in order to allow compiler\noptimizations, and expresses these assumptions as guards that check\nparticular values at runtime. If any of these guards fail, Dynamo will\nrecompile that function (or part) up totorch._dynamo.config.recompile_limittimes. If your program is\nhitting the cache limit, you will first need to determine which guard is\nfailing and what part of your program is triggering it. The\nUseTORCH_TRACE/tlparseorTORCH_LOGS=recompilesto trace the root of the issue, checktorch.compile Troubleshootingfor more details.\ntorch._dynamo.config.recompile_limit\nTORCH_TRACE/tlparse\nTORCH_LOGS=recompiles\n\n## Why are you recompiling in production?#\n\nIn some cases, you may not want unexpected compiles after a program has\nwarmed up. For example, if you are serving production traffic in a\nlatency critical application. For this, TorchDynamo provides an\nalternate mode where prior compiled graphs are used, but no new ones are\ngenerated:\n\n```python\nfrozen_toy_example = dynamo.run(toy_example)\nfrozen_toy_example(torch.randn(10), torch.randn(10))\n\n```\n\n\n## How are you speeding up my code?#\n\nThere are 3 major ways to accelerate PyTorch code:\nKernel fusion via vertical fusions which fuse sequential operations to avoid\nexcessive read/writes. For example, fuse 2 subsequent cosines means you\ncan can do 1 read 1 write instead 2 reads 2 writes 2. Horizontal fusion:\nthe simplest example being batching where a single matrix is multiplied\nwith a batch of examples but the more general scenario is a grouped GEMM\nwhere a group of matrix multiplications are scheduled together\nOut of order execution: A general optimization for compilers, by looking ahead\nat the exact data dependencies within a graph we can decide on the most\nopportune time to execute a node and which buffers can be reused\nAutomatic work placement: Similar of the out of order execution point,\nbut by matching nodes of a graph to resources like physical hardware or\nmemory we can design an appropriate schedule\nThe above are general principles for accelerating PyTorch code but\ndifferent backends will each make different tradeoffs on what to\noptimize. For example Inductor first takes care of fusing whatever it\ncan and only then generatesTritonkernels.\nTriton in addition offers speedups because of automatic memory\ncoalescing, memory management and scheduling within each Streaming\nMultiprocessor and has been designed to handle tiled computations.\nHowever, regardless of the backend you use it\u2019s best to use a benchmark\nand see approach so try out the PyTorch profiler, visually inspect the\ngenerated kernels and try to see what\u2019s going on for yourself.\n\n## Why am I not seeing speedups?#\n\n\n## Graph Breaks#\n\nThe main reason you won\u2019t see the speedups you\u2019d like to by using dynamo\nis excessive graph breaks. So what\u2019s a graph break?\nGiven a program like:\n\n```python\ndef some_fun(x):\n    ...\n\ntorch.compile(some_fun)(x)\n...\n\n```\n\nTorchdynamo will attempt to compile all of the torch/tensor operations\nwithinsome_fun()into a single FX graph, but it may fail to capture\neverything into one graph.\nsome_fun()\nSome graph break reasons are insurmountable to TorchDynamo like calling\ninto a C extension other than PyTorch is invisible to TorchDynamo, and\ncould do arbitrary things without TorchDynamo being able to introduce\nnecessary guards to ensure that the compiled program would be safe to reuse.\nTo maximize performance, it\u2019s important to have as few graph breaks\nas possible.\n\n## Identifying the cause of a graph break#\n\nTo identify all graph breaks in a program and the associated reasons for\nthe breaks,torch._dynamo.explaincan be used. This tool runs\nTorchDynamo on the supplied function and aggregates the graph breaks\nthat are encountered. Here is an example usage:\ntorch._dynamo.explain\n\n```python\nimport torch\nimport torch._dynamo as dynamo\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    print(\"woo\")\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nexplanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))\nprint(explanation)\n\"\"\"\nGraph Count: 3\nGraph Break Count: 2\nOp Count: 5\nBreak Reasons:\n  Break Reason 1:\n    Reason: builtin: print [<class 'torch._dynamo.variables.constant.ConstantVariable'>] False\n    User Stack:\n      <FrameSummary file foo.py, line 5 in toy_example>\n  Break Reason 2:\n    Reason: generic_jump TensorVariable()\n    User Stack:\n      <FrameSummary file foo.py, line 6 in torch_dynamo_resume_in_toy_example_at_5>\nOps per Graph:\n  ...\nOut Guards:\n  ...\n\"\"\"\n\n```\n\nTo throw an error on the first graph break encountered you can\ndisable python fallbacks by usingfullgraph=True, this should be\nfamiliar if you\u2019ve worked with export based compilers.\nfullgraph=True\n\n```python\ndef toy_example(a, b):\n   ...\n\ntorch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)\n\n```\n\n\n## Why didn\u2019t my code recompile when I changed it?#\n\nIf you enabled dynamic shapes by settingenvTORCHDYNAMO_DYNAMIC_SHAPES=1pythonmodel.pythen your code\nwon\u2019t recompile on shape changes. We\u2019ve added support for dynamic shapes\nwhich avoids recompilations in the case when shapes vary by less than a\nfactor of 2. This is especially useful in scenarios like varying image\nsizes in CV or variable sequence length in NLP. In inference scenarios\nit\u2019s often not possible to know what a batch size will be beforehand\nbecause you take what you can get from different client apps.\nenvTORCHDYNAMO_DYNAMIC_SHAPES=1pythonmodel.py\nIn general, TorchDynamo tries very hard not to recompile things\nunnecessarily so if for example TorchDynamo finds 3 graphs and your\nchange only modified one graph then only that graph will recompile. So\nanother tip to avoid potentially slow compilation times is to warmup a\nmodel by compiling it once after which subsequent compilations will be\nmuch faster. Cold start compile times is still a metric we track\nvisibly.\n\n## Why am I getting incorrect results?#\n\nAccuracy issues can also be minified if you set the environment variableTORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect\nmodel and a full repro might be something likeTORCHDYNAMO_REPRO_AFTER=\"aot\"TORCHDYNAMO_REPRO_LEVEL=4the reason\nwe need this is downstream compilers will codegen code whether it\u2019s\nTriton code or the C++ backend, the numerics from those downstream\ncompilers can be different in subtle ways yet have dramatic impact on\nyour training stability. So the accuracy debugger is very useful for us\nto detect bugs in our codegen or with a backend compiler.\nTORCHDYNAMO_REPRO_LEVEL=4\nTORCHDYNAMO_REPRO_AFTER=\"aot\"TORCHDYNAMO_REPRO_LEVEL=4\nIf you\u2019d like to ensure that random number generation is the same across both torch\nand triton then you can enabletorch._inductor.config.fallback_random=True\ntorch._inductor.config.fallback_random=True\n\n## Why am I getting OOMs?#\n\nDynamo is still an alpha product so there\u2019s a few sources of OOMs and if\nyou\u2019re seeing an OOM try disabling the following configurations in this\norder and then open an issue on GitHub so we can solve the root problem\n1. If you\u2019re using dynamic shapes try disabling them, we\u2019ve disabled\nthem by default:envTORCHDYNAMO_DYNAMIC_SHAPES=0pythonmodel.py2.\nCUDA graphs with Triton are enabled by default in inductor but removing\nthem may alleviate some OOM issues:torch._inductor.config.triton.cudagraphs=False.\nenvTORCHDYNAMO_DYNAMIC_SHAPES=0pythonmodel.py\ntorch._inductor.config.triton.cudagraphs=False\n\n## Doestorch.funcwork withtorch.compile(forgradandvmaptransforms)?#\n\ntorch.func\ntorch.compile\ngrad\nvmap\nApplying atorch.functransform to a function that usestorch.compiledoes work:\ntorch.func\ntorch.compile\n\n```python\nimport torch\n\n@torch.compile\ndef f(x):\n    return torch.sin(x)\n\ndef g(x):\n    return torch.grad(f)(x)\n\nx = torch.randn(2, 3)\ng(x)\n\n```\n\n\n## Callingtorch.functransform inside of a function handled withtorch.compile#\n\ntorch.func\ntorch.compile\n\n## Compilingtorch.func.gradwithtorch.compile#\n\ntorch.func.grad\ntorch.compile\n\n```python\nimport torch\n\ndef wrapper_fn(x):\n    return torch.func.grad(lambda x: x.sin().sum())(x)\n\nx = torch.randn(3, 3, 3)\ngrad_x = torch.compile(wrapper_fn)(x)\n\n```\n\n\n## Compilingtorch.vmapwithtorch.compile#\n\ntorch.vmap\ntorch.compile\n\n```python\nimport torch\n\ndef my_fn(x):\n    return torch.vmap(lambda x: x.sum(1))(x)\n\nx = torch.randn(3, 3, 3)\noutput = torch.compile(my_fn)(x)\n\n```\n\n\n## Compiling functions besides the ones which are supported (escape hatch)#\n\nFor other transforms, as a workaround, usetorch._dynamo.allow_in_graph\ntorch._dynamo.allow_in_graph\nallow_in_graphis an escape hatch. If your code does not work withtorch.compile, which introspects Python bytecode, but you believe it\nwill work via a symbolic tracing approach (likejax.jit), then useallow_in_graph.\nallow_in_graph\ntorch.compile\njax.jit\nallow_in_graph\nBy usingallow_in_graphto annotate a function, you must make sure\nyour code meets the following requirements:\nallow_in_graph\nAll outputs in your function only depend on the inputs and\ndo not depend on any captured Tensors.\nYour function is functional. That is, it does not mutate any state. This may\nbe relaxed; we actually support functions that appear to be functional from\nthe outside: they may have in-place PyTorch operations, but may not mutate\nglobal state or inputs to the function.\nYour function does not raise data-dependent errors.\n\n```python\nimport torch\n\n@torch.compile\ndef f(x):\n    return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x)\n\nx = torch.randn(2, 3)\nf(x)\n\n```\n\nA common pitfall is usingallow_in_graphto annotate a function that\ninvokes annn.Module. This is because the outputs now depend on the\nparameters of thenn.Module. To get this to work, usetorch.func.functional_callto extract the module state.\nallow_in_graph\nnn.Module\nnn.Module\ntorch.func.functional_call\n\n## Does NumPy work withtorch.compile?#\n\ntorch.compile\nStarting in 2.1,torch.compileunderstands native NumPy programs that\nwork on NumPy arrays, and mixed PyTorch-NumPy programs that convert from PyTorch\nto NumPy and back viax.numpy(),torch.from_numpy, and related functions.\ntorch.compile\nx.numpy()\ntorch.from_numpy\n\n## Which NumPy features doestorch.compilesupport?#\n\ntorch.compile\nNumPy withintorch.compilefollows NumPy 2.0 pre-release.\ntorch.compile\nGenerally,torch.compileis able to trace through most NumPy constructions,\nand when it cannot, it falls back to eager and lets NumPy execute that piece of\ncode. Even then, there are a few features wheretorch.compilesemantics\nslightly deviate from those of NumPy:\ntorch.compile\ntorch.compile\nNumPy scalars: We model them as 0-D arrays. That is,np.float32(3)returns\na 0-D array undertorch.compile. To avoid a graph break, it is best to use this 0-D\narray. If this breaks your code, you can workaround this by casting the NumPy scalar\nto the relevant Python scalar typebool/int/float.\nnp.float32(3)\ntorch.compile\nbool/int/float\nNegative strides:np.flipand slicing with a negative step return a copy.\nnp.flip\nType promotion: NumPy\u2019s type promotion will change in NumPy 2.0. The new rules\nare described inNEP 50.torch.compileimplements NEP 50 rather than the current soon-to-be deprecated rules.\ntorch.compile\n{tril,triu}_indices_from/{tril,triu}_indicesreturn arrays rather than a tuple of arrays.\n{tril,triu}_indices_from/{tril,triu}_indices\nThere are other features for which we do not support tracing and we gracefully\nfallback to NumPy for their execution:\nNon-numeric dtypes like datetimes, strings, chars, void, structured dtypes and recarrays.\nLong dtypesnp.float128/np.complex256and some unsigned dtypesnp.uint16/np.uint32/np.uint64.\nnp.float128/np.complex256\nnp.uint16/np.uint32/np.uint64\nndarraysubclasses.\nndarray\nMasked arrays.\nEsoteric ufunc machinery likeaxes=[(n,k),(k,m)->(n,m)]and ufunc methods (e.g.,np.add.reduce).\naxes=[(n,k),(k,m)->(n,m)]\nnp.add.reduce\nSorting / orderingcomplex64/complex128arrays.\ncomplex64/complex128\nNumPynp.poly1dandnp.polynomial.\nnp.poly1d\nnp.polynomial\nPositionalout1,out2args in functions with 2 or more returns (out=tupledoes work).\nout1,out2\nout=tuple\n__array_function__,__array_interface__and__array_wrap__.\n__array_function__\n__array_interface__\n__array_wrap__\nndarray.ctypesattribute.\nndarray.ctypes\n\n## Can I compile NumPy code usingtorch.compile?#\n\ntorch.compile\nOf course you do!torch.compileunderstands NumPy code natively, and treats it\nas if it were PyTorch code. To do so, simply wrap NumPy code with thetorch.compiledecorator.\ntorch.compile\ntorch.compile\n\n```python\nimport torch\nimport numpy as np\n\n@torch.compile\ndef numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = np.random.randn(1024, 64)\nY = np.random.randn(1024, 64)\nZ = numpy_fn(X, Y)\nassert isinstance(Z, np.ndarray)\n\n```\n\nExecuting this example with the environment variableTORCH_LOGS=output_code, we can see\nthattorch.compilewas able to fuse the multiplication and the sum into one C++ kernel.\nIt was also able to execute them in parallel using OpenMP (native NumPy is single-threaded).\nThis can easily make your NumPy codentimes faster, wherenis the number of cores\nin your processor!\nTORCH_LOGS=output_code\ntorch.compile\nn\nn\nTracing NumPy code this way also supports graph breaks within the compiled code.\n\n## Can I execute NumPy code on CUDA and compute gradients viatorch.compile?#\n\ntorch.compile\nYes you can! To do so, you may simply execute your code within atorch.device(\"cuda\")context. Consider the example\ntorch.device(\"cuda\")\n\n```python\nimport torch\nimport numpy as np\n\n@torch.compile\ndef numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = np.random.randn(1024, 64)\nY = np.random.randn(1024, 64)\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\nassert isinstance(Z, np.ndarray)\n\n```\n\nIn this example,numpy_fnwill be executed in CUDA. For this to be\npossible,torch.compileautomatically movesXandYfrom CPU\nto CUDA, and then it moves the resultZfrom CUDA to CPU. If we are\nexecuting this function several times in the same program run, we may want\nto avoid all these rather expensive memory copies. To do so, we just need\nto tweak ournumpy_fnso that it accepts cuda Tensors and returns tensors.\nWe can do so by usingtorch.compiler.wrap_numpy:\nnumpy_fn\ntorch.compile\nX\nY\nZ\nnumpy_fn\ntorch.compiler.wrap_numpy\n\n```python\n@torch.compile(fullgraph=True)\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = torch.randn(1024, 64, device=\"cuda\")\nY = torch.randn(1024, 64, device=\"cuda\")\nZ = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nassert Z.device.type == \"cuda\"\n\n```\n\nHere, we explicitly create the tensors in CUDA memory, and pass them to the\nfunction, which performs all the computations on the CUDA device.wrap_numpyis in charge of marking anytorch.Tensorinput as an input\nwithnp.ndarraysemantics at atorch.compilelevel. Marking tensors\ninside the compiler is a very cheap operation, so no data copy or data movement\nhappens during runtime.\nwrap_numpy\ntorch.Tensor\nnp.ndarray\ntorch.compile\nUsing this decorator, we can also differentiate through NumPy code!\n\n```python\n@torch.compile(fullgraph=True)\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    return np.mean(np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1)))\n\nX = torch.randn(1024, 64, device=\"cuda\", requires_grad=True)\nY = torch.randn(1024, 64, device=\"cuda\")\nZ = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nZ.backward()\n# X.grad now holds the gradient of the computation\nprint(X.grad)\n\n```\n\nWe have been usingfullgraph=Trueas graph break are problematic in this context.\nWhen a graph break occurs, we need to materialize the NumPy arrays. Since NumPy arrays\ndo not have a notion ofdeviceorrequires_grad, this information is lost during\na graph break.\nfullgraph=True\ndevice\nrequires_grad\nWe cannot propagate gradients through a graph break, as the graph break code may execute\narbitrary code that don\u2019t know how to differentiate. On the other hand, in the case of\nthe CUDA execution, we can work around this problem as we did in the first example, by\nusing thetorch.device(\"cuda\")context manager:\ntorch.device(\"cuda\")\n\n```python\n@torch.compile\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    prod = X[:, :, None] * Y[:, None, :]\n    print(\"oops, a graph break!\")\n    return np.sum(prod, axis=(-2, -1))\n\nX = torch.randn(1024, 64, device=\"cuda\")\nY = torch.randn(1024, 64, device=\"cuda\")\n\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nassert Z.device.type == \"cuda\"\n\n```\n\nDuring the graph break, the intermediary tensors still need to be moved to CPU, but when the\ntracing is resumed after the graph break, the rest of the graph is still traced on CUDA.\nGiven this CUDA <> CPU and CPU <> CUDA movement, graph breaks are fairly costly in the NumPy\ncontext and should be avoided, but at least they allow tracing through complex pieces of code.\n\n## How do I debug NumPy code undertorch.compile?#\n\ntorch.compile\nDebugging JIT compiled code is challenging, given the complexity of modern\ncompilers and the daunting errors that they raise.The torch.compile troubleshooting doccontains a few tips and tricks on how to tackle this task.\nIf the above is not enough to pinpoint the origin of the issue, there are still\na few other NumPy-specific tools we can use. We can discern whether the bug\nis entirely in the PyTorch code by disabling tracing through NumPy functions:\n\n```python\nfrom torch._dynamo import config\nconfig.trace_numpy = False\n\n```\n\nIf the bug lies in the traced NumPy code, we can execute the NumPy code eagerly (withouttorch.compile)\nusing PyTorch as a backend by importingimporttorch._numpyasnp.\nThis should just be used fordebugging purposesand is in no way a\nreplacement for the PyTorch API, as it ismuch less performantand, as a\nprivate API,may change without notice. At any rate,torch._numpyis a\nPython implementation of NumPy in terms of PyTorch and it is used internally bytorch.compileto\ntransform NumPy code into Pytorch code. It is rather easy to read and modify,\nso if you find any bug in it feel free to submit a PR fixing it or simply open\nan issue.\ntorch.compile\nimporttorch._numpyasnp\ntorch._numpy\ntorch.compile\nIf the program does work when importingtorch._numpyasnp, chances are\nthat the bug is in TorchDynamo. If this is the case, please feel free to open an issue\nwith aminimal reproducer.\ntorch._numpyasnp\n\n## Itorch.compilesome NumPy code and I did not see any speed-up.#\n\ntorch.compile\nThe best place to start is thetutorial with general advice for how to debug these sort of torch.compile issues.\nSome graph breaks may happen because of the use of unsupported features. SeeWhich NumPy features does torch.compile support?. More generally, it is useful to keep in mind\nthat some widely used NumPy features do not play well with compilers. For\nexample, in-place modifications make reasoning difficult within the compiler and\noften yield worse performance than their out-of-place counterparts.As such, it is best to avoid\nthem. Same goes for the use of theout=parameter. Instead, prefer\nout-of-place ops and lettorch.compileoptimize the memory use. Same goes\nfor data-dependent ops like masked indexing through boolean masks, or\ndata-dependent control flow likeiforwhileconstructions.\nout=\ntorch.compile\nif\nwhile\n\n## Which API to use for fine grain tracing?#\n\nIn some cases, you might need to exclude small parts of your code from the\ntorch.compile compilations. This section provides some of the answers and\nyou can find more information inTorchDynamo APIs for fine-grained tracing.\n\n## How do I graph break on a function?#\n\nGraph break on a function is not enough to sufficiently express what you want\nPyTorch to do. You need to be more specific about your use case. Some of the\nmost common use cases you might want to consider:\nIf you want to disable compilation on this function frame and the recursively\ninvoked frames, usetorch._dynamo.disable.\ntorch._dynamo.disable\nIf you want a particular operator, such asfbgemmto use the eager mode,\nusetorch._dynamo.disallow_in_graph.\nfbgemm\ntorch._dynamo.disallow_in_graph\nSome of the uncommon use cases include:\nIf you want to disable TorchDynamo on the function frame but enable it back\non the recursively invoked frames \u2013 usetorch._dynamo.disable(recursive=False).\ntorch._dynamo.disable(recursive=False)\nIf you want to prevent inlining of a function frame \u2013 usetorch._dynamo.graph_breakat the beginning of the function you want to prevent inlining.\ntorch._dynamo.graph_break\n\n## What\u2019s the difference betweentorch._dynamo.disableandtorch._dynamo.disallow_in_graph#\n\ntorch._dynamo.disable\ntorch._dynamo.disallow_in_graph\nDisallow-in-graph works at the level of operators, or more specifically,\nthe operators that you see in the TorchDynamo extracted graphs.\nDisable works at the function frame level and decides if TorchDynamo\nshould look into the function frame or not.\n\n## What\u2019s the difference betweentorch._dynamo.disableandtorch._dynamo_skip#\n\ntorch._dynamo.disable\ntorch._dynamo_skip\nNote\ntorch._dynamo_skipis deprecated.\ntorch._dynamo_skip\nYou most likely needtorch._dynamo.disable. But in an unlikely scenario, you\nmight need even finer control. Suppose you want to disable the tracing on just\nthea_fnfunction, but want to continue the tracing back inaa_fnandab_fn. The image below demonstrates this use case:\ntorch._dynamo.disable\na_fn\naa_fn\nab_fn\nIn this case, you can usetorch._dynamo.disable(recursive=False).\nIn previous versions, this functionality was provided bytorch._dynamo.skip.\nThis is now supported by therecursiveflag insidetorch._dynamo.disable.\ntorch._dynamo.disable(recursive=False)\ntorch._dynamo.skip\nrecursive\ntorch._dynamo.disable",
  "url": "https://pytorch.org/docs/stable/torch.compiler_faq.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
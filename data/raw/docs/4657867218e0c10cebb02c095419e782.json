{
  "doc_id": "4657867218e0c10cebb02c095419e782",
  "source": "pytorch_docs",
  "title": "TorchInductor and AOTInductor Provenance Tracking \u2014 PyTorch 2.9 documentation",
  "text": "\n## TorchInductor and AOTInductor Provenance Tracking#\n\nCreated On: May 09, 2025 | Last Updated On: May 23, 2025\nWarning\nThis feature is a prototype under active development and there will be\nbreaking change in future releases.\nThe current compatibility of this tool is limited to the latest nightly build of PyTorch.\nThis section describes how to use the provenance tracking feature for TorchInductor and AOTInductor intlparse.\nProvenance tracking helps you visualize the relationships between the input GraphModule to (AOT)Inductor and the optimized code generated. This feature allows you to trace how your original operations are transformed during compilation.\ntlparse\nSome example screenshots of the provenance tracking tool are shown below.\nThe tool visualizes the mapping between nodes in the input graph (panel 1), the post grad graph (panel 2), and the Inductor generated code (panel 3).\nTheboldedlines represent nodes/kernels covered by the current provenance tracing functionality.\nWe currently cover triton kernels, cpp kernels, and combo kernels.\nThe yellow highlighting shows the provenance of the nodes/kernels.\n\n## Using the Provenance Tracking Highlighter#\n\nFollow these steps to enable and use provenance tracking in your PyTorch project:\nInstalltlparsebycargoinstalltlparse. If you don\u2019t havecargo, seeThe Cargo Bookfor instructions to install.\ntlparse\ncargoinstalltlparse\ncargo\nRun your program with required flags:\n\n```python\nTORCH_TRACE=~/my_trace_log_dir TORCH_LOGS=\"+inductor\" TORCH_COMPILE_DEBUG=1 python your_program.py\n\n```\n\nThis will generate a log file in~/my_trace_log_dir. The log file will be used by tlparse to generate the provenance tracking highlighter.\n~/my_trace_log_dir\nRuntlparseon the log with--inductor-provenanceflag. For example:\ntlparse\n--inductor-provenance\n\n```python\ntlparse log_file_name.log --inductor-provenance\n\n```\n\nEven if you don\u2019t add the--inductor-provenanceflag, you should be able to see the mapping in json format in theinductor_provenance_tracking_node_mappings_<number>.jsonfile in theindex.htmltlparse output.\n--inductor-provenance\ninductor_provenance_tracking_node_mappings_<number>.json\nindex.html\nRuntlparedirectly on the log file. It might not work if you run \u201ctlparse parse <folder_name>  \u2013inductor-provenance\u201d.\ntlpare\nThetlparseartifacts used by the provenance tracking highlighter are:\ntlparse\nbefore_pre_grad_graph.txt\nbefore_pre_grad_graph.txt\nafter_post_grad_graph.txt\nafter_post_grad_graph.txt\ninductor_aot_wrapper_code.txt\ninductor_aot_wrapper_code.txt\ninductor_output_code.txt\ninductor_output_code.txt\ninductor_provenance_tracking_node_mappings.json\ninductor_provenance_tracking_node_mappings.json\nAfter runningtlparse<file_name>--inductor-provenance, you should see an additional \u201cProvenance Tracking\u201d section in the tlparse output. Clicking into the link(s) to access the provenance tracking tool.\nFor a demo, see:pytorch/tlparse#93\ntlparse<file_name>--inductor-provenance\n\n## See Also#\n\ntlparseis a tool written in Rust.\ntlparse\nLink to the tlparse GitHub repo:pytorch/tlparse\nLearn more abouttlparseattorch.compile Troubleshooting\ntlparse",
  "url": "https://pytorch.org/docs/stable/torch.compiler_inductor_provenance.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
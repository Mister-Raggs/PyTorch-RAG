{
  "doc_id": "1d9d89ac304e7c878377d88e332931ce",
  "source": "pytorch_docs",
  "title": "Meta device \u2014 PyTorch 2.9 documentation",
  "text": "\n## Meta device#\n\nCreated On: Jun 17, 2025 | Last Updated On: Jun 17, 2025\nThe \u201cmeta\u201d device is an abstract device which denotes a tensor which records\nonly metadata, but no actual data.  Meta tensors have two primary use cases:\nModels can be loaded on the meta device, allowing you to load a\nrepresentation of the model without actually loading the actual parameters\ninto memory.  This can be helpful if you need to make transformations on\nthe model before you load the actual data.\nMost operations can be performed on meta tensors, producing new meta\ntensors that describe what the result would have been if you performed\nthe operation on a real tensor.  You can use this to perform abstract\nanalysis without needing to spend time on compute or space to represent\nthe actual tensors.  Because meta tensors do not have real data, you cannot\nperform data-dependent operations liketorch.nonzero()oritem().  In some cases, not all device types (e.g., CPU\nand CUDA) have exactly the same output metadata for an operation; we\ntypically prefer representing the CUDA behavior faithfully in this\nsituation.\ntorch.nonzero()\nitem()\nWarning\nAlthough in principle meta tensor computation should always be faster than\nan equivalent CPU/CUDA computation, many meta tensor implementations are\nimplemented in Python and have not been ported to C++ for speed, so you\nmay find that you get lower absolute framework latency with small CPU tensors.\n\n## Idioms for working with meta tensors#\n\nAn object can be loaded withtorch.load()onto meta device by specifyingmap_location='meta':\ntorch.load()\nmap_location='meta'\n\n```python\n>>> torch.save(torch.randn(2), 'foo.pt')\n>>> torch.load('foo.pt', map_location='meta')\ntensor(..., device='meta', size=(2,))\n\n```\n\nIf you have some arbitrary code which performs some tensor construction without\nexplicitly specifying a device, you can override it to instead construct on meta device by using\nthetorch.device()context manager:\ntorch.device()\n\n```python\n>>> with torch.device('meta'):\n...     print(torch.randn(30, 30))\n...\ntensor(..., device='meta', size=(30, 30))\n\n```\n\nThis is especially helpful NN module construction, where you often are not\nable to explicitly pass in a device for initialization:\n\n```python\n>>> from torch.nn.modules import Linear\n>>> with torch.device('meta'):\n...     print(Linear(20, 30))\n...\nLinear(in_features=20, out_features=30, bias=True)\n\n```\n\nYou cannot convert a meta tensor directly to a CPU/CUDA tensor, because the\nmeta tensor stores no data and we do not know what the correct data values for\nyour new tensor are:\n\n```python\n>>> torch.ones(5, device='meta').to(\"cpu\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNotImplementedError: Cannot copy out of meta tensor; no data!\n\n```\n\nUse a factory function liketorch.empty_like()to explicitly specify how\nyou would like the missing data to be filled in.\ntorch.empty_like()\nNN modules have a convenience methodtorch.nn.Module.to_empty()that\nallows you to move the module to another device, leaving all parameters\nuninitialized.  You are expected to explicitly reinitialize the parameters\nmanually:\ntorch.nn.Module.to_empty()\n\n```python\n>>> from torch.nn.modules import Linear\n>>> with torch.device('meta'):\n...     m = Linear(20, 30)\n>>> m.to_empty(device=\"cpu\")\nLinear(in_features=20, out_features=30, bias=True)\n\n```\n\ntorch._subclasses.meta_utilscontains undocumented utilities for taking\nan arbitrary Tensor and constructing an equivalent meta Tensor with high\nfidelity.  These APIs are experimental and may be changed in a BC breaking way\nat any time.\ntorch._subclasses.meta_utils",
  "url": "https://pytorch.org/docs/stable/meta.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
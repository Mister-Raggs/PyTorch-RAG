{
  "doc_id": "51cfb37cce931ad002e01e4c60bc9550",
  "source": "pytorch_docs",
  "title": "torch.compiler API reference \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.compiler API reference#\n\nCreated On: Jun 02, 2023 | Last Updated On: Jun 22, 2025\nFor a quick overview oftorch.compiler, seetorch.compiler.\ntorch.compiler\ncompile\n\ncompile\nSeetorch.compile()for details on the arguments for this function.\ntorch.compile()\nreset\n\nreset\nThis function clears all compilation caches and restores the system to its initial state.\nallow_in_graph\n\nallow_in_graph\nTells the compiler frontend (Dynamo) to skip symbolic introspection of the function and instead directly write it to the graph when encountered.\nsubstitute_in_graph\n\nsubstitute_in_graph\nRegister a polyfill handler for a function, usually a C function from the C extension, to be used in place of the original function when inlining the original function in the graph.\nassume_constant_result\n\nassume_constant_result\nThis function is used to mark a functionfnas having a constant result.\nlist_backends\n\nlist_backends\nReturn valid strings that can be passed totorch.compile(..., backend=\"name\").\ndisable\n\ndisable\nThis function provides a decorator to disable compilation on a function.\nset_stance\n\nset_stance\nSet the current stance of the compiler.\nset_enable_guard_collectives\n\nset_enable_guard_collectives\nEnables use of collectivesduringguard evaluation to synchronize behavior across ranks.\ncudagraph_mark_step_begin\n\ncudagraph_mark_step_begin\nIndicates that a new iteration of inference or training is about to begin.\nis_compiling\n\nis_compiling\nIndicates whether a graph is executed/traced as part of torch.compile() or torch.export().\nis_dynamo_compiling\n\nis_dynamo_compiling\nIndicates whether a graph is traced via TorchDynamo.\nis_exporting\n\nis_exporting\nIndicated whether we're under exporting.\nskip_guard_on_inbuilt_nn_modules_unsafe\n\nskip_guard_on_inbuilt_nn_modules_unsafe\nA common function to skip guards on the inbuilt nn modules like torch.nn.Linear.\nskip_guard_on_all_nn_modules_unsafe\n\nskip_guard_on_all_nn_modules_unsafe\nA common function to skip guards on all nn modules, both user defined as well inbuilt nn modules (like torch.nn.Linear).\nkeep_tensor_guards_unsafe\n\nkeep_tensor_guards_unsafe\nA common function to keep tensor guards on all tensors.\nskip_guard_on_globals_unsafe\n\nskip_guard_on_globals_unsafe\nA common function to skip guards on all globals.\nnested_compile_region\n\nnested_compile_region\nTells``torch.compile``that the marked set of operations forms a nested compile region (which is often repeated in the full model) whose code can be compiled once and safely reused.",
  "url": "https://pytorch.org/docs/stable/torch.compiler_api.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
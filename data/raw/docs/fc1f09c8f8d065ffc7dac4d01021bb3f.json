{
  "doc_id": "fc1f09c8f8d065ffc7dac4d01021bb3f",
  "source": "pytorch_docs",
  "title": "LibTorch Stable ABI \u2014 PyTorch 2.9 documentation",
  "text": "\n## LibTorch Stable ABI#\n\nCreated On: Mar 17, 2025 | Last Updated On: Nov 07, 2025\n\n## Overview#\n\nThe LibTorch Stable ABI (Application Binary Interface) provides a limited interface for extending PyTorch functionality without being tightly coupled to specific PyTorch versions. This enables the development of custom operators and extensions that remain compatible across PyTorch releases. This limited set of APIs is not intended to replace existing LibTorch, but rather to provide a stable foundation for a majority of custom extension use cases. If there is any API you would like to see added to the stable ABI, please file a request through anew issue on the PyTorch repo.\nThe limited stable ABI consists of three main components:\nStable C headers- Low-level C API implemented by libtorch (primarilytorch/csrc/inductor/aoti_torch/c/shim.h)\ntorch/csrc/inductor/aoti_torch/c/shim.h\nHeader-only C++ library- Standalone utilities implemented in only headers such that there is no dependence on libtorch (torch/headeronly/*)\ntorch/headeronly/*\nStable C++ wrappers- High-level C++ convenience wrappers (torch/csrc/stable/*)\ntorch/csrc/stable/*\nWe discuss each of these in detail\n\n## torch/headeronly#\n\ntorch/headeronly\nThe inlined C++ headers living intorch/headeronlyare completely decoupled from LibTorch. The headers consist of certain utilities that might be familiar to custom extension writers. For example, thec10::ScalarTypeenum lives here astorch::headeronly::ScalarType, as well as a libtorch-independent version ofTORCH_CHECKthat isSTD_TORCH_CHECK. You can trust all APIs in thetorch::headeronlynamespace to not depend onlibtorch.so. These APIs are also globally listed intorch/header_only_apis.txt.\ntorch/headeronly\nc10::ScalarType\ntorch::headeronly::ScalarType\nTORCH_CHECK\nSTD_TORCH_CHECK\ntorch::headeronly\nlibtorch.so\n\n## torch/csrc/stable#\n\ntorch/csrc/stable\nThis is a set of inlined C++ headers that provide wrappers around the C API that handle the rough edges\ndiscussed below.\nIt consists of\ntorch/csrc/stable/library.h: Provides a stable version of TORCH_LIBRARY and similar macros.\ntorch/csrc/stable/tensor_struct.h: Provides torch::stable::Tensor, a stable version of at::Tensor.\ntorch/csrc/stable/ops.h: Provides a stable interface for calling ATen ops fromnative_functions.yaml.\nnative_functions.yaml\ntorch/csrc/stable/accelerator.h: Provides a stable interface for device-generic objects and APIs\n(e.g.getCurrentStream,DeviceGuard).\ngetCurrentStream\nDeviceGuard\nWe are continuing to improve coverage in ourtorch/csrc/stableAPIs. Please file an issue if you\u2019d like to see support for particular APIs in your custom extension.\ntorch/csrc/stable\n\n## Stable C headers#\n\nThe stable C headers started by AOTInductor form the foundation of the stable ABI. Presently, the available C headers include:\ntorch/csrc/inductor/aoti_torch/c/shim.h: Includes C-style shim APIs for commonly used regarding Tensors, dtypes, CUDA, and the like.\ntorch/csrc/inductor/aoti_torch/generated/c_shim_aten.h: Includes C-style shim APIs for ATen ops fromnative_functions.yaml(e.g.aoti_torch_aten_new_empty).\nnative_functions.yaml\naoti_torch_aten_new_empty\ntorch/csrc/inductor/aoti_torch/generated/c_shim_*.h: Includes C-style shim APIs for specific backend kernels dispatched fromnative_functions.yaml(e.g.aoti_torch_cuda_pad). These APIs should only be used for the specific backend they are named after (e.g.aoti_torch_cuda_padshould only be used within CUDA kernels), as they opt out of the dispatcher.\nnative_functions.yaml\naoti_torch_cuda_pad\naoti_torch_cuda_pad\ntorch/csrc/stable/c/shim.h: We are building out more ABIs to logically live intorch/csrc/stable/cinstead of continuing the AOTI naming that no longer makes sense for our general use case.\ntorch/csrc/stable/c\nThese headers are promised to be ABI stable across releases and adhere to a stronger backwards compatibility policy than LibTorch. Specifically, we promise not to modify them for at least 2 years after they are released. However, this isuse at your own risk. For example, users must handle the memory lifecycle of objects returned by certain APIs. Further, the stack-based APIs discussed below which allow the user to call into the PyTorch dispatcher do not provide strong guarantees on forward and backward compatibility of the underlying op that is called.\nUnless absolutely necessary, we recommend the high-level C++ API intorch/csrc/stablewhich will handle all the rough edges of the C API for the user.\ntorch/csrc/stable\n\n## Migrating your kernel to the LibTorch stable ABI#\n\nIf you\u2019d like your kernel to be ABI stable with LibTorch, meaning you\u2019d the ability to build for one version and run on another, your kernel must only use the limited stable ABI. This following section goes through some steps of migrating an existing kernel and APIs we imagine you would need to swap over.\nFirstly, instead of registering kernels throughTORCH_LIBRARY, LibTorch ABI stable kernels must be registered viaSTABLE_TORCH_LIBRARY. Note that, for the time being, implementations registered viaSTABLE_TORCH_LIBRARYmust be boxed unlikeTORCH_LIBRARY. See the simple example below or our docs onStack-based APIsfor more details. For kernels that are registered viapybind, before using the stable ABI, it would be useful to migrate to register them viaTORCH_LIBRARY.\nTORCH_LIBRARY\nSTABLE_TORCH_LIBRARY\nSTABLE_TORCH_LIBRARY\nTORCH_LIBRARY\npybind\nTORCH_LIBRARY\nWhile previously your kernels might have included APIs from<torch/*.h>(for example,<torch/all.h>), they are now limited to including from the 3 categories of headers mentioned above (torch/csrc/stable/*.h,torch/headeronly/*.hand the stable C headers). This means that your extension should no longer use any utilities from theat::orc10::namespaces but instead use their replacements intorch::stableandtorch::headeronly. To provide a couple examples of the necessary migrations:\n<torch/*.h>\n<torch/all.h>\ntorch/csrc/stable/*.h\ntorch/headeronly/*.h\nat::\nc10::\ntorch::stable\ntorch::headeronly\nall uses ofat::Tensormust be replaced withtorch::stable::Tensor\nat::Tensor\ntorch::stable::Tensor\nall uses ofTORCH_CHECKmust be replaced withSTD_TORCH_CHECK\nTORCH_CHECK\nSTD_TORCH_CHECK\nall uses ofat::kCUDAmust be replaced withtorch::headeronly::kCUDAetc.\nat::kCUDA\ntorch::headeronly::kCUDA\nnative functions such asat::padmust be replaced withtorch::stable::pad\nat::pad\ntorch::stable::pad\nnative functions that are called as Tensor methods (e.g.,Tensor.pad) must be replaced with the ATen variant throughtorch::stable::pad.\nTensor.pad\ntorch::stable::pad\nAs mentioned above, the LibTorch stable ABI is still under development. If there is any API or feature you would like to see added to the stable ABI/torch::headeronly/torch::stable, please file a request through anew issue on the PyTorch repo.\ntorch::headeronly\ntorch::stable\nBelow is a simple example of migrating an existing kernel that usesTORCH_LIBRARYto the stable ABI (TORCH_STABLE_LIBRARY). For a larger end to end example you can take a look at the FA3 repository. Specifically the diff betweenflash_api.cppand the stable variantflash_api_stable.cpp.\nTORCH_LIBRARY\nTORCH_STABLE_LIBRARY\nflash_api.cpp\nflash_api_stable.cpp\n\n## Original Version withTORCH_LIBRARY#\n\nTORCH_LIBRARY\n\n```python\n// original_kernel.cpp - Using TORCH_LIBRARY (not stable ABI)\n#include <torch/torch.h>\n#include <ATen/ATen.h>\n\nnamespace myops {\n\n// Simple kernel that adds a scalar value to each element of a tensor\nat::Tensor add_scalar(const at::Tensor& input, double scalar) {\n  TORCH_CHECK(input.scalar_type() == at::kFloat, \"Input must be float32\");\n\n  return input.add(scalar);\n}\n\n// Register the operator\nTORCH_LIBRARY(myops, m) {\n  m.def(\"add_scalar(Tensor input, float scalar) -> Tensor\", &add_scalar);\n}\n\n// Register the implementation\nTORCH_LIBRARY_IMPL(myops, CompositeExplicitAutograd, m) {\n  m.impl(\"add_scalar\", &add_scalar);\n}\n\n} // namespace myops\n\n```\n\n\n## Migrated Version withSTABLE_TORCH_LIBRARY#\n\nSTABLE_TORCH_LIBRARY\n\n```python\n// stable_kernel.cpp - Using STABLE_TORCH_LIBRARY (stable ABI)\n\n// (1) Don't include <torch/torch.h> <ATen/ATen.h>\n//     only include APIs from torch/csrc/stable, torch/headeronly and C-shims\n#include <torch/csrc/stable/library.h>\n#include <torch/csrc/stable/tensor_struct.h>\n#include <torch/csrc/stable/ops.h>\n#include <torch/csrc/stable/stableivalue_conversions.h>\n#include <torch/headeronly/core/ScalarType.h>\n#include <torch/headeronly/macros/Macros.h>\n\nnamespace myops {\n\n// Simple kernel that adds a scalar value to each element of a tensor\ntorch::stable::Tensor add_scalar(const torch::stable::Tensor& input, double scalar) {\n  // (2) use STD_TORCH_CHECK instead of TORCH_CHECK\n  STD_TORCH_CHECK(\n      // (3) use torch::headeronly::kFloat instead of at:kFloat\n      input.scalar_type() == torch::headeronly::kFloat,\n      \"Input must be float32\");\n\n  // (4) Use stable ops namespace instead of input.add\n  return torch::stable::add(input, scalar);\n}\n\n// (5) Add Boxed wrapper required for STABLE_TORCH_LIBRARY\nvoid boxed_add_scalar(StableIValue* stack, uint64_t num_args, uint64_t num_outputs) {\n  // Extract arguments from stack using `to<T>`\n  auto input = to<torch::stable::Tensor>(stack[0]);\n  auto scalar = to<double>(stack[1]);\n\n  // Call the actual kernel\n  auto result = add_scalar(input, scalar);\n\n  // Put result back on stack using `from()`\n  // Stack slot 0 now holds the return value\n  stack[0] = from(result);\n}\n\n// (6) Register the operator using STABLE_TORCH_LIBRARY\nSTABLE_TORCH_LIBRARY(myops, m) {\n  m.def(\"add_scalar(Tensor input, float scalar) -> Tensor\", &boxed_add_scalar);\n}\n\n// (7) Register the implementation using STABLE_TORCH_LIBRARY_IMPL\nSTABLE_TORCH_LIBRARY_IMPL(myops, CompositeExplicitAutograd, m) {\n  m.impl(\"add_scalar\", &boxed_add_scalar);\n}\n\n} // namespace myops\n\n```\n\n\n## How are objects passed across the ABI boundary when interacting with the dispatcher?#\n\nWhen interacting with the dispatcher via the stable APIs (STABLE_TORCH_LIBRARYetc.) we use a boxed convention. Arguments and returns are represented as a stack ofStableIValuewhich correlates with atorch::jit::stackof IValues. We discuss the following below\nSTABLE_TORCH_LIBRARY\nStableIValue\ntorch::jit::stack\nStableIValue Conversions\nStableIValue stack Conventions\nStable APIs that interact with the dispatcher\n\n## StableIValue Conversions#\n\nWe provide utilities for users to convert objects to and from StableIValues with the synonymoustoandfromAPIs intorch/csrc/stable/stableivalue_conversions.h. We document the stable custom extension representation, libtorch representation and StableIValue\nrepresentations below. Our confidently supported types are the ones in the table that have completed\nrows. You can rely on this subset for proper ABI stability, meaning that you can callto<T_custom_ext>(arg/ret)orfrom(T)on these types.\nto\nfrom\ntorch/csrc/stable/stableivalue_conversions.h\nto<T_custom_ext>(arg/ret)\nfrom(T)\nFor a limited set of use cases, we also implicitly support any literal type that is representable within 64 bits as StableIValues, as the default reinterpret_cast will succeed. (For example: c10::Device.) These types are currently ABI-stable on best effort but might break in the future and thus should be used for short term testing only.\nYou can always work with StableIValue abstractions in your custom kernel for types such as c10::Device even if there is no standard defined representation of device in custom extensions by not introspecting into the StableIValue. For example, a custom operator can take as argument a StableIValue device and directly pass it through to an aten operator withaoti_torch_call_dispatcher.\naoti_torch_call_dispatcher\ntype in custom extension: type used within the end user custom library.\nStableIValue representation: a stable conversion of the type to liaison between the user model vs libtorch.so in an ABI-stable manner.\ntype in libtorch: type used within libtorch.so (or any code binary locked with libtorch).\nSchema Type: type as described by the schema, which we hail as the source of truth for both ATen ops in native_functions.yaml and for user defined custom operators registered to the dispatcher via TORCH_LIBRARY or torch.library.\ntype in custom extension\nStableIValue representation\ntype in libtorch\nSchema Type\nstd::optional<S>\nif there is a value, raw bitwise copy into leading bytes of uint64_t of pointer to a new StableIValue representing S. if there is no value, nullptr.\nstd::optional<T>\nType?\ntorch::stable::Tensor\nraw bitwise copy of underlying AtenTensorHandle into leading bytes of uint64_t\nat::Tensor\nTensor\nRAIIATH (outdated)\nraw bitwise copy of underlying AtenTensorHandle into leading bytes of uint64_t\nat::Tensor\nTensor\ntorch::headeronly::ScalarType\nraw bitwise copy of the translated underlying enum into leading bytes of uint64_t\ntorch::headeronly::ScalarType\nScalarType\nint32_t\nraw bitwise copy into leading bytes of uint64_t\nat::Layout\nLayout\nint32_t\nraw bitwise copy into leading bytes of uint64_t\nat::MemoryFormat\nMemoryFormat\nbool\nraw bitwise copy into leading bytes of uint64_t\nbool\nbool\nint64_t\nraw bitwise copy into leading bytes of uint64_t\nint64_t\nint\ndouble\nraw bitwise copy into leading bytes of uint64_t\ndouble\nfloat\n?\n?\nc10::Device\nDevice\n?\n?\nc10::Stream\nStream\n?\n?\nc10::complex\ncomplex\n?\n?\nat::Scalar\nScalar\n?\n?\nstd::string/const char*/ivalue::ConstantString\nstr\n?\n?\nat::Storage\nStorage\n?\n?\nat::Generator\nGenerator\n?\n?\nc10::List<T>\nType[]\n?\n?\nivalue::Tuple<T>\n(Type, \u2026)\n?\n?\nc10::SymInt\nSymInt\n?\n?\nc10::SymFloat\nSymFloat\n?\n?\nc10::SymBool\nSymBool\n?\n?\nat::QScheme\nQScheme\n\n## Stack Conventions#\n\nThere are two invariants for the stack:\nThe stack is populated left to right.\na. For example, a stack representing argumentsarg0,arg1, andarg2will havearg0at index 0,arg1at index 1, andarg2at index 2.\nb. Returns are also populated left to right, e.g.,ret0will be at index 0 andret1will be at index 1, and so on.\narg0\narg1\narg2\narg0\narg1\narg2\nret0\nret1\nThe stack always has ownership of the objects it holds.\na. When calling a stack-based API, you must give owning references to the calling stack and steal references from the returned stack.\nb. When registering your function to be called with a stack, you must steal references from your argument stack and push onto the stack new references.\n\n## Stack-based APIs#\n\nThe above is relevant in two places:\nSTABLE_TORCH_LIBRARYUnlikeTORCH_LIBRARY, the dispatcher expects kernels registered viaSTABLE_TORCH_LIBRARYto be boxed. This means they must have the signature(StableIValue*stack,uint64_tnum_args,uint64_tnum_outputs)->void.We plan to eventually abstract away the need for manual boxing, but, for the time being, please usefromandto.\nSTABLE_TORCH_LIBRARY\nTORCH_LIBRARY\nSTABLE_TORCH_LIBRARY\n(StableIValue*stack,uint64_tnum_args,uint64_tnum_outputs)->void\nfrom\nto\n\n```python\nTensor my_amax_vec(Tensor t) {\n    std::vector<int64_t> v = {0,1};\n    return amax(t, v, false);\n}\n\nvoid boxed_my_amax_vec(StableIValue* stack, uint64_t num_args, uint64_t num_outputs) {\n    auto res = my_amax_vec(to<Tensor>(stack[0]));\n    stack[0] = from(res);\n}\n\n```\n\naoti_torch_call_dispatcherThis API allows you to call the PyTorch dispatcher from C/C++ code. It has the following signature:\naoti_torch_call_dispatcher\n\n```python\naoti_torch_call_dispatcher(const char* opName, const char* overloadName, StableIValue* stack);\n\n```\n\naoti_torch_call_dispatcherwill call the op overload defined by a givenopName,overloadName, and a stack of\nStableIValues. This call will populate any return values of the op into the stack in their StableIValue form,\nwithret0at index 0,ret1at index 1, and so on.\naoti_torch_call_dispatcher\nopName\noverloadName\nret0\nret1",
  "url": "https://pytorch.org/docs/stable/notes/libtorch_stable_abi.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
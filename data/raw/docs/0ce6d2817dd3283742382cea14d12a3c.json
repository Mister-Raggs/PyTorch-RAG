{
  "doc_id": "0ce6d2817dd3283742382cea14d12a3c",
  "source": "pytorch_docs",
  "title": "Joint with descriptors \u2014 PyTorch 2.9 documentation",
  "text": "\n## Joint with descriptors#\n\nCreated On: Aug 11, 2025 | Last Updated On: Aug 11, 2025\nJoint with descriptors is an experimental API for exporting a traced joint\ngraph that supports all of torch.compile\u2019s features in full generality and,\nafter processing, can be converted back into a differentiable callable that\ncan be executed as normal.  For example, it is used to implement autoparallel,\na system that takes a model and reshards inputs and parameters to make it\na distributed SPMD program.\nThis API captures the joint graph for an nn.Module.  However, unlike\naot_export_joint_simple or aot_export_module(trace_joint=True), the\ncalling convention of the produced joint graph follows no fixed positional\nschema; for example, you cannot rely on the second argument of the traced\njoint graph to correspond to the second argument of the module you traced.\nHowever, the inputs and outputs of the traced graph are schematized\nwithdescriptors, annotated on meta[\u2018desc\u2019] on the placeholder and\nreturn FX nodes, which you can use to determine the meaning of arguments.\nThe major benefit of using this export rather than aot_export_joint_simple\nis that we have feature parity with all situations that torch.compile\nsupports (via aot_module_simplified), including handling for more\ncomplicated cases such as multiple differentiable outputs, input mutations\nthat must be handled outside of the graph, tensor subclasses, etc.\nWhat can you do with one of these joint graphs with descriptors?  The\nmotivating use case (autoparallel) involves taking the joint graph, doing\noptimizations on it, and then turning it back into a callable so it can be\ntorch.compile\u2019d at a later point in time.  This cannot be done as a\ntraditional torch.compile joint graph pass for two reasons:\nThe sharding of parameters must be decided before parameter\ninitialization / checkpoint load, far before torch.compile would\nordinarily run.\nWe need to change the meaning of parameters (e.g., we might replace\na replicated parameter with a sharded version of it, changing its\ninput size).  torch.compile is ordinarily semantics preserving, and\nnot allowed to change the meaning of inputs.\nSome descriptors can be quite exotic, so we recommend thinking carefully\nif there is a safe fallback you can apply to descriptors you don\u2019t understand.\nFor example, you should have some way to handle not finding a particular\ninput exactly as is in the final FX graph inputs.\nNote: When using this API, you must create and enter an ExitStack context\nmanager, which will be passed into this function.  This context manager\nmust remain active if you call the compile function to finish compilation.\n(TODO: We may relax this requirement by having AOTAutograd keep track of\nhow to reconstruct all the context managers at a later point in time.)\nNB: You\u2019re not obligated to do a /full/ compile in stage2; instead you can\nleave the forward/backward compilers unspecified in which case the\npartitioned FX graphs will directly run.  The overall autograd Function\ncan be allowed in graph so you can reprocess it in the context of a\n(potentially larger) compiled region later.\nNB: These APIs do NOT hit cache, as we only ever cache the final compile results,\nnot the intermediate export result.\nNB: If the passed nn.Module has parameters and buffers on it, we will\ngenerate extra implicit parameter/buffer arguments and assign ParamAOTInput\nand BufferAOTInput descriptors to them.  However, if you generate the input\nnn.Module from a mechanism like Dynamo, you will NOT get these descriptors\n(because Dynamo will already have taken care of lifting the parameters/buffers\ninto arguments!)  In that case, it would be necessary to analyze the Sources\nof the inputs to determine if inputs are parameters and their FQNs.\nJointWithDescriptors\nCompanion function for aot_export_joint_with_descriptors which compiles the joint\ngraph into a callable function that follows a standard calling convention.\nparams_flat all are arguments.\nNote: We do NOT instantiate the module; this gives you the flexibility to subclass it and\ncustomize its behavior without having to worry about FQN rebinding.\nTODO: Consider if we should allow_in_graph the result by default.\ncallable\n\n## Descriptors#\n\nDescribes where an input from an AOTAutograd produced FX graph comes from\nTrue if this input is a buffer or derived from a buffer (e.g., subclass attr)\nbool\nTrue if this input is a parameter or derived from a parameter (e.g., subclass attr)\nbool\nTrue if this input is a tangent or derived from a tangent (e.g., subclass attr)\nbool\nDescribes where an output from an AOTAutograd produced FX graph will\neventually be bundled into the final output\nTrue if this output is a grad or derived from a grad (e.g., subclass attr)\nbool\nThe world token which is threaded through side-effectful operations, for backwards\nThe world token output for side-effectful calls, returned so we cannot DCE it, backward only\nThe input is a buffer, whose FQN is target\nIn some circumstances, we want to call into a function that expects AOTInput, but\nwe don\u2019t actually care about that logic (most typically, because some code is being used\nfor both compile-time and run-time; AOTInput processing is not needed in this situation.\nPass a dummy in this situation; but it is better to just have a version of the function\nthat doesn\u2019t have this at all.\nFor cases when you don\u2019t actually care about descriptor propagation, do not use under normal\ncircumstances.\nAn output representing the computed gradient for a differentiable input, in the joint graph\nThe mutated value of an input tensor, returned so we can appropriately propagate autograd.\nAn intermediate base of multiple outputs which alias each other.  We only report ONE of\nthe outputs that contributed to this base\nThe input is a parameter, whose FQN is target\nThe offset for functionalized Philox RNG calls, specifically for backward graph.\nThe seed for functionalized Philox RNG calls, specifically for backward graph.\nThe offset for functionalized Philox RNG calls, specifically for forward graph.\nThe seed for functionalized Philox RNG calls, specifically for forward graph.\nThe final offset from the functionalized RNG calls, backward only\nThe final offset from the functionalized RNG calls, forward only\nThe input is a plain input, corresponding to a particular positional index.\nNote that AOTInput is always relative to a function with aflatcalling convention,\ne.g., as accepted byaot_module_simplified.  There are some AOTAutograd APIs that\nflatten pytrees, and we don\u2019t record PyTree key paths from the flattening (but we\ncould and should!)\nA plain tensor output at position idx of the output tuple\nSubclass inputs get unpacked into their constituent pieces before going into an FX\ngraph.  This tells you which particular attribute of the subclass this particular\ninput corresponds to (of the \u2018base\u2019 originally subclass argument.)\nThis output will be bundled into a subclass at this location\nWhich subclass this particular outer size SymInt input (at dim idx) came from.\nThis output size will be bundled into a subclass at this location\nWhich subclass this particular outer stride SymInt input (at dim idx) came from.\nThis output stride will be bundled into a subclass at this location\nThis is similar to ViewBaseAOTInput, but this happens when none of the views were differentiable, so\nwe weren\u2019t able to get our hands on the true original view and constructed a synthetic one instead\nfor the sake of autograd.\nWhen multiple differentiable inputs are views of the same input, AOTAutograd will replace all of these\nviews with a single input representing the base.  If this is undesirable, you can clone the views\nexample inputs before passing them into AOTAutograd.\nTODO: In principle we could report ALL of the inputs who this is a base of.\n\n## FX utilities#\n\nThis module contains utility functions for working with joint FX graphs with descriptors\nthat are produced by AOTAutograd.  They will NOT work on generic FX graphs.  See alsotorch._functorch.aot_autograd.aot_export_joint_with_descriptors().  We also\nrecommend reading :mod:torch._functorch._aot_autograd.descriptors`.\ntorch._functorch.aot_autograd.aot_export_joint_with_descriptors()\nGiven a joint graph with descriptors (meta[\u2018desc\u2019] on placeholders and\noutput), returns the node for every input and its corresponding grad\noutput node if it exists.  These tuples are in a dict that is indexed by\nthe AOTInput descriptor that describes the input.\nNB:allforward tensor inputs are returned, including non-differentiable\ninputs (which simply have a None grad), so it is safe to use this function\nto perform operations on all inputs.  (Non-tensor inputs like symbolic\nintegers, tokens or RNG state are NOT traversed by this function.)\ng(Graph) \u2013 The FX joint graph with descriptors\nA dictionary mapping each DifferentiableAOTInput descriptor to a tuple\ncontaining:\n- The input node itself\n- The grad (output) node if it exists, None otherwise\nRuntimeError\u2013 If the joint graph has subclass tensor inputs/outputs; this\nis not supported by API as there is not necessarily a 1-1 correspondence\u2013\nbetween inputs and grads when subclasses are involved.\u2013\ndict[torch._functorch._aot_autograd.descriptors.DifferentiableAOTInput,tuple[torch.fx.node.Node,Optional[torch.fx.node.Node]]]\nGet all output nodes and their corresponding tangent nodes from a joint graph.\nSimilar to get_all_input_and_grad_nodes, but returns output nodes paired with\ntheir tangent nodes (if they exist). This function traverses the graph to find\nall differentiable outputs and matches them with their corresponding tangent\ninputs used in forward-mode autodiff.\nNB:allforward tensor output sare turned, including non-differentiable outputs,\nso you can use this function to perform operations on all outputs.\ng(Graph) \u2013 The FX joint graph with descriptors\nA dictionary mapping each DifferentiableAOTOutput descriptor to a tuple\ncontaining:\n- The output node itself\n- The tangent (input) node if it exists, None otherwise\nRuntimeError\u2013 If the joint graph has subclass tensor inputs/outputs; this\nis not supported by API as there is not necessarily a 1-1 correspondence\u2013\nbetween outputs and tangents when subclasses are involved.\u2013\ndict[torch._functorch._aot_autograd.descriptors.DifferentiableAOTOutput,tuple[torch.fx.node.Node,Optional[torch.fx.node.Node]]]\nGet all buffer nodes from a graph as a list.\nYou can rely on this providing the correct order of buffers you need\nto feed into the joint graph (after parameters).\ngraph(Graph) \u2013 The FX joint graph with descriptors\nA list of FX nodes representing all buffers in the graph.\nRuntimeError\u2013 If subclass tensors are encountered (not yet supported), as\nit is not clear if you wanted each individual constituent pieceofthe\u2013\nsubclasses, orhave them grouped up in some way.\u2013\nlist[torch.fx.node.Node]\nGet buffer nodes mapped by their fully qualified names.\nThis function traverses the graph to find all buffer input nodes and\nreturns them in a dictionary where keys are the buffer names (FQNs)\nand values are the corresponding FX nodes.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nA dictionary mapping buffer names (str) to their corresponding FX nodes.\nRuntimeError\u2013 If subclass tensors are encountered (not yet supported), as\nwith subclasses a FQN does not necessarily map to a single plain tensor.\u2013\ndict[str,torch.fx.node.Node]\nGet parameter nodes mapped by their fully qualified names.\nThis function traverses the graph to find all parameter input nodes and\nreturns them in a dictionary where keys are the parameter names (FQNs)\nand values are the corresponding FX nodes.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nA dictionary mapping parameter names (str) to their corresponding FX nodes.\nRuntimeError\u2013 If subclass tensors are encountered (not yet supported), as\nwith subclasses a FQN does not necessarily map to a single plain tensor.\u2013\ndict[str,torch.fx.node.Node]\nGet parameter nodes and their corresponding gradient nodes from a joint graph.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nThe parameter input nodeThe gradient (output) node if it exists, None otherwise\nThe parameter input node\nThe gradient (output) node if it exists, None otherwise\nA dictionary mapping each ParamAOTInput descriptor to a tuple containing\nGet all parameter nodes from a graph as a list.\nYou can rely on this providing the correct order of parameters you need\nto feed into the joint graph (at the very beginning of the argument list,\nbefore buffers).\ngraph(Graph) \u2013 The FX joint graph with descriptors\nA list of FX nodes representing all parameters in the graph.\nRuntimeError\u2013 If subclass tensors are encountered (not yet supported), as\nit is not clear if you wanted each individual constituent pieceofthe\u2013\nsubclasses, orhave them grouped up in some way.\u2013\nlist[torch.fx.node.Node]\nGet plain input nodes and their corresponding gradient nodes from a joint graph.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nThe plain input nodeThe gradient (output) node if it exists, None otherwise\nThe plain input node\nThe gradient (output) node if it exists, None otherwise\nA dictionary mapping each PlainAOTInput descriptor to a tuple containing\nGet plain output nodes and their corresponding tangent nodes from a joint graph.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nThe plain output nodeThe tangent (input) node if it exists, None otherwise\nThe plain output node\nThe tangent (input) node if it exists, None otherwise\nA dictionary mapping each PlainAOTOutput descriptor to a tuple containing",
  "url": "https://pytorch.org/docs/stable/export/joint_with_descriptors.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
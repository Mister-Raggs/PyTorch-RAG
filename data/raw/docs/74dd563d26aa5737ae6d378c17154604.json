{
  "doc_id": "74dd563d26aa5737ae6d378c17154604",
  "source": "pytorch_docs",
  "title": "Multiprocessing package - torch.multiprocessing \u2014 PyTorch 2.9 documentation",
  "text": "\n## Multiprocessing package - torch.multiprocessing#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jun 08, 2025\ntorch.multiprocessing is a wrapper around the nativemultiprocessingmodule.\nmultiprocessing\nIt registers custom reducers, that use shared memory to provide shared\nviews on the same data in different processes. Once the tensor/storage is moved\nto shared_memory (seeshare_memory_()), it will be possible\nto send it to other processes without making any copies.\nshare_memory_()\nThe API is 100% compatible with the original module - it\u2019s enough to changeimportmultiprocessingtoimporttorch.multiprocessingto have all the\ntensors sent through the queues or shared via other mechanisms, moved to shared\nmemory.\nimportmultiprocessing\nimporttorch.multiprocessing\nBecause of the similarity of APIs we do not document most of this package\ncontents, and we recommend referring to very good docs of the original module.\nWarning\nIf the main process exits abruptly (e.g. because of an incoming signal),\nPython\u2019smultiprocessingsometimes fails to clean up its children.\nIt\u2019s a known caveat, so if you\u2019re seeing any resource leaks after\ninterrupting the interpreter, it probably means that this has just happened\nto you.\nmultiprocessing\n\n## Strategy management#\n\nReturn a set of sharing strategies supported on a current system.\nReturn the current strategy for sharing CPU tensors.\nSet the strategy for sharing CPU tensors.\nnew_strategy(str) \u2013 Name of the selected strategy. Should be one of\nthe values returned byget_all_sharing_strategies().\nget_all_sharing_strategies()\n\n## Sharing CUDA tensors#\n\nSharing CUDA tensors between processes is supported only in Python 3, using\naspawnorforkserverstart methods.\nspawn\nforkserver\nUnlike CPU tensors, the sending process is required to keep the original tensor\nas long as the receiving process retains a copy of the tensor. The refcounting is\nimplemented under the hood but requires users to follow the next best practices.\nWarning\nIf the consumer process dies abnormally to a fatal signal, the shared tensor\ncould be forever kept in memory as long as the sending process is running.\nRelease memory ASAP in the consumer.\n\n```python\n## Good\nx = queue.get()\n# do somethings with x\ndel x\n\n```\n\n\n```python\n## Bad\nx = queue.get()\n# do somethings with x\n# do everything else (producer have to keep x in memory)\n\n```\n\nKeep producer process running until all consumers exits. This will prevent\nthe situation when the producer process releasing memory which is still in use\nby the consumer.\n\n```python\n## producer\n# send tensors, do something\nevent.wait()\n\n```\n\n\n```python\n## consumer\n# receive tensors and use them\nevent.set()\n\n```\n\nDon\u2019t pass received tensors.\n\n```python\n# not going to work\nx = queue.get()\nqueue_2.put(x)\n\n```\n\n\n```python\n# you need to create a process-local copy\nx = queue.get()\nx_clone = x.clone()\nqueue_2.put(x_clone)\n\n```\n\n\n```python\n# putting and getting from the same queue in the same process will likely end up with segfault\nqueue.put(tensor)\nx = queue.get()\n\n```\n\n\n## Sharing strategies#\n\nThis section provides a brief overview into how different sharing strategies\nwork. Note that it applies only to CPU tensor - CUDA tensors will always use\nthe CUDA API, as that\u2019s the only way they can be shared.\n\n## File descriptor -file_descriptor#\n\nfile_descriptor\nNote\nThis is the default strategy (except for macOS and OS X where it\u2019s not\nsupported).\nThis strategy will use file descriptors as shared memory handles. Whenever a\nstorage is moved to shared memory, a file descriptor obtained fromshm_openis cached with the object, and when it\u2019s going to be sent to other processes,\nthe file descriptor will be transferred (e.g. via UNIX sockets) to it. The\nreceiver will also cache the file descriptor andmmapit, to obtain a shared\nview onto the storage data.\nshm_open\nmmap\nNote that if there will be a lot of tensors shared, this strategy will keep a\nlarge number of file descriptors open most of the time. If your system has low\nlimits for the number of open file descriptors, and you can\u2019t raise them, you\nshould use thefile_systemstrategy.\nfile_system\n\n## File system -file_system#\n\nfile_system\nThis strategy will use file names given toshm_opento identify the shared\nmemory regions. This has a benefit of not requiring the implementation to cache\nthe file descriptors obtained from it, but at the same time is prone to shared\nmemory leaks. The file can\u2019t be deleted right after its creation, because other\nprocesses need to access it to open their views. If the processes fatally\ncrash, or are killed, and don\u2019t call the storage destructors, the files will\nremain in the system. This is very serious, because they keep using up the\nmemory until the system is restarted, or they\u2019re freed manually.\nshm_open\nTo counter the problem of shared memory file leaks,torch.multiprocessingwill spawn a daemon namedtorch_shm_managerthat will isolate itself from\nthe current process group, and will keep track of all shared memory allocations.\nOnce all processes connected to it exit, it will wait a moment to ensure there\nwill be no new connections, and will iterate over all shared memory files\nallocated by the group. If it finds that any of them still exist, they will be\ndeallocated. We\u2019ve tested this method and it proved to be robust to various\nfailures. Still, if your system has high enough limits, andfile_descriptoris a supported strategy, we do not recommend switching to this one.\ntorch.multiprocessing\ntorch_shm_manager\nfile_descriptor\n\n## Spawning subprocesses#\n\nNote\nAvailable for Python >= 3.4.\nThis depends on thespawnstart method in Python\u2019smultiprocessingpackage.\nspawn\nmultiprocessing\nSpawning a number of subprocesses to perform some function can be done\nby creatingProcessinstances and callingjointo wait for\ntheir completion. This approach works fine when dealing with a single\nsubprocess but presents potential issues when dealing with multiple\nprocesses.\nProcess\njoin\nNamely, joining processes sequentially implies they will terminate\nsequentially. If they don\u2019t, and the first process does not terminate,\nthe process termination will go unnoticed. Also, there are no native\nfacilities for error propagation.\nThespawnfunction below addresses these concerns and takes care\nof error propagation, out of order termination, and will actively\nterminate processes upon detecting an error in one of them.\nspawn\nSpawnsnprocsprocesses that runfnwithargs.\nnprocs\nfn\nargs\nIf one of the processes exits with a non-zero exit status, the\nremaining processes are killed and an exception is raised with the\ncause of termination. In the case an exception was caught in the\nchild process, it is forwarded and its traceback is included in\nthe exception raised in the parent process.\nfn(function) \u2013Function is called as the entrypoint of the\nspawned process. This function must be defined at the top\nlevel of a module so it can be pickled and spawned. This\nis a requirement imposed by multiprocessing.The function is called asfn(i,*args), whereiis\nthe process index andargsis the passed through tuple\nof arguments.\nFunction is called as the entrypoint of the\nspawned process. This function must be defined at the top\nlevel of a module so it can be pickled and spawned. This\nis a requirement imposed by multiprocessing.\nThe function is called asfn(i,*args), whereiis\nthe process index andargsis the passed through tuple\nof arguments.\nfn(i,*args)\ni\nargs\nargs(tuple) \u2013 Arguments passed tofn.\nfn\nnprocs(int) \u2013 Number of processes to spawn.\njoin(bool) \u2013 Perform a blocking join on all processes.\ndaemon(bool) \u2013 The spawned processes\u2019 daemon flag. If set to True,\ndaemonic processes will be created.\nstart_method(str) \u2013 (deprecated) this method will always usespawnas the start method. To use a different start method\nusestart_processes().\nspawn\nstart_processes()\nNone ifjoinisTrue,ProcessContextifjoinisFalse\njoin\nTrue\nProcessContext\njoin\nFalse\nReturned byspawn()when called withjoin=False.\nspawn()\njoin=False\nJoin one or more processes within spawn context.\nAttempt to join one or more processes in this spawn context.\nIf one of them exited with a non-zero exit status, this function\nkills the remaining processes (optionally with a grace period)\nand raises an exception with the cause of the first process exiting.\nReturnsTrueif all processes have been joined successfully,Falseif there are more processes that need to be joined.\nTrue\nFalse\ntimeout(float) \u2013 Wait this long (in seconds) before giving up on waiting.\ngrace_period(float) \u2013 When any processes fail, wait this long (in seconds)\nfor others to shutdown gracefully before terminating them. If they\nstill don\u2019t exit, wait another grace period before killing them.",
  "url": "https://pytorch.org/docs/stable/multiprocessing.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
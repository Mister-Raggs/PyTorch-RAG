{
  "doc_id": "1d45959feadb705ac1ae978926b4ee73",
  "source": "pytorch_docs",
  "title": "Automatic differentiation package - torch.autograd \u2014 PyTorch 2.9 documentation",
  "text": "\n## Automatic differentiation package - torch.autograd#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jun 12, 2025\ntorch.autogradprovides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\ntorch.autograd\nIt requires minimal changes to the existing code - you only need to declareTensors\nfor which gradients should be computed with therequires_grad=Truekeyword.\nAs of now, we only support autograd for floating pointTensortypes (\nhalf, float, double and bfloat16) and complexTensortypes (cfloat, cdouble).\nTensor\nrequires_grad=True\nTensor\nTensor\nbackward\n\nbackward\nCompute the sum of gradients of given tensors with respect to graph leaves.\ngrad\n\ngrad\nCompute and return the sum of gradients of outputs with respect to the inputs.\n\n## Forward-mode Automatic Differentiation#\n\nWarning\nThis API is in beta. Even though the function signatures are very unlikely to change, improved\noperator coverage is planned before we consider this stable.\nPlease see theforward-mode AD tutorialfor detailed steps on how to use this API.\nforward_ad.dual_level\nforward_ad.dual_level\nContext-manager for forward AD, where all forward AD computation must occur within thedual_levelcontext.\ndual_level\nforward_ad.make_dual\nforward_ad.make_dual\nAssociate a tensor value with its tangent to create a \"dual tensor\" for forward AD gradient computation.\nforward_ad.unpack_dual\nforward_ad.unpack_dual\nUnpack a \"dual tensor\" to get both its Tensor value and its forward AD gradient.\nforward_ad.enter_dual_level\nforward_ad.enter_dual_level\nEnter a new forward grad level.\nforward_ad.exit_dual_level\nforward_ad.exit_dual_level\nExit a forward grad level.\nforward_ad.UnpackedDualTensor\nforward_ad.UnpackedDualTensor\nNamedtuple returned byunpack_dual()containing the primal and tangent components of the dual tensor.\nunpack_dual()\n\n## Functional higher level API#\n\nWarning\nThis API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable.\nThis section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.\nThis API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a functionfthat takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag asf(input,constant,flag=flag)you can use it asfunctional.jacobian(lambdax:f(x,constant,flag=flag),input).\nf\nf(input,constant,flag=flag)\nfunctional.jacobian(lambdax:f(x,constant,flag=flag),input)\nfunctional.jacobian\nfunctional.jacobian\nCompute the Jacobian of a given function.\nfunctional.hessian\nfunctional.hessian\nCompute the Hessian of a given scalar function.\nfunctional.vjp\nfunctional.vjp\nCompute the dot product between a vectorvand the Jacobian of the given function at the point given by the inputs.\nv\nfunctional.jvp\nfunctional.jvp\nCompute the dot product between the Jacobian of the given function at the point given by the inputs and a vectorv.\nv\nfunctional.vhp\nfunctional.vhp\nCompute the dot product between vectorvand Hessian of a  given scalar function at a specified point.\nv\nfunctional.hvp\nfunctional.hvp\nCompute the dot product between the scalar function's Hessian and a vectorvat a specified point.\nv\n\n## Locally disabling gradient computation#\n\nSeeLocally disabling gradient computationfor more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two. Also seeLocally disabling gradient computationfor a list of functions that can be used to locally disable gradients.\n\n## Default gradient layouts#\n\nWhen a non-sparseparamreceives a non-sparse gradient duringtorch.autograd.backward()ortorch.Tensor.backward()param.gradis accumulated as follows.\nparam\ntorch.autograd.backward()\ntorch.Tensor.backward()\nparam.grad\nIfparam.gradis initiallyNone:\nparam.grad\nNone\nIfparam\u2019s memory is non-overlapping and dense,.gradis\ncreated with strides matchingparam(thus matchingparam\u2019s\nlayout).\nparam\n.grad\nparam\nparam\nOtherwise,.gradis created with rowmajor-contiguous strides.\n.grad\nIfparamalready has a non-sparse.gradattribute:\nparam\n.grad\nIfcreate_graph=False,backward()accumulates into.gradin-place, which preserves its strides.\ncreate_graph=False\nbackward()\n.grad\nIfcreate_graph=True,backward()replaces.gradwith a\nnew tensor.grad+newgrad, which attempts (but does not guarantee)\nmatching the preexisting.grad\u2019s strides.\ncreate_graph=True\nbackward()\n.grad\n.grad+newgrad\n.grad\nThe default behavior (letting.grads beNonebefore the firstbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls tomodel.zero_grad()oroptimizer.zero_grad()will not affect.gradlayouts.\n.grad\nNone\nbackward()\nmodel.zero_grad()\noptimizer.zero_grad()\n.grad\nIn fact, resetting all.grads toNonebefore each\naccumulation phase, e.g.:\n.grad\nNone\n\n```python\nfor iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n\n```\n\nsuch that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative tomodel.zero_grad()oroptimizer.zero_grad()that may improve performance for some networks.\nmodel.zero_grad()\noptimizer.zero_grad()\n\n## Manual gradient layouts#\n\nIf you need manual control over.grad\u2019s strides,\nassignparam.grad=a zeroed tensor with desired strides\nbefore the firstbackward(), and never reset it toNone.\n3 guarantees your layout is preserved as long ascreate_graph=False.\n4 indicates your layout islikelypreserved even ifcreate_graph=True.\n.grad\nparam.grad=\nbackward()\nNone\ncreate_graph=False\ncreate_graph=True\n\n## In-place operations on Tensors#\n\nSupporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.\n\n## In-place correctness checks#\n\nAllTensors keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.\nTensor\n\n## Variable (deprecated)#\n\nWarning\nThe Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors withrequires_gradset toTrue. Below please find a quick guide on what\nhas changed:\nrequires_grad\nTrue\nVariable(tensor)andVariable(tensor,requires_grad)still work as expected,\nbut they return Tensors instead of Variables.\nVariable(tensor)\nVariable(tensor,requires_grad)\nvar.datais the same thing astensor.data.\nvar.data\ntensor.data\nMethods such asvar.backward(),var.detach(),var.register_hook()now work on tensors\nwith the same method names.\nvar.backward(),var.detach(),var.register_hook()\nIn addition, one can now create tensors withrequires_grad=Trueusing factory\nmethods such astorch.randn(),torch.zeros(),torch.ones(), and others\nlike the following:\nrequires_grad=True\ntorch.randn()\ntorch.zeros()\ntorch.ones()\nautograd_tensor=torch.randn((2,3,4),requires_grad=True)\nautograd_tensor=torch.randn((2,3,4),requires_grad=True)\n\n## Tensor autograd functions#\n\ntorch.Tensor.grad\ntorch.Tensor.grad\nThis attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself.\nNone\nbackward()\nself\ntorch.Tensor.requires_grad\ntorch.Tensor.requires_grad\nIsTrueif gradients need to be computed for this Tensor,Falseotherwise.\nTrue\nFalse\ntorch.Tensor.is_leaf\ntorch.Tensor.is_leaf\nAll Tensors that haverequires_gradwhich isFalsewill be leaf Tensors by convention.\nrequires_grad\nFalse\ntorch.Tensor.backward([gradient,\u00a0...])\ntorch.Tensor.backward\nComputes the gradient of current tensor wrt graph leaves.\ntorch.Tensor.detach\ntorch.Tensor.detach\nReturns a new Tensor, detached from the current graph.\ntorch.Tensor.detach_\ntorch.Tensor.detach_\nDetaches the Tensor from the graph that created it, making it a leaf.\ntorch.Tensor.register_hook(hook)\ntorch.Tensor.register_hook\nRegisters a backward hook.\ntorch.Tensor.register_post_accumulate_grad_hook(hook)\ntorch.Tensor.register_post_accumulate_grad_hook\nRegisters a backward hook that runs after grad accumulation.\ntorch.Tensor.retain_grad()\ntorch.Tensor.retain_grad\nEnables this Tensor to have theirgradpopulated duringbackward().\ngrad\nbackward()\n\n## Function#\n\nBase class to create customautograd.Function.\nTo create a customautograd.Function, subclass this class and implement\ntheforward()andbackward()static methods. Then, to use your custom\nop in the forward pass, call the class methodapply. Do not callforward()directly.\nforward()\nbackward()\napply\nforward()\nTo ensure correctness and best performance, make sure you are calling the\ncorrect methods onctxand validating your backward function usingtorch.autograd.gradcheck().\nctx\ntorch.autograd.gradcheck()\nSeeExtending torch.autogradfor more details on how to use this class.\nExamples:\n\n```python\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)\n\n```\n\nFunction.forward\nFunction.forward\nDefine the forward of the custom autograd Function.\nFunction.backward\nFunction.backward\nDefine a formula for differentiating the operation with backward mode automatic differentiation.\nFunction.jvp\nFunction.jvp\nDefine a formula for differentiating the operation with forward mode automatic differentiation.\nFunction.vmap\nFunction.vmap\nDefine the behavior for this autograd.Function underneathtorch.vmap().\ntorch.vmap()\n\n## Context method mixins#\n\nWhen creating a newFunction, the following methods are available toctx.\nFunction\nctx\nfunction.FunctionCtx.mark_dirty\nfunction.FunctionCtx.mark_dirty\nMark given tensors as modified in an in-place operation.\nfunction.FunctionCtx.mark_non_differentiable\nfunction.FunctionCtx.mark_non_differentiable\nMark outputs as non-differentiable.\nfunction.FunctionCtx.save_for_backward\nfunction.FunctionCtx.save_for_backward\nSave given tensors for a future call tobackward().\nbackward()\nfunction.FunctionCtx.set_materialize_grads\nfunction.FunctionCtx.set_materialize_grads\nSet whether to materialize grad tensors.\n\n## Custom Function utilities#\n\nDecorator for backward method.\nfunction.once_differentiable\nfunction.once_differentiable\n\nBase customFunctionused to build PyTorch utilities\nFunction\nfunction.BackwardCFunction\nfunction.BackwardCFunction\nThis class is used for internal autograd work.\nfunction.InplaceFunction\nfunction.InplaceFunction\nThis class is here only for backward compatibility reasons.\nfunction.NestedIOFunction\nfunction.NestedIOFunction\nThis class is here only for backward compatibility reasons.\n\n## Numerical gradient checking#\n\ngradcheck\n\ngradcheck\nCheck gradients computed via small finite differences against analytical gradients wrt tensors ininputsthat are of floating point or complex type and withrequires_grad=True.\ninputs\nrequires_grad=True\ngradgradcheck\n\ngradgradcheck\nCheck gradients of gradients computed via small finite differences against analytical gradients wrt tensors ininputsandgrad_outputsthat are of floating point or complex type and withrequires_grad=True.\ninputs\ngrad_outputs\nrequires_grad=True\nGradcheckError\n\nGradcheckError\nError raised bygradcheck()andgradgradcheck().\ngradcheck()\ngradgradcheck()\n\n## Profiler#\n\nAutograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are three modes\nimplemented at the moment - CPU-only usingprofile.\nnvprof based (registers both CPU and GPU activity) usingemit_nvtx.\nand vtune profiler based usingemit_itt.\nprofile\nemit_nvtx\nemit_itt\nContext manager that manages autograd profiler state and holds a summary of results.\nNote\nThis is the backend, most people should usetorch.profilerinstead.\ntorch.profiler\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks\nenabled(bool,optional) \u2013 Setting this to False makes this context manager a no-op.\nuse_cuda(bool,optional) \u2013 Enables timing of CUDA events as well\nusing the cudaEvent API. (will be deprecated)\nuse_device(str,optional) \u2013 Enables timing of device events.\nAdds approximately 4us of overhead to each tensor operation when use cuda.\nThe valid devices options are \u2018cuda\u2019, \u2018xpu\u2019, \u2018mtia\u2019 and \u2018privateuseone\u2019.\nrecord_shapes(bool,optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection.\nwith_flops(bool,optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPs (floating point operations) value using the operator\u2019s input shape.\nThis allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators.\nprofile_memory(bool,optional) \u2013 track tensor memory allocation/deallocation.\nwith_stack(bool,optional) \u2013 record source information (file and line number) for the ops.\nwith_modules(bool) \u2013 record module hierarchy (including function names)\ncorresponding to the callstack of the op. e.g. If module A\u2019s forward call\u2019s\nmodule B\u2019s forward which contains an aten::add op,\nthen aten::add\u2019s module hierarchy is A.B\nNote that this support exist, at the moment, only for TorchScript models\nand not eager mode models.\nuse_kineto(bool,optional) \u2013 experimental, enable profiling with Kineto profiler.\nuse_cpu(bool,optional) \u2013 profile CPU events; setting toFalserequiresuse_kineto=Trueand can be used to lower the overhead for GPU-only profiling.\nFalse\nuse_kineto=True\nexperimental_config(_ExperimentalConfig) \u2013 A set of experimental options\nused by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.\nacc_events(bool) \u2013 Enable the accumulation of FunctionEvents across multiple profiling cycles\nWarning\nEnabling memory profiling or source attribution incurs additional profiler\noverhead\nWarning\nThis context managers should not be called recursively, i.e. no nested\ninstances are allowed\nWarning\nDue to some CUDA multiprocessing limitations (seeCUDA in multiprocessing),\none cannot use the profiler withuse_device='cuda'to benchmark\nDataLoaders withnum_workers>0. If you wish to benchmark data loading,\nplease useuse_device=Noneornum_workers=0.\nuse_device='cuda'\nnum_workers>0\nuse_device=None\nnum_workers=0\nExample\n\n```python\n>>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>>         y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n\n```\n\nprofiler.profile.export_chrome_trace\nprofiler.profile.export_chrome_trace\nExport an EventList as a Chrome tracing tools file.\nprofiler.profile.key_averages\nprofiler.profile.key_averages\nAverages all function events over their keys.\nprofiler.profile.self_cpu_time_total\nprofiler.profile.self_cpu_time_total\nReturns total time spent on CPU.\nprofiler.profile.total_average\nprofiler.profile.total_average\nAverages all events.\nprofiler.parse_nvprof_trace\nprofiler.parse_nvprof_trace\n\nprofiler.EnforceUnique\nprofiler.EnforceUnique\nRaises an error if a key is seen more than once.\nprofiler.KinetoStepTracker\nprofiler.KinetoStepTracker\nProvides an abstraction for incrementing the step count globally.\nprofiler.record_function\nprofiler.record_function\nContext manager/function decorator that adds a label to a code block/function when running autograd profiler.\nprofiler_util.Interval\nprofiler_util.Interval\n\nprofiler_util.Kernel\nprofiler_util.Kernel\n\nprofiler_util.MemRecordsAcc\nprofiler_util.MemRecordsAcc\nAcceleration structure for accessing mem_records in interval.\nprofiler_util.StringTable\nprofiler_util.StringTable\n\nContext manager that makes every autograd operation emit an NVTX range.\nIt is useful when running the program under nvprof:\n\n```python\nnvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n\n```\n\nUnfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, ortorch.autograd.profiler.load_nvprof()can load the results for inspection\ne.g. in Python REPL.\ntorch.autograd.profiler.load_nvprof()\nenabled(bool,optional) \u2013 Settingenabled=Falsemakes this context manager a no-op.\nDefault:True.\nenabled=False\nTrue\nrecord_shapes(bool,optional) \u2013 Ifrecord_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:[[arg0.size(0),arg0.size(1),...],[arg1.size(0),arg1.size(1),...],...]Non-tensor arguments will be represented by[].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.\nDefault:False\nrecord_shapes=True\n[[arg0.size(0),arg0.size(1),...],[arg1.size(0),arg1.size(1),...],...]\n[]\nFalse\nExample\n\n```python\n>>> with torch.cuda.profiler.profile():\n...     model(x)  # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n\n```\n\nForward-backward correlation\nWhen viewing a profile created usingemit_nvtxin the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task,emit_nvtxappends sequence number information to the ranges it\ngenerates.\nemit_nvtx\nemit_nvtx\nDuring the forward pass, each function range is decorated withseq=<N>.seqis a running\ncounter, incremented each time a new backward Function object is created and stashed for backward.\nThus, theseq=<N>annotation associated with each forward function range tells you that\nif a backward Function object is created by this forward function,\nthe backward object will receive sequence number N.\nDuring the backward pass, the top-level range wrapping each C++ backward Function\u2019sapply()call is decorated withstashedseq=<M>.Mis the sequence number that\nthe backward object was created with.  By comparingstashedseqnumbers in backward withseqnumbers in forward, you can track down which forward op created each backward Function.\nseq=<N>\nseq\nseq=<N>\napply()\nstashedseq=<M>\nM\nstashedseq\nseq\nAny functions executed during the backward pass are also decorated withseq=<N>.  During\ndefault backward (withcreate_graph=False) this information is irrelevant, and in fact,Nmay simply be 0 for all such functions.  Only the top-level ranges associated with\nbackward Function objects\u2019apply()methods are useful, as a way to correlate these Function\nobjects with the earlier forward pass.\nseq=<N>\ncreate_graph=False\nN\napply()\nDouble-backward\nIf, on the other hand, a backward pass withcreate_graph=Trueis underway (in other words,\nif you are setting up for a double-backward), each function\u2019s execution during backward\nis given a nonzero, usefulseq=<N>.  Those functions may themselves create Function objects\nto be executed later during double-backward, just as the original functions in the forward pass did.\nThe relationship between backward and double-backward is conceptually the same as the relationship\nbetween forward and backward: The functions still emit current-sequence-number-tagged ranges,\nthe Function objects they create still stash those sequence numbers, and during the eventual\ndouble-backward, the Function objects\u2019apply()ranges are still tagged withstashedseqnumbers, which can be compared toseqnumbers from the backward pass.\ncreate_graph=True\nseq=<N>\napply()\nstashedseq\nContext manager that makes every autograd operation emit an ITT range.\nIt is useful when running the program under Intel(R) VTune Profiler:\n\n```python\nvtune <--vtune-flags> <regular command here>\n\n```\n\nThe Instrumentation and Tracing Technology (ITT) API enables your application to generate and\ncontrol the collection of trace data during its execution across different Intel tools.\nThis context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager,\nyou will be able to see labeled ranges in Intel(R) VTune Profiler GUI.\nenabled(bool,optional) \u2013 Settingenabled=Falsemakes this context manager a no-op.\nDefault:True.\nenabled=False\nTrue\nrecord_shapes(bool,optional) \u2013 Ifrecord_shapes=True, the itt range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:[[arg0.size(0),arg0.size(1),...],[arg1.size(0),arg1.size(1),...],...]Non-tensor arguments will be represented by[].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of itt range creation.\nDefault:False\nrecord_shapes=True\n[[arg0.size(0),arg0.size(1),...],[arg1.size(0),arg1.size(1),...],...]\n[]\nFalse\nExample\n\n```python\n>>> with torch.autograd.profiler.emit_itt():\n...     model(x)\n\n```\n\nprofiler.load_nvprof\nprofiler.load_nvprof\nOpen an nvprof trace file and parses autograd annotations.\n\n## Debugging and anomaly detection#\n\nContext-manager that enable anomaly detection for the autograd engine.\nThis does two things:\nRunning the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function.\nIfcheck_nanisTrue, any backward computation that generate \u201cnan\u201d\nvalue will raise an error. DefaultTrue.\ncheck_nan\nTrue\nTrue\nWarning\nThis mode should be enabled only for debugging as the different tests\nwill slow down your program execution.\nExample\n\n```python\n>>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n\n```\n\nContext-manager that sets the anomaly detection for the autograd engine on or off.\nset_detect_anomalywill enable or disable the autograd anomaly detection\nbased on its argumentmode.\nIt can be used as a context-manager or as a function.\nset_detect_anomaly\nmode\nSeedetect_anomalyabove for details of the anomaly detection behaviour.\ndetect_anomaly\nmode(bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).\nTrue\nFalse\ncheck_nan(bool) \u2013 Flag whether to raise an error when the backward\ngenerate \u201cnan\u201d\ngrad_mode.set_multithreading_enabled\ngrad_mode.set_multithreading_enabled\nContext-manager that sets multithreaded backwards on or off.\n\n## Autograd graph#\n\nAutograd exposes methods that allow one to inspect the graph and interpose behavior during\nthe backward pass.\nThegrad_fnattribute of atorch.Tensorholds atorch.autograd.graph.Nodeif the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is\nenabled and at least one of the inputs required gradients), orNoneotherwise.\ngrad_fn\ntorch.Tensor\ntorch.autograd.graph.Node\nNone\ngraph.Node.name\ngraph.Node.name\nReturn the name.\ngraph.Node.metadata\ngraph.Node.metadata\nReturn the metadata.\ngraph.Node.next_functions\ngraph.Node.next_functions\n\ngraph.Node.register_hook\ngraph.Node.register_hook\nRegister a backward hook.\ngraph.Node.register_prehook\ngraph.Node.register_prehook\nRegister a backward pre-hook.\ngraph.increment_version\ngraph.increment_version\nUpdate autograd metadata tracking whether the given Tensor was modified in place.\nSome operations need intermediary results to be saved during the forward pass\nin order to execute the backward pass.\nThese intermediary results are saved as attributes on thegrad_fnand can be accessed.\nFor example:\ngrad_fn\n\n```python\n>>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> b = a.exp()\n>>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))\nTrue\n>>> print(dir(b.grad_fn))\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n>>> print(torch.allclose(b.grad_fn._saved_result, b))\nTrue\n\n```\n\nYou can also define how these saved tensors should be packed / unpacked using hooks.\nA common application is to trade compute for memory by saving those intermediary results\nto disk or to CPU instead of leaving them on the GPU. This is especially useful if you\nnotice your model fits on GPU during evaluation, but not training.\nAlso seeHooks for saved tensors.\nContext-manager that sets a pair of pack / unpack hooks for saved tensors.\nUse this context-manager to define how intermediary results of an operation\nshould be packed before saving, and unpacked on retrieval.\nIn that context, thepack_hookfunction will be called every time an\noperation saves a tensor for backward (this includes intermediary results\nsaved usingsave_for_backward()but\nalso those recorded by a PyTorch-defined operation). The output ofpack_hookis then stored in the computation graph instead of the\noriginal tensor.\npack_hook\nsave_for_backward()\npack_hook\nTheunpack_hookis called when the saved tensor needs to be accessed,\nnamely when executingtorch.Tensor.backward()ortorch.autograd.grad(). It takes as argument thepackedobject\nreturned bypack_hookand should return a tensor which has the same\ncontent as the original tensor (passed as input to the correspondingpack_hook).\nunpack_hook\ntorch.Tensor.backward()\ntorch.autograd.grad()\npack_hook\npack_hook\nThe hooks should have the following signatures:\npack_hook(tensor: Tensor) -> Any\nunpack_hook(Any) -> Tensor\nwhere the return value ofpack_hookis a valid input tounpack_hook.\npack_hook\nunpack_hook\nIn general, you wantunpack_hook(pack_hook(t))to be equal totin terms\nof value, size, dtype and device.\nunpack_hook(pack_hook(t))\nt\nExample:\n\n```python\n>>> def pack_hook(x):\n...     print(\"Packing\", x)\n...     return x.detach()\n>>>\n>>> def unpack_hook(x):\n...     print(\"Unpacking\", x)\n...     return x\n>>>\n>>> a = torch.ones(5, requires_grad=True)\n>>> b = torch.ones(5, requires_grad=True) * 2\n>>> with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n...     y = a * b\nPacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nPacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n>>> y.sum().backward()\nUnpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nUnpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n\n```\n\nWarning\nPerforming an inplace operation on the input to either hooks may lead\nto undefined behavior.\nWarning\nOnly one pair of hooks is allowed at a time. When recursively nesting this\ncontext-manager, only the inner-most pair of hooks will be applied.\nWarning\nTo avoid reference cycle, the return value ofpack_hookcannot hold a\nreference to the input tensor. For example, uselambda x: x.detach()instead oflambda x: xas the pack hook.\npack_hook\nContext manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.\nWhen performing operations within this context manager, intermediary\nresults saved in the graph during the forward pass will be moved to CPU,\nthen copied back to the original device when needed for the backward pass.\nIf the graph was already on CPU, no tensor copy is performed.\nUse this context-manager to trade compute for GPU memory usage (e.g.\nwhen your model doesn\u2019t fit in GPU memory during training).\npin_memory(bool) \u2013 IfTruetensors will be saved to CPU pinned memory\nduring packing and copied to GPU asynchronously during unpacking.\nDefaults toFalse.\nAlso seeUse pinned memory buffers.\nTrue\nFalse\nExample:\n\n```python\n>>> a = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> b = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> c = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>>\n>>> def f(a, b, c):\n...     prod_1 = a * b           # a and b are saved on GPU\n...     with torch.autograd.graph.save_on_cpu():\n...         prod_2 = prod_1 * c  # prod_1 and c are saved on CPU\n...     y = prod_2 * a           # prod_2 and a are saved on GPU\n...     return y\n>>>\n>>> y = f(a, b, c)\n>>> del a, b, c  # for illustration only\n>>> # the content of a, b, and prod_2 are still alive on GPU\n>>> # the content of prod_1 and c only live on CPU\n>>> y.sum().backward()  # all CPU tensors are moved back to GPU, for backward\n>>> # all intermediary tensors are released (deleted) after the call to backward\n\n```\n\nContext-manager that disables the saved tensors default hooks feature.\nUseful for if you are creating a feature that does not work with saved\ntensors default hooks.\nerror_message(str) \u2013 When saved tensors default hooks are used when they\nhave been are disabled, a RuntimeError with this\nerror message gets raised.\nGenerator[None, None, None]\nExample:\n\n```python\n>>> message = \"saved tensors default hooks are disabled\"\n>>> with torch.autograd.graph.disable_saved_tensors_hooks(message):\n...     # Raises RuntimeError: saved tensors default hooks are disabled\n...     with torch.autograd.graph.save_on_cpu():\n...         pass\n\n```\n\nRegister a multi-grad backward hook.\nThere are two supported modes:\"all\"and\"any\".\n\"all\"\n\"any\"\nUnder the\"all\"mode, the hook will be called after gradients with respect to every tensor intensorshave been computed. If a tensor is intensorsbut\nis not part of the graph, or if a tensor is not needed to compute the gradients\nfor anyinputsspecified for the current.backward()or.grad()call,\nthis tensor will be ignored and the hook will not wait for its gradient to be\ncomputed.\n\"all\"\ntensors\ntensors\ninputs\n.backward()\n.grad()\nAfter every non-ignored tensor\u2019s gradient has been computed,fnwill be\ncalled with those gradients.Nonewill be passed for tensors that did not\nhave their gradients computed.\nfn\nNone\nUnder the\"any\"mode, the hook will be called after the first gradient\nwith respect to a tensor intensorshas been computed. The hook\nwill be called with that gradient as its argument.\n\"any\"\ntensors\nThe hook should not modify its arguments.\nThis function returns a handle with a methodhandle.remove()that removes the hook.\nhandle.remove()\nNote\nSeeBackward Hooks executionfor more information on how when this hook\nis executed, and how its execution is ordered relative to other hooks.\nExample:\n\n```python\n>>> import torch\n>>>\n>>> a = torch.rand(2, 3, requires_grad=True)\n>>> b = torch.rand(2, 3, requires_grad=True)\n>>> c = a * b\n>>> d = a * b\n>>>\n>>> def fn(grads):\n...     print([g is not None for g in grads])\n...\n>>> torch.autograd.graph.register_multi_grad_hook((a, b, c, d), fn)\n>>>\n>>> c.sum().backward(retain_graph=True)\n[True, True, True, False]\n>>> c.sum().backward(inputs=(a,), retain_graph=True)\n[True, False, True, False]\n>>>\n\n```\n\nRemovableHandle\nContext manager under which mutating tensors saved for backward is allowed.\nUnder this context manager, tensors saved for backward are cloned on mutation,\nso the original version can still be used during backward. Normally, mutating a tensor\nsaved for backward will result in an error raised when it\u2019s used during backward.\nTo ensure the correct behavior, both the forward and backward should be run under\nthe same context manager.\nAn _AllowMutationOnSavedContext object storing the state managed by this\ncontext manager. This object can be useful for debugging purposes. The state\nmanaged by the context manager is automatically cleared upon exiting.\nGenerator[_AllowMutationOnSavedContext, None, None]\nExample:\n\n```python\n>>> import torch\n>>> with torch.autograd.graph.allow_mutation_on_saved_tensors():\n...     # forward\n...     a = torch.ones(2, 3, requires_grad=True)\n...     b = a.clone()\n...     out = (b**2).sum()\n...     b.sin_()\n...     # backward\n...     out.sum().backward()\n...\ntensor([[0.8415, 0.8415, 0.8415],\n        [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>)\n\n```\n\nObject representing a given gradient edge within the autograd graph.\nTo get the gradient edge where a given Tensor gradient will be computed,\nyou can doedge=autograd.graph.get_gradient_edge(tensor).\nedge=autograd.graph.get_gradient_edge(tensor)\nGet the gradient edge for computing the gradient of the given Tensor.\nIn particular, it is equivalent to callg=autograd.grad(loss,input)andg=autograd.grad(loss,get_gradient_edge(input)).\ng=autograd.grad(loss,input)\ng=autograd.grad(loss,get_gradient_edge(input))\nGradientEdge",
  "url": "https://pytorch.org/docs/stable/autograd.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
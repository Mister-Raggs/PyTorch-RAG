{
  "doc_id": "8532364d8979e6ecd0e3e1e24de244d8",
  "source": "pytorch_docs",
  "title": "HIP (ROCm) semantics \u2014 PyTorch 2.9 documentation",
  "text": "\n## HIP (ROCm) semantics#\n\nCreated On: May 12, 2021 | Last Updated On: Aug 08, 2025\nROCm\u2122 is AMD\u2019s open source software platform for GPU-accelerated high\nperformance computing and machine learning. HIP is ROCm\u2019s C++ dialect designed\nto ease conversion of CUDA applications to portable C++ code. HIP is used when\nconverting existing CUDA applications like PyTorch to portable C++ and for new\nprojects that require portability between AMD and NVIDIA.\n\n## HIP Interfaces Reuse the CUDA Interfaces#\n\nPyTorch for HIP intentionally reuses the existingtorch.cudainterfaces.\nThis helps to accelerate the porting of existing PyTorch code and models because\nvery few code changes are necessary, if any.\ntorch.cuda\nThe example fromCUDA semanticswill work exactly the same for HIP:\n\n```python\ncuda = torch.device('cuda')     # Default HIP device\ncuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n\n```\n\n\n## Checking for HIP#\n\nWhether you are using PyTorch for CUDA or HIP, the result of callingis_available()will be the same. If you are using a PyTorch\nthat has been built with GPU support, it will returnTrue. If you must check\nwhich version of PyTorch you are using, refer to this example below:\nis_available()\n\n```python\nif torch.cuda.is_available() and torch.version.hip:\n    # do something specific for HIP\nelif torch.cuda.is_available() and torch.version.cuda:\n    # do something specific for CUDA\n\n```\n\n\n## TensorFloat-32(TF32) on ROCm#\n\nTF32 is not supported on ROCm.\n\n## Memory management#\n\nPyTorch uses a caching memory allocator to speed up memory allocations. This\nallows fast memory deallocation without device synchronizations. However, the\nunused memory managed by the allocator will still show as if used inrocm-smi. You can usememory_allocated()andmax_memory_allocated()to monitor memory occupied by\ntensors, and usememory_reserved()andmax_memory_reserved()to monitor the total amount of memory\nmanaged by the caching allocator. Callingempty_cache()releases allunusedcached memory from PyTorch so that those can be used\nby other GPU applications. However, the occupied GPU memory by tensors will not\nbe freed so it can not increase the amount of GPU memory available for PyTorch.\nrocm-smi\nmemory_allocated()\nmax_memory_allocated()\nmemory_reserved()\nmax_memory_reserved()\nempty_cache()\nFor more advanced users, we offer more comprehensive memory benchmarking viamemory_stats(). We also offer the capability to capture a\ncomplete snapshot of the memory allocator state viamemory_snapshot(), which can help you understand the\nunderlying allocation patterns produced by your code.\nmemory_stats()\nmemory_snapshot()\nTo debug memory errors, setPYTORCH_NO_HIP_MEMORY_CACHING=1in your environment to disable caching.PYTORCH_NO_CUDA_MEMORY_CACHING=1is also accepted for ease of porting.\nPYTORCH_NO_HIP_MEMORY_CACHING=1\nPYTORCH_NO_CUDA_MEMORY_CACHING=1\n\n## hipBLAS workspaces#\n\nFor each combination of hipBLAS handle and HIP stream, a hipBLAS workspace will be allocated if that\nhandle and stream combination executes a hipBLAS kernel that requires a workspace.  In order to\navoid repeatedly allocating workspaces, these workspaces are not deallocated unlesstorch._C._cuda_clearCublasWorkspaces()is called; note that it\u2019s the same function for CUDA or\nHIP. The workspace size per allocation can be specified via the environment variableHIPBLAS_WORKSPACE_CONFIGwith the format:[SIZE]:[COUNT].  As an example, the environment\nvariableHIPBLAS_WORKSPACE_CONFIG=:4096:2:16:8specifies a total size of2*4096+8*16KiBor 8 MIB. The default workspace size is 32 MiB; MI300 and newer defaults to 128 MiB. To force\nhipBLAS to avoid using workspaces, setHIPBLAS_WORKSPACE_CONFIG=:0:0. For convenience,CUBLAS_WORKSPACE_CONFIGis also accepted.\ntorch._C._cuda_clearCublasWorkspaces()\nHIPBLAS_WORKSPACE_CONFIG\n:[SIZE]:[COUNT]\nHIPBLAS_WORKSPACE_CONFIG=:4096:2:16:8\n2*4096+8*16KiB\nHIPBLAS_WORKSPACE_CONFIG=:0:0\nCUBLAS_WORKSPACE_CONFIG\n\n## hipFFT/rocFFT plan cache#\n\nSetting the size of the cache for hipFFT/rocFFT plans is not supported.\n\n## torch.distributed backends#\n\nCurrently, only the \u201cnccl\u201d and \u201cgloo\u201d backends for torch.distributed are supported on ROCm.\n\n## CUDA API to HIP API mappings in C++#\n\nPlease refer:https://rocm.docs.amd.com/projects/HIP/en/latest/reference/api_syntax.html\nNOTE: The CUDA_VERSION macro, cudaRuntimeGetVersion and cudaDriverGetVersion APIs do not\nsemantically map to the same values as HIP_VERSION macro, hipRuntimeGetVersion and\nhipDriverGetVersion APIs. Please do not use them interchangeably when doing version checks.\nFor example: Instead of using\n#ifdefined(CUDA_VERSION)&&CUDA_VERSION>=11000to implicitly exclude ROCm/HIP,\n#ifdefined(CUDA_VERSION)&&CUDA_VERSION>=11000\nuse the following to not take the code path for ROCm/HIP:\n#ifdefined(CUDA_VERSION)&&CUDA_VERSION>=11000&&!defined(USE_ROCM)\n#ifdefined(CUDA_VERSION)&&CUDA_VERSION>=11000&&!defined(USE_ROCM)\nAlternatively, if it is desired to take the code path for ROCm/HIP:\n#if(defined(CUDA_VERSION)&&CUDA_VERSION>=11000)||defined(USE_ROCM)\n#if(defined(CUDA_VERSION)&&CUDA_VERSION>=11000)||defined(USE_ROCM)\nOr if it is desired to take the code path for ROCm/HIP only for specific HIP versions:\n#if(defined(CUDA_VERSION)&&CUDA_VERSION>=11000)||(defined(USE_ROCM)&&ROCM_VERSION>=40300)\n#if(defined(CUDA_VERSION)&&CUDA_VERSION>=11000)||(defined(USE_ROCM)&&ROCM_VERSION>=40300)\n\n## Refer to CUDA Semantics doc#\n\nFor any sections not listed here, please refer to the CUDA semantics doc:CUDA semantics\n\n## Enabling kernel asserts#\n\nKernel asserts are supported on ROCm, but they are disabled due to performance overhead. It can be enabled\nby recompiling the PyTorch from source.\nPlease add below line as an argument to cmake command parameters:\n\n```python\n-DROCM_FORCE_ENABLE_GPU_ASSERTS:BOOL=ON\n\n```\n\n\n## Enabling/Disabling ROCm Composable Kernel#\n\nEnabling composable_kernel (CK) for both SDPA and GEMMs is a two-part process. First the user must have built\npytorch while setting the corresponding environment variable to \u20181\u2019\nSDPA:USE_ROCM_CK_SDPA=1\nUSE_ROCM_CK_SDPA=1\nGEMMs:USE_ROCM_CK_GEMM=1\nUSE_ROCM_CK_GEMM=1\nSecond, the user must explicitly request that CK be used as the backend library via the corresponding python\ncall\nSDPA:setROCmFAPreferredBackend('<choice>')\nsetROCmFAPreferredBackend('<choice>')\nGEMMs:setBlasPreferredBackend('<choice>')\nsetBlasPreferredBackend('<choice>')\nTo enable CK in either scenario, simply pass \u2018ck\u2019 to those functions.\nIn order to set the backend to CK, the user MUST have built with the correct environment variable. If not,\nPyTorch will print a warning and use the \u201cdefault\u201d backend. For GEMMs, this will route to hipblas and\nfor SDPA it routes to aotriton.",
  "url": "https://pytorch.org/docs/stable/notes/hip.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
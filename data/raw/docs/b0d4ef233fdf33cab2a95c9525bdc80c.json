{
  "doc_id": "b0d4ef233fdf33cab2a95c9525bdc80c",
  "source": "pytorch_docs",
  "title": "torch.sparse \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.sparse#\n\nCreated On: Apr 26, 2017 | Last Updated On: Jun 18, 2025\nWarning\nThe PyTorch API of sparse tensors is in beta and may change in the near future.\nWe highly welcome feature requests, bug reports and general suggestions as GitHub issues.\n\n## Why and when to use sparsity#\n\nBy default, PyTorch storestorch.Tensorelements contiguously in\nphysical memory. This leads to efficient implementations of various array\nprocessing algorithms that require fast access to elements.\ntorch.Tensor\nNow, some users might decide to represent data such as graph adjacency\nmatrices, pruned weights or points clouds by Tensors whoseelements are\nmostly zero valued. We recognize these are important applications and aim\nto provide performance optimizations for these use cases via sparse storage formats.\nVarious sparse storage formats such as COO, CSR/CSC, semi-structured, LIL, etc. have been\ndeveloped over the years. While they differ in exact layouts, they all\ncompress data through efficient representation of zero valued elements.\nWe call the uncompressed valuesspecifiedin contrast tounspecified,\ncompressed elements.\nBy compressing repeat zeros sparse storage formats aim to save memory\nand computational resources on various CPUs and GPUs. Especially for high\ndegrees of sparsity or highly structured sparsity this can have significant\nperformance implications. As such sparse storage formats can be seen as a\nperformance optimization.\nLike many other performance optimization sparse storage formats are not\nalways advantageous. When trying sparse formats for your use case\nyou might find your execution time to increase rather than decrease.\nPlease feel encouraged to open a GitHub issue if you analytically\nexpected to see a stark increase in performance but measured a\ndegradation instead. This helps us prioritize the implementation\nof efficient kernels and wider performance optimizations.\nWe make it easy to try different sparsity layouts, and convert between them,\nwithout being opinionated on what\u2019s best for your particular application.\n\n## Functionality overview#\n\nWe want it to be straightforward to construct a sparse Tensor from a\ngiven dense Tensor by providing conversion routines for each layout.\nIn the next example we convert a 2D Tensor with default dense (strided)\nlayout to a 2D Tensor backed by the COO memory layout. Only values and\nindices of non-zero elements are stored in this case.\n\n```python\n>>> a = torch.tensor([[0, 2.], [3, 0]])\n>>> a.to_sparse()\ntensor(indices=tensor([[0, 1],\n                       [1, 0]]),\n       values=tensor([2., 3.]),\n       size=(2, 2), nnz=2, layout=torch.sparse_coo)\n\n```\n\nPyTorch currently supportsCOO,CSR,CSC,BSR, andBSC.\nWe also have a prototype implementation to support :ref:semi-structured sparsity<sparse-semi-structured-docs>.\nPlease see the references for more details.\nNote that we provide slight generalizations of these formats.\nBatching: Devices such as GPUs require batching for optimal performance and\nthus we support batch dimensions.\nWe currently offer a very simple version of batching where each component of a sparse format\nitself is batched. This also requires the same number of specified elements per batch entry.\nIn this example we construct a 3D (batched) CSR Tensor from a 3D dense Tensor.\n\n```python\n>>> t = torch.tensor([[[1., 0], [2., 3.]], [[4., 0], [5., 6.]]])\n>>> t.dim()\n3\n>>> t.to_sparse_csr()\ntensor(crow_indices=tensor([[0, 1, 3],\n                            [0, 1, 3]]),\n       col_indices=tensor([[0, 0, 1],\n                           [0, 0, 1]]),\n       values=tensor([[1., 2., 3.],\n                      [4., 5., 6.]]), size=(2, 2, 2), nnz=3,\n       layout=torch.sparse_csr)\n\n```\n\nDense dimensions: On the other hand, some data such as Graph embeddings might be\nbetter viewed as sparse collections of vectors instead of scalars.\nIn this example we create a 3D Hybrid COO Tensor with 2 sparse and 1 dense dimension\nfrom a 3D strided Tensor. If an entire row in the 3D strided Tensor is zero, it is\nnot stored. If however any of the values in the row are non-zero, they are stored\nentirely. This reduces the number of indices since we need one index one per row instead\nof one per element. But it also increases the amount of storage for the values. Since\nonly rows that areentirelyzero can be emitted and the presence of any non-zero\nvalued elements cause the entire row to be stored.\n\n```python\n>>> t = torch.tensor([[[0., 0], [1., 2.]], [[0., 0], [3., 4.]]])\n>>> t.to_sparse(sparse_dim=2)\ntensor(indices=tensor([[0, 1],\n                       [1, 1]]),\n       values=tensor([[1., 2.],\n                      [3., 4.]]),\n       size=(2, 2, 2), nnz=2, layout=torch.sparse_coo)\n\n```\n\n\n## Operator overview#\n\nFundamentally, operations on Tensor with sparse storage formats behave the same as\noperations on Tensor with strided (or other) storage formats. The particularities of\nstorage, that is the physical layout of the data, influences the performance of\nan operation but should not influence the semantics.\nWe are actively increasing operator coverage for sparse tensors. Users should not\nexpect support same level of support as for dense Tensors yet.\nSee ouroperatordocumentation for a list.\n\n```python\n>>> b = torch.tensor([[0, 0, 1, 2, 3, 0], [4, 5, 0, 6, 0, 0]])\n>>> b_s = b.to_sparse_csr()\n>>> b_s.cos()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: unsupported tensor layout: SparseCsr\n>>> b_s.sin()\ntensor(crow_indices=tensor([0, 3, 6]),\n       col_indices=tensor([2, 3, 4, 0, 1, 3]),\n       values=tensor([ 0.8415,  0.9093,  0.1411, -0.7568, -0.9589, -0.2794]),\n       size=(2, 6), nnz=6, layout=torch.sparse_csr)\n\n```\n\nAs shown in the example above, we don\u2019t support non-zero preserving unary\noperators such as cos. The output of a non-zero preserving unary operation\nwill not be able to take advantage of sparse storage formats to the same\nextent as the input and potentially result in a catastrophic increase in memory.\nWe instead rely on the user to explicitly convert to a dense Tensor first and\nthen run the operation.\n\n```python\n>>> b_s.to_dense().cos()\ntensor([[ 1.0000, -0.4161],\n        [-0.9900,  1.0000]])\n\n```\n\nWe are aware that some users want to ignore compressed zeros for operations such\nascosinstead of preserving the exact semantics of the operation. For this we\ncan point to torch.masked and its MaskedTensor, which is in turn also backed and\npowered by sparse storage formats and kernels.\nAlso note that, for now, the user doesn\u2019t have a choice of the output layout. For example,\nadding a sparse Tensor to a regular strided Tensor results in a strided Tensor. Some\nusers might prefer for this to stay a sparse layout, because they know the result will\nstill be sufficiently sparse.\n\n```python\n>>> a + b.to_sparse()\ntensor([[0., 3.],\n        [3., 0.]])\n\n```\n\nWe acknowledge that access to kernels that can efficiently produce different output\nlayouts can be very useful. A subsequent operation might significantly benefit from\nreceiving a particular layout. We are working on an API to control the result layout\nand recognize it is an important feature to plan a more optimal path of execution for\nany given model.\n\n## Sparse Semi-Structured Tensors#\n\nWarning\nSparse semi-structured tensors are currently a prototype feature and subject to change. Please feel free to open an issue to report a bug or if you have feedback to share.\nSemi-Structured sparsity is a sparse data layout that was first introduced in NVIDIA\u2019s Ampere architecture. It is also referred to asfine-grained structured sparsityor2:4 structured sparsity.\nThis sparse layout storesnelements out of every2nelements, withnbeing determined by the width of the Tensor\u2019s data type (dtype). The most frequently used dtype is float16, wheren=2, thus the term \u201c2:4 structured sparsity.\u201d\nSemi-structured sparsity is explained in greater detail inthis NVIDIA blog post.\nIn PyTorch, semi-structured sparsity is implemented via a Tensor subclass.\nBy subclassing, we can override__torch_dispatch__, allowing us to use faster sparse kernels when performing matrix multiplication.\nWe can also store the tensor in it\u2019s compressed form inside the subclass to reduce memory overhead.\n__torch_dispatch__\nIn this compressed form, the sparse tensor is stored by retaining only thespecifiedelements and some metadata, which encodes the mask.\nNote\nThe specified elements and metadata mask of a semi-structured sparse tensor are stored together in a single\nflat compressed tensor. They are appended to each other to form a contiguous chunk of memory.\ncompressed tensor = [ specified elements of original tensor |   metadata_mask ]\nFor an original tensor of size(r, c)we expect the firstm * k // 2elements to be the kept elements\nand the rest of the tensor is metadata.\nIn order to make it easier for the user to view the specified elements\nand mask, one can use.indices()and.values()to access the mask and specified elements respectively.\n.indices()\n.values()\n.values()returns the specified elements in a tensor of size(r, c//2)and with the same dtype as the dense matrix.\n.values()\n.indices()returns the metadata_mask in a tensor of size(r, c//2 )and with element typetorch.int16if dtype is torch.float16 or torch.bfloat16, and element typetorch.int32if dtype is torch.int8.\n.indices()\ntorch.int16\ntorch.int32\nFor 2:4 sparse tensors, the metadata overhead is minor - just 2 bits per specified element.\nNote\nIt\u2019s important to note thattorch.float32is only supported for 1:2 sparsity. Therefore, it does not follow the same formula as above.\ntorch.float32\nHere, we break down how to calculate the compression ratio ( size dense / size sparse) of a 2:4 sparse tensor.\nLet(r, c) = tensor.shapeande = bitwidth(tensor.dtype), soe = 16fortorch.float16andtorch.bfloat16ande = 8fortorch.int8.\ntorch.float16\ntorch.bfloat16\ntorch.int8\nUsing these calculations, we can determine the total memory footprint for both the original dense and the new sparse representation.\nThis gives us a simple formula for the compression ratio, which is dependent only on the bitwidth of the tensor datatype.\nBy using this formula, we find that the compression ratio is 56.25% fortorch.float16ortorch.bfloat16, and 62.5% fortorch.int8.\ntorch.float16\ntorch.bfloat16\ntorch.int8\n\n## Constructing Sparse Semi-Structured Tensors#\n\nYou can transform a dense tensor into a sparse semi-structured tensor by simply using thetorch.to_sparse_semi_structuredfunction.\ntorch.to_sparse_semi_structured\nPlease also note that we only support CUDA tensors since hardware compatibility for semi-structured sparsity is limited to NVIDIA GPUs.\nThe following datatypes are supported for semi-structured sparsity. Note that each datatype has its own shape constraints and compression factor.\nPyTorch dtype\nShape Constraints\nCompression Factor\nSparsity Pattern\ntorch.float16\ntorch.float16\nTensor must be 2D and (r, c) must both be a positive multiple of 64\n9/16\n2:4\ntorch.bfloat16\ntorch.bfloat16\nTensor must be 2D and (r, c) must both be a positive multiple of 64\n9/16\n2:4\ntorch.int8\ntorch.int8\nTensor must be 2D and (r, c) must both be a positive multiple of 128\n10/16\n2:4\nTo construct a semi-structured sparse tensor, start by creating a regular dense tensor that adheres to a 2:4 (or semi-structured) sparse format.\nTo do this we  tile a small 1x4 strip to create a 16x16 dense float16 tensor.\nAfterwards, we can callto_sparse_semi_structuredfunction to compress it for accelerated inference.\nto_sparse_semi_structured\n\n```python\n>>> from torch.sparse import to_sparse_semi_structured\n>>> A = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda()\ntensor([[0., 0., 1.,  ..., 0., 1., 1.],\n        [0., 0., 1.,  ..., 0., 1., 1.],\n        [0., 0., 1.,  ..., 0., 1., 1.],\n        ...,\n        [0., 0., 1.,  ..., 0., 1., 1.],\n        [0., 0., 1.,  ..., 0., 1., 1.],\n        [0., 0., 1.,  ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)\n>>> A_sparse = to_sparse_semi_structured(A)\nSparseSemiStructuredTensor(shape=torch.Size([128, 128]), transposed=False, values=tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16), metadata=tensor([[-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        ...,\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370]], device='cuda:0',\ndtype=torch.int16))\n\n```\n\n\n## Sparse Semi-Structured Tensor Operations#\n\nCurrently, the following operations are supported for semi-structured sparse tensors:\ntorch.addmm(bias, dense, sparse.t())\ntorch.mm(dense, sparse)\ntorch.mm(sparse, dense)\naten.linear.default(dense, sparse, bias)\naten.t.default(sparse)\naten.t.detach(sparse)\nTo use these ops, simply pass the output ofto_sparse_semi_structured(tensor)instead of usingtensoronce your tensor has 0s in a semi-structured sparse format, like this:\nto_sparse_semi_structured(tensor)\ntensor\n\n```python\n>>> a = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).half().cuda()\n>>> b = torch.rand(64, 64).half().cuda()\n>>> c = torch.mm(a, b)\n>>> a_sparse = to_sparse_semi_structured(a)\n>>> torch.allclose(c, torch.mm(a_sparse, b))\nTrue\n\n```\n\n\n## Accelerating nn.Linear with semi-structured sparsity#\n\nYou can accelerate the linear layers in your model if the weights are already semi-structured sparse with just a few lines of code:\n\n```python\n>>> input = torch.rand(64, 64).half().cuda()\n>>> mask = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool()\n>>> linear = nn.Linear(64, 64).half().cuda()\n>>> linear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0)))\n\n```\n\n\n## Sparse COO tensors#\n\nPyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular,\nthe indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64,\nindices\n(ndim,nse)\ntorch.int64\nthe corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type,\nvalues\n(nse,)\nwherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements.\nndim\nnse\nNote\nThe memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data).\n(ndim*8+<sizeofelementtypeinbytes>)*nse\nThe memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>.\nproduct(<tensorshape>)*<sizeofelementtypeinbytes>\nFor example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format.\n(2*8+4)*100000=2000000\n10000*10000*4=400000000\n\n## Construction#\n\nA sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor().\ntorch.sparse_coo_tensor()\nSuppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:\n\n```python\n>>> i = [[0, 1, 1], [2, 0, 2]]\n>>> v =  [3, 4, 5]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3, 4, 5]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n>>> s.to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n\n```\n\nNote that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:\ni\n\n```python\n>>> i = [[0, 2], [1, 0], [1, 2]]\n>>> v =  [3,      4,      5    ]\n>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\n>>> # Or another equivalent formulation to get s\n>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\n>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n\n```\n\nAn empty sparse COO tensor can be constructed by specifying its size\nonly:\n\n```python\n>>> torch.sparse_coo_tensor(size=(2, 3))\ntensor(indices=tensor([], size=(2, 0)),\n       values=tensor([], size=(0,)),\n       size=(2, 3), nnz=0, layout=torch.sparse_coo)\n\n```\n\n\n## Sparse hybrid COO tensors#\n\nPyTorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors.\nPyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave:\nvalues\nthe indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64,\nindices\n(sparse_dims,nse)\ntorch.int64\nthe corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type.\nvalues\n(nse,dense_dims)\nNote\nWe use (M + K)-dimensional tensor to denote a N-dimensional sparse\nhybrid tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.\nSuppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write\n\n```python\n>>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([[3, 4],\n                      [5, 6],\n                      [7, 8]]),\n       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)\n\n```\n\n\n```python\n>>> s.to_dense()\ntensor([[[0, 0],\n         [0, 0],\n         [3, 4]],\n        [[5, 6],\n         [0, 0],\n         [7, 8]]])\n\n```\n\nIn general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants:\ns\nM=s.sparse_dim()\nK=s.dense_dim()\nM+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions,\nM+K==len(s.shape)==s.ndim\ns.indices().shape==(M,nse)- sparse indices are stored\nexplicitly,\ns.indices().shape==(M,nse)\ns.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors,\ns.values().shape==(nse,)+s.shape[M:M+K]\ns.values().layout==torch.strided- values are stored as\nstrided tensors.\ns.values().layout==torch.strided\nNote\nDense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.\nNote\nTo be sure that a constructed sparse tensor has consistent indices,\nvalues, and size, the invariant checks can be enabled per tensor\ncreation viacheck_invariants=Truekeyword argument, or\nglobally usingtorch.sparse.check_sparse_tensor_invariantscontext manager instance. By default, the sparse tensor invariants\nchecks are disabled.\ncheck_invariants=True\ntorch.sparse.check_sparse_tensor_invariants\n\n## Uncoalesced sparse COO tensors#\n\nPyTorch sparse COO tensor format permits sparseuncoalescedtensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor:\n3\n4\n1\n\n```python\n>>> i = [[1, 1]]\n>>> v =  [3, 4]\n>>> s=torch.sparse_coo_tensor(i, v, (3,))\n>>> s\ntensor(indices=tensor([[1, 1]]),\n       values=tensor(  [3, 4]),\n       size=(3,), nnz=2, layout=torch.sparse_coo)\n\n```\n\nwhile the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:\n\n```python\n>>> s.coalesce()\ntensor(indices=tensor([[1]]),\n       values=tensor([7]),\n       size=(3,), nnz=1, layout=torch.sparse_coo)\n\n```\n\nIn general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties:\ntorch.Tensor.coalesce()\nthe indices of specified tensor elements are unique,\nthe indices are sorted in lexicographical order,\ntorch.Tensor.is_coalesced()returnsTrue.\ntorch.Tensor.is_coalesced()\nTrue\nNote\nFor the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a sparse coalesced or uncoalesced tensor.\nHowever, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.\nFor instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:\n\n```python\n>>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n>>> a + b\ntensor(indices=tensor([[0, 0, 1, 1]]),\n       values=tensor([7, 8, 5, 6]),\n       size=(2,), nnz=4, layout=torch.sparse_coo)\n\n```\n\nIf you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large.\ntorch.Tensor.add()\nOn the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products.\n\n## Working with sparse COO tensors#\n\nLet\u2019s consider the following example:\n\n```python\n>>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n\n```\n\nAs mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, one can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:\ntorch.Tensor\ntorch.Tensor.is_sparse\ntorch.Tensor.layout\n\n```python\n>>> isinstance(s, torch.Tensor)\nTrue\n>>> s.is_sparse\nTrue\n>>> s.layout == torch.sparse_coo\nTrue\n\n```\n\nThe number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance:\ntorch.Tensor.sparse_dim()\ntorch.Tensor.dense_dim()\n\n```python\n>>> s.sparse_dim(), s.dense_dim()\n(2, 1)\n\n```\n\nIfsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values().\ns\ntorch.Tensor.indices()\ntorch.Tensor.values()\nNote\nCurrently, one can acquire the COO format data only when the tensor\ninstance is coalesced:\n\n```python\n>>> s.indices()\nRuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first\n\n```\n\nFor acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices():\ntorch.Tensor._values()\ntorch.Tensor._indices()\n\n```python\n>>> s._indices()\ntensor([[0, 1, 1],\n        [2, 0, 2]])\n\n```\n\nWarning\nCallingtorch.Tensor._values()will return adetachedtensor.\nTo track gradients,torch.Tensor.coalesce().values()must be\nused instead.\ntorch.Tensor._values()\ntorch.Tensor.coalesce().values()\nConstructing a new sparse COO tensor results a tensor that is not\ncoalesced:\n\n```python\n>>> s.is_coalesced()\nFalse\n\n```\n\nbut one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:\ntorch.Tensor.coalesce()\n\n```python\n>>> s2 = s.coalesce()\n>>> s2.indices()\ntensor([[0, 1, 1],\n       [2, 0, 2]])\n\n```\n\nWhen working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on a sparse uncoalesced tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general.\nc*(a+b)==c*a+c*b\nsqrt(a+b)==sqrt(a)+sqrt(b)\nSlicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:\n\n```python\n>>> s[1]\ntensor(indices=tensor([[0, 2]]),\n       values=tensor([[5, 6],\n                      [7, 8]]),\n       size=(3, 2), nnz=2, layout=torch.sparse_coo)\n>>> s[1, 0, 1]\ntensor(6)\n>>> s[1, 0, 1:]\ntensor([6])\n\n```\n\nIn PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity.\ntorch.sparse.softmax()\n\n## Sparse Compressed Tensors#\n\nSparse Compressed Tensors represents a class of sparse tensors that\nhave a common feature of compressing the indices of a certain dimension\nusing an encoding that enables certain optimizations on linear algebra\nkernels of sparse compressed tensors. This encoding is based on theCompressed Sparse Row (CSR)format that PyTorch sparse compressed\ntensors extend with the support of sparse tensor batches, allowing\nmulti-dimensional tensor values, and storing sparse tensor values in\ndense blocks.\nNote\nWe use (B + M + K)-dimensional tensor to denote a N-dimensional\nsparse compressed hybrid tensor, where B, M, and K are the numbers\nof batch, sparse, and dense dimensions, respectively, such thatB+M+K==Nholds. The number of sparse dimensions for\nsparse compressed tensors is always two,M==2.\nB+M+K==N\nM==2\nNote\nWe say that an indices tensorcompressed_indicesuses CSR\nencoding if the following invariants are satisfied:\ncompressed_indices\ncompressed_indicesis a contiguous strided 32 or 64 bit\ninteger tensor\ncompressed_indices\ncompressed_indicesshape is(*batchsize,compressed_dim_size+1)wherecompressed_dim_sizeis the\nnumber of compressed dimensions (e.g. rows or columns)\ncompressed_indices\n(*batchsize,compressed_dim_size+1)\ncompressed_dim_size\ncompressed_indices[...,0]==0where...denotes batch\nindices\ncompressed_indices[...,0]==0\n...\ncompressed_indices[...,compressed_dim_size]==nsewherenseis the number of specified elements\ncompressed_indices[...,compressed_dim_size]==nse\nnse\n0<=compressed_indices[...,i]-compressed_indices[...,i-1]<=plain_dim_sizefori=1,...,compressed_dim_size,\nwhereplain_dim_sizeis the number of plain dimensions\n(orthogonal to compressed dimensions, e.g. columns or rows).\n0<=compressed_indices[...,i]-compressed_indices[...,i-1]<=plain_dim_size\ni=1,...,compressed_dim_size\nplain_dim_size\nTo be sure that a constructed sparse tensor has consistent indices,\nvalues, and size, the invariant checks can be enabled per tensor\ncreation viacheck_invariants=Truekeyword argument, or\nglobally usingtorch.sparse.check_sparse_tensor_invariantscontext manager instance. By default, the sparse tensor invariants\nchecks are disabled.\ncheck_invariants=True\ntorch.sparse.check_sparse_tensor_invariants\nNote\nThe generalization of sparse compressed layouts to N-dimensional\ntensors can lead to some confusion regarding the count of specified\nelements. When a sparse compressed tensor contains batch dimensions\nthe number of specified elements will correspond to the number of such\nelements per-batch. When a sparse compressed tensor has dense dimensions\nthe element considered is now the K-dimensional array. Also for block\nsparse compressed layouts the 2-D block is considered as the element\nbeing specified.  Take as an example a 3-dimensional block sparse\ntensor, with one batch dimension of lengthb, and a block\nshape ofp,q. If this tensor hasnspecified elements, then\nin fact we havenblocks specified per batch. This tensor would\nhavevalueswith shape(b,n,p,q). This interpretation of the\nnumber of specified elements comes from all sparse compressed layouts\nbeing derived from the compression of a 2-dimensional matrix. Batch\ndimensions are treated as stacking of sparse matrices, dense dimensions\nchange the meaning of the element from a simple scalar value to an\narray with its own dimensions.\nb\np,q\nn\nn\nvalues\n(b,n,p,q)\n\n## Sparse CSR Tensor#\n\nThe primary advantage of the CSR format over the COO format is better\nuse of storage and much faster computation operations such as sparse\nmatrix-vector multiplication using MKL and MAGMA backends.\nIn the simplest case, a (0 + 2 + 0)-dimensional sparse CSR tensor\nconsists of three 1-D tensors:crow_indices,col_indicesandvalues:\ncrow_indices\ncol_indices\nvalues\nThecrow_indicestensor consists of compressed row\nindices. This is a 1-D tensor of sizenrows+1(the number of\nrows plus 1). The last element ofcrow_indicesis the number\nof specified elements,nse. This tensor encodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the\nnumber before it denotes the number of elements in a given row.\ncrow_indices\nnrows+1\ncrow_indices\nnse\nvalues\ncol_indices\nThecol_indicestensor contains the column indices of each\nelement. This is a 1-D tensor of sizense.\ncol_indices\nnse\nThevaluestensor contains the values of the CSR tensor\nelements. This is a 1-D tensor of sizense.\nvalues\nnse\nNote\nThe index tensorscrow_indicesandcol_indicesshould have\nelement type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix operations,\nusetorch.int32. This is as a result of the default linking of\npytorch being with MKL LP64, which uses 32 bit integer indexing.\ncrow_indices\ncol_indices\ntorch.int64\ntorch.int32\ntorch.int32\nIn the general case, the (B + 2 + K)-dimensional sparse CSR tensor\nconsists of two (B + 1)-dimensional index tensorscrow_indicesandcol_indices, and of (1 + K)-dimensionalvaluestensor such\nthat\ncrow_indices\ncol_indices\nvalues\ncrow_indices.shape==(*batchsize,nrows+1)\ncrow_indices.shape==(*batchsize,nrows+1)\ncol_indices.shape==(*batchsize,nse)\ncol_indices.shape==(*batchsize,nse)\nvalues.shape==(nse,*densesize)\nvalues.shape==(nse,*densesize)\nwhile the shape of the sparse CSR tensor is(*batchsize,nrows,ncols,*densesize)wherelen(batchsize)==Bandlen(densesize)==K.\n(*batchsize,nrows,ncols,*densesize)\nlen(batchsize)==B\nlen(densesize)==K\nNote\nThe batches of sparse CSR tensors are dependent: the number of\nspecified elements in all batches must be the same. This somewhat\nartificial constraint allows efficient storage of the indices of\ndifferent CSR batches.\nNote\nThe number of sparse and dense dimensions can be acquired usingtorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim()methods. The batch dimensions can be computed from the tensor\nshape:batchsize=tensor.shape[:-tensor.sparse_dim()-tensor.dense_dim()].\ntorch.Tensor.sparse_dim()\ntorch.Tensor.dense_dim()\nbatchsize=tensor.shape[:-tensor.sparse_dim()-tensor.dense_dim()]\nNote\nThe memory consumption of a sparse CSR tensor is at least(nrows*8+(8+<sizeofelementtypeinbytes>*prod(densesize))*nse)*prod(batchsize)bytes (plus a constant\noverhead from storing other tensor data).\n(nrows*8+(8+<sizeofelementtypeinbytes>*prod(densesize))*nse)*prod(batchsize)\nWith the same example data ofthe note in sparse COO format\nintroduction, the memory consumption of a 10 000\nx 10 000 tensor with 100 000 non-zero 32-bit floating point numbers\nis at least(10000*8+(8+4*1)*100000)*1=1280000bytes when using CSR tensor layout. Notice the 1.6 and 310 fold\nsavings from using CSR storage format compared to using the COO and\nstrided formats, respectively.\n(10000*8+(8+4*1)*100000)*1=1280000\nSparse CSR tensors can be directly constructed by using thetorch.sparse_csr_tensor()function. The user must supply the row\nand column indices and values tensors separately where the row indices\nmust be specified using the CSR compression encoding.  Thesizeargument is optional and will be deduced from thecrow_indicesandcol_indicesif it is not present.\ntorch.sparse_csr_tensor()\nsize\ncrow_indices\ncol_indices\n\n```python\n>>> crow_indices = torch.tensor([0, 2, 4])\n>>> col_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\n>>> csr\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n       dtype=torch.float64)\n>>> csr.to_dense()\ntensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)\n\n```\n\nNote\nThe values of sparse dimensions in deducedsizeis computed\nfrom the size ofcrow_indicesand the maximal index value incol_indices. If the number of columns needs to be larger than\nin the deducedsizethen thesizeargument must be\nspecified explicitly.\nsize\ncrow_indices\ncol_indices\nsize\nsize\nThe simplest way of constructing a 2-D sparse CSR tensor from a\nstrided or sparse COO tensor is to usetorch.Tensor.to_sparse_csr()method. Any zeros in the (strided)\ntensor will be interpreted as missing values in the sparse tensor:\ntorch.Tensor.to_sparse_csr()\n\n```python\n>>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)\n>>> sp = a.to_sparse_csr()\n>>> sp\ntensor(crow_indices=tensor([0, 1, 3, 3]),\n      col_indices=tensor([2, 0, 1]),\n      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)\n\n```\n\nThe sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.\ntensor.matmul()\n\n```python\n>>> vec = torch.randn(4, 1, dtype=torch.float64)\n>>> sp.matmul(vec)\ntensor([[0.9078],\n        [1.3180],\n        [0.0000]], dtype=torch.float64)\n\n```\n\n\n## Sparse CSC Tensor#\n\nThe sparse CSC (Compressed Sparse Column) tensor format implements the\nCSC format for storage of 2 dimensional tensors with an extension to\nsupporting batches of sparse CSC tensors and values being\nmulti-dimensional tensors.\nNote\nSparse CSC tensor is essentially a transpose of the sparse CSR\ntensor when the transposition is about swapping the sparse\ndimensions.\nSimilarly tosparse CSR tensors, a sparse CSC\ntensor consists of three tensors:ccol_indices,row_indicesandvalues:\nccol_indices\nrow_indices\nvalues\nTheccol_indicestensor consists of compressed column\nindices. This is a (B + 1)-D tensor of shape(*batchsize,ncols+1).\nThe last element is the number of specified\nelements,nse. This tensor encodes the index invaluesandrow_indicesdepending on where the given column starts. Each\nsuccessive number in the tensor subtracted by the number before it\ndenotes the number of elements in a given column.\nccol_indices\n(*batchsize,ncols+1)\nnse\nvalues\nrow_indices\nTherow_indicestensor contains the row indices of each\nelement. This is a (B + 1)-D tensor of shape(*batchsize,nse).\nrow_indices\n(*batchsize,nse)\nThevaluestensor contains the values of the CSC tensor\nelements. This is a (1 + K)-D tensor of shape(nse,*densesize).\nvalues\n(nse,*densesize)\nSparse CSC tensors can be directly constructed by using thetorch.sparse_csc_tensor()function. The user must supply the row\nand column indices and values tensors separately where the column indices\nmust be specified using the CSR compression encoding.  Thesizeargument is optional and will be deduced from therow_indicesandccol_indicestensors if it is not present.\ntorch.sparse_csc_tensor()\nsize\nrow_indices\nccol_indices\n\n```python\n>>> ccol_indices = torch.tensor([0, 2, 4])\n>>> row_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csc = torch.sparse_csc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)\n>>> csc\ntensor(ccol_indices=tensor([0, 2, 4]),\n       row_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n       dtype=torch.float64, layout=torch.sparse_csc)\n>>> csc.to_dense()\ntensor([[1., 3.],\n        [2., 4.]], dtype=torch.float64)\n\n```\n\nNote\nThe sparse CSC tensor constructor function has the compressed\ncolumn indices argument before the row indices argument.\nThe (0 + 2 + 0)-dimensional sparse CSC tensors can be constructed from\nany two-dimensional tensor usingtorch.Tensor.to_sparse_csc()method. Any zeros in the (strided) tensor will be interpreted as\nmissing values in the sparse tensor:\ntorch.Tensor.to_sparse_csc()\n\n```python\n>>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)\n>>> sp = a.to_sparse_csc()\n>>> sp\ntensor(ccol_indices=tensor([0, 1, 2, 3, 3]),\n       row_indices=tensor([1, 1, 0]),\n       values=tensor([1., 2., 1.]), size=(3, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_csc)\n\n```\n\n\n## Sparse BSR Tensor#\n\nThe sparse BSR (Block compressed Sparse Row) tensor format implements the\nBSR format for storage of two-dimensional tensors with an extension to\nsupporting batches of sparse BSR tensors and values being blocks of\nmulti-dimensional tensors.\nA sparse BSR tensor consists of three tensors:crow_indices,col_indicesandvalues:\ncrow_indices\ncol_indices\nvalues\nThecrow_indicestensor consists of compressed row\nindices. This is a (B + 1)-D tensor of shape(*batchsize,nrowblocks+1).  The last element is the number of specified blocks,nse. This tensor encodes the index invaluesandcol_indicesdepending on where the given column block\nstarts. Each successive number in the tensor subtracted by the\nnumber before it denotes the number of blocks in a given row.\ncrow_indices\n(*batchsize,nrowblocks+1)\nnse\nvalues\ncol_indices\nThecol_indicestensor contains the column block indices of each\nelement. This is a (B + 1)-D tensor of shape(*batchsize,nse).\ncol_indices\n(*batchsize,nse)\nThevaluestensor contains the values of the sparse BSR tensor\nelements collected into two-dimensional blocks. This is a (1 + 2 +\nK)-D tensor of shape(nse,nrowblocks,ncolblocks,*densesize).\nvalues\n(nse,nrowblocks,ncolblocks,*densesize)\nSparse BSR tensors can be directly constructed by using thetorch.sparse_bsr_tensor()function. The user must supply the row\nand column block indices and values tensors separately where the row block indices\nmust be specified using the CSR compression encoding.\nThesizeargument is optional and will be deduced from thecrow_indicesandcol_indicestensors if it is not present.\ntorch.sparse_bsr_tensor()\nsize\ncrow_indices\ncol_indices\n\n```python\n>>> crow_indices = torch.tensor([0, 2, 4])\n>>> col_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],\n...                        [[3, 4, 5], [9, 10, 11]],\n...                        [[12, 13, 14], [18, 19, 20]],\n...                        [[15, 16, 17], [21, 22, 23]]])\n>>> bsr = torch.sparse_bsr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\n>>> bsr\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([[[ 0.,  1.,  2.],\n                       [ 6.,  7.,  8.]],\n                      [[ 3.,  4.,  5.],\n                       [ 9., 10., 11.]],\n                      [[12., 13., 14.],\n                       [18., 19., 20.]],\n                      [[15., 16., 17.],\n                       [21., 22., 23.]]]),\n       size=(4, 6), nnz=4, dtype=torch.float64, layout=torch.sparse_bsr)\n>>> bsr.to_dense()\ntensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9., 10., 11.],\n        [12., 13., 14., 15., 16., 17.],\n        [18., 19., 20., 21., 22., 23.]], dtype=torch.float64)\n\n```\n\nThe (0 + 2 + 0)-dimensional sparse BSR tensors can be constructed from\nany two-dimensional tensor usingtorch.Tensor.to_sparse_bsr()method that also requires the specification of the values block size:\ntorch.Tensor.to_sparse_bsr()\n\n```python\n>>> dense = torch.tensor([[0, 1, 2, 3, 4, 5],\n...                       [6, 7, 8, 9, 10, 11],\n...                       [12, 13, 14, 15, 16, 17],\n...                       [18, 19, 20, 21, 22, 23]])\n>>> bsr = dense.to_sparse_bsr(blocksize=(2, 3))\n>>> bsr\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([[[ 0,  1,  2],\n                       [ 6,  7,  8]],\n                      [[ 3,  4,  5],\n                       [ 9, 10, 11]],\n                      [[12, 13, 14],\n                       [18, 19, 20]],\n                      [[15, 16, 17],\n                       [21, 22, 23]]]), size=(4, 6), nnz=4,\n       layout=torch.sparse_bsr)\n\n```\n\n\n## Sparse BSC Tensor#\n\nThe sparse BSC (Block compressed Sparse Column) tensor format implements the\nBSC format for storage of two-dimensional tensors with an extension to\nsupporting batches of sparse BSC tensors and values being blocks of\nmulti-dimensional tensors.\nA sparse BSC tensor consists of three tensors:ccol_indices,row_indicesandvalues:\nccol_indices\nrow_indices\nvalues\nTheccol_indicestensor consists of compressed column\nindices. This is a (B + 1)-D tensor of shape(*batchsize,ncolblocks+1).  The last element is the number of specified blocks,nse. This tensor encodes the index invaluesandrow_indicesdepending on where the given row block\nstarts. Each successive number in the tensor subtracted by the\nnumber before it denotes the number of blocks in a given column.\nccol_indices\n(*batchsize,ncolblocks+1)\nnse\nvalues\nrow_indices\nTherow_indicestensor contains the row block indices of each\nelement. This is a (B + 1)-D tensor of shape(*batchsize,nse).\nrow_indices\n(*batchsize,nse)\nThevaluestensor contains the values of the sparse BSC tensor\nelements collected into two-dimensional blocks. This is a (1 + 2 +\nK)-D tensor of shape(nse,nrowblocks,ncolblocks,*densesize).\nvalues\n(nse,nrowblocks,ncolblocks,*densesize)\nSparse BSC tensors can be directly constructed by using thetorch.sparse_bsc_tensor()function. The user must supply the row\nand column block indices and values tensors separately where the column block indices\nmust be specified using the CSR compression encoding.\nThesizeargument is optional and will be deduced from theccol_indicesandrow_indicestensors if it is not present.\ntorch.sparse_bsc_tensor()\nsize\nccol_indices\nrow_indices\n\n```python\n>>> ccol_indices = torch.tensor([0, 2, 4])\n>>> row_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],\n...                        [[3, 4, 5], [9, 10, 11]],\n...                        [[12, 13, 14], [18, 19, 20]],\n...                        [[15, 16, 17], [21, 22, 23]]])\n>>> bsc = torch.sparse_bsc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)\n>>> bsc\ntensor(ccol_indices=tensor([0, 2, 4]),\n       row_indices=tensor([0, 1, 0, 1]),\n       values=tensor([[[ 0.,  1.,  2.],\n                       [ 6.,  7.,  8.]],\n                      [[ 3.,  4.,  5.],\n                       [ 9., 10., 11.]],\n                      [[12., 13., 14.],\n                       [18., 19., 20.]],\n                      [[15., 16., 17.],\n                       [21., 22., 23.]]]), size=(4, 6), nnz=4,\n       dtype=torch.float64, layout=torch.sparse_bsc)\n\n```\n\n\n## Tools for working with sparse compressed tensors#\n\nAll sparse compressed tensors \u2014 CSR, CSC, BSR, and BSC tensors \u2014\nare conceptually very similar in that their indices data is split\ninto two parts: so-called compressed indices that use the CSR\nencoding, and so-called plain indices that are orthogonal to the\ncompressed indices. This allows various tools on these tensors to\nshare the same implementations that are parameterized by tensor\nlayout.\nSparse CSR, CSC, BSR, and CSC tensors can be constructed by usingtorch.sparse_compressed_tensor()function that have the same\ninterface as the above discussed constructor functionstorch.sparse_csr_tensor(),torch.sparse_csc_tensor(),torch.sparse_bsr_tensor(), andtorch.sparse_bsc_tensor(),\nrespectively, but with an extra requiredlayoutargument. The\nfollowing example illustrates a method of constructing CSR and CSC\ntensors using the same input data by specifying the corresponding\nlayout parameter to thetorch.sparse_compressed_tensor()function:\ntorch.sparse_compressed_tensor()\ntorch.sparse_csr_tensor()\ntorch.sparse_csc_tensor()\ntorch.sparse_bsr_tensor()\ntorch.sparse_bsc_tensor()\nlayout\ntorch.sparse_compressed_tensor()\n\n```python\n>>> compressed_indices = torch.tensor([0, 2, 4])\n>>> plain_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csr = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csr)\n>>> csr\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,\n       layout=torch.sparse_csr)\n>>> csc = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csc)\n>>> csc\ntensor(ccol_indices=tensor([0, 2, 4]),\n       row_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,\n       layout=torch.sparse_csc)\n>>> (csr.transpose(0, 1).to_dense() == csc.to_dense()).all()\ntensor(True)\n\n```\n\n\n## Supported operations#\n\n\n## Linear Algebra operations#\n\nThe following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication.\nT[layout]\nM[layout]\nV[layout]\nf\n*\n@\nPyTorch operation\nSparse grad?\nLayout signature\ntorch.mv()\ntorch.mv()\nno\nM[sparse_coo]@V[strided]->V[strided]\nM[sparse_coo]@V[strided]->V[strided]\ntorch.mv()\ntorch.mv()\nno\nM[sparse_csr]@V[strided]->V[strided]\nM[sparse_csr]@V[strided]->V[strided]\ntorch.matmul()\ntorch.matmul()\nno\nM[sparse_coo]@M[strided]->M[strided]\nM[sparse_coo]@M[strided]->M[strided]\ntorch.matmul()\ntorch.matmul()\nno\nM[sparse_csr]@M[strided]->M[strided]\nM[sparse_csr]@M[strided]->M[strided]\ntorch.matmul()\ntorch.matmul()\nno\nM[SparseSemiStructured]@M[strided]->M[strided]\nM[SparseSemiStructured]@M[strided]->M[strided]\ntorch.matmul()\ntorch.matmul()\nno\nM[strided]@M[SparseSemiStructured]->M[strided]\nM[strided]@M[SparseSemiStructured]->M[strided]\ntorch.mm()\ntorch.mm()\nno\nM[sparse_coo]@M[strided]->M[strided]\nM[sparse_coo]@M[strided]->M[strided]\ntorch.mm()\ntorch.mm()\nno\nM[SparseSemiStructured]@M[strided]->M[strided]\nM[SparseSemiStructured]@M[strided]->M[strided]\ntorch.mm()\ntorch.mm()\nno\nM[strided]@M[SparseSemiStructured]->M[strided]\nM[strided]@M[SparseSemiStructured]->M[strided]\ntorch.sparse.mm()\ntorch.sparse.mm()\nyes\nM[sparse_coo]@M[strided]->M[strided]\nM[sparse_coo]@M[strided]->M[strided]\ntorch.smm()\ntorch.smm()\nno\nM[sparse_coo]@M[strided]->M[sparse_coo]\nM[sparse_coo]@M[strided]->M[sparse_coo]\ntorch.hspmm()\ntorch.hspmm()\nno\nM[sparse_coo]@M[strided]->M[hybridsparse_coo]\nM[sparse_coo]@M[strided]->M[hybridsparse_coo]\ntorch.bmm()\ntorch.bmm()\nno\nT[sparse_coo]@T[strided]->T[strided]\nT[sparse_coo]@T[strided]->T[strided]\ntorch.addmm()\ntorch.addmm()\nno\nf*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]\nf*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]\ntorch.addmm()\ntorch.addmm()\nno\nf*M[strided]+f*(M[SparseSemiStructured]@M[strided])->M[strided]\nf*M[strided]+f*(M[SparseSemiStructured]@M[strided])->M[strided]\ntorch.addmm()\ntorch.addmm()\nno\nf*M[strided]+f*(M[strided]@M[SparseSemiStructured])->M[strided]\nf*M[strided]+f*(M[strided]@M[SparseSemiStructured])->M[strided]\ntorch.sparse.addmm()\ntorch.sparse.addmm()\nyes\nf*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]\nf*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]\ntorch.sparse.spsolve()\ntorch.sparse.spsolve()\nno\nSOLVE(M[sparse_csr],V[strided])->V[strided]\nSOLVE(M[sparse_csr],V[strided])->V[strided]\ntorch.sspaddmm()\ntorch.sspaddmm()\nno\nf*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo]\nf*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo]\ntorch.lobpcg()\ntorch.lobpcg()\nno\nGENEIG(M[sparse_coo])->M[strided],M[strided]\nGENEIG(M[sparse_coo])->M[strided],M[strided]\ntorch.pca_lowrank()\ntorch.pca_lowrank()\nyes\nPCA(M[sparse_coo])->M[strided],M[strided],M[strided]\nPCA(M[sparse_coo])->M[strided],M[strided],M[strided]\ntorch.svd_lowrank()\ntorch.svd_lowrank()\nyes\nSVD(M[sparse_coo])->M[strided],M[strided],M[strided]\nSVD(M[sparse_coo])->M[strided],M[strided],M[strided]\nwhere \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments.\ntorch.smm()\nNote\nCurrently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t().\nM[strided]@M[sparse_coo]\nD@S==(S.t()@D.t()).t()\n\n## Tensor methods and sparse#\n\nThe following Tensor methods are related to sparse tensors:\nTensor.is_sparse\nTensor.is_sparse\nIsTrueif the Tensor uses sparse COO storage layout,Falseotherwise.\nTrue\nFalse\nTensor.is_sparse_csr\nTensor.is_sparse_csr\nIsTrueif the Tensor uses sparse CSR storage layout,Falseotherwise.\nTrue\nFalse\nTensor.dense_dim\nTensor.dense_dim\nReturn the number of dense dimensions in asparse tensorself.\nself\nTensor.sparse_dim\nTensor.sparse_dim\nReturn the number of sparse dimensions in asparse tensorself.\nself\nTensor.sparse_mask\nTensor.sparse_mask\nReturns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask.\nself\nmask\nTensor.to_sparse\nTensor.to_sparse\nReturns a sparse copy of the tensor.\nTensor.to_sparse_coo\nTensor.to_sparse_coo\nConvert a tensor tocoordinate format.\nTensor.to_sparse_csr\nTensor.to_sparse_csr\nConvert a tensor to compressed row storage format (CSR).\nTensor.to_sparse_csc\nTensor.to_sparse_csc\nConvert a tensor to compressed column storage (CSC) format.\nTensor.to_sparse_bsr\nTensor.to_sparse_bsr\nConvert a tensor to a block sparse row (BSR) storage format of given blocksize.\nTensor.to_sparse_bsc\nTensor.to_sparse_bsc\nConvert a tensor to a block sparse column (BSC) storage format of given blocksize.\nTensor.to_dense\nTensor.to_dense\nCreates a strided copy ofselfifselfis not a strided tensor, otherwise returnsself.\nself\nself\nself\nTensor.values\nTensor.values\nReturn the values tensor of asparse COO tensor.\nThe following Tensor methods are specific to sparse COO tensors:\nTensor.coalesce\nTensor.coalesce\nReturns a coalesced copy ofselfifselfis anuncoalesced tensor.\nself\nself\nTensor.sparse_resize_\nTensor.sparse_resize_\nResizesselfsparse tensorto the desired size and the number of sparse and dense dimensions.\nself\nTensor.sparse_resize_and_clear_\nTensor.sparse_resize_and_clear_\nRemoves all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions.\nself\nself\nTensor.is_coalesced\nTensor.is_coalesced\nReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise.\nTrue\nself\nFalse\nTensor.indices\nTensor.indices\nReturn the indices tensor of asparse COO tensor.\nThe following methods are specific tosparse CSR tensorsandsparse BSR tensors:\nTensor.crow_indices\nTensor.crow_indices\nReturns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.\nself\nself\nsparse_csr\nTensor.col_indices\nTensor.col_indices\nReturns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.\nself\nself\nsparse_csr\nThe following methods are specific tosparse CSC tensorsandsparse BSC tensors:\nTensor.row_indices\nTensor.row_indices\n\nTensor.ccol_indices\nTensor.ccol_indices\n\nThe following Tensor methods support sparse COO tensors:\nadd()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()\nadd()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()\n\n## Torch functions specific to sparse Tensors#\n\nsparse_coo_tensor\n\nsparse_coo_tensor\nConstructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.\nindices\nsparse_csr_tensor\n\nsparse_csr_tensor\nConstructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices.\ncrow_indices\ncol_indices\nsparse_csc_tensor\n\nsparse_csc_tensor\nConstructs asparse tensor in CSC (Compressed Sparse Column)with specified values at the givenccol_indicesandrow_indices.\nccol_indices\nrow_indices\nsparse_bsr_tensor\n\nsparse_bsr_tensor\nConstructs asparse tensor in BSR (Block Compressed Sparse Row))with specified 2-dimensional blocks at the givencrow_indicesandcol_indices.\ncrow_indices\ncol_indices\nsparse_bsc_tensor\n\nsparse_bsc_tensor\nConstructs asparse tensor in BSC (Block Compressed Sparse Column))with specified 2-dimensional blocks at the givenccol_indicesandrow_indices.\nccol_indices\nrow_indices\nsparse_compressed_tensor\n\nsparse_compressed_tensor\nConstructs asparse tensor in Compressed Sparse format - CSR, CSC, BSR, or BSC -with specified values at the givencompressed_indicesandplain_indices.\ncompressed_indices\nplain_indices\nsparse.sum\nsparse.sum\nReturn the sum of each row of the given sparse tensor.\nsparse.addmm\nsparse.addmm\nThis function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse COO matrixmat1.\ntorch.addmm()\nmat1\nsparse.sampled_addmm\nsparse.sampled_addmm\nPerforms a matrix multiplication of the dense matricesmat1andmat2at the locations specified by the sparsity pattern ofinput.\nmat1\nmat2\ninput\nsparse.mm\nsparse.mm\nPerforms a matrix multiplication of the sparse matrixmat1\nmat1\nsspaddmm\n\nsspaddmm\nMatrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.\nmat1\nmat2\ninput\nhspmm\n\nhspmm\nPerforms a matrix multiplication of asparse COO matrixmat1and a strided matrixmat2.\nmat1\nmat2\nsmm\n\nsmm\nPerforms a matrix multiplication of the sparse matrixinputwith the dense matrixmat.\ninput\nmat\nsparse.softmax\nsparse.softmax\nApplies a softmax function.\nsparse.spsolve\nsparse.spsolve\nComputes the solution of a square system of linear equations with a unique solution.\nsparse.log_softmax\nsparse.log_softmax\nApplies a softmax function followed by logarithm.\nsparse.spdiags\nsparse.spdiags\nCreates a sparse 2D tensor by placing the values from rows ofdiagonalsalong specified diagonals of the output\ndiagonals\n\n## Other functions#\n\nThe followingtorchfunctions support sparse tensors:\ntorch\ncat()dstack()empty()empty_like()hstack()index_select()is_complex()is_floating_point()is_nonzero()is_same_size()is_signed()is_tensor()lobpcg()mm()native_norm()pca_lowrank()select()stack()svd_lowrank()unsqueeze()vstack()zeros()zeros_like()\ncat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like()\nTo manage checking sparse tensor invariants, see:\nsparse.check_sparse_tensor_invariants\nsparse.check_sparse_tensor_invariants\nA tool to control checking sparse tensor invariants.\nTo use sparse tensors withgradcheck()function,\nsee:\ngradcheck()\nsparse.as_sparse_gradcheck\nsparse.as_sparse_gradcheck\nDecorate function, to extend gradcheck for sparse tensors.\n\n## Zero-preserving unary functions#\n\nWe aim to support all \u2018zero-preserving unary functions\u2019: functions of one argument that map zero to zero.\nIf you find that we are missing a zero-preserving unary function\nthat you need, please feel encouraged to open an issue for a feature request.\nAs always please kindly try the search function first before opening an issue.\nThe following operators currently support sparse COO/CSR/CSC/BSR/CSR tensor inputs.\nabs()asin()asinh()atan()atanh()ceil()conj_physical()floor()log1p()neg()round()sin()sinh()sign()sgn()signbit()tan()tanh()trunc()expm1()sqrt()angle()isinf()isposinf()isneginf()isnan()erf()erfinv()\nabs()\nasin()\nasinh()\natan()\natanh()\nceil()\nconj_physical()\nfloor()\nlog1p()\nneg()\nround()\nsin()\nsinh()\nsign()\nsgn()\nsignbit()\ntan()\ntanh()\ntrunc()\nexpm1()\nsqrt()\nangle()\nisinf()\nisposinf()\nisneginf()\nisnan()\nerf()\nerfinv()",
  "url": "https://pytorch.org/docs/stable/sparse.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
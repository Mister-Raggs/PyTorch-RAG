{
  "doc_id": "dde136a3f7bc89b1ec3348f05285d988",
  "source": "pytorch_docs",
  "title": "torch.compiler \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.compiler#\n\nCreated On: Jul 28, 2023 | Last Updated On: Jul 28, 2025\ntorch.compileris a namespace through which some of the internal compiler\nmethods are surfaced for user consumption. The main function and the feature in\nthis namespace istorch.compile.\ntorch.compiler\ntorch.compile\ntorch.compileis a PyTorch function introduced in PyTorch 2.x that aims to\nsolve the problem of accurate graph capturing in PyTorch and ultimately enable\nsoftware engineers to run their PyTorch programs faster.torch.compileis\nwritten in Python and it marks the transition of PyTorch from C++ to Python.\ntorch.compile\ntorch.compile\ntorch.compileleverages the following underlying technologies:\ntorch.compile\nTorchDynamo (torch._dynamo)is an internal API that uses a CPython\nfeature called the Frame Evaluation API to safely capture PyTorch graphs.\nMethods that are available externally for PyTorch users are surfaced\nthrough thetorch.compilernamespace.\ntorch.compiler\nTorchInductoris the defaulttorch.compiledeep learning compiler\nthat generates fast code for multiple accelerators and backends. You\nneed to use a backend compiler to make speedups throughtorch.compilepossible. For NVIDIA, AMD and Intel GPUs, it leverages OpenAI Triton as the key\nbuilding block.\ntorch.compile\ntorch.compile\nAOT Autogradcaptures not only the user-level code, but also backpropagation,\nwhich results in capturing the backwards pass \u201cahead-of-time\u201d. This enables\nacceleration of both forwards and backwards pass using TorchInductor.\nTo better understand howtorch.compiletracing behavior on your code, or to\nlearn more about the internals oftorch.compile, please refer to thetorch.compileprogramming model.\ntorch.compile\ntorch.compile\ntorch.compile\nNote\nIn some cases, the termstorch.compile, TorchDynamo,torch.compilermight be used interchangeably in this documentation.\ntorch.compile\ntorch.compiler\nAs mentioned above, to run your workflows faster,torch.compilethrough\nTorchDynamo requires a backend that converts the captured graphs into a fast\nmachine code. Different backends can result in various optimization gains.\nThe default backend is called TorchInductor, also known asinductor,\nTorchDynamo has a list of supported backends developed by our partners,\nwhich can be seen by runningtorch.compiler.list_backends()each of which\nwith its optional dependencies.\ntorch.compile\ntorch.compiler.list_backends()\nSome of the most commonly used backends include:\nTraining & inference backends\nBackend\nDescription\ntorch.compile(m,backend=\"inductor\")\ntorch.compile(m,backend=\"inductor\")\nUses the TorchInductor backend.Read more\ntorch.compile(m,backend=\"cudagraphs\")\ntorch.compile(m,backend=\"cudagraphs\")\nCUDA graphs with AOT Autograd.Read more\ntorch.compile(m,backend=\"ipex\")\ntorch.compile(m,backend=\"ipex\")\nUses IPEX on CPU.Read more\nInference-only backends\nBackend\nDescription\ntorch.compile(m,backend=\"tensorrt\")\ntorch.compile(m,backend=\"tensorrt\")\nUses Torch-TensorRT for inference optimizations. Requiresimporttorch_tensorrtin the calling script to register backend.Read more\nimporttorch_tensorrt\ntorch.compile(m,backend=\"ipex\")\ntorch.compile(m,backend=\"ipex\")\nUses IPEX for inference on CPU.Read more\ntorch.compile(m,backend=\"tvm\")\ntorch.compile(m,backend=\"tvm\")\nUses Apache TVM for inference optimizations.Read more\ntorch.compile(m,backend=\"openvino\")\ntorch.compile(m,backend=\"openvino\")\nUses OpenVINO for inference optimizations.Read more\n\n## Read More#\n\nGetting Started for PyTorch Users\ntorch.compile\n`torch.compile` Programming Model\nfullgraph=True\ntorch._dynamo.nonstrict_trace\nfullgraph=False\ncompile(model)\nmodel.compile()\nerror_on_graph_break\nerror_on_graph_break(False)\nerror_on_graph_break(True)\nerror_on_graph_break\nfullgraph\nfullgraph=True/False\nerror_on_graph_break\nDeep Dive for PyTorch Developers\nHowTo for PyTorch Backend Vendors",
  "url": "https://pytorch.org/docs/stable/torch.compiler.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
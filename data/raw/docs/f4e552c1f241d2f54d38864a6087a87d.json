{
  "doc_id": "f4e552c1f241d2f54d38864a6087a87d",
  "source": "pytorch_docs",
  "title": "Automatic Mixed Precision package - torch.amp \u2014 PyTorch 2.9 documentation",
  "text": "\n## Automatic Mixed Precision package - torch.amp#\n\nCreated On: Jun 12, 2025 | Last Updated On: Jun 12, 2025\ntorch.ampprovides convenience methods for mixed precision,\nwhere some operations use thetorch.float32(float) datatype and other operations\nuse lower precision floating point datatype (lower_precision_fp):torch.float16(half) ortorch.bfloat16. Some ops, like linear layers and convolutions,\nare much faster inlower_precision_fp. Other ops, like reductions, often require the dynamic\nrange offloat32. Mixed precision tries to match each op to its appropriate datatype.\ntorch.amp\ntorch.float32\nfloat\nlower_precision_fp\ntorch.float16\nhalf\ntorch.bfloat16\nlower_precision_fp\nfloat32\nOrdinarily, \u201cautomatic mixed precision training\u201d with datatype oftorch.float16usestorch.autocastandtorch.amp.GradScalertogether, as shown in theAutomatic Mixed Precision examplesandAutomatic Mixed Precision recipe.\nHowever,torch.autocastandtorch.GradScalerare modular, and may be used separately if desired.\nAs shown in the CPU example section oftorch.autocast, \u201cautomatic mixed precision training/inference\u201d on CPU with\ndatatype oftorch.bfloat16only usestorch.autocast.\ntorch.float16\ntorch.autocast\ntorch.amp.GradScaler\ntorch.autocast\ntorch.GradScaler\ntorch.autocast\ntorch.bfloat16\ntorch.autocast\nWarning\ntorch.cuda.amp.autocast(args...)andtorch.cpu.amp.autocast(args...)is deprecated. Please usetorch.amp.autocast(\"cuda\",args...)ortorch.amp.autocast(\"cpu\",args...)instead.torch.cuda.amp.GradScaler(args...)andtorch.cpu.amp.GradScaler(args...)is deprecated. Please usetorch.amp.GradScaler(\"cuda\",args...)ortorch.amp.GradScaler(\"cpu\",args...)instead.\ntorch.cuda.amp.autocast(args...)\ntorch.cpu.amp.autocast(args...)\ntorch.amp.autocast(\"cuda\",args...)\ntorch.amp.autocast(\"cpu\",args...)\ntorch.cuda.amp.GradScaler(args...)\ntorch.cpu.amp.GradScaler(args...)\ntorch.amp.GradScaler(\"cuda\",args...)\ntorch.amp.GradScaler(\"cpu\",args...)\ntorch.autocastandtorch.cpu.amp.autocastare new in version1.10.\ntorch.autocast\ntorch.cpu.amp.autocast\n1.10\nAutocasting\nGradient Scaling\nAutocast Op Reference\nOp Eligibility\nCUDA Op-Specific Behavior\nCUDA Ops that can autocast tofloat16\nfloat16\nCUDA Ops that can autocast tofloat32\nfloat32\nCUDA Ops that promote to the widest input type\nPreferbinary_cross_entropy_with_logitsoverbinary_cross_entropy\nbinary_cross_entropy_with_logits\nbinary_cross_entropy\nXPU Op-Specific Behavior (Experimental)\nXPU Ops that can autocast tofloat16\nfloat16\nXPU Ops that can autocast tofloat32\nfloat32\nXPU Ops that promote to the widest input type\nCPU Op-Specific Behavior\nCPU Ops that can autocast tobfloat16\nbfloat16\nCPU Ops that can autocast tofloat32\nfloat32\nCPU Ops that promote to the widest input type\n\n## Autocasting#\n\nReturn a bool indicating if autocast is available ondevice_type.\ndevice_type\ndevice_type(str) \u2013 Device type to use. Possible values are: \u2018cuda\u2019, \u2018cpu\u2019, \u2018mtia\u2019, \u2018maia\u2019, \u2018xpu\u2019, and so on.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\nbool\nInstances ofautocastserve as context managers or decorators that\nallow regions of your script to run in mixed precision.\nautocast\nIn these regions, ops run in an op-specific dtype chosen by autocast\nto improve performance while maintaining accuracy.\nSee theAutocast Op Referencefor details.\nWhen entering an autocast-enabled region, Tensors may be any type.\nYou should not callhalf()orbfloat16()on your model(s) or inputs when using autocasting.\nhalf()\nbfloat16()\nautocastshould wrap only the forward pass(es) of your network, including the loss\ncomputation(s).  Backward passes under autocast are not recommended.\nBackward ops run in the same type that autocast used for corresponding forward ops.\nautocast\nExample for CUDA Devices:\n\n```python\n# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with torch.autocast(device_type=\"cuda\"):\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n\n```\n\nSee theAutomatic Mixed Precision examplesfor usage (along with gradient scaling)\nin more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).\nautocastcan also be used as a decorator, e.g., on theforwardmethod of your model:\nautocast\nforward\n\n```python\nclass AutocastModel(nn.Module):\n    ...\n\n    @torch.autocast(device_type=\"cuda\")\n    def forward(self, input): ...\n\n```\n\nFloating-point Tensors produced in an autocast-enabled region may befloat16.\nAfter returning to an autocast-disabled region, using them with floating-point\nTensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)\nproduced in the autocast region back tofloat32(or other dtype if desired).\nIf a Tensor from the autocast region is alreadyfloat32, the cast is a no-op,\nand incurs no additional overhead.\nCUDA Example:\nfloat16\nfloat32\nfloat32\n\n```python\n# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n\n```\n\nCPU Training Example:\n\n```python\n# Creates model and optimizer in default precision\nmodel = Net()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Runs the forward pass with autocasting.\n        with torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        loss.backward()\n        optimizer.step()\n\n```\n\nCPU Inference Example:\n\n```python\n# Creates model in default precision\nmodel = Net().eval()\n\nwith torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n    for input in data:\n        # Runs the forward pass with autocasting.\n        output = model(input)\n\n```\n\nCPU Inference Example with Jit Trace:\n\n```python\nclass TestModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        return self.fc1(x)\n\n\ninput_size = 2\nnum_classes = 2\nmodel = TestModel(input_size, num_classes).eval()\n\n# For now, we suggest to disable the Jit Autocast Pass,\n# As the issue: https://github.com/pytorch/pytorch/issues/75956\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.cpu.amp.autocast(cache_enabled=False):\n    model = torch.jit.trace(model, torch.randn(1, input_size))\nmodel = torch.jit.freeze(model)\n# Models Run\nfor _ in range(3):\n    model(torch.randn(1, input_size))\n\n```\n\nType mismatch errorsinan autocast-enabled region are a bug; if this is what you observe,\nplease file an issue.\nautocast(enabled=False)subregions can be nested in autocast-enabled regions.\nLocally disabling autocast can be useful, for example, if you want to force a subregion\nto run in a particulardtype.  Disabling autocast gives you explicit control over\nthe execution type.  In the subregion, inputs from the surrounding region\nshould be cast todtypebefore use:\nautocast(enabled=False)\ndtype\ndtype\n\n```python\n# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    e_float16 = torch.mm(a_float32, b_float32)\n    with torch.autocast(device_type=\"cuda\", enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n\n```\n\nThe autocast state is thread-local.  If you want it enabled in a new thread, the context manager or decorator\nmust be invoked in that thread.  This affectstorch.nn.DataParallelandtorch.nn.parallel.DistributedDataParallelwhen used with more than one GPU per process\n(seeWorking with Multiple GPUs).\ntorch.nn.DataParallel\ntorch.nn.parallel.DistributedDataParallel\ndevice_type(str,required) \u2013 Device type to use. Possible values are: \u2018cuda\u2019, \u2018cpu\u2019, \u2018mtia\u2019, \u2018maia\u2019, \u2018xpu\u2019, and \u2018hpu\u2019.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\nenabled(bool,optional) \u2013 Whether autocasting should be enabled in the region.\nDefault:True\nTrue\ndtype(torch_dtype,optional) \u2013 Data type for ops run in autocast. It uses the default value\n(torch.float16for CUDA andtorch.bfloat16for CPU), given byget_autocast_dtype(), ifdtypeisNone.\nDefault:None\ntorch.float16\ntorch.bfloat16\nget_autocast_dtype()\ndtype\nNone\nNone\ncache_enabled(bool,optional) \u2013 Whether the weight cache inside autocast should be enabled.\nDefault:True\nTrue\nCreate a helper decorator forforwardmethods of custom autograd functions.\nforward\nAutograd functions are subclasses oftorch.autograd.Function.\nSee theexample pagefor more detail.\ntorch.autograd.Function\ndevice_type(str) \u2013 Device type to use. \u2018cuda\u2019, \u2018cpu\u2019, \u2018mtia\u2019, \u2018maia\u2019, \u2018xpu\u2019 and so on.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\ncast_inputs(torch.dtypeor None, optional, default=None) \u2013 If notNone,\nwhenforwardruns in an autocast-enabled region, casts incoming\nfloating-point Tensors to the target dtype (non-floating-point Tensors are not affected),\nthen executesforwardwith autocast disabled.\nIfNone,forward\u2019s internal ops execute with the current autocast state.\ntorch.dtype\nNone\nforward\nforward\nNone\nforward\nNote\nIf the decoratedforwardis called outside an autocast-enabled region,custom_fwdis a no-op andcast_inputshas no effect.\nforward\ncustom_fwd\ncast_inputs\nCreate a helper decorator for backward methods of custom autograd functions.\nAutograd functions are subclasses oftorch.autograd.Function.\nEnsures thatbackwardexecutes with the same autocast state asforward.\nSee theexample pagefor more detail.\ntorch.autograd.Function\nbackward\nforward\ndevice_type(str) \u2013 Device type to use. \u2018cuda\u2019, \u2018cpu\u2019, \u2018mtia\u2019, \u2018maia\u2019, \u2018xpu\u2019 and so on.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\nSeetorch.autocast.\ntorch.autocast\ntorch.cuda.amp.autocast(args...)is deprecated. Please usetorch.amp.autocast(\"cuda\",args...)instead.\ntorch.cuda.amp.autocast(args...)\ntorch.amp.autocast(\"cuda\",args...)\ntorch.cuda.amp.custom_fwd(args...)is deprecated. Please usetorch.amp.custom_fwd(args...,device_type='cuda')instead.\ntorch.cuda.amp.custom_fwd(args...)\ntorch.amp.custom_fwd(args...,device_type='cuda')\ntorch.cuda.amp.custom_bwd(args...)is deprecated. Please usetorch.amp.custom_bwd(args...,device_type='cuda')instead.\ntorch.cuda.amp.custom_bwd(args...)\ntorch.amp.custom_bwd(args...,device_type='cuda')\nSeetorch.autocast.torch.cpu.amp.autocast(args...)is deprecated. Please usetorch.amp.autocast(\"cpu\",args...)instead.\ntorch.autocast\ntorch.cpu.amp.autocast(args...)\ntorch.amp.autocast(\"cpu\",args...)\n\n## Gradient Scaling#\n\nIf the forward pass for a particular op hasfloat16inputs, the backward pass for\nthat op will producefloat16gradients.\nGradient values with small magnitudes may not be representable infloat16.\nThese values will flush to zero (\u201cunderflow\u201d), so the update for the corresponding parameters will be lost.\nfloat16\nfloat16\nfloat16\nTo prevent underflow, \u201cgradient scaling\u201d multiplies the network\u2019s loss(es) by a scale factor and\ninvokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are\nthen scaled by the same factor. In other words, gradient values have a larger magnitude,\nso they don\u2019t flush to zero.\nEach parameter\u2019s gradient (.gradattribute) should be unscaled before the optimizer\nupdates the parameters, so the scale factor does not interfere with the learning rate.\n.grad\nNote\nAMP/fp16 may not work for every model! For example, most bf16-pretrained models cannot operate in\nthe fp16 numerical range of max 65504 and will cause gradients to overflow instead of underflow. In\nthis case, the scale factor may decrease under 1 as an attempt to bring gradients to a number\nrepresentable in the fp16 dynamic range. While one may expect the scale to always be above 1, our\nGradScaler does NOT make this guarantee to maintain performance. If you encounter NaNs in your loss\nor gradients when running with AMP/fp16, verify your model is compatible.\nSeetorch.amp.GradScaler.torch.cuda.amp.GradScaler(args...)is deprecated. Please usetorch.amp.GradScaler(\"cuda\",args...)instead.\ntorch.amp.GradScaler\ntorch.cuda.amp.GradScaler(args...)\ntorch.amp.GradScaler(\"cuda\",args...)\nSeetorch.amp.GradScaler.torch.cpu.amp.GradScaler(args...)is deprecated. Please usetorch.amp.GradScaler(\"cpu\",args...)instead.\ntorch.amp.GradScaler\ntorch.cpu.amp.GradScaler(args...)\ntorch.amp.GradScaler(\"cpu\",args...)\n\n## Autocast Op Reference#\n\n\n## Op Eligibility#\n\nOps that run infloat64or non-floating-point dtypes are not eligible, and will\nrun in these types whether or not autocast is enabled.\nfloat64\nOnly out-of-place ops and Tensor methods are eligible.\nIn-place variants and calls that explicitly supply anout=...Tensor\nare allowed in autocast-enabled regions, but won\u2019t go through autocasting.\nFor example, in an autocast-enabled regiona.addmm(b,c)can autocast,\nbuta.addmm_(b,c)anda.addmm(b,c,out=d)cannot.\nFor best performance and stability, prefer out-of-place ops in autocast-enabled\nregions.\nout=...\na.addmm(b,c)\na.addmm_(b,c)\na.addmm(b,c,out=d)\nOps called with an explicitdtype=...argument are not eligible,\nand will produce output that respects thedtypeargument.\ndtype=...\ndtype\n\n## CUDA Op-Specific Behavior#\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of atorch.nn.Module,\nas a function, or as atorch.Tensormethod. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\ntorch.nn.Module\ntorch.Tensor\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type\nin which unlisted ops run if they\u2019re downstream from autocasted ops.\nIf an op is unlisted, we assume it\u2019s numerically stable infloat16.\nIf you believe an unlisted op is numerically unstable infloat16,\nplease file an issue.\nfloat16\nfloat16\nfloat16\n__matmul__,addbmm,addmm,addmv,addr,baddbmm,bmm,chain_matmul,multi_dot,conv1d,conv2d,conv3d,conv_transpose1d,conv_transpose2d,conv_transpose3d,GRUCell,linear,LSTMCell,matmul,mm,mv,prelu,RNNCell\n__matmul__\naddbmm\naddmm\naddmv\naddr\nbaddbmm\nbmm\nchain_matmul\nmulti_dot\nconv1d\nconv2d\nconv3d\nconv_transpose1d\nconv_transpose2d\nconv_transpose3d\nGRUCell\nlinear\nLSTMCell\nmatmul\nmm\nmv\nprelu\nRNNCell\nfloat32\n__pow__,__rdiv__,__rpow__,__rtruediv__,acos,asin,binary_cross_entropy_with_logits,cosh,cosine_embedding_loss,cdist,cosine_similarity,cross_entropy,cumprod,cumsum,dist,erfinv,exp,expm1,group_norm,hinge_embedding_loss,kl_div,l1_loss,layer_norm,log,log_softmax,log10,log1p,log2,margin_ranking_loss,mse_loss,multilabel_margin_loss,multi_margin_loss,nll_loss,norm,normalize,pdist,poisson_nll_loss,pow,prod,reciprocal,rsqrt,sinh,smooth_l1_loss,soft_margin_loss,softmax,softmin,softplus,sum,renorm,tan,triplet_margin_loss\n__pow__\n__rdiv__\n__rpow__\n__rtruediv__\nacos\nasin\nbinary_cross_entropy_with_logits\ncosh\ncosine_embedding_loss\ncdist\ncosine_similarity\ncross_entropy\ncumprod\ncumsum\ndist\nerfinv\nexp\nexpm1\ngroup_norm\nhinge_embedding_loss\nkl_div\nl1_loss\nlayer_norm\nlog\nlog_softmax\nlog10\nlog1p\nlog2\nmargin_ranking_loss\nmse_loss\nmultilabel_margin_loss\nmulti_margin_loss\nnll_loss\nnorm\nnormalize\npdist\npoisson_nll_loss\npow\nprod\nreciprocal\nrsqrt\nsinh\nsmooth_l1_loss\nsoft_margin_loss\nsoftmax\nsoftmin\nsoftplus\nsum\nrenorm\ntan\ntriplet_margin_loss\nThese ops don\u2019t require a particular dtype for stability, but take multiple inputs\nand require that the inputs\u2019 dtypes match. If all of the inputs arefloat16, the op runs infloat16. If any of the inputs isfloat32,\nautocast casts all inputs tofloat32and runs the op infloat32.\nfloat16\nfloat16\nfloat32\nfloat32\nfloat32\naddcdiv,addcmul,atan2,bilinear,cross,dot,grid_sample,index_put,scatter_add,tensordot\naddcdiv\naddcmul\natan2\nbilinear\ncross\ndot\ngrid_sample\nindex_put\nscatter_add\ntensordot\nSome ops not listed here (e.g., binary ops likeadd) natively promote\ninputs without autocasting\u2019s intervention. If inputs are a mixture offloat16andfloat32, these ops run infloat32and producefloat32output,\nregardless of whether autocast is enabled.\nadd\nfloat16\nfloat32\nfloat32\nfloat32\nbinary_cross_entropy_with_logits\nbinary_cross_entropy\nThe backward passes oftorch.nn.functional.binary_cross_entropy()(andtorch.nn.BCELoss, which wraps it)\ncan produce gradients that aren\u2019t representable infloat16. In autocast-enabled regions, the forward input\nmay befloat16, which means the backward gradient must be representable infloat16(autocastingfloat16forward inputs tofloat32doesn\u2019t help, because that cast must be reversed in backward).\nTherefore,binary_cross_entropyandBCELossraise an error in autocast-enabled regions.\ntorch.nn.functional.binary_cross_entropy()\ntorch.nn.BCELoss\nfloat16\nfloat16\nfloat16\nfloat16\nfloat32\nbinary_cross_entropy\nBCELoss\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers usingtorch.nn.functional.binary_cross_entropy_with_logits()ortorch.nn.BCEWithLogitsLoss.binary_cross_entropy_with_logitsandBCEWithLogitsare safe to autocast.\ntorch.nn.functional.binary_cross_entropy_with_logits()\ntorch.nn.BCEWithLogitsLoss\nbinary_cross_entropy_with_logits\nBCEWithLogits\n\n## XPU Op-Specific Behavior (Experimental)#\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of atorch.nn.Module,\nas a function, or as atorch.Tensormethod. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\ntorch.nn.Module\ntorch.Tensor\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type\nin which unlisted ops run if they\u2019re downstream from autocasted ops.\nIf an op is unlisted, we assume it\u2019s numerically stable infloat16.\nIf you believe an unlisted op is numerically unstable infloat16,\nplease file an issue.\nfloat16\nfloat16\nfloat16\naddbmm,addmm,addmv,addr,baddbmm,bmm,chain_matmul,multi_dot,conv1d,conv2d,conv3d,conv_transpose1d,conv_transpose2d,conv_transpose3d,GRUCell,linear,LSTMCell,matmul,mm,mv,RNNCell\naddbmm\naddmm\naddmv\naddr\nbaddbmm\nbmm\nchain_matmul\nmulti_dot\nconv1d\nconv2d\nconv3d\nconv_transpose1d\nconv_transpose2d\nconv_transpose3d\nGRUCell\nlinear\nLSTMCell\nmatmul\nmm\nmv\nRNNCell\nfloat32\n__pow__,__rdiv__,__rpow__,__rtruediv__,binary_cross_entropy_with_logits,cosine_embedding_loss,cosine_similarity,cumsum,dist,exp,group_norm,hinge_embedding_loss,kl_div,l1_loss,layer_norm,log,log_softmax,margin_ranking_loss,nll_loss,normalize,poisson_nll_loss,pow,reciprocal,rsqrt,soft_margin_loss,softmax,softmin,sum,triplet_margin_loss\n__pow__\n__rdiv__\n__rpow__\n__rtruediv__\nbinary_cross_entropy_with_logits\ncosine_embedding_loss\ncosine_similarity\ncumsum\ndist\nexp\ngroup_norm\nhinge_embedding_loss\nkl_div\nl1_loss\nlayer_norm\nlog\nlog_softmax\nmargin_ranking_loss\nnll_loss\nnormalize\npoisson_nll_loss\npow\nreciprocal\nrsqrt\nsoft_margin_loss\nsoftmax\nsoftmin\nsum\ntriplet_margin_loss\nThese ops don\u2019t require a particular dtype for stability, but take multiple inputs\nand require that the inputs\u2019 dtypes match. If all of the inputs arefloat16, the op runs infloat16. If any of the inputs isfloat32,\nautocast casts all inputs tofloat32and runs the op infloat32.\nfloat16\nfloat16\nfloat32\nfloat32\nfloat32\nbilinear,cross,grid_sample,index_put,scatter_add,tensordot\nbilinear\ncross\ngrid_sample\nindex_put\nscatter_add\ntensordot\nSome ops not listed here (e.g., binary ops likeadd) natively promote\ninputs without autocasting\u2019s intervention. If inputs are a mixture offloat16andfloat32, these ops run infloat32and producefloat32output,\nregardless of whether autocast is enabled.\nadd\nfloat16\nfloat32\nfloat32\nfloat32\n\n## CPU Op-Specific Behavior#\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of atorch.nn.Module,\nas a function, or as atorch.Tensormethod. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\ntorch.nn.Module\ntorch.Tensor\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type\nin which unlisted ops run if they\u2019re downstream from autocasted ops.\nIf an op is unlisted, we assume it\u2019s numerically stable inbfloat16.\nIf you believe an unlisted op is numerically unstable inbfloat16,\nplease file an issue.float16shares the lists ofbfloat16.\nbfloat16\nbfloat16\nfloat16\nbfloat16\nbfloat16\nconv1d,conv2d,conv3d,bmm,mm,linalg_vecdot,baddbmm,addmm,addbmm,linear,matmul,_convolution,conv_tbc,mkldnn_rnn_layer,conv_transpose1d,conv_transpose2d,conv_transpose3d,prelu,scaled_dot_product_attention,_native_multi_head_attention\nconv1d\nconv2d\nconv3d\nbmm\nmm\nlinalg_vecdot\nbaddbmm\naddmm\naddbmm\nlinear\nmatmul\n_convolution\nconv_tbc\nmkldnn_rnn_layer\nconv_transpose1d\nconv_transpose2d\nconv_transpose3d\nprelu\nscaled_dot_product_attention\n_native_multi_head_attention\nfloat32\navg_pool3d,binary_cross_entropy,grid_sampler,grid_sampler_2d,_grid_sampler_2d_cpu_fallback,grid_sampler_3d,polar,prod,quantile,nanquantile,stft,cdist,trace,view_as_complex,cholesky,cholesky_inverse,cholesky_solve,inverse,lu_solve,orgqr,inverse,ormqr,pinverse,max_pool3d,max_unpool2d,max_unpool3d,adaptive_avg_pool3d,reflection_pad1d,reflection_pad2d,replication_pad1d,replication_pad2d,replication_pad3d,mse_loss,cosine_embedding_loss,nll_loss,nll_loss2d,hinge_embedding_loss,poisson_nll_loss,cross_entropy_loss,l1_loss,huber_loss,margin_ranking_loss,soft_margin_loss,triplet_margin_loss,multi_margin_loss,ctc_loss,kl_div,multilabel_margin_loss,binary_cross_entropy_with_logits,fft_fft,fft_ifft,fft_fft2,fft_ifft2,fft_fftn,fft_ifftn,fft_rfft,fft_irfft,fft_rfft2,fft_irfft2,fft_rfftn,fft_irfftn,fft_hfft,fft_ihfft,linalg_cond,linalg_matrix_rank,linalg_solve,linalg_cholesky,linalg_svdvals,linalg_eigvals,linalg_eigvalsh,linalg_inv,linalg_householder_product,linalg_tensorinv,linalg_tensorsolve,fake_quantize_per_tensor_affine,geqrf,_lu_with_info,qr,svd,triangular_solve,fractional_max_pool2d,fractional_max_pool3d,adaptive_max_pool3d,multilabel_margin_loss_forward,linalg_qr,linalg_cholesky_ex,linalg_svd,linalg_eig,linalg_eigh,linalg_lstsq,linalg_inv_ex\navg_pool3d\nbinary_cross_entropy\ngrid_sampler\ngrid_sampler_2d\n_grid_sampler_2d_cpu_fallback\ngrid_sampler_3d\npolar\nprod\nquantile\nnanquantile\nstft\ncdist\ntrace\nview_as_complex\ncholesky\ncholesky_inverse\ncholesky_solve\ninverse\nlu_solve\norgqr\ninverse\normqr\npinverse\nmax_pool3d\nmax_unpool2d\nmax_unpool3d\nadaptive_avg_pool3d\nreflection_pad1d\nreflection_pad2d\nreplication_pad1d\nreplication_pad2d\nreplication_pad3d\nmse_loss\ncosine_embedding_loss\nnll_loss\nnll_loss2d\nhinge_embedding_loss\npoisson_nll_loss\ncross_entropy_loss\nl1_loss\nhuber_loss\nmargin_ranking_loss\nsoft_margin_loss\ntriplet_margin_loss\nmulti_margin_loss\nctc_loss\nkl_div\nmultilabel_margin_loss\nbinary_cross_entropy_with_logits\nfft_fft\nfft_ifft\nfft_fft2\nfft_ifft2\nfft_fftn\nfft_ifftn\nfft_rfft\nfft_irfft\nfft_rfft2\nfft_irfft2\nfft_rfftn\nfft_irfftn\nfft_hfft\nfft_ihfft\nlinalg_cond\nlinalg_matrix_rank\nlinalg_solve\nlinalg_cholesky\nlinalg_svdvals\nlinalg_eigvals\nlinalg_eigvalsh\nlinalg_inv\nlinalg_householder_product\nlinalg_tensorinv\nlinalg_tensorsolve\nfake_quantize_per_tensor_affine\ngeqrf\n_lu_with_info\nqr\nsvd\ntriangular_solve\nfractional_max_pool2d\nfractional_max_pool3d\nadaptive_max_pool3d\nmultilabel_margin_loss_forward\nlinalg_qr\nlinalg_cholesky_ex\nlinalg_svd\nlinalg_eig\nlinalg_eigh\nlinalg_lstsq\nlinalg_inv_ex\nThese ops don\u2019t require a particular dtype for stability, but take multiple inputs\nand require that the inputs\u2019 dtypes match. If all of the inputs arebfloat16, the op runs inbfloat16. If any of the inputs isfloat32,\nautocast casts all inputs tofloat32and runs the op infloat32.\nbfloat16\nbfloat16\nfloat32\nfloat32\nfloat32\ncat,stack,index_copy\ncat\nstack\nindex_copy\nSome ops not listed here (e.g., binary ops likeadd) natively promote\ninputs without autocasting\u2019s intervention. If inputs are a mixture ofbfloat16andfloat32, these ops run infloat32and producefloat32output,\nregardless of whether autocast is enabled.\nadd\nbfloat16\nfloat32\nfloat32\nfloat32",
  "url": "https://pytorch.org/docs/stable/amp.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
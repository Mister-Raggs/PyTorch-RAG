{
  "doc_id": "f273df8453cdde5cd2cd485bd4d4df31",
  "source": "pytorch_docs",
  "title": "IRs \u2014 PyTorch 2.9 documentation",
  "text": "\n## IRs#\n\nCreated On: Dec 13, 2022 | Last Updated On: Jul 16, 2025\nPyTorch 2.0 offers two set of IRs for backends to interface with: Core Aten IR and Prims IR.\n\n## Core Aten IR#\n\nCore aten ops is the core subset of aten operators that can be used to compose other operators.\nCore aten IR is fully functional, and there is noinplaceor_outvariants in this opset.\nIn contrast to Prims IR, core aten ops reuses the existing aten ops in \u201cnative_functions.yaml\u201d,\nand it doesn\u2019t further decompose ops into explicit type promotion and broadcasting ops.\nThis opset is designed to serve as the functional IR to interface with backends.\ninplace\n_out\nWarning\nThis opset is still under active development, more ops will be added in the future.\nOperator\nSchema\naten._adaptive_avg_pool2d\naten._adaptive_avg_pool2d\n_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor\naten._adaptive_avg_pool2d_backward\naten._adaptive_avg_pool2d_backward\n_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor\naten._adaptive_avg_pool3d\naten._adaptive_avg_pool3d\n_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor\naten._cdist_forward\naten._cdist_forward\n_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor\naten._embedding_bag\naten._embedding_bag\n_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)\naten._fft_r2c\naten._fft_r2c\n_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor\naten._local_scalar_dense\naten._local_scalar_dense\n_local_scalar_dense(Tensor self) -> Scalar\naten._log_softmax\naten._log_softmax\n_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor\naten._native_batch_norm_legit\naten._native_batch_norm_legit\n_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)\naten._native_batch_norm_legit.no_stats\naten._native_batch_norm_legit.no_stats\n_native_batch_norm_legit.no_stats(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)\naten._native_batch_norm_legit_no_training\naten._native_batch_norm_legit_no_training\n_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)\naten._pdist_forward\naten._pdist_forward\n_pdist_forward(Tensor self, float p=2) -> Tensor\naten._softmax\naten._softmax\n_softmax(Tensor self, int dim, bool half_to_float) -> Tensor\naten._to_copy\naten._to_copy\n_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor\naten.abs\naten.abs\nabs(Tensor self) -> Tensor\naten.acos\naten.acos\nacos(Tensor self) -> Tensor\naten.acosh\naten.acosh\nacosh(Tensor self) -> Tensor\naten.adaptive_avg_pool1d\naten.adaptive_avg_pool1d\nadaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor\naten.add.Scalar\naten.add.Scalar\nadd.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor\naten.add.Tensor\naten.add.Tensor\nadd.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\naten.addmm\naten.addmm\naddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\naten.alias\naten.alias\nalias(Tensor(a) self) -> Tensor(a)\naten.amax\naten.amax\namax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor\naten.amin\naten.amin\namin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor\naten.any\naten.any\nany(Tensor self) -> Tensor\naten.any.dim\naten.any.dim\nany.dim(Tensor self, int dim, bool keepdim=False) -> Tensor\naten.any.dims\naten.any.dims\nany.dims(Tensor self, int[]? dim=None, bool keepdim=False) -> Tensor\naten.arange.start_step\naten.arange.start_step\narange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.argmax\naten.argmax\nargmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor\naten.argmin\naten.argmin\nargmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor\naten.as_strided\naten.as_strided\nas_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)\naten.asin\naten.asin\nasin(Tensor self) -> Tensor\naten.asinh\naten.asinh\nasinh(Tensor self) -> Tensor\naten.atan\naten.atan\natan(Tensor self) -> Tensor\naten.atan2\naten.atan2\natan2(Tensor self, Tensor other) -> Tensor\naten.atan2.out\naten.atan2.out\natan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)\naten.atanh\naten.atanh\natanh(Tensor self) -> Tensor\naten.avg_pool1d\naten.avg_pool1d\navg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor\naten.avg_pool2d\naten.avg_pool2d\navg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor\naten.avg_pool2d_backward\naten.avg_pool2d_backward\navg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor\naten.avg_pool3d\naten.avg_pool3d\navg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor\naten.bitwise_and.Scalar\naten.bitwise_and.Scalar\nbitwise_and.Scalar(Tensor self, Scalar other) -> Tensor\naten.bitwise_and.Tensor\naten.bitwise_and.Tensor\nbitwise_and.Tensor(Tensor self, Tensor other) -> Tensor\naten.bitwise_not\naten.bitwise_not\nbitwise_not(Tensor self) -> Tensor\naten.bitwise_or.Scalar\naten.bitwise_or.Scalar\nbitwise_or.Scalar(Tensor self, Scalar other) -> Tensor\naten.bitwise_or.Tensor\naten.bitwise_or.Tensor\nbitwise_or.Tensor(Tensor self, Tensor other) -> Tensor\naten.bitwise_xor.Scalar\naten.bitwise_xor.Scalar\nbitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor\naten.bitwise_xor.Tensor\naten.bitwise_xor.Tensor\nbitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor\naten.bmm\naten.bmm\nbmm(Tensor self, Tensor mat2) -> Tensor\naten.cat\naten.cat\ncat(Tensor[] tensors, int dim=0) -> Tensor\naten.ceil\naten.ceil\nceil(Tensor self) -> Tensor\naten.clamp\naten.clamp\nclamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor\naten.clamp.Tensor\naten.clamp.Tensor\nclamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor\naten.clone\naten.clone\nclone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor\naten.col2im\naten.col2im\ncol2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor\naten.constant_pad_nd\naten.constant_pad_nd\nconstant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor\naten.convolution\naten.convolution\nconvolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor\naten.convolution_backward\naten.convolution_backward\nconvolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)\naten.copy\naten.copy\ncopy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor\naten.cos\naten.cos\ncos(Tensor self) -> Tensor\naten.cosh\naten.cosh\ncosh(Tensor self) -> Tensor\naten.cumsum\naten.cumsum\ncumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor\naten.diagonal\naten.diagonal\ndiagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)\naten.div.Scalar\naten.div.Scalar\ndiv.Scalar(Tensor self, Scalar other) -> Tensor\naten.div.Scalar_mode\naten.div.Scalar_mode\ndiv.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor\naten.div.Tensor\naten.div.Tensor\ndiv.Tensor(Tensor self, Tensor other) -> Tensor\naten.div.Tensor_mode\naten.div.Tensor_mode\ndiv.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor\naten.elu\naten.elu\nelu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor\naten.embedding\naten.embedding\nembedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor\naten.embedding_dense_backward\naten.embedding_dense_backward\nembedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor\naten.empty.memory_format\naten.empty.memory_format\nempty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor\naten.empty_strided\naten.empty_strided\nempty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.eq.Scalar\naten.eq.Scalar\neq.Scalar(Tensor self, Scalar other) -> Tensor\naten.eq.Tensor\naten.eq.Tensor\neq.Tensor(Tensor self, Tensor other) -> Tensor\naten.erf\naten.erf\nerf(Tensor self) -> Tensor\naten.exp\naten.exp\nexp(Tensor self) -> Tensor\naten.expand\naten.expand\nexpand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)\naten.expm1\naten.expm1\nexpm1(Tensor self) -> Tensor\naten.fill.Scalar\naten.fill.Scalar\nfill.Scalar(Tensor self, Scalar value) -> Tensor\naten.flip\naten.flip\nflip(Tensor self, int[] dims) -> Tensor\naten.floor\naten.floor\nfloor(Tensor self) -> Tensor\naten.fmod.Scalar\naten.fmod.Scalar\nfmod.Scalar(Tensor self, Scalar other) -> Tensor\naten.fmod.Tensor\naten.fmod.Tensor\nfmod.Tensor(Tensor self, Tensor other) -> Tensor\naten.full\naten.full\nfull(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.full_like\naten.full_like\nfull_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor\naten.gather\naten.gather\ngather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor\naten.ge.Scalar\naten.ge.Scalar\nge.Scalar(Tensor self, Scalar other) -> Tensor\naten.ge.Tensor\naten.ge.Tensor\nge.Tensor(Tensor self, Tensor other) -> Tensor\naten.gelu\naten.gelu\ngelu(Tensor self, *, str approximate=\u2019none\u2019) -> Tensor\naten.grid_sampler_2d\naten.grid_sampler_2d\ngrid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor\naten.gt.Scalar\naten.gt.Scalar\ngt.Scalar(Tensor self, Scalar other) -> Tensor\naten.gt.Tensor\naten.gt.Tensor\ngt.Tensor(Tensor self, Tensor other) -> Tensor\naten.hardtanh\naten.hardtanh\nhardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor\naten.index.Tensor\naten.index.Tensor\nindex.Tensor(Tensor self, Tensor?[] indices) -> Tensor\naten.index_put\naten.index_put\nindex_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor\naten.index_select\naten.index_select\nindex_select(Tensor self, int dim, Tensor index) -> Tensor\naten.isinf\naten.isinf\nisinf(Tensor self) -> Tensor\naten.isnan\naten.isnan\nisnan(Tensor self) -> Tensor\naten.le.Scalar\naten.le.Scalar\nle.Scalar(Tensor self, Scalar other) -> Tensor\naten.le.Tensor\naten.le.Tensor\nle.Tensor(Tensor self, Tensor other) -> Tensor\naten.leaky_relu\naten.leaky_relu\nleaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor\naten.log\naten.log\nlog(Tensor self) -> Tensor\naten.log10\naten.log10\nlog10(Tensor self) -> Tensor\naten.log1p\naten.log1p\nlog1p(Tensor self) -> Tensor\naten.log2\naten.log2\nlog2(Tensor self) -> Tensor\naten.logical_and\naten.logical_and\nlogical_and(Tensor self, Tensor other) -> Tensor\naten.logical_not\naten.logical_not\nlogical_not(Tensor self) -> Tensor\naten.logical_or\naten.logical_or\nlogical_or(Tensor self, Tensor other) -> Tensor\naten.logical_xor\naten.logical_xor\nlogical_xor(Tensor self, Tensor other) -> Tensor\naten.lt.Scalar\naten.lt.Scalar\nlt.Scalar(Tensor self, Scalar other) -> Tensor\naten.lt.Tensor\naten.lt.Tensor\nlt.Tensor(Tensor self, Tensor other) -> Tensor\naten.masked_scatter\naten.masked_scatter\nmasked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor\naten.max.dim\naten.max.dim\nmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)\naten.max_pool2d_with_indices\naten.max_pool2d_with_indices\nmax_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)\naten.max_pool2d_with_indices_backward\naten.max_pool2d_with_indices_backward\nmax_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor\naten.max_pool3d_with_indices\naten.max_pool3d_with_indices\nmax_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)\naten.maximum\naten.maximum\nmaximum(Tensor self, Tensor other) -> Tensor\naten.mean\naten.mean\nmean(Tensor self, *, ScalarType? dtype=None) -> Tensor\naten.mean.dim\naten.mean.dim\nmean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor\naten.min.dim\naten.min.dim\nmin.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)\naten.minimum\naten.minimum\nminimum(Tensor self, Tensor other) -> Tensor\naten.mm\naten.mm\nmm(Tensor self, Tensor mat2) -> Tensor\naten.mul.Scalar\naten.mul.Scalar\nmul.Scalar(Tensor self, Scalar other) -> Tensor\naten.mul.Tensor\naten.mul.Tensor\nmul.Tensor(Tensor self, Tensor other) -> Tensor\naten.native_dropout\naten.native_dropout\nnative_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)\naten.native_group_norm\naten.native_group_norm\nnative_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)\naten.native_group_norm_backward\naten.native_group_norm_backward\nnative_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)\naten.native_layer_norm\naten.native_layer_norm\nnative_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)\naten.native_layer_norm_backward\naten.native_layer_norm_backward\nnative_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)\naten.ne.Scalar\naten.ne.Scalar\nne.Scalar(Tensor self, Scalar other) -> Tensor\naten.ne.Tensor\naten.ne.Tensor\nne.Tensor(Tensor self, Tensor other) -> Tensor\naten.neg\naten.neg\nneg(Tensor self) -> Tensor\naten.nonzero\naten.nonzero\nnonzero(Tensor self) -> Tensor\naten.permute\naten.permute\npermute(Tensor(a) self, int[] dims) -> Tensor(a)\naten.pow.Scalar\naten.pow.Scalar\npow.Scalar(Scalar self, Tensor exponent) -> Tensor\naten.pow.Tensor_Scalar\naten.pow.Tensor_Scalar\npow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor\naten.pow.Tensor_Tensor\naten.pow.Tensor_Tensor\npow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor\naten.prod\naten.prod\nprod(Tensor self, *, ScalarType? dtype=None) -> Tensor\naten.prod.dim_int\naten.prod.dim_int\nprod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor\naten.rand\naten.rand\nrand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.randn\naten.randn\nrandn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.randperm\naten.randperm\nrandperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.reciprocal\naten.reciprocal\nreciprocal(Tensor self) -> Tensor\naten.reflection_pad1d\naten.reflection_pad1d\nreflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor\naten.reflection_pad2d\naten.reflection_pad2d\nreflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor\naten.reflection_pad3d\naten.reflection_pad3d\nreflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor\naten.relu\naten.relu\nrelu(Tensor self) -> Tensor\naten.remainder.Scalar\naten.remainder.Scalar\nremainder.Scalar(Tensor self, Scalar other) -> Tensor\naten.remainder.Tensor\naten.remainder.Tensor\nremainder.Tensor(Tensor self, Tensor other) -> Tensor\naten.repeat\naten.repeat\nrepeat(Tensor self, SymInt[] repeats) -> Tensor\naten.replication_pad2d\naten.replication_pad2d\nreplication_pad2d(Tensor self, SymInt[4] padding) -> Tensor\naten.replication_pad3d\naten.replication_pad3d\nreplication_pad3d(Tensor self, SymInt[6] padding) -> Tensor\naten.resize_\naten.resize_\nresize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)\naten.round\naten.round\nround(Tensor self) -> Tensor\naten.rsqrt\naten.rsqrt\nrsqrt(Tensor self) -> Tensor\naten.scalar_tensor\naten.scalar_tensor\nscalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.scatter.src\naten.scatter.src\nscatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor\naten.scatter.value\naten.scatter.value\nscatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor\naten.scatter_add\naten.scatter_add\nscatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor\naten.scatter_reduce.two\naten.scatter_reduce.two\nscatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor\naten.select.int\naten.select.int\nselect.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)\naten.select_scatter\naten.select_scatter\nselect_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor\naten.sigmoid\naten.sigmoid\nsigmoid(Tensor self) -> Tensor\naten.sign\naten.sign\nsign(Tensor self) -> Tensor\naten.sin\naten.sin\nsin(Tensor self) -> Tensor\naten.sinh\naten.sinh\nsinh(Tensor self) -> Tensor\naten.slice.Tensor\naten.slice.Tensor\nslice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)\naten.slice_scatter\naten.slice_scatter\nslice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor\naten.sort\naten.sort\nsort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)\naten.split_with_sizes\naten.split_with_sizes\nsplit_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]\naten.sqrt\naten.sqrt\nsqrt(Tensor self) -> Tensor\naten.squeeze.dim\naten.squeeze.dim\nsqueeze.dim(Tensor(a) self, int dim) -> Tensor(a)\naten.squeeze.dims\naten.squeeze.dims\nsqueeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)\naten.sub.Scalar\naten.sub.Scalar\nsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor\naten.sub.Tensor\naten.sub.Tensor\nsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\naten.sum.dim_IntList\naten.sum.dim_IntList\nsum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor\naten.sym_is_contiguous\naten.sym_is_contiguous\nsym_is_contiguous(Tensor self, MemoryFormat memory_format=contiguous_format) -> SymBool\naten.sym_numel\naten.sym_numel\nsym_numel(Tensor self) -> SymInt\naten.sym_size.int\naten.sym_size.int\nsym_size.int(Tensor self, int dim) -> SymInt\naten.sym_storage_offset\naten.sym_storage_offset\nsym_storage_offset(Tensor self) -> SymInt\naten.sym_stride.int\naten.sym_stride.int\nsym_stride.int(Tensor self, int dim) -> SymInt\naten.tan\naten.tan\ntan(Tensor self) -> Tensor\naten.tanh\naten.tanh\ntanh(Tensor self) -> Tensor\naten.topk\naten.topk\ntopk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)\naten.trunc\naten.trunc\ntrunc(Tensor self) -> Tensor\naten.unsqueeze\naten.unsqueeze\nunsqueeze(Tensor(a) self, int dim) -> Tensor(a)\naten.upsample_bilinear2d.vec\naten.upsample_bilinear2d.vec\nupsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor\naten.upsample_nearest2d.vec\naten.upsample_nearest2d.vec\nupsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor\naten.var.correction\naten.var.correction\nvar.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor\naten.var.dim\naten.var.dim\nvar.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor\naten.view\naten.view\nview(Tensor(a) self, SymInt[] size) -> Tensor(a)\naten.where.self\naten.where.self\nwhere.self(Tensor condition, Tensor self, Tensor other) -> Tensor\n\n## Prims IR#\n\nPrims IR is a set of primitive operators that can be used to compose other operators.\nPrims IR is a lower level opset than core aten IR, and it further decomposes ops into explicit\ntype promotion and broadcasting ops: prims.convert_element_type and prims.broadcast_in_dim.\nThis opset is designed to interface with compiler backends.\nWarning\nThis opset is still under active development, more ops will be added in the future.\nOperator\nSchema\nprims.abs\nprims.abs\n(Tensor self) -> Tensor\nprims.acos\nprims.acos\n(Tensor self) -> Tensor\nprims.acosh\nprims.acosh\n(Tensor self) -> Tensor\nprims.asin\nprims.asin\n(Tensor self) -> Tensor\nprims.asinh\nprims.asinh\n(Tensor self) -> Tensor\nprims.atan\nprims.atan\n(Tensor self) -> Tensor\nprims.atanh\nprims.atanh\n(Tensor self) -> Tensor\nprims.cos\nprims.cos\n(Tensor self) -> Tensor\nprims.cosh\nprims.cosh\n(Tensor self) -> Tensor\nprims.bessel_i0\nprims.bessel_i0\n(Tensor self) -> Tensor\nprims.bessel_i0e\nprims.bessel_i0e\n(Tensor self) -> Tensor\nprims.bessel_i1\nprims.bessel_i1\n(Tensor self) -> Tensor\nprims.bessel_i1e\nprims.bessel_i1e\n(Tensor self) -> Tensor\nprims.bessel_j0\nprims.bessel_j0\n(Tensor self) -> Tensor\nprims.bessel_j1\nprims.bessel_j1\n(Tensor self) -> Tensor\nprims.bitwise_not\nprims.bitwise_not\n(Tensor self) -> Tensor\nprims.cbrt\nprims.cbrt\n(Tensor self) -> Tensor\nprims.ceil\nprims.ceil\n(Tensor self) -> Tensor\nprims.conj_physical\nprims.conj_physical\n(Tensor self) -> Tensor\nprims.digamma\nprims.digamma\n(Tensor self) -> Tensor\nprims.erf\nprims.erf\n(Tensor self) -> Tensor\nprims.erf_inv\nprims.erf_inv\n(Tensor self) -> Tensor\nprims.erfc\nprims.erfc\n(Tensor self) -> Tensor\nprims.erfcx\nprims.erfcx\n(Tensor self) -> Tensor\nprims.exp\nprims.exp\n(Tensor self) -> Tensor\nprims.expm1\nprims.expm1\n(Tensor self) -> Tensor\nprims.exp2\nprims.exp2\n(Tensor self) -> Tensor\nprims.fill\nprims.fill\n(Tensor self, Scalar value) -> Tensor\nprims.floor\nprims.floor\n(Tensor self) -> Tensor\nprims.imag\nprims.imag\n(Tensor(a) self) -> Tensor(a)\nprims.isfinite\nprims.isfinite\n(Tensor self) -> Tensor\nprims.lgamma\nprims.lgamma\n(Tensor self) -> Tensor\nprims.log\nprims.log\n(Tensor self) -> Tensor\nprims.log1p\nprims.log1p\n(Tensor self) -> Tensor\nprims.log2\nprims.log2\n(Tensor self) -> Tensor\nprims.log10\nprims.log10\n(Tensor self) -> Tensor\nprims.ndtri\nprims.ndtri\n(Tensor self) -> Tensor\nprims.neg\nprims.neg\n(Tensor self) -> Tensor\nprims.real\nprims.real\n(Tensor(a) self) -> Tensor(a)\nprims.reciprocal\nprims.reciprocal\n(Tensor self) -> Tensor\nprims.round\nprims.round\n(Tensor self) -> Tensor\nprims.sign\nprims.sign\n(Tensor self) -> Tensor\nprims.signbit\nprims.signbit\n(Tensor self) -> Tensor\nprims.sin\nprims.sin\n(Tensor self) -> Tensor\nprims.sinh\nprims.sinh\n(Tensor self) -> Tensor\nprims.spherical_bessel_j0\nprims.spherical_bessel_j0\n(Tensor self) -> Tensor\nprims.sqrt\nprims.sqrt\n(Tensor self) -> Tensor\nprims.tan\nprims.tan\n(Tensor self) -> Tensor\nprims.tanh\nprims.tanh\n(Tensor self) -> Tensor\nprims.trunc\nprims.trunc\n(Tensor self) -> Tensor\nprims.add\nprims.add\n(Tensor self, Tensor other) -> Tensor\nprims.atan2\nprims.atan2\n(Tensor self, Tensor other) -> Tensor\nprims.bitwise_and\nprims.bitwise_and\n(Tensor self, Tensor other) -> Tensor\nprims.bitwise_or\nprims.bitwise_or\n(Tensor self, Tensor other) -> Tensor\nprims.bitwise_xor\nprims.bitwise_xor\n(Tensor self, Tensor other) -> Tensor\nprims.div\nprims.div\n(Tensor self, Tensor other) -> Tensor\nprims.eq\nprims.eq\n(Tensor self, Tensor other) -> Tensor\nprims.fmax\nprims.fmax\n(Tensor self, Tensor other) -> Tensor\nprims.fmin\nprims.fmin\n(Tensor self, Tensor other) -> Tensor\nprims.fmod\nprims.fmod\n(Tensor self, Tensor other) -> Tensor\nprims.frexp\nprims.frexp\n(Tensor self) -> (Tensor mantissa, Tensor exponent)\nprims.gcd\nprims.gcd\n(Tensor self, Tensor other) -> Tensor\nprims.ge\nprims.ge\n(Tensor self, Tensor other) -> Tensor\nprims.gt\nprims.gt\n(Tensor self, Tensor other) -> Tensor\nprims.hypot\nprims.hypot\n(Tensor self, Tensor other) -> Tensor\nprims.igamma\nprims.igamma\n(Tensor self, Tensor other) -> Tensor\nprims.igammac\nprims.igammac\n(Tensor self, Tensor other) -> Tensor\nprims.le\nprims.le\n(Tensor self, Tensor other) -> Tensor\nprims.lt\nprims.lt\n(Tensor self, Tensor other) -> Tensor\nprims.maximum\nprims.maximum\n(Tensor self, Tensor other) -> Tensor\nprims.minimum\nprims.minimum\n(Tensor self, Tensor other) -> Tensor\nprims.mul\nprims.mul\n(Tensor self, Tensor other) -> Tensor\nprims.ne\nprims.ne\n(Tensor self, Tensor other) -> Tensor\nprims.nextafter\nprims.nextafter\n(Tensor self, Tensor other) -> Tensor\nprims.pow\nprims.pow\n(Tensor self, Tensor other) -> Tensor\nprims.remainder\nprims.remainder\n(Tensor self, Tensor other) -> Tensor\nprims.rsqrt\nprims.rsqrt\n(Tensor self) -> Tensor\nprims.shift_left\nprims.shift_left\n(Tensor self, Tensor other) -> Tensor\nprims.shift_right_arithmetic\nprims.shift_right_arithmetic\n(Tensor self, Tensor other) -> Tensor\nprims.sub\nprims.sub\n(Tensor self, Tensor other) -> Tensor\nprims.zeta\nprims.zeta\n(Tensor self, Tensor other) -> Tensor\nprims.as_strided\nprims.as_strided\n(Tensor(a!) a, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor(a!)\nprims.broadcast_in_dim\nprims.broadcast_in_dim\n(Tensor(a) a, SymInt[] shape, int[] broadcast_dimensions) -> Tensor(a)\nprims.collapse_view\nprims.collapse_view\n(Tensor(a) a, int start, int end) -> Tensor(a)\nprims.conj\nprims.conj\n(Tensor(a) a) -> Tensor(a)\nprims.split_dim\nprims.split_dim\n(Tensor(a) a, int dim, SymInt outer_length) -> Tensor(a)\nprims.squeeze\nprims.squeeze\n(Tensor(a) a, int[] dimensions) -> Tensor(a)\nprims.transpose\nprims.transpose\n(Tensor(a) a, int[] permutation) -> Tensor(a)\nprims.view_of\nprims.view_of\n(Tensor(a) a) -> Tensor(a)\nprims.view_of_dtype\nprims.view_of_dtype\n(Tensor(a) a, ScalarType dtype) -> Tensor(a)\nprims.as_strided_scatter\nprims.as_strided_scatter\n(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor\nprims.collapse\nprims.collapse\n(Tensor a, int start, int end) -> Tensor\nprims.cat\nprims.cat\n(Tensor[] tensors, int dim) -> Tensor\nprims.reshape\nprims.reshape\n(Tensor a, SymInt[] shape) -> Tensor\nprims.rev\nprims.rev\n(Tensor a, int[] dims) -> Tensor\nprims.where\nprims.where\n(Tensor pred, Tensor a, Tensor b) -> Tensor\nprims.clone\nprims.clone\n(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor\nprims.convert_element_type\nprims.convert_element_type\n(Tensor a, ScalarType dtype) -> Tensor\nprims.device_put\nprims.device_put\n(Tensor a, Device device, bool non_blocking=False) -> Tensor\nprims.item\nprims.item\n(Tensor a) -> Scalar\nprims.maximum_value\nprims.maximum_value\n(ScalarType dtype) -> Scalar\nprims.minimum_value\nprims.minimum_value\n(ScalarType dtype) -> Scalar\nprims.copy_strided\nprims.copy_strided\n(Tensor a, SymInt[] stride) -> Tensor\nprims.copy_to\nprims.copy_to\n(Tensor(a!) a, Tensor b) -> Tensor(a!)\nprims.resize\nprims.resize\n(Tensor(a!) a, SymInt[] shape) -> Tensor(a!)\nprims.amax\nprims.amax\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.amin\nprims.amin\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.prod\nprims.prod\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.sum\nprims.sum\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.xor_sum\nprims.xor_sum\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.var\nprims.var\n(Tensor inp, int[]? dims, float? correction=1, *, ScalarType? output_dtype=None) -> Tensor\nprims.empty_strided\nprims.empty_strided\n(SymInt[] shape, SymInt[] strides, *, ScalarType dtype, Device device, bool requires_grad) -> Tensor\nprims.empty_permuted\nprims.empty_permuted\n(SymInt[] shape, int[] physical_layout, *, ScalarType dtype, Device device, bool requires_grad) -> Tensor\nprims.scalar_tensor\nprims.scalar_tensor\n(Scalar s, *, ScalarType? dtype=None, Device? device=None) -> Tensor\nprims.iota\nprims.iota\n(SymInt length, *, SymInt start, SymInt step, ScalarType dtype, Device device, bool requires_grad) -> Tensor\nprims.svd\nprims.svd\n(Tensor A, *, bool full_matrices) -> (Tensor U, Tensor S, Tensor Vh)\nprims.normal\nprims.normal\n(SymInt[] shape, *, Scalar mean, Scalar std, ScalarType dtype, Device device, bool requires_grad, Generator? generator=None) -> Tensor\nprims.uniform\nprims.uniform\n(SymInt[] shape, *, Scalar low, Scalar high, ScalarType dtype, Device device, Generator? generator=None) -> Tensor\nprims.fft_r2c\nprims.fft_r2c\n(Tensor self, *, int[] dim, bool onesided) -> Tensor\nprims.fft_c2c\nprims.fft_c2c\n(Tensor self, *, int[] dim, bool forward) -> Tensor\nprims.fft_c2r\nprims.fft_c2r\n(Tensor self, *, int[] dim, SymInt last_dim_size) -> Tensor\nprims._make_token\nprims._make_token\n() -> Tensor\nprims._sink_tokens\nprims._sink_tokens\n(Tensor[] tokens) -> ()",
  "url": "https://pytorch.org/docs/stable/torch.compiler_ir.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
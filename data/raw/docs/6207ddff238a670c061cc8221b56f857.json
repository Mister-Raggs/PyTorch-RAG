{
  "doc_id": "6207ddff238a670c061cc8221b56f857",
  "source": "pytorch_docs",
  "title": "Quantization API Reference \u2014 PyTorch 2.9 documentation",
  "text": "\n## Quantization API Reference#\n\nCreated On: Jul 25, 2020 | Last Updated On: Jun 18, 2025\n\n## torch.ao.quantization#\n\nThis module contains Eager mode quantization APIs.\n\n## Top level APIs#\n\nquantize\n\nquantize\nQuantize the input float model with post training static quantization.\nquantize_dynamic\n\nquantize_dynamic\nConverts a float model to dynamic (i.e.\nquantize_qat\n\nquantize_qat\nDo quantization aware training and output a quantized model\nprepare\n\nprepare\nPrepares a copy of the model for quantization calibration or quantization-aware training.\nprepare_qat\n\nprepare_qat\nPrepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.\nconvert\n\nconvert\nConverts submodules in input module to a different module according tomappingby callingfrom_floatmethod on the target module class.\n\n## Preparing model for quantization#\n\nfuse_modules.fuse_modules\nfuse_modules.fuse_modules\nFuse a list of modules into a single module.\nQuantStub\n\nQuantStub\nQuantize stub module, before calibration, this is same as an observer, it will be swapped asnnq.Quantizeinconvert.\nDeQuantStub\n\nDeQuantStub\nDequantize stub module, before calibration, this is same as identity, this will be swapped asnnq.DeQuantizeinconvert.\nQuantWrapper\n\nQuantWrapper\nA wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.\nadd_quant_dequant\n\nadd_quant_dequant\nWrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.\n\n## Utility functions#\n\nswap_module\n\nswap_module\nSwaps the module if it has a quantized counterpart and it has anobserverattached.\npropagate_qconfig_\n\npropagate_qconfig_\nPropagate qconfig through the module hierarchy and assignqconfigattribute on each leaf module\ndefault_eval_fn\n\ndefault_eval_fn\nDefine the default evaluation function.\n\n## torch.ao.quantization.quantize_fx#\n\nThis module contains FX graph mode quantization APIs (prototype).\nprepare_fx\n\nprepare_fx\nPrepare a model for post training quantization\nprepare_qat_fx\n\nprepare_qat_fx\nPrepare a model for quantization aware training\nconvert_fx\n\nconvert_fx\nConvert a calibrated or trained model to a quantized model\nfuse_fx\n\nfuse_fx\nFuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.\n\n## torch.ao.quantization.qconfig_mapping#\n\nThis module contains QConfigMapping for configuring FX graph mode quantization.\nQConfigMapping\n\nQConfigMapping\nMapping from model ops totorch.ao.quantization.QConfigs.\ntorch.ao.quantization.QConfig\nget_default_qconfig_mapping\n\nget_default_qconfig_mapping\nReturn the default QConfigMapping for post training quantization.\nget_default_qat_qconfig_mapping\n\nget_default_qat_qconfig_mapping\nReturn the default QConfigMapping for quantization aware training.\n\n## torch.ao.quantization.backend_config#\n\nThis module contains BackendConfig, a config object that defines how quantization is supported\nin a backend. Currently only used by FX Graph Mode Quantization, but we may extend Eager Mode\nQuantization to work with this as well.\nBackendConfig\n\nBackendConfig\nConfig that defines the set of patterns that can be quantized on a given backend, and how reference quantized models can be produced from these patterns.\nBackendPatternConfig\n\nBackendPatternConfig\nConfig object that specifies quantization behavior for a given operator pattern.\nDTypeConfig\n\nDTypeConfig\nConfig object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases.\nDTypeWithConstraints\n\nDTypeWithConstraints\nConfig for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used inDTypeConfig.\nDTypeConfig\nObservationType\n\nObservationType\nAn enum that represents different ways of how an operator/operator pattern should be observed\n\n## torch.ao.quantization.fx.custom_config#\n\nThis module contains a few CustomConfig classes that\u2019s used in both eager mode and FX graph mode quantization\nFuseCustomConfig\n\nFuseCustomConfig\nCustom configuration forfuse_fx().\nfuse_fx()\nPrepareCustomConfig\n\nPrepareCustomConfig\nCustom configuration forprepare_fx()andprepare_qat_fx().\nprepare_fx()\nprepare_qat_fx()\nConvertCustomConfig\n\nConvertCustomConfig\nCustom configuration forconvert_fx().\nconvert_fx()\nStandaloneModuleConfigEntry\n\nStandaloneModuleConfigEntry\n\n\n## torch.ao.quantization.quantizer#\n\n\n## torch.ao.quantization.pt2e (quantization in pytorch 2.0 export implementation)#\n\n\n## torch.ao.quantization.pt2e.export_utils#\n\nmodel_is_exported\n\nmodel_is_exported\nReturn True if thetorch.nn.Modulewas exported, False otherwise (e.g.\n\n## torch.ao.quantization.pt2e.lowering#\n\nlower_pt2e_quantized_to_x86\n\nlower_pt2e_quantized_to_x86\nLower a PT2E-qantized model to x86 backend.\n\n## PT2 Export (pt2e) Numeric Debugger#\n\ngenerate_numeric_debug_handle\n\ngenerate_numeric_debug_handle\nAttach numeric_debug_handle_id for all nodes in the graph module of the given ExportedProgram, like conv2d, squeeze, conv1d, etc, except for placeholder.\nCUSTOM_KEY\n\nCUSTOM_KEY\nstr(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str\nNUMERIC_DEBUG_HANDLE_KEY\n\nNUMERIC_DEBUG_HANDLE_KEY\nstr(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str\nprepare_for_propagation_comparison\n\nprepare_for_propagation_comparison\nAdd output loggers to node that has numeric_debug_handle\nextract_results_from_loggers\n\nextract_results_from_loggers\nFor a given model, extract the tensors stats and related information for each debug handle.\ncompare_results\n\ncompare_results\nGiven two dict mapping fromdebug_handle_id(int) to list of tensors return a map fromdebug_handle_idtoNodeAccuracySummarythat contains comparison information like SQNR, MSE etc.\n\n## torch (quantization related functions)#\n\nThis describes the quantization related functions of thetorchnamespace.\ntorch\nquantize_per_tensor\n\nquantize_per_tensor\nConverts a float tensor to a quantized tensor with given scale and zero point.\nquantize_per_channel\n\nquantize_per_channel\nConverts a float tensor to a per-channel quantized tensor with given scales and zero points.\ndequantize\n\ndequantize\nReturns an fp32 Tensor by dequantizing a quantized Tensor\n\n## torch.Tensor (quantization related methods)#\n\nQuantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor.\nview\n\nview\nReturns a new tensor with the same data as theselftensor but of a differentshape.\nself\nshape\nas_strided\n\nas_strided\nSeetorch.as_strided()\ntorch.as_strided()\nexpand\n\nexpand\nReturns a new view of theselftensor with singleton dimensions expanded to a larger size.\nself\nflatten\n\nflatten\nSeetorch.flatten()\ntorch.flatten()\nselect\n\nselect\nSeetorch.select()\ntorch.select()\nne\n\nne\nSeetorch.ne().\ntorch.ne()\neq\n\neq\nSeetorch.eq()\ntorch.eq()\nge\n\nge\nSeetorch.ge().\ntorch.ge()\nle\n\nle\nSeetorch.le().\ntorch.le()\ngt\n\ngt\nSeetorch.gt().\ntorch.gt()\nlt\n\nlt\nSeetorch.lt().\ntorch.lt()\ncopy_\n\ncopy_\nCopies the elements fromsrcintoselftensor and returnsself.\nsrc\nself\nself\nclone\n\nclone\nSeetorch.clone()\ntorch.clone()\ndequantize\n\ndequantize\nGiven a quantized Tensor, dequantize it and return the dequantized float Tensor.\nequal\n\nequal\nSeetorch.equal()\ntorch.equal()\nint_repr\n\nint_repr\nGiven a quantized Tensor,self.int_repr()returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.\nself.int_repr()\nmax\n\nmax\nSeetorch.max()\ntorch.max()\nmean\n\nmean\nSeetorch.mean()\ntorch.mean()\nmin\n\nmin\nSeetorch.min()\ntorch.min()\nq_scale\n\nq_scale\nGiven a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().\nq_zero_point\n\nq_zero_point\nGiven a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().\nq_per_channel_scales\n\nq_per_channel_scales\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.\nq_per_channel_zero_points\n\nq_per_channel_zero_points\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.\nq_per_channel_axis\n\nq_per_channel_axis\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.\nresize_\n\nresize_\nResizesselftensor to the specified size.\nself\nsort\n\nsort\nSeetorch.sort()\ntorch.sort()\ntopk\n\ntopk\nSeetorch.topk()\ntorch.topk()\n\n## torch.ao.quantization.observer#\n\nThis module contains observers which are used to collect statistics about\nthe values observed during calibration (PTQ) or training (QAT).\nObserverBase\n\nObserverBase\nBase observer Module.\nMinMaxObserver\n\nMinMaxObserver\nObserver module for computing the quantization parameters based on the running min and max values.\nMovingAverageMinMaxObserver\n\nMovingAverageMinMaxObserver\nObserver module for computing the quantization parameters based on the moving average of the min and max values.\nPerChannelMinMaxObserver\n\nPerChannelMinMaxObserver\nObserver module for computing the quantization parameters based on the running per channel min and max values.\nMovingAveragePerChannelMinMaxObserver\n\nMovingAveragePerChannelMinMaxObserver\nObserver module for computing the quantization parameters based on the running per channel min and max values.\nHistogramObserver\n\nHistogramObserver\nThe module records the running histogram of tensor values along with min/max values.\nPlaceholderObserver\n\nPlaceholderObserver\nObserver that doesn't do anything and just passes its configuration to the quantized module's.from_float().\n.from_float()\nRecordingObserver\n\nRecordingObserver\nThe module is mainly for debug and records the tensor values during runtime.\nNoopObserver\n\nNoopObserver\nObserver that doesn't do anything and just passes its configuration to the quantized module's.from_float().\n.from_float()\nget_observer_state_dict\n\nget_observer_state_dict\nReturns the state dict corresponding to the observer stats.\nload_observer_state_dict\n\nload_observer_state_dict\nGiven input model and a state_dict containing model observer stats, load the stats back into the model.\ndefault_observer\n\ndefault_observer\nDefault observer for static quantization, usually used for debugging.\ndefault_placeholder_observer\n\ndefault_placeholder_observer\nDefault placeholder observer, usually used for quantization to torch.float16.\ndefault_debug_observer\n\ndefault_debug_observer\nDefault debug-only observer.\ndefault_weight_observer\n\ndefault_weight_observer\nDefault weight observer.\ndefault_histogram_observer\n\ndefault_histogram_observer\nDefault histogram observer, usually used for PTQ.\ndefault_per_channel_weight_observer\n\ndefault_per_channel_weight_observer\nDefault per-channel weight observer, usually used on backends where per-channel weight quantization is supported, such asfbgemm.\ndefault_dynamic_quant_observer\n\ndefault_dynamic_quant_observer\nDefault observer for dynamic quantization.\ndefault_float_qparams_observer\n\ndefault_float_qparams_observer\nDefault observer for a floating point zero-point.\nAffineQuantizedObserverBase\n\nAffineQuantizedObserverBase\nObserver module for affine quantization (pytorch/ao)\nGranularity\n\nGranularity\nBase class for representing the granularity of quantization.\nMappingType\n\nMappingType\nHow floating point number is mapped to integer number\nPerAxis\n\nPerAxis\nRepresents per-axis granularity in quantization.\nPerBlock\n\nPerBlock\nRepresents per-block granularity in quantization.\nPerGroup\n\nPerGroup\nRepresents per-channel group granularity in quantization.\nPerRow\n\nPerRow\nRepresents row-wise granularity in quantization.\nPerTensor\n\nPerTensor\nRepresents per-tensor granularity in quantization.\nPerToken\n\nPerToken\nRepresents per-token granularity in quantization.\nTorchAODType\n\nTorchAODType\nPlaceholder for dtypes that do not exist in PyTorch core yet.\nZeroPointDomain\n\nZeroPointDomain\nEnum that indicate whether zero_point is in integer domain or floating point domain\nget_block_size\n\nget_block_size\nGet the block size based on the input shape and granularity type.\n\n## torch.ao.quantization.fake_quantize#\n\nThis module implements modules which are used to perform fake quantization\nduring QAT.\nFakeQuantizeBase\n\nFakeQuantizeBase\nBase fake quantize module.\nFakeQuantize\n\nFakeQuantize\nSimulate the quantize and dequantize operations in training time.\nFixedQParamsFakeQuantize\n\nFixedQParamsFakeQuantize\nSimulate quantize and dequantize in training time.\nFusedMovingAvgObsFakeQuantize\n\nFusedMovingAvgObsFakeQuantize\nDefine a fused module to observe the tensor.\ndefault_fake_quant\n\ndefault_fake_quant\nDefault fake_quant for activations.\ndefault_weight_fake_quant\n\ndefault_weight_fake_quant\nDefault fake_quant for weights.\ndefault_per_channel_weight_fake_quant\n\ndefault_per_channel_weight_fake_quant\nDefault fake_quant for per-channel weights.\ndefault_histogram_fake_quant\n\ndefault_histogram_fake_quant\nFake_quant for activations using a histogram..\ndefault_fused_act_fake_quant\n\ndefault_fused_act_fake_quant\nFused version ofdefault_fake_quant, with improved performance.\ndefault_fused_wt_fake_quant\n\ndefault_fused_wt_fake_quant\nFused version ofdefault_weight_fake_quant, with improved performance.\ndefault_fused_per_channel_wt_fake_quant\n\ndefault_fused_per_channel_wt_fake_quant\nFused version ofdefault_per_channel_weight_fake_quant, with improved performance.\ndisable_fake_quant\n\ndisable_fake_quant\nDisable fake quantization for the module.\nenable_fake_quant\n\nenable_fake_quant\nEnable fake quantization for the module.\ndisable_observer\n\ndisable_observer\nDisable observation for this module.\nenable_observer\n\nenable_observer\nEnable observation for this module.\n\n## torch.ao.quantization.qconfig#\n\nThis module definesQConfigobjects which are used\nto configure quantization settings for individual ops.\nQConfig\nQConfig\n\nQConfig\nDescribes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.\ndefault_qconfig\n\ndefault_qconfig\nDefault qconfig configuration.\ndefault_debug_qconfig\n\ndefault_debug_qconfig\nDefault qconfig configuration for debugging.\ndefault_per_channel_qconfig\n\ndefault_per_channel_qconfig\nDefault qconfig configuration for per channel weight quantization.\ndefault_dynamic_qconfig\n\ndefault_dynamic_qconfig\nDefault dynamic qconfig.\nfloat16_dynamic_qconfig\n\nfloat16_dynamic_qconfig\nDynamic qconfig with weights quantized totorch.float16.\nfloat16_static_qconfig\n\nfloat16_static_qconfig\nDynamic qconfig with both activations and weights quantized totorch.float16.\nper_channel_dynamic_qconfig\n\nper_channel_dynamic_qconfig\nDynamic qconfig with weights quantized per channel.\nfloat_qparams_weight_only_qconfig\n\nfloat_qparams_weight_only_qconfig\nDynamic qconfig with weights quantized with a floating point zero_point.\ndefault_qat_qconfig\n\ndefault_qat_qconfig\nDefault qconfig for QAT.\ndefault_weight_only_qconfig\n\ndefault_weight_only_qconfig\nDefault qconfig for quantizing weights only.\ndefault_activation_only_qconfig\n\ndefault_activation_only_qconfig\nDefault qconfig for quantizing activations only.\ndefault_qat_qconfig_v2\n\ndefault_qat_qconfig_v2\nFused version ofdefault_qat_config, has performance benefits.\n\n## torch.ao.nn.intrinsic#\n\nThis module implements the combined (fused) modules conv + relu which can\nthen be quantized.\nConvReLU1d\n\nConvReLU1d\nThis is a sequential container which calls the Conv1d and ReLU modules.\nConvReLU2d\n\nConvReLU2d\nThis is a sequential container which calls the Conv2d and ReLU modules.\nConvReLU3d\n\nConvReLU3d\nThis is a sequential container which calls the Conv3d and ReLU modules.\nLinearReLU\n\nLinearReLU\nThis is a sequential container which calls the Linear and ReLU modules.\nConvBn1d\n\nConvBn1d\nThis is a sequential container which calls the Conv 1d and Batch Norm 1d modules.\nConvBn2d\n\nConvBn2d\nThis is a sequential container which calls the Conv 2d and Batch Norm 2d modules.\nConvBn3d\n\nConvBn3d\nThis is a sequential container which calls the Conv 3d and Batch Norm 3d modules.\nConvBnReLU1d\n\nConvBnReLU1d\nThis is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules.\nConvBnReLU2d\n\nConvBnReLU2d\nThis is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules.\nConvBnReLU3d\n\nConvBnReLU3d\nThis is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules.\nBNReLU2d\n\nBNReLU2d\nThis is a sequential container which calls the BatchNorm 2d and ReLU modules.\nBNReLU3d\n\nBNReLU3d\nThis is a sequential container which calls the BatchNorm 3d and ReLU modules.\n\n## torch.ao.nn.intrinsic.qat#\n\nThis module implements the versions of those fused operations needed for\nquantization aware training.\nLinearReLU\n\nLinearReLU\nA LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBn1d\n\nConvBn1d\nA ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBnReLU1d\n\nConvBnReLU1d\nA ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBn2d\n\nConvBn2d\nA ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBnReLU2d\n\nConvBnReLU2d\nA ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvReLU2d\n\nConvReLU2d\nA ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.\nConvBn3d\n\nConvBn3d\nA ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBnReLU3d\n\nConvBnReLU3d\nA ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvReLU3d\n\nConvReLU3d\nA ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.\nupdate_bn_stats\n\nupdate_bn_stats\n\nfreeze_bn_stats\n\nfreeze_bn_stats\n\n\n## torch.ao.nn.intrinsic.quantized#\n\nThis module implements the quantized implementations of fused operations\nlike conv + relu. No BatchNorm variants as it\u2019s usually folded into convolution\nfor inference.\nBNReLU2d\n\nBNReLU2d\nA BNReLU2d module is a fused module of BatchNorm2d and ReLU\nBNReLU3d\n\nBNReLU3d\nA BNReLU3d module is a fused module of BatchNorm3d and ReLU\nConvReLU1d\n\nConvReLU1d\nA ConvReLU1d module is a fused module of Conv1d and ReLU\nConvReLU2d\n\nConvReLU2d\nA ConvReLU2d module is a fused module of Conv2d and ReLU\nConvReLU3d\n\nConvReLU3d\nA ConvReLU3d module is a fused module of Conv3d and ReLU\nLinearReLU\n\nLinearReLU\nA LinearReLU module fused from Linear and ReLU modules\n\n## torch.ao.nn.intrinsic.quantized.dynamic#\n\nThis module implements the quantized dynamic implementations of fused operations\nlike linear + relu.\nLinearReLU\n\nLinearReLU\nA LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization.\n\n## torch.ao.nn.qat#\n\nThis module implements versions of the key nn modulesConv2d()andLinear()which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization.\nConv2d\n\nConv2d\nA Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.\nConv3d\n\nConv3d\nA Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training.\nLinear\n\nLinear\nA linear module attached with FakeQuantize modules for weight, used for quantization aware training.\n\n## torch.ao.nn.qat.dynamic#\n\nThis module implements versions of the key nn modules such asLinear()which run in FP32 but with rounding applied to simulate the effect of INT8\nquantization and will be dynamically quantized during inference.\nLinear\n\nLinear\nA linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training.\n\n## torch.ao.nn.quantized#\n\nThis module implements the quantized versions of the nn layers such as~torch.nn.Conv2dandtorch.nn.ReLU.\n~torch.nn.Conv2d\ntorch.nn.ReLU\nReLU6\n\nReLU6\nApplies the element-wise function:\nHardswish\n\nHardswish\nThis is the quantized version ofHardswish.\nHardswish\nELU\n\nELU\nThis is the quantized equivalent ofELU.\nELU\nLeakyReLU\n\nLeakyReLU\nThis is the quantized equivalent ofLeakyReLU.\nLeakyReLU\nSigmoid\n\nSigmoid\nThis is the quantized equivalent ofSigmoid.\nSigmoid\nBatchNorm2d\n\nBatchNorm2d\nThis is the quantized version ofBatchNorm2d.\nBatchNorm2d\nBatchNorm3d\n\nBatchNorm3d\nThis is the quantized version ofBatchNorm3d.\nBatchNorm3d\nConv1d\n\nConv1d\nApplies a 1D convolution over a quantized input signal composed of several quantized input planes.\nConv2d\n\nConv2d\nApplies a 2D convolution over a quantized input signal composed of several quantized input planes.\nConv3d\n\nConv3d\nApplies a 3D convolution over a quantized input signal composed of several quantized input planes.\nConvTranspose1d\n\nConvTranspose1d\nApplies a 1D transposed convolution operator over an input image composed of several input planes.\nConvTranspose2d\n\nConvTranspose2d\nApplies a 2D transposed convolution operator over an input image composed of several input planes.\nConvTranspose3d\n\nConvTranspose3d\nApplies a 3D transposed convolution operator over an input image composed of several input planes.\nEmbedding\n\nEmbedding\nA quantized Embedding module with quantized packed weights as inputs.\nEmbeddingBag\n\nEmbeddingBag\nA quantized EmbeddingBag module with quantized packed weights as inputs.\nFloatFunctional\n\nFloatFunctional\nState collector class for float operations.\nFXFloatFunctional\n\nFXFloatFunctional\nmodule to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly\nQFunctional\n\nQFunctional\nWrapper class for quantized operations.\nLinear\n\nLinear\nA quantized linear module with quantized tensor as inputs and outputs.\nLayerNorm\n\nLayerNorm\nThis is the quantized version ofLayerNorm.\nLayerNorm\nGroupNorm\n\nGroupNorm\nThis is the quantized version ofGroupNorm.\nGroupNorm\nInstanceNorm1d\n\nInstanceNorm1d\nThis is the quantized version ofInstanceNorm1d.\nInstanceNorm1d\nInstanceNorm2d\n\nInstanceNorm2d\nThis is the quantized version ofInstanceNorm2d.\nInstanceNorm2d\nInstanceNorm3d\n\nInstanceNorm3d\nThis is the quantized version ofInstanceNorm3d.\nInstanceNorm3d\n\n## torch.ao.nn.quantized.functional#\n\nFunctional interface (quantized).\nThis module implements the quantized versions of the functional layers such as~torch.nn.functional.conv2dandtorch.nn.functional.relu. Note:torch.nn.functional.relu~torch.nn.functional.relutorch.nn.functional.relusupports quantized inputs.\navg_pool2d\n\navg_pool2d\nApplies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.\navg_pool3d\n\navg_pool3d\nApplies 3D average-pooling operation inkDtimeskH\u00d7kWkD \\ times kH \\times kWkDtimeskH\u00d7kWregions by step sizesD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sWsteps.\nadaptive_avg_pool2d\n\nadaptive_avg_pool2d\nApplies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.\nadaptive_avg_pool3d\n\nadaptive_avg_pool3d\nApplies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes.\nconv1d\n\nconv1d\nApplies a 1D convolution over a quantized 1D input composed of several input planes.\nconv2d\n\nconv2d\nApplies a 2D convolution over a quantized 2D input composed of several input planes.\nconv3d\n\nconv3d\nApplies a 3D convolution over a quantized 3D input composed of several input planes.\ninterpolate\n\ninterpolate\nDown/up samples the input to either the givensizeor the givenscale_factor\nsize\nscale_factor\nlinear\n\nlinear\nApplies a linear transformation to the incoming quantized data:y=xAT+by = xA^T + by=xAT+b.\nmax_pool1d\n\nmax_pool1d\nApplies a 1D max pooling over a quantized input signal composed of several quantized input planes.\nmax_pool2d\n\nmax_pool2d\nApplies a 2D max pooling over a quantized input signal composed of several quantized input planes.\ncelu\n\ncelu\nApplies the quantized CELU function element-wise.\nleaky_relu\n\nleaky_relu\nQuantized version of the.\nhardtanh\n\nhardtanh\nThis is the quantized version ofhardtanh().\nhardtanh()\nhardswish\n\nhardswish\nThis is the quantized version ofhardswish().\nhardswish()\nthreshold\n\nthreshold\nApplies the quantized version of the threshold function element-wise:\nelu\n\nelu\nThis is the quantized version ofelu().\nelu()\nhardsigmoid\n\nhardsigmoid\nThis is the quantized version ofhardsigmoid().\nhardsigmoid()\nclamp\n\nclamp\nfloat(input, min_, max_) -> Tensor\nupsample\n\nupsample\nUpsamples the input to either the givensizeor the givenscale_factor\nsize\nscale_factor\nupsample_bilinear\n\nupsample_bilinear\nUpsamples the input, using bilinear upsampling.\nupsample_nearest\n\nupsample_nearest\nUpsamples the input, using nearest neighbours' pixel values.\n\n## torch.ao.nn.quantizable#\n\nThis module implements the quantizable versions of some of the nn layers.\nThese modules can be used in conjunction with the custom module mechanism,\nby providing thecustom_module_configargument to both prepare and convert.\ncustom_module_config\nLSTM\n\nLSTM\nA quantizable long short-term memory (LSTM).\nMultiheadAttention\n\nMultiheadAttention\n\n\n## torch.ao.nn.quantized.dynamic#\n\nDynamically quantizedLinear,LSTM,LSTMCell,GRUCell, andRNNCell.\nLinear\nLSTM\nLSTMCell\nGRUCell\nRNNCell\nLinear\n\nLinear\nA dynamic quantized linear module with floating point tensor as inputs and outputs.\nLSTM\n\nLSTM\nA dynamic quantized LSTM module with floating point tensor as inputs and outputs.\nGRU\n\nGRU\nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\nRNNCell\n\nRNNCell\nAn Elman RNN cell with tanh or ReLU non-linearity.\nLSTMCell\n\nLSTMCell\nA long short-term memory (LSTM) cell.\nGRUCell\n\nGRUCell\nA gated recurrent unit (GRU) cell\n\n## Quantized dtypes and quantization schemes#\n\nNote that operator implementations currently only\nsupport per channel quantization for weights of theconvandlinearoperators. Furthermore, the input data is\nmapped linearly to the quantized data and vice versa\nas follows:\nwhereclamp(.)\\text{clamp}(.)clamp(.)is the same asclamp()while the\nscalesssand zero pointzzzare then computed\nas described inMinMaxObserver, specifically:\nclamp()\nMinMaxObserver\nwhere :math:[x_\\text{min},x_\\text{max}]denotes the range of the input data while\n:math:Q_\\text{min}and :math:Q_\\text{max}are respectively the minimum and maximum values of the quantized dtype.\n[x_\\text{min},x_\\text{max}]\nQ_\\text{min}\nQ_\\text{max}\nNote that the choice of :math:sand :math:zimplies that zero is represented with no quantization error whenever zero is within\nthe range of the input data or symmetric quantization is being used.\ns\nz\nAdditional data types and quantization schemes can be implemented through\nthecustomoperatormechanism<https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>_.\ncustomoperatormechanism<https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>\ntorch.qscheme\u2014 Type to describe the quantization scheme of a tensor.\nSupported types:\ntorch.qscheme\ntorch.per_tensor_affine\u2014 per tensor, asymmetric\ntorch.per_tensor_affine\ntorch.per_channel_affine\u2014 per channel, asymmetric\ntorch.per_channel_affine\ntorch.per_tensor_symmetric\u2014 per tensor, symmetric\ntorch.per_tensor_symmetric\ntorch.per_channel_symmetric\u2014 per channel, symmetric\ntorch.per_channel_symmetric\ntorch.dtype\u2014 Type to describe the data. Supported types:\ntorch.dtype\ntorch.quint8\u2014 8-bit unsigned integer\ntorch.quint8\ntorch.qint8\u2014 8-bit signed integer\ntorch.qint8\ntorch.qint32\u2014 32-bit signed integer\ntorch.qint32\nQAT Modules.\nThis package is in the process of being deprecated.\nPlease, usetorch.ao.nn.qat.modulesinstead.\nQAT Dynamic Modules.\nThis package is in the process of being deprecated.\nPlease, usetorch.ao.nn.qat.dynamicinstead.\nThis file is in the process of migration totorch/ao/quantization, and\nis kept here for compatibility while the migration process is ongoing.\nIf you are adding a new entry/functionality, please, add it to the\nappropriate files undertorch/ao/quantization/fx/, while adding an import statement\nhere.\nQAT Dynamic Modules.\nThis package is in the process of being deprecated.\nPlease, usetorch.ao.nn.qat.dynamicinstead.\nQuantized Modules.\nThetorch.nn.quantizednamespace is in the process of being deprecated.\nPlease, usetorch.ao.nn.quantizedinstead.\nQuantized Dynamic Modules.\nThis file is in the process of migration totorch/ao/nn/quantized/dynamic,\nand is kept here for compatibility while the migration process is ongoing.\nIf you are adding a new entry/functionality, please, add it to the\nappropriate file under thetorch/ao/nn/quantized/dynamic,\nwhile adding an import statement here.",
  "url": "https://pytorch.org/docs/stable/quantization-support.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
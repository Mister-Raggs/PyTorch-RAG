{
  "doc_id": "dc05bc37cace212b3ec5d7b17c3a5186",
  "source": "pytorch_docs",
  "title": "Reproducibility \u2014 PyTorch 2.9 documentation",
  "text": "\n## Reproducibility#\n\nCreated On: Sep 11, 2018 | Last Updated On: Nov 26, 2024\nCompletely reproducible results are not guaranteed across PyTorch releases,\nindividual commits, or different platforms. Furthermore, results may not be\nreproducible between CPU and GPU executions, even when using identical seeds.\nHowever, there are some steps you can take to limit the number of sources of\nnondeterministic behavior for a specific platform, device, and PyTorch release.\nFirst, you can control sources of randomness that can cause multiple executions\nof your application to behave differently. Second, you can configure PyTorch\nto avoid using nondeterministic algorithms for some operations, so that multiple\ncalls to those operations, given the same inputs, will produce the same result.\nWarning\nDeterministic operations are often slower than nondeterministic operations, so\nsingle-run performance may decrease for your model. However, determinism may\nsave time in development by facilitating experimentation, debugging, and\nregression testing.\n\n## Controlling sources of randomness#\n\n\n## PyTorch random number generator#\n\nYou can usetorch.manual_seed()to seed the RNG for all devices (both\nCPU and CUDA):\ntorch.manual_seed()\n\n```python\nimport torch\ntorch.manual_seed(0)\n\n```\n\nSome PyTorch operations may use random numbers internally.torch.svd_lowrank()does this, for instance. Consequently, calling it\nmultiple times back-to-back with the same input arguments may give different\nresults. However, as long astorch.manual_seed()is set to a constant\nat the beginning of an application and all other sources of nondeterminism have\nbeen eliminated, the same series of random numbers will be generated each time\nthe application is run in the same environment.\ntorch.svd_lowrank()\ntorch.manual_seed()\nIt is also possible to obtain identical results from an operation that uses\nrandom numbers by settingtorch.manual_seed()to the same value between\nsubsequent calls.\ntorch.manual_seed()\n\n## Python#\n\nFor custom operators, you might need to set python seed as well:\n\n```python\nimport random\nrandom.seed(0)\n\n```\n\n\n## Random number generators in other libraries#\n\nIf you or any of the libraries you are using rely on NumPy, you can seed the global\nNumPy RNG with:\n\n```python\nimport numpy as np\nnp.random.seed(0)\n\n```\n\nHowever, some applications and libraries may use NumPy Random Generator objects,\nnot the global RNG\n(https://numpy.org/doc/stable/reference/random/generator.html), and those will\nneed to be seeded consistently as well.\nIf you are using any other libraries that use random number generators, refer to\nthe documentation for those libraries to see how to set consistent seeds for them.\n\n## CUDA convolution benchmarking#\n\nThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism\nacross multiple executions of an application. When a cuDNN convolution is called with a\nnew set of size parameters, an optional feature can run multiple convolution algorithms,\nbenchmarking them to find the fastest one. Then, the fastest algorithm will be used\nconsistently during the rest of the process for the corresponding set of size parameters.\nDue to benchmarking noise and different hardware, the benchmark may select different\nalgorithms on subsequent runs, even on the same machine.\nDisabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced\nperformance.\ntorch.backends.cudnn.benchmark=False\nHowever, if you do not need reproducibility across multiple executions of your application,\nthen performance might improve if the benchmarking feature is enabled withtorch.backends.cudnn.benchmark=True.\ntorch.backends.cudnn.benchmark=True\nNote that this setting is different from thetorch.backends.cudnn.deterministicsetting discussed below.\ntorch.backends.cudnn.deterministic\n\n## Avoiding nondeterministic algorithms#\n\ntorch.use_deterministic_algorithms()lets you configure PyTorch to use\ndeterministic algorithms instead of nondeterministic ones where available, and\nto throw an error if an operation is known to be nondeterministic (and without\na deterministic alternative).\ntorch.use_deterministic_algorithms()\nPlease check the documentation fortorch.use_deterministic_algorithms()for a full list of affected operations. If an operation does not act correctly\naccording to the documentation, or if you need a deterministic implementation\nof an operation that does not have one, please submit an issue:pytorch/pytorch#issues\ntorch.use_deterministic_algorithms()\nFor example, running the nondeterministic CUDA implementation oftorch.Tensor.index_add_()will throw an error:\ntorch.Tensor.index_add_()\n\n```python\n>>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\n'torch.use_deterministic_algorithms(True)'. ...\n\n```\n\nWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a\nnondeterministic algorithm, but when the deterministic flag is turned on, its alternate\ndeterministic implementation will be used:\ntorch.bmm()\n\n```python\n>>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\ntensor([[[ 1.1900, -2.3409],\n         [ 0.4796,  0.8003]],\n        [[ 0.1509,  1.8027],\n         [ 0.0333, -1.1444]]], device='cuda:0')\n\n```\n\nFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you\nshould set the environment variableCUBLAS_WORKSPACE_CONFIGaccording to CUDA documentation:https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n\n## CUDA convolution determinism#\n\nWhile disabling CUDA convolution benchmarking (discussed above) ensures that\nCUDA selects the same algorithm each time an application is run, that algorithm\nitself may be nondeterministic, unless eithertorch.use_deterministic_algorithms(True)ortorch.backends.cudnn.deterministic=Trueis set. The latter setting\ncontrols only this behavior, unliketorch.use_deterministic_algorithms()which will make other PyTorch operations behave deterministically, too.\ntorch.use_deterministic_algorithms(True)\ntorch.backends.cudnn.deterministic=True\ntorch.use_deterministic_algorithms()\n\n## CUDA RNN and LSTM#\n\nIn some versions of CUDA, RNNs and LSTM networks may have non-deterministic behavior.\nSeetorch.nn.RNN()andtorch.nn.LSTM()for details and workarounds.\ntorch.nn.RNN()\ntorch.nn.LSTM()\n\n## Filling uninitialized memory#\n\nOperations liketorch.empty()andtorch.Tensor.resize_()can return\ntensors with uninitialized memory that contain undefined values. Using such a\ntensor as an input to another operation is invalid if determinism is required,\nbecause the output will be nondeterministic. But there is nothing to actually\nprevent such invalid code from being run. So for safety,torch.utils.deterministic.fill_uninitialized_memoryis set toTrueby default, which will fill the uninitialized memory with a known value iftorch.use_deterministic_algorithms(True)is set. This will prevent the\npossibility of this kind of nondeterministic behavior.\ntorch.empty()\ntorch.Tensor.resize_()\ntorch.utils.deterministic.fill_uninitialized_memory\nTrue\ntorch.use_deterministic_algorithms(True)\nHowever, filling uninitialized memory is detrimental to performance. So if your\nprogram is valid and does not use uninitialized memory as the input to an\noperation, then this setting can be turned off for better performance.\n\n## DataLoader#\n\nDataLoader will reseed workers followingRandomness in multi-process data loadingalgorithm.\nUseworker_init_fn()andgeneratorto preserve reproducibility:\nworker_init_fn()\n\n```python\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker,\n    generator=g,\n)\n\n```\n",
  "url": "https://pytorch.org/docs/stable/notes/randomness.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
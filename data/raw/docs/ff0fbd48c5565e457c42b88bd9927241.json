{
  "doc_id": "ff0fbd48c5565e457c42b88bd9927241",
  "source": "pytorch_docs",
  "title": "torch.Storage \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.Storage#\n\nCreated On: Dec 30, 2016 | Last Updated On: Apr 14, 2025\nIn PyTorch, a regular tensor is a multi-dimensional array that is defined by the following components:\nStorage: The actual data of the tensor, stored as a contiguous, one-dimensional array of bytes.\ndtype: The data type of the elements in the tensor, such as torch.float32 or torch.int64.\ndtype\nshape: A tuple indicating the size of the tensor in each dimension.\nshape\nStride: The step size needed to move from one element to the next in each dimension.\nOffset: The starting point in the storage from which the tensor data begins. This will usually be 0 for newly\ncreated tensors.\nThese components together define the structure and data of a tensor, with the storage holding the\nactual data and the rest serving as metadata.\n\n## Untyped Storage API#\n\nAtorch.UntypedStorageis a contiguous, one-dimensional array of elements. Its length is equal to the number of\nbytes of the tensor. The storage serves as the underlying data container for tensors.\nIn general, a tensor created in PyTorch using regular constructors such aszeros(),zeros_like()ornew_zeros()will produce tensors where there is a one-to-one correspondence between the tensor\nstorage and the tensor itself.\ntorch.UntypedStorage\nzeros()\nzeros_like()\nnew_zeros()\nHowever, a storage is allowed to be shared by multiple tensors.\nFor instance, any view of a tensor (obtained throughview()or some, but not all, kinds of indexing\nlike integers and slices) will point to the same underlying storage as the original tensor.\nWhen serializing and deserializing tensors that share a common storage, the relationship is preserved, and the tensors\ncontinue to point to the same storage. Interestingly, deserializing multiple tensors that point to a single storage\ncan be faster than deserializing multiple independent tensors.\nview()\nA tensor storage can be accessed through theuntyped_storage()method. This will return an object of\ntypetorch.UntypedStorage.\nFortunately, storages have a unique identifier accessed through thetorch.UntypedStorage.data_ptr()method.\nIn regular settings, two tensors with the same data storage will have the same storagedata_ptr.\nHowever, tensors themselves can point to two separate storages, one for its data attribute and another for its grad\nattribute. Each will require adata_ptr()of its own. In general, there is no guarantee that atorch.Tensor.data_ptr()andtorch.UntypedStorage.data_ptr()match and this should not be assumed to be true.\nuntyped_storage()\ntorch.UntypedStorage\ntorch.UntypedStorage.data_ptr()\ndata_ptr\ndata_ptr()\ntorch.Tensor.data_ptr()\ntorch.UntypedStorage.data_ptr()\nUntyped storages are somewhat independent of the tensors that are built on them. Practically, this means that tensors\nwith different dtypes or shape can point to the same storage.\nIt also implies that a tensor storage can be changed, as the following example shows:\n\n```python\n>>> t = torch.ones(3)\n>>> s0 = t.untyped_storage()\n>>> s0\n 0\n 0\n 128\n 63\n 0\n 0\n 128\n 63\n 0\n 0\n 128\n 63\n[torch.storage.UntypedStorage(device=cpu) of size 12]\n>>> s1 = s0.clone()\n>>> s1.fill_(0)\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n[torch.storage.UntypedStorage(device=cpu) of size 12]\n>>> # Fill the tensor with a zeroed storage\n>>> t.set_(s1, storage_offset=t.storage_offset(), stride=t.stride(), size=t.size())\ntensor([0., 0., 0.])\n\n```\n\nWarning\nPlease note that directly modifying a tensor\u2019s storage as shown in this example is not a recommended practice.\nThis low-level manipulation is illustrated solely for educational purposes, to demonstrate the relationship between\ntensors and their underlying storages. In general, it\u2019s more efficient and safer to use standardtorch.Tensormethods, such asclone()andfill_(), to achieve the same results.\ntorch.Tensor\nclone()\nfill_()\nOther thandata_ptr, untyped storage also have other attributes such asfilename(in case the storage points to a file on disk),deviceoris_cudafor device checks. A storage can also be manipulated in-place or\nout-of-place with methods likecopy_,fill_orpin_memory. FOr more information, check the API\nreference below. Keep in mind that modifying storages is a low-level API and comes with risks!\nMost of these APIs also exist on the tensor level: if present, they should be prioritized over their storage\ncounterparts.\ndata_ptr\nfilename\ndevice\nis_cuda\ncopy_\nfill_\npin_memory\n\n## Special cases#\n\nWe mentioned that a tensor that has a non-Nonegradattribute has actually two pieces of data within it.\nIn this case,untyped_storage()will return the storage of thedataattribute,\nwhereas the storage of the gradient can be obtained throughtensor.grad.untyped_storage().\ngrad\nuntyped_storage()\ndata\ntensor.grad.untyped_storage()\n\n```python\n>>> t = torch.zeros(3, requires_grad=True)\n>>> t.sum().backward()\n>>> assert list(t.untyped_storage()) == [0] * 12  # the storage of the tensor is just 0s\n>>> assert list(t.grad.untyped_storage()) != [0] * 12  # the storage of the gradient isn't\n\n```\n\nTensors on\"meta\"device: Tensors on the\"meta\"device are used for shape inference\nand do not hold actual data.\n\"meta\"\n\"meta\"\nFake Tensors: Another internal tool used by PyTorch\u2019s compiler isFakeTensorwhich is based on a similar idea.\nTensor subclasses or tensor-like objects can also display unusual behaviours. In general, we do not\nexpect many use cases to require operating at the Storage level!\nCasts this storage to bfloat16 type.\nCasts this storage to bool type.\nCasts this storage to byte type.\nSwap bytes in underlying data.\nCasts this storage to char type.\nReturn a copy of this storage.\nCasts this storage to complex double type.\nCasts this storage to complex float type.\nReturn a CPU copy of this storage if it\u2019s not already on the CPU.\nReturns a copy of this object in CUDA memory.\nIf this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned.\ndevice(int) \u2013 The destination GPU id. Defaults to the current device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nUnion[_StorageBase,TypedStorage]\nCasts this storage to double type.\nReturns the file name associated with this storage.\nThe file name will be a string if the storage is on CPU and was created viafrom_file()withsharedasTrue. This attribute isNoneotherwise.\nfrom_file()\nshared\nTrue\nNone\nCasts this storage to float type.\nCasts this storage to float8_e4m3fn type\nCasts this storage to float8_e4m3fnuz type\nCasts this storage to float8_e5m2 type\nCasts this storage to float8_e5m2fnuz type\nCreates a CPU storage backed by a memory-mapped file.\nIfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.\nshared\nTrue\nshared\nFalse\nnbytesis the number of bytes of storage. IfsharedisFalse,\nthen the file must contain at leastnbytesbytes. IfsharedisTruethe file will be created if needed. (Note that forUntypedStoragethis argument differs from that ofTypedStorage.from_file)\nnbytes\nshared\nFalse\nnbytes\nshared\nTrue\nUntypedStorage\nTypedStorage.from_file\nfilename(str) \u2013 file name to map\nshared(bool) \u2013 whether to share memory (whetherMAP_SHAREDorMAP_PRIVATEis passed to the\nunderlyingmmap(2) call)\nMAP_SHARED\nMAP_PRIVATE\nnbytes(int) \u2013 number of bytes of storage\nint\nCasts this storage to half type.\nReturns a copy of this object in HPU memory.\nIf this object is already in HPU memory and on the correct device, then\nno copy is performed and the original object is returned.\ndevice(int) \u2013 The destination HPU id. Defaults to the current device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nUnion[_StorageBase,TypedStorage]\nCasts this storage to int type.\nDetermine whether the CPU storage is already pinned on device.\ndevice(strortorch.device) \u2013 The device to pin memory on (default:'cuda').\nThis argument is discouraged and subject to deprecated.\n'cuda'\nA boolean variable.\nCasts this storage to long type.\nReturn a MPS copy of this storage if it\u2019s not already on the MPS.\nCopy the CPU storage to pinned memory, if it\u2019s not already pinned.\ndevice(strortorch.device) \u2013 The device to pin memory on (default:'cuda').\nThis argument is discouraged and subject to deprecated.\n'cuda'\nA pinned CPU storage.\nMoves the storage to shared memory.\nThis is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized.\nNote that to mitigate issues likethisit is thread safe to call this function from multiple threads on the same object.\nIt is NOT thread safe though to call any other function on self without proper\nsynchronization. Please seeMultiprocessing best practicesfor more details.\nNote\nWhen all references to a storage in shared memory are deleted, the associated shared memory\nobject will also be deleted. PyTorch has a special cleanup process to ensure that this happens\neven if the current process exits unexpectedly.\nIt is worth noting the difference betweenshare_memory_()andfrom_file()withshared=True\nshare_memory_()\nfrom_file()\nshared=True\nshare_memory_usesshm_open(3)to create a\nPOSIX shared memory object whilefrom_file()usesopen(2)to open the filename passed by the user.\nshare_memory_\nfrom_file()\nBoth use anmmap(2) callwithMAP_SHAREDto map the file/object into the current virtual address space\nMAP_SHARED\nshare_memory_will callshm_unlink(3)on the object after mapping it to make sure the shared memory\nobject is freed when no process has the object open.torch.from_file(shared=True)does not unlink the\nfile. This file is persistent and will remain until it is deleted by the user.\nshare_memory_\nshm_unlink(3)\ntorch.from_file(shared=True)\nself\nself\nCasts this storage to short type.\nint\nReturn a list containing the elements of this storage.\nUnion[_StorageBase,TypedStorage]\n\n## Legacy Typed Storage#\n\nWarning\nFor historical context, PyTorch previously used typed storage classes, which are\nnow deprecated and should be avoided. The following details this API in case you\nshould encounter it, although its usage is highly discouraged.\nAll storage classes except fortorch.UntypedStoragewill be removed\nin the future, andtorch.UntypedStoragewill be used in all cases.\ntorch.UntypedStorage\ntorch.UntypedStorage\ntorch.Storageis an alias for the storage class that corresponds with\nthe default data type (torch.get_default_dtype()). For example, if the\ndefault data type istorch.float,torch.Storageresolves totorch.FloatStorage.\ntorch.Storage\ntorch.get_default_dtype()\ntorch.float\ntorch.Storage\ntorch.FloatStorage\nThetorch.<type>Storageandtorch.cuda.<type>Storageclasses,\nliketorch.FloatStorage,torch.IntStorage, etc., are not\nactually ever instantiated. Calling their constructors creates\natorch.TypedStoragewith the appropriatetorch.dtypeandtorch.device.torch.<type>Storageclasses have all of the\nsame class methods thattorch.TypedStoragehas.\ntorch.<type>Storage\ntorch.cuda.<type>Storage\ntorch.FloatStorage\ntorch.IntStorage\ntorch.TypedStorage\ntorch.dtype\ntorch.device\ntorch.<type>Storage\ntorch.TypedStorage\nAtorch.TypedStorageis a contiguous, one-dimensional array of\nelements of a particulartorch.dtype. It can be given anytorch.dtype, and the internal data will be interpreted appropriately.torch.TypedStoragecontains atorch.UntypedStoragewhich\nholds the data as an untyped array of bytes.\ntorch.TypedStorage\ntorch.dtype\ntorch.dtype\ntorch.TypedStorage\ntorch.UntypedStorage\nEvery stridedtorch.Tensorcontains atorch.TypedStorage,\nwhich stores all of the data that thetorch.Tensorviews.\ntorch.Tensor\ntorch.TypedStorage\ntorch.Tensor\nCasts this storage to bfloat16 type.\nCasts this storage to bool type.\nCasts this storage to byte type.\nCasts this storage to char type.\nReturn a copy of this storage.\nCasts this storage to complex double type.\nCasts this storage to complex float type.\nReturn a CPU copy of this storage if it\u2019s not already on the CPU.\nReturns a copy of this object in CUDA memory.\nIf this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned.\ndevice(int) \u2013 The destination GPU id. Defaults to the current device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nSelf\nCasts this storage to double type.\nReturns the file name associated with this storage if the storage was memory mapped from a file.\norNoneif the storage was not created by memory mapping a file.\nNone\nCasts this storage to float type.\nCasts this storage to float8_e4m3fn type\nCasts this storage to float8_e4m3fnuz type\nCasts this storage to float8_e5m2 type\nCasts this storage to float8_e5m2fnuz type\nCreates a CPU storage backed by a memory-mapped file.\nIfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.\nshared\nTrue\nshared\nFalse\nsizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize*sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be created if needed.\nsize\nshared\nFalse\nsize*sizeof(Type)\nType\nshared\nTrue\nfilename(str) \u2013 file name to map\nshared(bool) \u2013whether to share memory (whetherMAP_SHAREDorMAP_PRIVATEis passed to the\nunderlyingmmap(2) call)\nwhether to share memory (whetherMAP_SHAREDorMAP_PRIVATEis passed to the\nunderlyingmmap(2) call)\nMAP_SHARED\nMAP_PRIVATE\nsize(int) \u2013 number of elements in the storage\nint\nCasts this storage to half type.\nReturns a copy of this object in HPU memory.\nIf this object is already in HPU memory and on the correct device, then\nno copy is performed and the original object is returned.\ndevice(int) \u2013 The destination HPU id. Defaults to the current device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nSelf\nCasts this storage to int type.\nDetermine whether the CPU TypedStorage is already pinned on device.\ndevice(strortorch.device) \u2013 The device to pin memory on (default:'cuda').\nThis argument is discouraged and subject to deprecated.\n'cuda'\nA boolean variable.\nCasts this storage to long type.\nCopy the CPU TypedStorage to pinned memory, if it\u2019s not already pinned.\ndevice(strortorch.device) \u2013 The device to pin memory on (default:'cuda').\nThis argument is discouraged and subject to deprecated.\n'cuda'\nA pinned CPU storage.\nSeetorch.UntypedStorage.share_memory_()\ntorch.UntypedStorage.share_memory_()\nCasts this storage to short type.\nReturns a copy of this object in device memory.\nIf this object is already on the correct device, then no copy is performed\nand the original object is returned.\ndevice(int) \u2013 The destination device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nSelf\nReturn a list containing the elements of this storage.\nReturns the type ifdtypeis not provided, else casts this object to\nthe specified type.\nIf this is already of the correct type, no copy is performed and the\noriginal object is returned.\ndtype(typeorstring) \u2013 The desired type\nnon_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.\nTrue\n**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated.\nasync\nnon_blocking\nasync\nUnion[_StorageBase,TypedStorage,str]\nReturn the internaltorch.UntypedStorage.\ntorch.UntypedStorage",
  "url": "https://pytorch.org/docs/stable/storage.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
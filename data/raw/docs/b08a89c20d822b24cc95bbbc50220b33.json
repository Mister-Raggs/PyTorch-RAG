{
  "doc_id": "b08a89c20d822b24cc95bbbc50220b33",
  "source": "pytorch_docs",
  "title": "Fake tensor \u2014 PyTorch 2.9 documentation",
  "text": "\n## Fake tensor#\n\nCreated On: May 19, 2023 | Last Updated On: Jun 13, 2025\nCode:fake_tensor.py\n\n## Motivation#\n\nWhen doing Dynamo symbolic evaluation and compiler passes, we often want to be able to run tensor operations to understand what output sizes/dtypes/devices are, without actually running those operations (or trashing preexisting tensors), which would be slower (if you\u2019re doing a lot of compute) and take a lot of memory (it\u2019s bad if your compiler needs to use GPU memory while you are compiling the program). A fake tensor is like a real tensor in all respects, except that it doesn\u2019t actually have any data. For example, when we do Dynamo tracing, we need to trace through user Tensor code and answer questions about intermediates (e.g., if a user does a conditional on an intermediate tensor). Without fake tensor, we would not have accurate information for these queries.\nSimilarly, suppose you want to store metadata for a tensor, e.g., on an FX IR node (meta[\u2018val\u2019]). You can instead store a fake tensor directly on the node, which will give you all the metadata you need for the tensor, including subtle stuff that you probably wouldn\u2019t have handled (e.g., aliasing relationships).\n\n## Related work#\n\nA meta tensor is a tensor with device=\u2019meta\u2019. This is actually a lot of what you want for fake tensor, but meta tensors don\u2019t model devices, and sometimes stride behavior varies depending on your device, so fake tensors really can get a lot more accurate info this way. Also, meta tensors are \u201cglobal\u201d (they exist on their own, similar to how a CPU/CUDA tensor exist on their own), whereas fake tensors are scoped to a FakeTensorMode.\nA tensor subclass lets you subclass torch.Tensor and customize their behavior. Fake tensors are implemented as a tensor subclass; that means almost all of its implementation lives in Python! For more simple examples of tensor subclasses check outsubclass_zoo.\nDynamic shapes allow you to create tensors with symbolic sizes rather than only concrete sizes, and propagate these sizes symbolically through operations. Dynamic shapes maintain state in a ShapeEnv, which is always associated with a FakeTensorMode (so fake tensors also are responsible for managing symbolic sizes.) In general, whenever we compile a subgraph with PT2, there is a tracing context associated with this compilation, which contains, among other things, a FakeTensorMode and (possibly) a ShapeEnv.\n\n## Overall architecture#\n\nAll fake tensors are associated with a FakeTensorMode. Because fake tensor\u2019s primary use case is to do analysis on real tensors, the general workflow is you have a bunch of real tensors, you allocate a FakeTensorMode, and then you use from_real_tensor to convert all those real tensors into fake tensors, and then you do things to the fake tensors. In particular, the FakeTensorMode maintains a memo table persistently mapping tensors (and storages) to the same storages. If you fakeify the same tensor multiple times, you will get the same fake tensor; if you fakeify two tensors which alias each other, you will get two fake tensors which alias the same fake storage. FakeTensors are tensor subclasses, so if you do operations on them, you\u2019ll automatically get a fake tensor, but in general you will want to do operations on fake tensors (e.g., if you\u2019re running an FX pass) with the FakeTensorMode active; what a tensor operation will do is automatically turn on the fake tensor mode and try again.\nA fake tensor is represented as a __torch_dispatch__ tensor subclass of a meta tensor. This means under the hood, fake tensors are meta device tensors; they then use extra extensibility hooks, specifically dispatch_device, to lie about what the actual device of the tensor is. This was one of the more error-prone parts of fake tensors in the early days: sometimes, fake tensors were too good at lying about being CPU/CUDA whatever, and you\u2019d end up with a CPU kernel getting called with a fake tensor trying to dereference the data pointer, which obviously won\u2019t work. If you are segfaulting in fake tensor code, this is the first thing you should check: is the C++ backtrace in a CPU kernel (unexpected!) or a meta kernel (expected!) A meta kernel is like a real kernel, but all it does is allocate the outputs, it doesn\u2019t do any data compute.\nA tensor subclass has to define how to implement various operations. Here is the general fake tensor recipe:\nRun the meta kernel on the input fake tensors, reinterpreting them as meta tensors. This is done via a magic context manager in_kernel_invocation_manager which instructs all of PyTorch to view fake tensors as their underlying meta tensors, rather than \u201cunwrapping\u201d fake tensors into meta tensors (a fake tensor is a meta tensor). Fake tensors are represented this way to avoid having to keep two sets of metadata in sync (the meta tensor\u2019s metadata, and the fake tensor\u2019s metadata); the \u201cis a\u201d relationship ensures there is only one canonical copy of metadata.\nIf you\u2019re a factory function, you\u2019ll instead call the underlying factory function with device=\u2019meta\u2019.\nConvert the resulting meta tensor into a fake tensor, computing what the output device of the tensor should be (this is usually trivial, but sometimes it is not, e.g., cpu scalar promotion, or device-converting operations.)\n\n## API: the important bits#\n\nNon-PT2 usage (check out test/test_fake_tensor.py for more examples):\n\n```python\n# Create a fake mode\nfrom torch._subclasses.fake_tensor import FakeTensorMode\nfake_mode = FakeTensorMode()\nconverter = fake_mode.fake_tensor_converter\n# Fakeify some real tensors\nfake_x = converter.from_real_tensor(fake_mode, x)\nwith fake_mode:\n    # Do some operations on the fake tensors\n    fake_y = fake_x * 2\n    # Factory operations automatically get fakeified in the context manager\n    fake_z = torch.empty(20)\n\n```\n\nQ: Why do you have real tensors as inputs?\nA: In a PT2 context, this is because you typically are compiling just-in-time, so for all the inputs to a graph you\u2019re compiling, you already have the \u201creal\u201d inputs, because you\u2019re compiling while you\u2019re executing the program.\nPT2 pre-AOTAutograd usage (this is unusual, you probably don\u2019t want to do this):\n\n```python\n# Fake mode is not enabled!\nfrom torch._guards import detect_fake_mode\nfake_mode = detect_fake_mode(args)\n# if fake_mode isn't None\nconverter = fake_mode.fake_tensor_converter\nfake_args = [converter.from_real_tensor(fake_mode, arg) for arg in args]\nwith fake_mode:\n    ... # do stuff with the fake args, if needed ...\n\n```\n\ndetect_fake_mode will search a number of locations to try to find \u201cthe\u201d fake tensor mode associated with the lifecycle. Typically it will be pulled off of the tracing context.\nPT2 post-AOTAutograd usage:\n\n```python\n# Fake mode is enabled! example_inputs is typically fake already\n# TODO: we probably want to change this\n# Still do this to access fake mode\nfake_mode = detect_fake_mode(example_inputs)\n# But in general you don't have to turn it on\n\n```\n\nOther useful stuff:\n\n```python\nfrom torch._subclasses.fake_tensor import unset_fake_temporarily\nwith unset_fake_temporarily():\n    ... # fake mode is disabled here, you can do real tensor compute\n\n```\n\nWhen might you want to disable fake tensor mode? Usually you don\u2019t want to do this. One niche case where we\u2019ve found it useful is to implement constant propagation on fake tensors: in this case, we need to do some actual tensor computation even though we\u2019re in a fake tensor mode.\n\n```python\nimport FakeTensorProp from torch.fx.passes.fake_tensor_prop\ngm: GraphModule\nreal_inputs: List[Tensor]\nFakeTensorProp(gm).propagate(*real_inputs)\n# This will populate meta['val'] on all the FX nodes with a fake tensor\n# or if you have a preexisting fake mode, you should use it\nFakeTensorProp(gm, mode=fake_mode).propagate(*real_inputs)\n# There is also propagate_dont_convert_inputs if your inputs are already fake\nfake_inputs: List[FakeTensor]\nFakeTensorProp(gm, mode=fake_mode).propagate_dont_convert_inputs(*fake_inputs)\n\n```\n\n\n## Details#\n\nAuto-convert or not?\nOriginally, FakeTensorMode would not automatically fakeify real tensors if you tried to do compute on them inside a FakeTensorMode region. The motivation behind this was to prevent the following footgun:\n\n```python\nwith FakeTensorMode():\n    real_tensor.t_()\n\n```\n\nWhat should this code do? It would be surprising if we actually modified the metadata on the real tensor. But at the same time, there isn\u2019t any obvious opportunity to create a FakeTensor. So we conservatively decided to make this raise an error: \u201cInvoking operators with non-Fake Tensor inputs in FakeTensorMode is not yet supported. Please convert all Tensors to FakeTensors first.\u201d\nThis error is pretty annoying in practice. For example, suppose you have a real nn.Module and you want to feed fake tensors through it. You need to somehow fakeify the nn.Module. This motivated FakeCopyMode.\nEventually, we gave up and added automatic fakeification. However, this is still not yet enabled by default in many uses of FakeTensorMode.\nMetadata mutation on fake tensor\nIf you have a fake tensor, and you t_() it, the metadata on the fake tensor changes. This is reasonable on its face, but sometimes you want to also store fake tensors as metadata on FX nodes; mutating a fake tensor is bad because this will invalidate old metadata!\nIn fact, there is a fundamental tension here, which is that fake tensors maintain extremely accurate metadata about tensors, up to and including object identity. If object metadata changes over time in an FX graph, there is not actually any way to represent this change over time. Most of the time, our serious FX analyses are done on functionalized graphs, which don\u2019t have this, but occasionally you need to do an analysis on a non-functionalized graph. Maybe it was a mistake to put fake tensor in meta[\u2018val\u2019]\n\n## About the tensor subclass#\n\nFake tensor uses both a subclass and a mode tensor subclass pattern, where FakeTensor.__torch_dispatch__ enables the FakeTensorMode associated with the fake tensor, and then redispatches (relying on FakeTensorMode to do the heavy lifting). If fake tensor operations get a subclass argument it doesn\u2019t recognize, it will return NotImplemented, giving the other subclass a chance to run first (hopefully desugaring into plain tensor operations), before it tries again. This can cause infinite loops.\n\n## How is each individual operator implemented?#\n\nUnfortunately, there is a pretty complicated set of places where any given operator may be implemented. Some important cases to know about:\nTensor subclasses support limited constant propagation if the number of elements is very small (this helps deal with some cases where we immediately call item() on such tensors.)\nWe have some fastpath implementations for certain operators, which are done entirely in fake tensor, for performance reasons.\nIf you use @custom_op to generate a custom tensor, these will register impl_abstract directly to fake tensor.\nFake tensor itself has some hardcoded special cases for device-converting operations.\nIf there is no meta implementation nor any decomposition, we will generate real zero-filled tensors and attempt to run the operator directly to find out what the results will be. This can cause segfaults if the operator attempts to do indexing with data, so we don\u2019t turn this on by default for custom ops.\n\n## How does the converter work?#\n\nBecause fake tensors are used in situations that are very sensitive to the exact properties of a tensor, fake tensors do conversion very carefully, preserving leaf-ness, requires_grad\u2019ness, aliasing, and a whole host of other properties. The bulk of the heavy lifting is in MetaConverter.\n\n## Performance characteristics#\n\nYou would think fake tensors are fast because they don\u2019t do any tensor compute. But at small tensor sizes we are actually entirely overhead bound, and, well, fake tensor is in Python, and we often do a LOT of work to do a single tensor operation (because they are implemented as decompositions). So fake tensors are actually pretty slow in practice, especially when symbolic shapes are involved. There are two important fastpaths we currently have in fake tensor that make a big difference in practice:\nPointwise ops don\u2019t go through PrimTorch decomps, instead we\u2019ve hand-coded their propagation rule.\nIf possible, we should.\n\n## Fake tensor of fake tensor?#\n\nThere is interest in sending fake tensors as user inputs into the PT2 stack, which would imply we would need to be able to create a fake tensor of a fake tensor. This isn\u2019t really supported right now, but maybe it would not be too difficult to do.\n\n## Interaction with dynamic shapes#\n\nEvery FakeTensorMode contains a ShapeEnv, which tracks all symbolic shapes information. Their lifetimes are typically tied: they live and die together.\nBecause FakeTensorMode has a ShapeEnv (but meta implementations do not), meta functions that are data-dependent and require allocating an unbacked SymInt live in fake tensor. Fake tensor also takes care of memoizing unbacked SymInts, so that, e.g., if you call nonzero() on the same fake tensor twice, you get the same symbolic size.\n\n## Other resources#\n\nColab Tutorial On Using FakeTensor To Determine Max Batch Size",
  "url": "https://pytorch.org/docs/stable/torch.compiler_fake_tensor.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
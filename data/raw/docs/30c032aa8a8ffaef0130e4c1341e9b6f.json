{
  "doc_id": "30c032aa8a8ffaef0130e4c1341e9b6f",
  "source": "pytorch_docs",
  "title": "torch.utils.cpp_extension \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.utils.cpp_extension#\n\nCreated On: Mar 07, 2018 | Last Updated On: Feb 16, 2025\nCreate asetuptools.Extensionfor C++.\nsetuptools.Extension\nConvenience method that creates asetuptools.Extensionwith the\nbare minimum (but often sufficient) arguments to build a C++ extension.\nsetuptools.Extension\nAll arguments are forwarded to thesetuptools.Extensionconstructor. Full list arguments can be found athttps://setuptools.pypa.io/en/latest/userguide/ext_modules.html#extension-api-reference\nsetuptools.Extension\nWarning\nThe PyTorch python API (as provided in libtorch_python) cannot be built\nwith the flagpy_limited_api=True.  When this flag is passed, it is\nthe user\u2019s responsibility in their library to not use APIs from\nlibtorch_python (in particular pytorch/python bindings) and to only use\nAPIs from libtorch (aten objects, operators and the dispatcher). For\nexample, to give access to custom ops from python, the library should\nregister the ops through the dispatcher.\npy_limited_api=True\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API\nas a compile flag when py_limited_api is specified as an option for\nthe \u201cbdist_wheel\u201d command insetup, PyTorch does! We will specify\n-DPy_LIMITED_API=min_supported_cpython to best enforce consistency,\nsafety, and sanity in order to encourage best practices. To target a\ndifferent version, set min_supported_cpython to the hexcode of the\nCPython version of choice.\nsetup\nExample\n\n```python\n>>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\n>>> setup(\n...     name='extension',\n...     ext_modules=[\n...         CppExtension(\n...             name='extension',\n...             sources=['extension.cpp'],\n...             extra_compile_args=['-g'],\n...             extra_link_args=['-Wl,--no-as-needed', '-lm'])\n...     ],\n...     cmdclass={\n...         'build_ext': BuildExtension\n...     })\n\n```\n\nCreate asetuptools.Extensionfor CUDA/C++.\nsetuptools.Extension\nConvenience method that creates asetuptools.Extensionwith the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary.\nsetuptools.Extension\nAll arguments are forwarded to thesetuptools.Extensionconstructor. Full list arguments can be found athttps://setuptools.pypa.io/en/latest/userguide/ext_modules.html#extension-api-reference\nsetuptools.Extension\nWarning\nThe PyTorch python API (as provided in libtorch_python) cannot be built\nwith the flagpy_limited_api=True.  When this flag is passed, it is\nthe user\u2019s responsibility in their library to not use APIs from\nlibtorch_python (in particular pytorch/python bindings) and to only use\nAPIs from libtorch (aten objects, operators and the dispatcher). For\nexample, to give access to custom ops from python, the library should\nregister the ops through the dispatcher.\npy_limited_api=True\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API\nas a compile flag when py_limited_api is specified as an option for\nthe \u201cbdist_wheel\u201d command insetup, PyTorch does! We will specify\n-DPy_LIMITED_API=min_supported_cpython to best enforce consistency,\nsafety, and sanity in order to encourage best practices. To target a\ndifferent version, set min_supported_cpython to the hexcode of the\nCPython version of choice.\nsetup\nExample\n\n```python\n>>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n>>> setup(\n...     name='cuda_extension',\n...     ext_modules=[\n...         CUDAExtension(\n...                 name='cuda_extension',\n...                 sources=['extension.cpp', 'extension_kernel.cu'],\n...                 extra_compile_args={'cxx': ['-g'],\n...                                     'nvcc': ['-O2']},\n...                 extra_link_args=['-Wl,--no-as-needed', '-lcuda'])\n...     ],\n...     cmdclass={\n...         'build_ext': BuildExtension\n...     })\n\n```\n\nCompute capabilities:\nBy default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, PyTorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX).\nYou can override the default behavior usingTORCH_CUDA_ARCH_LISTto explicitly specify which\nCCs you want the extension to support:\nTORCH_CUDA_ARCH_LIST=\"6.18.6\"pythonbuild_my_extension.pyTORCH_CUDA_ARCH_LIST=\"5.26.06.17.07.58.08.6+PTX\"pythonbuild_my_extension.py\nTORCH_CUDA_ARCH_LIST=\"6.18.6\"pythonbuild_my_extension.py\nTORCH_CUDA_ARCH_LIST=\"5.26.06.17.07.58.08.6+PTX\"pythonbuild_my_extension.py\nThe +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.\nNote that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch.\nNote that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows.\nTo workaround the issue, move python binding logic to pure C++ file.\n#include <ATen/ATen.h>\nat::Tensor SigmoidAlphaBlendForwardCuda(\u2026.)\n#include <torch/extension.h>\ntorch::Tensor SigmoidAlphaBlendForwardCuda(\u2026)\nCurrently open issue for nvcc bug:pytorch/pytorch#69460Complete workaround code example:facebookresearch/pytorch3d\nRelocatable device code linking:\nIf you want to reference device symbols across compilation units (across object files),\nthe object files need to be built withrelocatable device code(-rdc=true or -dc).\nAn exception to this rule is \u201cdynamic parallelism\u201d (nested kernel launches)  which is not used a lot anymore.Relocatable device codeis less optimized so it needs to be used only on object files that need it.\nUsing-dlto(Device Link Time Optimization) at the device code compilation step anddlinkstep\nhelps reduce the protentional perf degradation of-rdc.\nNote that it needs to be used at both steps to be useful.\nIf you haverdcobjects you need to have an extra-dlink(device linking) step before the CPU symbol linking step.\nThere is also a case where-dlinkis used without-rdc:\nwhen an extension is linked against a static lib containing rdc-compiled objects\nlike the [NVSHMEM library](https://developer.nvidia.com/nvshmem).\nNote: Ninja is required to build a CUDA Extension with RDC linking.\nExample\n\n```python\n>>> CUDAExtension(\n...        name='cuda_extension',\n...        sources=['extension.cpp', 'extension_kernel.cu'],\n...        dlink=True,\n...        dlink_libraries=[\"dlink_lib\"],\n...        extra_compile_args={'cxx': ['-g'],\n...                            'nvcc': ['-O2', '-rdc=true']})\n\n```\n\nCreates asetuptools.Extensionfor SYCL/C++.\nsetuptools.Extension\nConvenience method that creates asetuptools.Extensionwith the\nbare minimum (but often sufficient) arguments to build a SYCL/C++\nextension.\nsetuptools.Extension\nAll arguments are forwarded to thesetuptools.Extensionconstructor.\nsetuptools.Extension\nWarning\nThe PyTorch python API (as provided in libtorch_python) cannot be built\nwith the flagpy_limited_api=True.  When this flag is passed, it is\nthe user\u2019s responsibility in their library to not use APIs from\nlibtorch_python (in particular pytorch/python bindings) and to only use\nAPIs from libtorch (aten objects, operators and the dispatcher). For\nexample, to give access to custom ops from python, the library should\nregister the ops through the dispatcher.\npy_limited_api=True\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API\nas a compile flag when py_limited_api is specified as an option for\nthe \u201cbdist_wheel\u201d command insetup, PyTorch does! We will specify\n-DPy_LIMITED_API=min_supported_cpython to best enforce consistency,\nsafety, and sanity in order to encourage best practices. To target a\ndifferent version, set min_supported_cpython to the hexcode of the\nCPython version of choice.\nsetup\nExample\n\n```python\n>>> from torch.utils.cpp_extension import BuildExtension, SyclExtension\n>>> setup(\n...     name='xpu_extension',\n...     ext_modules=[\n...     SyclExtension(\n...                 name='xpu_extension',\n...                 sources=['extension.cpp', 'extension_kernel.cpp'],\n...                 extra_compile_args={'cxx': ['-g', '-std=c++20', '-fPIC']})\n...     ],\n...     cmdclass={\n...         'build_ext': BuildExtension\n...     })\n\n```\n\nBy default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension. If down the road a new card is installed the\nextension may need to be recompiled. You can override the default behavior usingTORCH_XPU_ARCH_LISTto explicitly specify which device architectures you want the extension\nto support:\nTORCH_XPU_ARCH_LIST=\"pvc,xe-lpg\"pythonbuild_my_extension.py\nTORCH_XPU_ARCH_LIST=\"pvc,xe-lpg\"pythonbuild_my_extension.py\nNote that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch.\nNote: Ninja is required to build SyclExtension.\nA customsetuptoolsbuild extension .\nsetuptools\nThissetuptools.build_extsubclass takes care of passing the\nminimum required compiler flags (e.g.-std=c++17) as well as mixed\nC++/CUDA/SYCL compilation (and support for CUDA/SYCL files in general).\nsetuptools.build_ext\n-std=c++17\nWhen usingBuildExtension, it is allowed to supply a dictionary\nforextra_compile_args(rather than the usual list) that maps from\nlanguages/compilers (the only expected values arecxx,nvccorsycl) to a list of additional compiler flags to supply to the compiler.\nThis makes it possible to supply different flags to the C++, CUDA and SYCL\ncompiler during mixed compilation.\nBuildExtension\nextra_compile_args\ncxx\nnvcc\nsycl\nuse_ninja(bool): Ifuse_ninjaisTrue(default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standardsetuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available.\nuse_ninja\nuse_ninja\nTrue\nsetuptools.build_ext\nNote\nBy default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting theMAX_JOBSenvironment\nvariable to a non-negative number.\nLoad a PyTorch C++ extension just-in-time (JIT).\nTo load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.\nBy default, the directory to which the build file is emitted and the\nresulting library compiled to is<tmp>/torch_extensions/<name>, where<tmp>is the temporary folder on the current platform and<name>the name of the extension. This location can be overridden in two ways.\nFirst, if theTORCH_EXTENSIONS_DIRenvironment variable is set, it\nreplaces<tmp>/torch_extensionsand all extensions will be compiled\ninto subfolders of this directory. Second, if thebuild_directoryargument to this function is supplied, it overrides the entire path, i.e.\nthe library will be compiled into that folder directly.\n<tmp>/torch_extensions/<name>\n<tmp>\n<name>\nTORCH_EXTENSIONS_DIR\n<tmp>/torch_extensions\nbuild_directory\nTo compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting theCXXenvironment variable. To pass\nadditional arguments to the compilation process,extra_cflagsorextra_ldflagscan be provided. For example, to compile your extension\nwith optimizations, passextra_cflags=['-O3']. You can also useextra_cflagsto pass further include directories.\nc++\nCXX\nextra_cflags\nextra_ldflags\nextra_cflags=['-O3']\nextra_cflags\nCUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cuor.cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linkingcudart. You can pass additional flags to nvcc viaextra_cuda_cflags, just like withextra_cflagsfor C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting theCUDA_HOMEenvironment variable is the\nsafest option.\n.cu\n.cuh\ncudart\nextra_cuda_cflags\nextra_cflags\nCUDA_HOME\nSYCL support with mixed compilation is provided. Simply pass SYCL source\nfiles (.sycl) along with other sources. Such files will be detected\nand compiled with SYCL compiler (such as Intel DPC++ Compiler) rather\nthan the C++ compiler. You can pass additional flags to SYCL compiler\nviaextra_sycl_cflags, just like withextra_cflagsfor C++.\nSYCL compiler is expected to be found via system PATH environment\nvariable.\n.sycl\nextra_sycl_cflags\nextra_cflags\nname\u2013 The name of the extension to build. This MUST be the same as the\nname of the pybind11 module!\nsources(Union[str,list[str]]) \u2013 A list of relative or absolute paths to C++ source files.\nextra_cflags\u2013 optional list of compiler flags to forward to the build.\nextra_cuda_cflags\u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources.\nextra_sycl_cflags\u2013 optional list of compiler flags to forward to SYCL\ncompiler when building SYCL sources.\nextra_ldflags\u2013 optional list of linker flags to forward to the build.\nextra_include_paths\u2013 optional list of include directories to forward\nto the build.\nbuild_directory\u2013 optional path to use as build workspace.\nverbose\u2013 IfTrue, turns on verbose logging of load steps.\nTrue\nwith_cuda(Optional[bool]) \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set toNone(default), this value is\nautomatically determined based on the existence of.cuor.cuhinsources. Set it toTrue`to force CUDA headers\nand libraries to be included.\nNone\n.cu\n.cuh\nsources\nwith_sycl(Optional[bool]) \u2013 Determines whether SYCL headers and libraries are added to\nthe build. If set toNone(default), this value is\nautomatically determined based on the existence of.syclinsources. Set it toTrue`to force SYCL headers and\nlibraries to be included.\nNone\n.sycl\nsources\nis_python_module\u2013 IfTrue(default), imports the produced shared\nlibrary as a Python module. IfFalse, behavior depends onis_standalone.\nTrue\nFalse\nis_standalone\nis_standalone\u2013 IfFalse(default) loads the constructed extension\ninto the process as a plain dynamic library. IfTrue, build a\nstandalone executable.\nFalse\nTrue\nReturns the loaded PyTorch extension as a Python module.Ifis_python_moduleisFalseandis_standaloneisFalse:Returns nothing. (The shared library is loaded into the process as\na side effect.)Ifis_standaloneisTrue.Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.)\nReturns the loaded PyTorch extension as a Python module.\nis_python_module\nFalse\nis_standalone\nFalse\nReturns nothing. (The shared library is loaded into the process as\na side effect.)\nis_standalone\nTrue\nReturn the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.)\nIfis_python_moduleisTrue\nis_python_module\nTrue\nExample\n\n```python\n>>> from torch.utils.cpp_extension import load\n>>> module = load(\n...     name='extension',\n...     sources=['extension.cpp', 'extension_kernel.cu'],\n...     extra_cflags=['-O2'],\n...     verbose=True)\n\n```\n\nLoad a PyTorch C++ extension just-in-time (JIT) from string sources.\nThis function behaves exactly likeload(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior ofload_inline()is\nidentical toload().\nload()\nload_inline()\nload()\nSeethe\ntestsfor good examples of using this function.\nSources may omit two required parts of a typical non-inline C++ extension:\nthe necessary header includes, as well as the (pybind11) binding code. More\nprecisely, strings passed tocpp_sourcesare first concatenated into a\nsingle.cppfile. This file is then prepended with#include<torch/extension.h>\ncpp_sources\n.cpp\n#include<torch/extension.h>\nFurthermore, if thefunctionsargument is supplied, bindings will be\nautomatically generated for each function specified.functionscan\neither be a list of function names, or a dictionary mapping from function\nnames to docstrings. If a list is given, the name of each function is used\nas its docstring.\nfunctions\nfunctions\nThe sources incuda_sourcesare concatenated into a separate.cufile and  prepended withtorch/types.h,cuda.handcuda_runtime.hincludes. The.cppand.cufiles are compiled\nseparately, but ultimately linked into a single library. Note that no\nbindings are generated for functions incuda_sourcesper se. To bind\nto a CUDA kernel, you must create a C++ function that calls it, and either\ndeclare or define this C++ function in one of thecpp_sources(and\ninclude its name infunctions).\ncuda_sources\n.cu\ntorch/types.h\ncuda.h\ncuda_runtime.h\n.cpp\n.cu\ncuda_sources\ncpp_sources\nfunctions\nThe sources insycl_sourcesare concatenated into a separate.syclfile and  prepended withtorch/types.h,sycl/sycl.hppincludes.\nThe.cppand.syclfiles are compiled separately, but ultimately\nlinked into a single library. Note that no bindings are generated for\nfunctions insycl_sourcesper se. To bind to a SYCL kernel, you must\ncreate a C++ function that calls it, and either declare or define this\nC++ function in one of thecpp_sources(and include its name\ninfunctions).\nsycl_sources\n.sycl\ntorch/types.h\nsycl/sycl.hpp\n.cpp\n.sycl\nsycl_sources\ncpp_sources\nfunctions\nSeeload()for a description of arguments omitted below.\nload()\ncpp_sources\u2013 A string, or list of strings, containing C++ source code.\ncuda_sources\u2013 A string, or list of strings, containing CUDA source code.\nsycl_sources\u2013 A string, or list of strings, containing SYCL source code.\nfunctions\u2013 A list of function names for which to generate function\nbindings. If a dictionary is given, it should map function names to\ndocstrings (which are otherwise just the function names).\nwith_cuda\u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set toNone(default), this value is\nautomatically determined based on whethercuda_sourcesis\nprovided. Set it toTrueto force CUDA headers\nand libraries to be included.\nNone\ncuda_sources\nTrue\nwith_sycl\u2013 Determines whether SYCL headers and libraries are added to\nthe build. If set toNone(default), this value is\nautomatically determined based on whethersycl_sourcesis\nprovided. Set it toTrueto force SYCL headers\nand libraries to be included.\nNone\nsycl_sources\nTrue\nwith_pytorch_error_handling\u2013 Determines whether pytorch error and\nwarning macros are handled by pytorch instead of pybind. To do\nthis, each functionfoois called via an intermediary_safe_foofunction. This redirection might cause issues in obscure cases\nof cpp. This flag should be set toFalsewhen this redirect\ncauses issues.\nfoo\n_safe_foo\nFalse\nno_implicit_headers\u2013 IfTrue, skips automatically adding headers, most notably#include<torch/extension.h>and#include<torch/types.h>lines.\nUse this option to improve cold start times when you\nalready include the necessary headers in your source code. Default:False.\nTrue\n#include<torch/extension.h>\n#include<torch/types.h>\nFalse\nExample\n\n```python\n>>> from torch.utils.cpp_extension import load_inline\n>>> source = \"\"\"\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\n  return x.sin() + y.sin();\n}\n\"\"\"\n>>> module = load_inline(name='inline_extension',\n...                      cpp_sources=[source],\n...                      functions=['sin_add'])\n\n```\n\nNote\nSince load_inline will just-in-time compile the source code, please ensure\nthat you have the right toolchains installed in the runtime. For example,\nwhen loading C++, make sure a C++ compiler is available. If you\u2019re loading\na CUDA extension, you will need to additionally install the corresponding CUDA\ntoolkit (nvcc and any other dependencies your code has). Compiling toolchains\nare not included when you install torch and must be additionally installed.\nDuring compiling, by default, the Ninja backend uses #CPUS + 2 workers to build\nthe extension. This may use up too many resources on some systems. One\ncan control the number of workers by setting theMAX_JOBSenvironment\nvariable to a non-negative number.\nGet the include paths required to build a C++ or CUDA or SYCL extension.\ndevice_type(str) \u2013 Defaults to \u201ccpu\u201d.\nA list of include path strings.\nlist[str]\nDetermine if the given compiler is ABI-compatible with PyTorch alongside its version.\ncompiler(str) \u2013 The compiler executable name to check (e.g.g++).\nMust be executable in a shell process.\ng++\nA tuple that contains a boolean that defines if the compiler is (likely) ABI-incompatible with PyTorch,\nfollowed by aTorchVersionstring that contains the compiler version separated by dots.\ntuple[bool, torch.torch_version.TorchVersion]\nRaiseRuntimeErrorifninjabuild system is not available on the system, does nothing otherwise.\nRuntimeError\nReturnTrueif theninjabuild system is available on the system,Falseotherwise.\nTrue\nFalse",
  "url": "https://pytorch.org/docs/stable/cpp_extension.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
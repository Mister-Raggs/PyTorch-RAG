{
  "doc_id": "5e20dd8c02cb74171e7b1ee7276ae5de",
  "source": "pytorch_docs",
  "title": "CUDA semantics \u2014 PyTorch 2.9 documentation",
  "text": "\n## CUDA semantics#\n\nCreated On: Jan 16, 2017 | Last Updated On: Sep 04, 2025\ntorch.cudais used to set up and run CUDA operations. It keeps track of\nthe currently selected GPU, and all CUDA tensors you allocate will by default be\ncreated on that device. The selected device can be changed with atorch.cuda.devicecontext manager.\ntorch.cuda\ntorch.cuda.device\nHowever, once a tensor is allocated, you can do operations on it irrespective\nof the selected device, and the results will be always placed on the same\ndevice as the tensor.\nCross-GPU operations are not allowed by default, with the exception ofcopy_()and other methods with copy-like functionality\nsuch asto()andcuda().\nUnless you enable peer-to-peer memory access, any attempts to launch ops on\ntensors spread across different devices will raise an error.\ncopy_()\nto()\ncuda()\nBelow you can find a small example showcasing this:\n\n```python\ncuda = torch.device('cuda')     # Default CUDA device\ncuda0 = torch.device('cuda:0')\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n\n```\n\n\n## TensorFloat-32 (TF32) on Ampere (and later) devices#\n\nAfter Pytorch 2.9, we provide a new sets of APIs to control the TF32 behavior in a more fine-grained way, and\nsuggest to use the new APIs for better control.\nWe can set float32 precision per backend and per operators. We can also override the global setting for a specific operator.\n\n```python\ntorch.backends.fp32_precision = \"ieee\"\ntorch.backends.cuda.matmul.fp32_precision = \"ieee\"\ntorch.backends.cudnn.fp32_precision = \"ieee\"\ntorch.backends.cudnn.conv.fp32_precision = \"tf32\"\ntorch.backends.cudnn.rnn.fp32_precision = \"tf32\"\n\n```\n\nThe fp32_precision can be set toieeeortf32forcuda/cudnn.ieeefp32_precision indicate that we will useFP32as internal computation precision.tf32fp32_precision indicate that we will allow to useTF32as internal computation precision.\nWe can override a generic setting for a specific operator if the fp32_precision is set toieee.\n\n```python\ntorch.backends.cudnn.fp32_precision = \"tf32\"\ntorch.backends.cudnn.conv.fp32_precision = \"ieee\"\ntorch.backends.cudnn.rnn.fp32_precision = \"ieee\"\n\n```\n\nWe can also override a generic setting for a specific backend if the fp32_precision is set toieee.\n\n```python\ntorch.backends.fp32_precision = \"tf32\"\ntorch.backends.cudnn.fp32_precision = \"ieee\"\ntorch.backends.cudnn.conv.fp32_precision = \"ieee\"\ntorch.backends.cudnn.rnn.fp32_precision = \"ieee\"\n\n```\n\nFor above 2 cases, bothtorch.backends.cudnn.conv.fp32_precisionandtorch.backends.cudnn.rnn.fp32_precisionis overridden toieee.\nWe suggest to use the new settings for better control. And we do not support to use mix of old and new settings.\nWarning\nOld settings withallow_tf32as follows is going to be deprecated. We suggest to use the above new settings for\nbetter control. And we do not support to use mix of old and new settings.\nStarting in PyTorch 1.7, there is a new flag calledallow_tf32. This flag\ndefaults to True in PyTorch 1.7 to PyTorch 1.11, and False in PyTorch 1.12 and later.\nThis flag controls whether PyTorch is allowed to use the TensorFloat32 (TF32) tensor cores,\navailable on NVIDIA GPUs since Ampere, internally to compute matmul (matrix multiplies\nand batched matrix multiplies) and convolutions.\nTF32 tensor cores are designed to achieve better performance on matmul and convolutions ontorch.float32tensors by rounding input data to have 10 bits of mantissa, and accumulating\nresults with FP32 precision, maintaining FP32 dynamic range.\nmatmuls and convolutions are controlled separately, and their corresponding flags can be accessed at:\n\n```python\n# The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n# in PyTorch 1.12 and later.\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\ntorch.backends.cudnn.allow_tf32 = True\n\n```\n\nThe precision of matmuls can also be set more broadly (limited not just to CUDA) viaset_float32_matmul_precision().\nNote that besides matmuls and convolutions themselves, functions and nn modules that internally uses\nmatmuls or convolutions are also affected. These includenn.Linear,nn.Conv*, cdist, tensordot,\naffine grid and grid sample, adaptive log softmax, GRU and LSTM.\nset_float32_matmul_precision()\nTo get an idea of the precision and speed, see the example code and benchmark data (on A100) below:\n\n```python\na_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\nb_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\nab_full = a_full @ b_full\nmean = ab_full.abs().mean()  # 80.7277\n\na = a_full.float()\nb = b_full.float()\n\n# Do matmul at TF32 mode.\ntorch.backends.cuda.matmul.allow_tf32 = True\nab_tf32 = a @ b  # takes 0.016s on GA100\nerror = (ab_tf32 - ab_full).abs().max()  # 0.1747\nrelative_error = error / mean  # 0.0022\n\n# Do matmul with TF32 disabled.\ntorch.backends.cuda.matmul.allow_tf32 = False\nab_fp32 = a @ b  # takes 0.11s on GA100\nerror = (ab_fp32 - ab_full).abs().max()  # 0.0031\nrelative_error = error / mean  # 0.000039\n\n```\n\nFrom the above example, we can see that with TF32 enabled, the speed is ~7x faster on A100, and that\nrelative error compared to double precision is approximately 2 orders of magnitude larger. Note that\nthe exact ratio of TF32 to single precision speed depends on the hardware generation, as properties\nsuch as the ratio of memory bandwidth to compute as well as the ratio of TF32 to FP32 matmul throughput\nmay vary from generation to generation or model to model.\nIf full FP32 precision is needed, users can disable TF32 by:\n\n```python\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.allow_tf32 = False\n\n```\n\nTo toggle the TF32 flags off in C++, you can do\n\n```python\nat::globalContext().setAllowTF32CuBLAS(false);\nat::globalContext().setAllowTF32CuDNN(false);\n\n```\n\nFor more information about TF32, see:\nTensorFloat-32\nCUDA 11\nAmpere architecture\n\n## Reduced Precision Reduction in FP16 GEMMs#\n\n(Distinct from full FP16 accumulation that is intended for hardware that has higher throughput\nwith FP16 accumulation than FP32 accumulation, seeFull FP16 accumulation)\nfp16 GEMMs are potentially done with some intermediate reduced precision reductions (e.g., in fp16 rather than fp32). These selective reductions in precision can allow for higher performance on certain workloads (particularly those with a largekdimension) and GPU architectures at the cost of numerical precision and potential for overflow.\nSome example benchmark data on V100:\n\n```python\n[--------------------------- bench_gemm_transformer --------------------------]\n      [  m ,  k  ,  n  ]    |  allow_fp16_reduc=True  |  allow_fp16_reduc=False\n1 threads: --------------------------------------------------------------------\n      [4096, 4048, 4096]    |           1634.6        |           1639.8\n      [4096, 4056, 4096]    |           1670.8        |           1661.9\n      [4096, 4080, 4096]    |           1664.2        |           1658.3\n      [4096, 4096, 4096]    |           1639.4        |           1651.0\n      [4096, 4104, 4096]    |           1677.4        |           1674.9\n      [4096, 4128, 4096]    |           1655.7        |           1646.0\n      [4096, 4144, 4096]    |           1796.8        |           2519.6\n      [4096, 5096, 4096]    |           2094.6        |           3190.0\n      [4096, 5104, 4096]    |           2144.0        |           2663.5\n      [4096, 5112, 4096]    |           2149.1        |           2766.9\n      [4096, 5120, 4096]    |           2142.8        |           2631.0\n      [4096, 9728, 4096]    |           3875.1        |           5779.8\n      [4096, 16384, 4096]   |           6182.9        |           9656.5\n(times in microseconds).\n\n```\n\nIf full precision reductions are needed, users can disable reduced precision reductions in fp16 GEMMs with:\n\n```python\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\n```\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\n```python\nat::globalContext().setAllowFP16ReductionCuBLAS(false);\n\n```\n\n\n## Reduced Precision Reduction in BF16 GEMMs#\n\nA similar flag (as above) exists for BFloat16 GEMMs.\nNote that this switch is set toTrueby default for BF16, if you observe\nnumerical instability in your workload, you may wish to set it toFalse.\nIf reduced precision reductions are not desired, users can disable reduced\nprecision reductions in bf16 GEMMs with:\n\n```python\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\n```\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\n```python\nat::globalContext().setAllowBF16ReductionCuBLAS(true);\n\n```\n\n\n## Full FP16 Accmumulation in FP16 GEMMs#\n\nCertain GPUs have increased performance when doing _all_ FP16 GEMM accumulation\nin FP16, at the cost of numerical precision and greater likelihood of overflow.\nNote that this setting only has an effect on GPUs of compute capability 7.0 (Volta)\nor newer.\nThis behavior can be enabled via:\n\n```python\ntorch.backends.cuda.matmul.allow_fp16_accumulation = True\n\n```\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\n```python\nat::globalContext().setAllowFP16AccumulationCuBLAS(true);\n\n```\n\n\n## Asynchronous execution#\n\nBy default, GPU operations are asynchronous.  When you call a function that\nuses the GPU, the operations areenqueuedto the particular device, but not\nnecessarily executed until later.  This allows us to execute more computations\nin parallel, including operations on CPU or other GPUs.\nIn general, the effect of asynchronous computation is invisible to the caller,\nbecause (1) each device executes operations in the order they are queued, and\n(2) PyTorch automatically performs necessary synchronization when copying data\nbetween CPU and GPU or between two GPUs.  Hence, computation will proceed as if\nevery operation was executed synchronously.\nYou can force synchronous computation by setting environment variableCUDA_LAUNCH_BLOCKING=1.  This can be handy when an error occurs on the GPU.\n(With asynchronous execution, such an error isn\u2019t reported until after the\noperation is actually executed, so the stack trace does not show where it was\nrequested.)\nCUDA_LAUNCH_BLOCKING=1\nA consequence of the asynchronous computation is that time measurements without\nsynchronizations are not accurate. To get precise measurements, one should either\ncalltorch.cuda.synchronize()before measuring, or usetorch.cuda.Eventto record times as following:\ntorch.cuda.synchronize()\ntorch.cuda.Event\n\n```python\nstart_event = torch.cuda.Event(enable_timing=True)\nend_event = torch.cuda.Event(enable_timing=True)\nstart_event.record()\n\n# Run some things here\n\nend_event.record()\ntorch.cuda.synchronize()  # Wait for the events to be recorded!\nelapsed_time_ms = start_event.elapsed_time(end_event)\n\n```\n\nAs an exception, several functions such asto()andcopy_()admit an explicitnon_blockingargument,\nwhich lets the caller bypass synchronization when it is unnecessary.\nAnother exception is CUDA streams, explained below.\nto()\ncopy_()\nnon_blocking\n\n## CUDA streams#\n\nACUDA streamis a linear sequence of execution that belongs to a specific\ndevice.  You normally do not need to create one explicitly: by default, each\ndevice uses its own \u201cdefault\u201d stream.\nOperations inside each stream are serialized in the order they are created,\nbut operations from different streams can execute concurrently in any\nrelative order, unless explicit synchronization functions (such assynchronize()orwait_stream()) are\nused.  For example, the following code is incorrect:\nsynchronize()\nwait_stream()\n\n```python\ncuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\nwith torch.cuda.stream(s):\n    # sum() may start execution before normal_() finishes!\n    B = torch.sum(A)\n\n```\n\nWhen the \u201ccurrent stream\u201d is the default stream, PyTorch automatically performs\nnecessary synchronization when data is moved around, as explained above.\nHowever, when using non-default streams, it is the user\u2019s responsibility to\nensure proper synchronization.  The fixed version of this example is:\n\n```python\ncuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\ns.wait_stream(torch.cuda.default_stream(cuda))  # NEW!\nwith torch.cuda.stream(s):\n    B = torch.sum(A)\nA.record_stream(s)  # NEW!\n\n```\n\nThere are two new additions.  Thetorch.cuda.Stream.wait_stream()call\nensures that thenormal_()execution has finished before we start runningsum(A)on a side stream.  Thetorch.Tensor.record_stream()(see for\nmore details) ensures that we do not deallocate A beforesum(A)has\ncompleted.  You can also manually wait on the stream at some later point in\ntime withtorch.cuda.default_stream(cuda).wait_stream(s)(note that it\nis pointless to wait immediately, since that will prevent the stream execution\nfrom running in parallel with other work on the default stream.)  See the\ndocumentation fortorch.Tensor.record_stream()on more details on when\nto use one or another.\ntorch.cuda.Stream.wait_stream()\nnormal_()\nsum(A)\ntorch.Tensor.record_stream()\nsum(A)\ntorch.cuda.default_stream(cuda).wait_stream(s)\ntorch.Tensor.record_stream()\nNote that this synchronization is necessary even when there is no\nread dependency, e.g., as seen in this example:\n\n```python\ncuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda)\ns.wait_stream(torch.cuda.default_stream(cuda))  # STILL REQUIRED!\nwith torch.cuda.stream(s):\n    A.normal_(0.0, 1.0)\n    A.record_stream(s)\n\n```\n\nDespite the computation onsnot reading the contents ofAand no\nother uses ofA, it is still necessary to synchronize, becauseAmay correspond to memory reallocated by the CUDA caching allocator, with\npending operations from the old (deallocated) memory.\ns\nA\nA\nA\n\n## Stream semantics of backward passes#\n\nEach backward CUDA op runs on the same stream that was used for its corresponding forward op.\nIf your forward pass runs independent ops in parallel on different streams,\nthis helps the backward pass exploit that same parallelism.\nThe stream semantics of a backward call with respect to surrounding ops are the same\nas for any other call. The backward pass inserts internal syncs to ensure this even when\nbackward ops run on multiple streams as described in the previous paragraph.\nMore concretely, when callingautograd.backward,autograd.grad, ortensor.backward,\nand optionally supplying CUDA tensor(s) as the  initial gradient(s) (e.g.,autograd.backward(...,grad_tensors=initial_grads),autograd.grad(...,grad_outputs=initial_grads), ortensor.backward(...,gradient=initial_grad)),\nthe acts of\nautograd.backward\nautograd.grad\ntensor.backward\nautograd.backward(...,grad_tensors=initial_grads)\nautograd.grad(...,grad_outputs=initial_grads)\ntensor.backward(...,gradient=initial_grad)\noptionally populating initial gradient(s),\ninvoking the backward pass, and\nusing the gradients\nhave the same stream-semantics relationship as any group of ops:\n\n```python\ns = torch.cuda.Stream()\n\n# Safe, grads are used in the same stream context as backward()\nwith torch.cuda.stream(s):\n    loss.backward()\n    use grads\n\n# Unsafe\nwith torch.cuda.stream(s):\n    loss.backward()\nuse grads\n\n# Safe, with synchronization\nwith torch.cuda.stream(s):\n    loss.backward()\ntorch.cuda.current_stream().wait_stream(s)\nuse grads\n\n# Safe, populating initial grad and invoking backward are in the same stream context\nwith torch.cuda.stream(s):\n    loss.backward(gradient=torch.ones_like(loss))\n\n# Unsafe, populating initial_grad and invoking backward are in different stream contexts,\n# without synchronization\ninitial_grad = torch.ones_like(loss)\nwith torch.cuda.stream(s):\n    loss.backward(gradient=initial_grad)\n\n# Safe, with synchronization\ninitial_grad = torch.ones_like(loss)\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    initial_grad.record_stream(s)\n    loss.backward(gradient=initial_grad)\n\n```\n\nIn prior versions of PyTorch (1.9 and earlier), the autograd engine always synced\nthe default stream with all backward ops, so the following pattern:\n\n```python\nwith torch.cuda.stream(s):\n    loss.backward()\nuse grads\n\n```\n\nwas safe as long asusegradshappened on the default stream.\nIn present PyTorch, that pattern is no longer safe. Ifbackward()andusegradsare in different stream contexts, you must sync the streams:\nusegrads\nbackward()\nusegrads\n\n```python\nwith torch.cuda.stream(s):\n    loss.backward()\ntorch.cuda.current_stream().wait_stream(s)\nuse grads\n\n```\n\neven ifusegradsis on the default stream.\nusegrads\n\n## Memory management#\n\nPyTorch uses a caching memory allocator to speed up memory allocations. This\nallows fast memory deallocation without device synchronizations. However, the\nunused memory managed by the allocator will still show as if used innvidia-smi. You can usememory_allocated()andmax_memory_allocated()to monitor memory occupied by\ntensors, and usememory_reserved()andmax_memory_reserved()to monitor the total amount of memory\nmanaged by the caching allocator. Callingempty_cache()releases allunusedcached memory from PyTorch so that those can be used\nby other GPU applications. However, the occupied GPU memory by tensors will not\nbe freed so it can not increase the amount of GPU memory available for PyTorch.\nnvidia-smi\nmemory_allocated()\nmax_memory_allocated()\nmemory_reserved()\nmax_memory_reserved()\nempty_cache()\nTo better understand how CUDA memory is being used over time,Understanding CUDA Memory Usagedescribes tools for capturing and visualizing traces of memory use.\nFor more advanced users, we offer more comprehensive memory benchmarking viamemory_stats(). We also offer the capability to capture a\ncomplete snapshot of the memory allocator state viamemory_snapshot(), which can help you understand the\nunderlying allocation patterns produced by your code.\nmemory_stats()\nmemory_snapshot()\n\n## Optimizing memory usage  withPYTORCH_CUDA_ALLOC_CONF#\n\nPYTORCH_CUDA_ALLOC_CONF\nUse of a caching allocator can interfere with memory checking tools such ascuda-memcheck.  To debug memory errors usingcuda-memcheck, setPYTORCH_NO_CUDA_MEMORY_CACHING=1in your environment to disable caching.\ncuda-memcheck\ncuda-memcheck\nPYTORCH_NO_CUDA_MEMORY_CACHING=1\nThe behavior of the caching allocator can be controlled via the environment variablePYTORCH_CUDA_ALLOC_CONF.\nThe format isPYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...Available options:\nPYTORCH_CUDA_ALLOC_CONF\nPYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...\nbackendallows selecting the underlying allocator implementation.\nCurrently, valid options arenative, which uses PyTorch\u2019s native\nimplementation, andcudaMallocAsync, which usesCUDA\u2019s built-in asynchronous allocator.cudaMallocAsyncrequires CUDA 11.4 or newer. The default isnative.backendapplies to all devices used by the process, and can\u2019t be\nspecified on a per-device basis.\nbackend\nnative\ncudaMallocAsync\ncudaMallocAsync\nnative\nbackend\nmax_split_size_mbprevents the native allocator\nfrom splitting blocks larger than this size (in MB). This can reduce\nfragmentation and may allow some borderline workloads to complete without\nrunning out of memory. Performance cost can range from \u2018zero\u2019 to \u2018substantial\u2019\ndepending on allocation patterns.  Default value is unlimited, i.e. all blocks\ncan be split. Thememory_stats()andmemory_summary()methods are useful for tuning.  This\noption should be used as a last resort for a workload that is aborting\ndue to \u2018out of memory\u2019 and showing a large amount of inactive split blocks.max_split_size_mbis only meaningful withbackend:native.\nWithbackend:cudaMallocAsync,max_split_size_mbis ignored.\nmax_split_size_mb\nmemory_stats()\nmemory_summary()\nmax_split_size_mb\nbackend:native\nbackend:cudaMallocAsync\nmax_split_size_mb\nroundup_power2_divisionshelps with rounding the requested allocation\nsize to nearest power-2 division and making better use of the blocks. In\nthe native CUDACachingAllocator, the sizes are rounded up in multiple\nof blocks size of 512, so this works fine for smaller sizes. However, this\ncan be inefficient for large near-by allocations as each will go to different\nsize of blocks and reuse of those blocks are minimized. This might create\nlots of unused blocks and will waste GPU memory capacity. This option enables\nthe rounding of allocation size to nearest power-2 division. For example, if\nwe need to round-up size of 1200 and if number of divisions is 4,\nthe size 1200 lies between 1024 and 2048 and if we do 4 divisions between\nthem, the values are 1024, 1280, 1536, and 1792. So, allocation size of 1200\nwill be rounded to 1280 as the nearest ceiling of power-2 division.\nSpecify a single value to apply for all allocation sizes or specify an\narray of key value pairs to set power-2 division individually for each\npower of two interval. For example to set 1 division for all allocations\nunder 256MB, 2 division for allocations between 256MB and 512MB, 4 divisions\nfor allocations between 512MB and 1GB and 8 divisions for any larger allocations,\nset the knob value to: [256:1,512:2,1024:4,>:8].roundup_power2_divisionsis only meaningful withbackend:native.\nWithbackend:cudaMallocAsync,roundup_power2_divisionsis ignored.\nroundup_power2_divisions\nroundup_power2_divisions\nbackend:native\nbackend:cudaMallocAsync\nroundup_power2_divisions\nmax_non_split_rounding_mb\na 1024MB cached block can be reused for a 512MB allocation request. In the default\ncase, we only allow up to 20MB of rounding of non-split blocks, so a 512MB block\ncan only be served with between 512-532 MB size block. If we set the value of this\noption to 1024, it will allow 512-1536 MB size blocks to be used for a 512MB block\nwhich increases reuse of larger blocks. This will also help in reducing the stalls\nin avoiding expensive cudaMalloc calls.\ngarbage_collection_thresholdhelps actively reclaiming unused GPU memory to\navoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks),\nwhich can be unfavorable to latency-critical GPU applications (e.g., servers).\nUpon setting this threshold (e.g., 0.8), the allocator will start reclaiming\nGPU memory blocks if the GPU memory capacity usage exceeds the threshold (i.e.,\n80% of the total memory allocated to the GPU application). The algorithm prefers\nto free old & unused blocks first to avoid freeing blocks that are actively being\nreused. The threshold value should be between greater than 0.0 and less than 1.0.\nThe default value is set at 1.0.\ngarbage_collection_threshold\ngarbage_collection_thresholdis only meaningful withbackend:native.\nWithbackend:cudaMallocAsync,garbage_collection_thresholdis ignored.\ngarbage_collection_threshold\nbackend:native\nbackend:cudaMallocAsync\ngarbage_collection_threshold\nexpandable_segments(experimental, default:False) If set toTrue, this setting instructs\nthe allocator to create CUDA allocations that can later be expanded to better handle cases\nwhere a job changing allocation sizes frequently, such as having a changing batch size.\nNormally for large (>2MB) allocations, the allocator calls cudaMalloc to get allocations\nthat are the same size as what the user requests. In the future, parts of these\nallocations can be reused for other requests if they are free. This works well\nwhen the program makes many requests of exactly the same size or of sizes that\neven multiples of that size. Many deep learning models follow this behavior.\nHowever, one common exception is when the batch size changes slightly from one\niteration to the next, e.g. in batched inference. When the program runs\ninitially with batch sizeN, it will make allocations appropriate for that size.\nIf in the future, it runs at sizeN - 1, the existing allocations will still be\nbig enough. However, if it runs at sizeN + 1, then it will have to make new\nallocations that are slightly larger. Not all the tensors are the same size.\nSome might be(N + 1)*Aand others(N + 1)*A*BwhereAandBare some non-batch\ndimensions in the model. Because the allocator reuses existing allocations when\nthey are big enough, some number of(N + 1)*Aallocations will actually fit in\nthe already existingN*B*Asegments, though not perfectly. As the model runs it\nwill partially fill up all of these segments leaving unusable free slices of\nmemory at the end of these segments. The allocator at some point will need tocudaMalloca new(N + 1)*A*Bsegment. If there is not enough memory, there is\nnow no way to recover the slices of memory that are free at the end of existing\nsegments. With models 50+ layers deep, this pattern might repeat 50+ times\ncreating many slivers.\nexpandable_segments\nexpandable_segmentsallows the allocator to create a segment initially and then\nexpand its size later when more memory is needed. Instead of making one segment\nper allocation, it tries to make one segment (per stream) that grows as\nnecessary. Now when theN + 1case runs, the allocations will tile nicely into\nthe one large segment until it fills up. Then more memory is requested and\nappended to the end of the segment. This process does not create as many slivers\nof unusable memory, so it is more likely to succeed at finding this memory.\npinned_use_cuda_host_registeroption is a boolean flag that determines whether to\nuse the CUDA API\u2019s cudaHostRegister function for allocating pinned memory instead\nof the default cudaHostAlloc. When set to True, the memory is allocated using regular\nmalloc and then pages are mapped to the memory before calling cudaHostRegister.\nThis pre-mapping of pages helps reduce the lock time during the execution\nof cudaHostRegister.\npinned_num_register_threadsoption is only valid when pinned_use_cuda_host_register\nis set to True. By default, one thread is used to map the pages. This option allows\nusing more threads to parallelize the page mapping operations to reduce the overall\nallocation time of pinned memory. A good value for this option is 8 based on\nbenchmarking results.\npinned_use_background_threadsoption is a boolean flag to enable background thread\nfor processing events. This avoids any slow path associated with querying/processing of\nevents in the fast allocation path. This feature is disabled by default.\ngraph_capture_record_stream_reuse(experimental, default:False)\nIf set toTrue, the CUDA caching allocator will attempt to reclaim device memory during\nCUDA Graph capture by using the graph topology (instead of CUDA events) to determine\nwhen a freed block is safe to reuse. This can reduce peak memory during long captures that free\nand reallocate buffers across multiple streams, especially when the capture DAG frequently\nreaches joined frontiers. Note: Enabling this option can significantly increase the time spent\ncapturing the graph.\ngraph_capture_record_stream_reuse\nNote\nSome stats reported by theCUDA memory management APIare specific tobackend:native, and are not meaningful withbackend:cudaMallocAsync.\nSee each function\u2019s docstring for details.\nbackend:native\nbackend:cudaMallocAsync\n\n## Using custom memory allocators for CUDA#\n\nIt is possible to define allocators as simple functions in C/C++ and compile\nthem as a shared library, the code below shows a basic allocator that just\ntraces all the memory operations.\n\n```python\n#include <sys/types.h>\n#include <cuda_runtime_api.h>\n#include <iostream>\n// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC\nextern \"C\" {\nvoid* my_malloc(ssize_t size, int device, cudaStream_t stream) {\n   void *ptr;\n   cudaMalloc(&ptr, size);\n   std::cout<<\"alloc \"<<ptr<<size<<std::endl;\n   return ptr;\n}\n\nvoid my_free(void* ptr, ssize_t size, int device, cudaStream_t stream) {\n   std::cout<<\"free \"<<ptr<< \" \"<<stream<<std::endl;\n   cudaFree(ptr);\n}\n}\n\n```\n\nThis can be used in python through thetorch.cuda.memory.CUDAPluggableAllocator.\nThe user is responsible for supplying the path to the.sofile and the name\nof the alloc/free functions that match the signatures specified above.\ntorch.cuda.memory.CUDAPluggableAllocator\n\n```python\nimport torch\n\n# Load the allocator\nnew_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n    'alloc.so', 'my_malloc', 'my_free')\n# Swap the current allocator\ntorch.cuda.memory.change_current_allocator(new_alloc)\n# This will allocate memory in the device using the new allocator\nb = torch.zeros(10, device='cuda')\n\n```\n\n\n```python\nimport torch\n\n# Do an initial memory allocator\nb = torch.zeros(10, device='cuda')\n# Load the allocator\nnew_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n    'alloc.so', 'my_malloc', 'my_free')\n# This will error since the current allocator was already instantiated\ntorch.cuda.memory.change_current_allocator(new_alloc)\n\n```\n\n\n## Mixing different CUDA system allocators in the same program#\n\nDepending on your use case,change_current_allocator()may not be what you\nwant to use, since it swaps the CUDA allocator for the entire program (similar toPYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync). For instance, if the swapped allocator doesn\u2019t\nhave caching mechanism, you will lose all the benefits of PyTorch\u2019s CUDACachingAllocator. Instead,\nyou can selectively mark a region of PyTorch code to use a custom allocator usingtorch.cuda.MemPool. This will let you use multiple CUDA system allocators in the same\nPyTorch program, along with most of the benefits of the CUDACachingAllocator (e.g. caching).\nUsingtorch.cuda.MemPool, you can utilize custom allocators that enable several features,\nsuch as:\nchange_current_allocator()\nPYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\ntorch.cuda.MemPool\ntorch.cuda.MemPool\nAllocating output buffers for an all-reduce usingncclMemAllocallocator can enable NVLink\nSwitch Reductions (NVLS). This can reduce contention between overlapping compute and communication\nkernels on GPU resources (SMs, and Copy Engines), especially on tensor-parallel workloads.\nncclMemAlloc\nFor Grace CPU based systems, allocating host outputs buffers for an all-gather usingcuMemCreateand specifyingCU_MEM_LOCATION_TYPE_HOST_NUMAcan enable Extended GPU Memory (EGM) based memory transfers\nfrom source GPUs to the destination CPU. This accelerates the all-gather since the transfer\nhappens over NVLinks, which otherwise would have happened over bandwidth-limited, Network Interface\nCard (NIC) links. Such an accelerated all-gather can in turn speed up model checkpointing.\ncuMemCreate\nCU_MEM_LOCATION_TYPE_HOST_NUMA\nIf you are crafting a model and don\u2019t want to think about the optimal memory placements of a memory\nintensive module at first (e.g. an embedding table), or perhaps you have a module which is not\nperformance sensitive and doesn\u2019t fit in the GPU, then you could just allocate that module withcudaMallocManagedwith preferred CPU location and get your model working first.\ncudaMallocManaged\nNote\nWhilecudaMallocManagedoffers convenient automatic memory management using CUDA Unified Virtual Memory (UVM),\nit is not recommended for DL workloads. For DL workloads that fit in GPU memory, explicit placement consistently\noutperforms UVM, since there are no page faults and access patterns remain predictable. When GPU memory gets\nsaturated, UVM has to perform costly double transfers, evicting pages to CPU before bringing in new ones.\ncudaMallocManaged\nThe code below showsncclMemAllocwrapped in atorch.cuda.memory.CUDAPluggableAllocator.\nncclMemAlloc\ntorch.cuda.memory.CUDAPluggableAllocator\n\n```python\nimport os\n\nimport torch\nimport torch.distributed as dist\nfrom torch.cuda.memory import CUDAPluggableAllocator\nfrom torch.distributed.distributed_c10d import _get_default_group\nfrom torch.utils import cpp_extension\n\n\n# create allocator\nnccl_allocator_source = \"\"\"\n#include <nccl.h>\n#include <iostream>\nextern \"C\" {\n\nvoid* nccl_alloc_plug(size_t size, int device, void* stream) {\n  std::cout << \"Using ncclMemAlloc\" << std::endl;\n  void* ptr;\n  ncclResult_t err = ncclMemAlloc(&ptr, size);\n  return ptr;\n\n}\n\nvoid nccl_free_plug(void* ptr, size_t size, int device, void* stream) {\n  std::cout << \"Using ncclMemFree\" << std::endl;\n  ncclResult_t err = ncclMemFree(ptr);\n}\n\n}\n\"\"\"\nnccl_allocator_libname = \"nccl_allocator\"\nnccl_allocator = torch.utils.cpp_extension.load_inline(\n    name=nccl_allocator_libname,\n    cpp_sources=nccl_allocator_source,\n    with_cuda=True,\n    extra_ldflags=[\"-lnccl\"],\n    verbose=True,\n    is_python_module=False,\n    build_directory=\"./\",\n)\n\nallocator = CUDAPluggableAllocator(\n    f\"./{nccl_allocator_libname}.so\", \"nccl_alloc_plug\", \"nccl_free_plug\"\n).allocator()\n\n# setup distributed\nrank = int(os.getenv(\"RANK\"))\nlocal_rank = int(os.getenv(\"LOCAL_RANK\"))\nworld_size = int(os.getenv(\"WORLD_SIZE\"))\ntorch.cuda.set_device(local_rank)\ndist.init_process_group(backend=\"nccl\")\ndevice = torch.device(f\"cuda:{local_rank}\")\ndefault_pg = _get_default_group()\nbackend = default_pg._get_backend(device)\n\n# Note: for convenience, ProcessGroupNCCL backend provides\n# the ncclMemAlloc allocator as backend.mem_allocator\nallocator = backend.mem_allocator\n\n```\n\nYou can now define a new memory pool by passing this allocator totorch.cuda.MemPool:\ntorch.cuda.MemPool\n\n```python\npool = torch.cuda.MemPool(allocator)\n\n```\n\nThe pool can then be used with thetorch.cuda.use_mem_poolcontext manager to\nallocate tensors into that pool:\ntorch.cuda.use_mem_pool\n\n```python\nwith torch.cuda.use_mem_pool(pool):\n    # tensor gets allocated with ncclMemAlloc passed in the pool\n    tensor = torch.arange(1024 * 1024 * 2, device=device)\n    print(f\"tensor ptr on rank {rank} is {hex(tensor.data_ptr())}\")\n\n# register user buffers using ncclCommRegister (called under the hood)\nbackend.register_mem_pool(pool)\n\n# Collective uses Zero Copy NVLS\ndist.all_reduce(tensor[0:4])\ntorch.cuda.synchronize()\nprint(tensor[0:4])\n\n```\n\nNote the usage ofregister_mem_poolin the above example. This is an extra step for\nNVLS reductions, where the user buffers need to be registered with NCCL. A user can\nde-register the buffers with a similarderegister_mem_poolcall.\nregister_mem_pool\nderegister_mem_pool\nTo reclaim memory, users will first need to ensure nothing is using the pool. When none\nof the tensors are holding a reference to the pool,empty_cache()will\nbe called internally on deletion of the pool, hence returning all the memory to the system.\nempty_cache()\n\n```python\ndel tensor, del pool\n\n```\n\nUsers can optionally specify ause_on_oombool (which is False by default) during MemPool\ncreation. If true, then the CUDACachingAllocator will be able to use memory in this pool as\na last resort instead of OOMing.\nuse_on_oom\n\n```python\npool = torch.cuda.MemPool(allocator, use_on_oom=True)\nwith torch.cuda.use_mem_pool(pool):\n    a = torch.randn(40 * 1024 * 1024, dtype=torch.uint8, device=\"cuda\")\ndel a\n\n# at the memory limit, this will succeed by using pool's memory in order to avoid the oom\nb = torch.randn(40 * 1024 * 1024, dtype=torch.uint8, device=\"cuda\")\n\n```\n\nThe followingtorch.cuda.MemPool.use_count()andtorch.cuda.MemPool.snapshot()APIs can be used for debugging purposes:\ntorch.cuda.MemPool.use_count()\ntorch.cuda.MemPool.snapshot()\n\n```python\npool = torch.cuda.MemPool(allocator)\n\n# pool's use count should be 1 at this point as MemPool object\n# holds a reference\nassert pool.use_count() == 1\n\nnelem_1mb = 1024 * 1024 // 4\n\nwith torch.cuda.use_mem_pool(pool):\n    out_0 = torch.randn(nelem_1mb, device=\"cuda\")\n\n    # pool's use count should be 2 at this point as use_mem_pool\n    # holds a reference\n    assert pool.use_count() == 2\n\n# pool's use count should be back to 1 at this point as use_mem_pool\n# released its reference\nassert pool.use_count() == 1\n\nwith torch.cuda.use_mem_pool(pool):\n    # pool should have 1 segment since we made a small allocation (1 MB)\n    # above and so the CUDACachingAllocator packed it into a 2 MB buffer\n    assert len(pool.snapshot()) == 1\n\n    out_1 = torch.randn(nelem_1mb, device=\"cuda\")\n\n    # pool should still have 1 segment since we made another small allocation\n    # (1 MB) that got packed into the existing 2 MB buffer\n    assert len(pool.snapshot()) == 1\n\n    out_2 = torch.randn(nelem_1mb, device=\"cuda\")\n\n    # pool now should have 2 segments since the CUDACachingAllocator had\n    # to make a new 2 MB buffer to accommodate out_2\n    assert len(pool.snapshot()) == 2\n\n```\n\nNote\ntorch.cuda.MemPoolholds a reference to the pool. When you use thetorch.cuda.use_mem_poolcontext manager, it will also acquire another reference\nto the pool. On exit of the context manager, it will release its reference. After that,\nideally it should only be tensors holding references to the pool. Once the tensors release\ntheir references, the use count of the pool will be 1, reflecting that only thetorch.cuda.MemPoolobject is holding a reference. Only at that point, can the memory\nheld by the pool be returned to the system when the pool\u2019s destructor is called usingdel.\ntorch.cuda.MemPool\ntorch.cuda.use_mem_pool\ntorch.cuda.MemPool\ndel\ntorch.cuda.MemPooldoesn\u2019t currently supportexpandable_segmentsmode of\nCUDACachingAllocator.\ntorch.cuda.MemPool\nexpandable_segments\nNCCL has specific requirementsfor a buffer to be compatible with NVLS reductions.\nThese requirements can be broken in a dynamic workload, for instance, the buffer being\nsent to NCCL by the CUDACachingAllocator might be split and hence, not correctly aligned.\nIn those cases, NCCL can use a fallback algorithm instead of NVLS.\nAllocators likencclMemAlloccan use more memory than requested, due to alignment\nrequirements (CU_MULTICAST_GRANULARITY_RECOMMENDED,CU_MULTICAST_GRANULARITY_MINIMUM),\nand can cause your workload to run out of memory.\nncclMemAlloc\nCU_MULTICAST_GRANULARITY_RECOMMENDED\nCU_MULTICAST_GRANULARITY_MINIMUM\n\n## Tuning NVLink Performance with Custom Memory Allocator on H100/H200 GPUs#\n\nIn rare cases, performance of NVLink on H100/H200 GPUs can be influenced by the physical memory\nlayout of data, creating an opportunity for developers to tune their applications for optimal\nthroughput.\nAn example of how physical memory layout of data affects performance is when communication\nkernels issue unbalanced NVLink read/write operations. In the following figure, we can see\nthat each warp accesses memory addresses with a consistent strided pattern in each single wave.\nWe can have a more balanced load by tuning the stride size in the workload or we can implement\na custom CUDA allocator.\n\n```python\n_______________________________  _______________________________      _______________________________\n| Warp 0 Reading | No-reading |  | Warp 1 Reading | No-reading |  ...  Warp N Reading | No-reading |\n_______________________________  _______________________________      _______________________________\n<----------------------------->\n        Stride size\n\n```\n\nSuch an allocator can maintain contiguous virtual memory addresses for the kernel while strategically\narranging the mapping to physical memory addresses (e.g., through shuffling). This technique allows\ndevelopers to explore different physical access patterns to find the most efficient one, unlocking\nhigher performance without modifying the kernel\u2019s logic. A practical implementation of such an allocator\ncan be achieved using PyTorch\u2019s custom allocator support as mentioned before, where the malloc and free\nfunctions are:\n\n```python\n// assuming a system with 8 GPUs\nstruct CustomAllocInfo {\n  void** devPtr;  // This will be the usable virtual memory address\n  CUdeviceptr dptr;\n  size_t totalSize;  // Total size of the allocated memory\n  size_t padded_size;\n  int device_id;\n  std::vector<CUmemGenericAllocationHandle> handles;  // Handles to physical memory allocations\n};\n\n// loop over pages\ncudaError_t customCudaMalloc(CustomAllocInfo* info) {\n    if (!info) return cudaErrorInvalidValue;\n\n    CUdeviceptr dptr;\n\n    // Handles to redundant physical memory allocations which help truncate stride pattern in physical memory\n    std::vector<CUmemGenericAllocationHandle> handles_redundant;\n\n    size_t granularity = 0;\n    CUmemAllocationProp prop = {};\n\n    int currentDev = info->device_id;\n    size_t totalSize = info->totalSize;\n\n    prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;\n    prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n    prop.location.id = currentDev;\n    cuMemGetAllocationGranularity(&granularity, &prop, CU_MEM_ALLOC_GRANULARITY_MINIMUM);\n    size_t padded_size = ROUND_UP(totalSize, granularity);\n\n    info->padded_size = padded_size;\n\n    // loop over pages\n    size_t iter_granularity = granularity * 64; // 64 * granularity with shift_size = 2 works\n    uint32_t iteration_count = (totalSize + iter_granularity - 1) / iter_granularity;\n\n    cuMemAddressReserve(&dptr, padded_size, 0ULL, 0ULL, 0ULL);\n\n    const int shift_size = 2;\n    for (size_t i = 0; i < iteration_count; i+=shift_size) {\n\n        CUmemGenericAllocationHandle allocHandle[shift_size];\n        for (int shift = 0; (shift < shift_size)&&(i+shift < iteration_count); shift++){\n            CHECK_CUDA(cuMemCreate(&allocHandle[shift], iter_granularity, &prop, 0));\n            info->handles.push_back(allocHandle[shift]);\n        }\n\n        for (int shift = 0; (shift < shift_size)&&(i+shift < iteration_count); shift++){\n\n            // mapping makes the shift (shift -> (shift+1)%shift_size  )\n            CHECK_CUDA(cuMemMap(dptr + (i+shift) * iter_granularity, iter_granularity, 0, allocHandle[(shift+1)%shift_size], 0));\n\n            setupMultiGPUAccess(dptr + (i+shift) * iter_granularity, iter_granularity, {0, 1, 2, 3, 4, 5, 6, 7}); // Enable access for all 8 GPUs\n        }\n\n        // std::cout << \"Here we allocate one redundant page (2MB)...\" << std::endl;\n        // this is an extra optimization on top of the swizzling. It helps \"break\"\n        // the physical access pattern even more. It can be left out if workload is already\n        // performing at SOL with just swizzling.\n        CUmemGenericAllocationHandle allocHandle_redundant;\n        CHECK_CUDA(cuMemCreate(&allocHandle_redundant, granularity, &prop, 0));\n        handles_redundant.push_back(allocHandle_redundant);\n    }\n\n    *info->devPtr = (void*)dptr;\n    info->dptr = dptr;\n\n    // Release each redundant allocation\n    for (auto handle : handles_redundant) {\n        // std::cout << \"Here we release one redundant page (2MB)...\" << std::endl;\n        CHECK_CUDA(cuMemRelease(handle));\n    }\n\n    return cudaSuccess;\n}\n\nvoid customCudaFree(CustomAllocInfo* info) {\n    if (!info) return;\n\n    // CHECK_CUDA(cudaSetDevice(info->device_id));\n\n    CHECK_CUDA(cuMemUnmap(info->dptr, info->padded_size));\n\n    // Unmap and release each allocation\n    for (auto handle : info->handles) {\n        CHECK_CUDA(cuMemRelease(handle));\n    }\n\n    // Unreserve the virtual address space\n    // CHECK_CUDA(cuMemAddressFree((CUdeviceptr)*info->devPtr, info->padded_size));\n    CHECK_CUDA(cuMemAddressFree(info->dptr, info->padded_size));\n}\n\n```\n\n\n## cuBLAS workspaces#\n\nFor each combination of cuBLAS handle and CUDA stream, a cuBLAS workspace will be allocated\nif that handle and stream combination executes a cuBLAS kernel that requires a workspace.\nIn order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unlesstorch._C._cuda_clearCublasWorkspaces()is called. The workspace size per allocation can be\nspecified via the environment variableCUBLAS_WORKSPACE_CONFIGwith the format:[SIZE]:[COUNT].\nAs an example, the default workspace size per allocation isCUBLAS_WORKSPACE_CONFIG=:4096:2:16:8which specifies a total size of2*4096+8*16KiB. To force cuBLAS to avoid using workspaces,\nsetCUBLAS_WORKSPACE_CONFIG=:0:0.\ntorch._C._cuda_clearCublasWorkspaces()\nCUBLAS_WORKSPACE_CONFIG\n:[SIZE]:[COUNT]\nCUBLAS_WORKSPACE_CONFIG=:4096:2:16:8\n2*4096+8*16KiB\nCUBLAS_WORKSPACE_CONFIG=:0:0\n\n## cuFFT plan cache#\n\nFor each CUDA device, an LRU cache of cuFFT plans is used to speed up repeatedly\nrunning FFT methods (e.g.,torch.fft.fft()) on CUDA tensors of same geometry\nwith same configuration. Because some cuFFT plans may allocate GPU memory,\nthese caches have a maximum capacity.\ntorch.fft.fft()\nYou may control and query the properties of the cache of current device with\nthe following APIs:\ntorch.backends.cuda.cufft_plan_cache.max_sizegives the capacity of the\ncache (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions).\nSetting this value directly modifies the capacity.\ntorch.backends.cuda.cufft_plan_cache.max_size\ntorch.backends.cuda.cufft_plan_cache.sizegives the number of plans\ncurrently residing in the cache.\ntorch.backends.cuda.cufft_plan_cache.size\ntorch.backends.cuda.cufft_plan_cache.clear()clears the cache.\ntorch.backends.cuda.cufft_plan_cache.clear()\nTo control and query plan caches of a non-default device, you can index thetorch.backends.cuda.cufft_plan_cacheobject with either atorch.deviceobject or a device index, and access one of the above attributes. E.g., to set\nthe capacity of the cache for device1, one can writetorch.backends.cuda.cufft_plan_cache[1].max_size=10.\ntorch.backends.cuda.cufft_plan_cache\ntorch.device\n1\ntorch.backends.cuda.cufft_plan_cache[1].max_size=10\n\n## Just-in-Time Compilation#\n\nPyTorch just-in-time compiles some operations, like torch.special.zeta, when\nperformed on CUDA tensors. This compilation can be time consuming\n(up to a few seconds depending on your hardware and software)\nand may occur multiple times for a single operator since many PyTorch operators actually\nselect from a variety of kernels, each of which must be compiled once, depending on their input.\nThis compilation occurs once per process, or just once if a kernel cache is used.\nBy default, PyTorch creates a kernel cache in $XDG_CACHE_HOME/torch/kernels if\nXDG_CACHE_HOME is defined and $HOME/.cache/torch/kernels if it\u2019s not (except on Windows,\nwhere the kernel cache is not yet supported). The caching behavior can be directly\ncontrolled with two environment variables. If USE_PYTORCH_KERNEL_CACHE is set to 0 then no\ncache will be used, and if PYTORCH_KERNEL_CACHE_PATH is set then that path will be used\nas a kernel cache instead of the default location.\n\n## Best practices#\n\n\n## Device-agnostic code#\n\nDue to the structure of PyTorch, you may need to explicitly write\ndevice-agnostic (CPU or GPU) code; an example may be creating a new tensor as\nthe initial hidden state of a recurrent neural network.\nThe first step is to determine whether the GPU should be used or not. A common\npattern is to use Python\u2019sargparsemodule to read in user arguments, and\nhave a flag that can be used to disable CUDA, in combination withis_available(). In the following,args.deviceresults in atorch.deviceobject that can be used to move tensors to CPU or CUDA.\nargparse\nis_available()\nargs.device\ntorch.device\n\n```python\nimport argparse\nimport torch\n\nparser = argparse.ArgumentParser(description='PyTorch Example')\nparser.add_argument('--disable-cuda', action='store_true',\n                    help='Disable CUDA')\nargs = parser.parse_args()\nargs.device = None\nif not args.disable_cuda and torch.cuda.is_available():\n    args.device = torch.device('cuda')\nelse:\n    args.device = torch.device('cpu')\n\n```\n\nNote\nWhen assessing the availability of CUDA in a given environment (is_available()), PyTorch\u2019s default\nbehavior is to call the CUDA Runtime API methodcudaGetDeviceCount. Because this call in turn initializes the\nCUDA Driver API (viacuInit) if it is not already initialized, subsequent forks of a process that has runis_available()will fail with a CUDA initialization error.\nis_available()\nis_available()\nOne can setPYTORCH_NVML_BASED_CUDA_CHECK=1in your environment before importing PyTorch modules that executeis_available()(or before executing it directly) in order to directis_available()to attempt an NVML-based assessment (nvmlDeviceGetCount_v2). If the\nNVML-based assessment is successful (i.e. NVML discovery/initialization does not fail),is_available()calls will not poison subsequent forks.\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nis_available()\nis_available()\nis_available()\nIf NVML discovery/initialization fails,is_available()will fallback to the standard CUDA Runtime\nAPI assessment and the aforementioned fork constraint will apply.\nis_available()\nNote that the above NVML-based CUDA availability assessment provides a weaker guarantee than the default CUDA\nRuntime API approach (which requires CUDA initialization to succeed). In some circumstances, the NVML-based check\nmay succeed while later CUDA initialization fails.\nNow that we haveargs.device, we can use it to create a Tensor on the\ndesired device.\nargs.device\n\n```python\nx = torch.empty((8, 42), device=args.device)\nnet = Network().to(device=args.device)\n\n```\n\nThis can be used in a number of cases to produce device agnostic code. Below\nis an example when using a dataloader:\n\n```python\ncuda0 = torch.device('cuda:0')  # CUDA GPU 0\nfor i, x in enumerate(train_loader):\n    x = x.to(cuda0)\n\n```\n\nWhen working with multiple GPUs on a system, you can use theCUDA_VISIBLE_DEVICESenvironment flag to manage which GPUs are available to\nPyTorch. As mentioned above, to manually control which GPU a tensor is created\non, the best practice is to use atorch.cuda.devicecontext manager.\nCUDA_VISIBLE_DEVICES\ntorch.cuda.device\n\n```python\nprint(\"Outside device is 0\")  # On device 0 (default in most scenarios)\nwith torch.cuda.device(1):\n    print(\"Inside device is 1\")  # On device 1\nprint(\"Outside device is still 0\")  # On device 0\n\n```\n\nIf you have a tensor and would like to create a new tensor of the same type on\nthe same device, then you can use atorch.Tensor.new_*method\n(seetorch.Tensor).\nWhilst the previously mentionedtorch.*factory functions\n(Creation Ops) depend on the current GPU context and\nthe attributes arguments you pass in,torch.Tensor.new_*methods preserve\nthe device and other attributes of the tensor.\ntorch.Tensor.new_*\ntorch.Tensor\ntorch.*\ntorch.Tensor.new_*\nThis is the recommended practice when creating modules in which new\ntensors need to be created internally during the forward pass.\n\n```python\ncuda = torch.device('cuda')\nx_cpu = torch.empty(2)\nx_gpu = torch.empty(2, device=cuda)\nx_cpu_long = torch.empty(2, dtype=torch.int64)\n\ny_cpu = x_cpu.new_full([3, 2], fill_value=0.3)\nprint(y_cpu)\n\n    tensor([[ 0.3000,  0.3000],\n            [ 0.3000,  0.3000],\n            [ 0.3000,  0.3000]])\n\ny_gpu = x_gpu.new_full([3, 2], fill_value=-5)\nprint(y_gpu)\n\n    tensor([[-5.0000, -5.0000],\n            [-5.0000, -5.0000],\n            [-5.0000, -5.0000]], device='cuda:0')\n\ny_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])\nprint(y_cpu_long)\n\n    tensor([[ 1,  2,  3]])\n\n```\n\nIf you want to create a tensor of the same type and size of another tensor, and\nfill it with either ones or zeros,ones_like()orzeros_like()are provided as convenient helper functions (which\nalso preservetorch.deviceandtorch.dtypeof a Tensor).\nones_like()\nzeros_like()\ntorch.device\ntorch.dtype\n\n```python\nx_cpu = torch.empty(2, 3)\nx_gpu = torch.empty(2, 3)\n\ny_cpu = torch.ones_like(x_cpu)\ny_gpu = torch.zeros_like(x_gpu)\n\n```\n\n\n## Use pinned memory buffers#\n\nWarning\nThis is an advanced tip. If you overuse pinned memory, it can cause serious\nproblems when running low on RAM, and you should be aware that pinning is\noften an expensive operation.\nHost to GPU copies are much faster when they originate from pinned (page-locked)\nmemory. CPU tensors and storages expose apin_memory()method, that returns a copy of the object, with data put in a pinned region.\npin_memory()\nAlso, once you pin a tensor or storage, you can use asynchronous GPU copies.\nJust pass an additionalnon_blocking=Trueargument to ato()or acuda()call. This can be used\nto overlap data transfers with computation.\nnon_blocking=True\nto()\ncuda()\nYou can make theDataLoaderreturn batches placed in\npinned memory by passingpin_memory=Trueto its constructor.\nDataLoader\npin_memory=True\n\n## Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel#\n\nMost use cases involving batched inputs and multiple GPUs should default to\nusingDistributedDataParallelto utilize more\nthan one GPU.\nDistributedDataParallel\nThere are significant caveats to using CUDA models withmultiprocessing; unless care is taken to meet the data handling\nrequirements exactly, it is likely that your program will have incorrect or\nundefined behavior.\nmultiprocessing\nIt is recommended to useDistributedDataParallel,\ninstead ofDataParallelto do multi-GPU training, even if\nthere is only a single node.\nDistributedDataParallel\nDataParallel\nThe difference betweenDistributedDataParallelandDataParallelis:DistributedDataParalleluses multiprocessing where a process is created for each GPU, whileDataParalleluses multithreading. By using multiprocessing,\neach GPU has its dedicated process, this avoids the performance overhead caused\nby GIL of Python interpreter.\nDistributedDataParallel\nDataParallel\nDistributedDataParallel\nDataParallel\nIf you useDistributedDataParallel, you could usetorch.distributed.launchutility to launch your program, seeLaunch utility.\nDistributedDataParallel\n\n## CUDA Graphs#\n\nA CUDA graph is a record of the work (mostly kernels and their arguments) that a\nCUDA stream and its dependent streams perform.\nFor general principles and details on the underlying CUDA API, seeGetting Started with CUDA Graphsand theGraphs sectionof the CUDA C Programming Guide.\nPyTorch supports the construction of CUDA graphs usingstream capture, which puts a\nCUDA stream incapture mode. CUDA work issued to a capturing stream doesn\u2019t actually\nrun on the GPU. Instead, the work is recorded in a graph.\nAfter capture, the graph can belaunchedto run the GPU work as many times as needed.\nEach replay runs the same kernels with the same arguments. For pointer arguments this\nmeans the same memory addresses are used.\nBy filling input memory with new data (e.g., from a new batch) before each replay,\nyou can rerun the same work on new data.\n\n## Why CUDA Graphs?#\n\nReplaying a graph sacrifices the dynamic flexibility of typical eager execution in exchange forgreatly reduced CPU overhead. A graph\u2019s arguments and kernels are fixed, so a graph replay\nskips all layers of argument setup and kernel dispatch, including Python, C++, and CUDA driver\noverheads. Under the hood, a replay submits the entire graph\u2019s work to the GPU with\na single call tocudaGraphLaunch.  Kernels in a replay also execute slightly faster\non the GPU, but eliding CPU overhead is the main benefit.\nYou should try CUDA graphs if all or part of your network is graph-safe (usually this means\nstatic shapes and static control flow, but see the otherconstraints)\nand you suspect its runtime is at least somewhat CPU-limited.\n\n## PyTorch API#\n\nWarning\nThis API is in beta and may change in future releases.\nPyTorch exposes graphs via a rawtorch.cuda.CUDAGraphclass\nand two convenience wrappers,torch.cuda.graphandtorch.cuda.make_graphed_callables.\ntorch.cuda.CUDAGraph\ntorch.cuda.graph\ntorch.cuda.make_graphed_callables\ntorch.cuda.graphis a simple, versatile context manager that\ncaptures CUDA work in its context.\nBefore capture, warm up the workload to be captured by running\na few eager iterations. Warmup must occur on a side stream.\nBecause the graph reads from and writes to the same memory addresses in every\nreplay, you must maintain long-lived references to tensors that hold\ninput and output data during capture.\nTo run the graph on new input data, copy new data to the capture\u2019s input tensor(s),\nreplay the graph, then read the new output from the capture\u2019s output tensor(s).\nExample:\ntorch.cuda.graph\n\n```python\ng = torch.cuda.CUDAGraph()\n\n# Placeholder input used for capture\nstatic_input = torch.empty((5,), device=\"cuda\")\n\n# Warmup before capture\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for _ in range(3):\n        static_output = static_input * 2\ntorch.cuda.current_stream().wait_stream(s)\n\n# Captures the graph\n# To allow capture, automatically sets a side stream as the current stream in the context\nwith torch.cuda.graph(g):\n    static_output = static_input * 2\n\n# Fills the graph's input memory with new data to compute on\nstatic_input.copy_(torch.full((5,), 3, device=\"cuda\"))\ng.replay()\n# static_output holds the results\nprint(static_output)  # full of 3 * 2 = 6\n\n# Fills the graph's input memory with more data to compute on\nstatic_input.copy_(torch.full((5,), 4, device=\"cuda\"))\ng.replay()\nprint(static_output)  # full of 4 * 2 = 8\n\n```\n\nSeeWhole-network capture,Usage with torch.cuda.amp, andUsage with multiple streamsfor realistic and advanced patterns.\nmake_graphed_callablesis more sophisticated.make_graphed_callablesaccepts Python functions andtorch.nn.Modules. For each passed function or Module,\nit creates separate graphs of the forward-pass and backward-pass work. SeePartial-network capture.\nmake_graphed_callables\nmake_graphed_callables\ntorch.nn.Module\nA set of ops iscapturableif it doesn\u2019t violate any of the following constraints.\nConstraints apply to all work in atorch.cuda.graphcontext and all work in the forward and backward passes\nof any callable you pass totorch.cuda.make_graphed_callables().\ntorch.cuda.graph\ntorch.cuda.make_graphed_callables()\nViolating any of these will likely cause a runtime error:\nCapture must occur on a non-default stream. (This is only a concern if you use the rawCUDAGraph.capture_beginandCUDAGraph.capture_endcalls.graphandmake_graphed_callables()set a side stream for you.)\nCUDAGraph.capture_begin\nCUDAGraph.capture_end\ngraph\nmake_graphed_callables()\nOps that synchronize the CPU with the GPU (e.g.,.item()calls) are prohibited.\n.item()\nCUDA RNG operations are permitted, and when using multipletorch.Generatorinstances within a graph,\nthey must be registered usingCUDAGraph.register_generator_statebefore graph capture.\nAvoid usingGenerator.get_stateandGenerator.set_stateduring capture;\ninstead, utilizeGenerator.graphsafe_set_stateandGenerator.graphsafe_get_statefor managing generator states safely within the graph context. This ensures proper RNG operation and generator management within CUDA graphs.\ntorch.Generator\nCUDAGraph.register_generator_state\nGenerator.get_state\nGenerator.set_state\nGenerator.graphsafe_set_state\nGenerator.graphsafe_get_state\nViolating any of these will likely cause silent numerical errors or undefined behavior:\nWithin a process, only one capture may be underway at a time.\nNo non-captured CUDA work may run in this process (on any thread) while capture is underway.\nCPU work is not captured. If the captured ops include CPU work, that work will be elided during replay.\nEvery replay reads from and writes to the same (virtual) memory addresses.\nDynamic control flow (based on CPU or GPU data) is prohibited.\nDynamic shapes are prohibited. The graph assumes every tensor in the captured op sequence\nhas the same size and layout in every replay.\nUsing multiple streams in a capture is allowed, but there arerestrictions.\nOnce captured, the graph may be replayed on any stream.\n\n## Whole-network capture#\n\nIf your entire network is capturable, you can capture and replay an entire iteration:\n\n```python\nN, D_in, H, D_out = 640, 4096, 2048, 1024\nmodel = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n                            torch.nn.Dropout(p=0.2),\n                            torch.nn.Linear(H, D_out),\n                            torch.nn.Dropout(p=0.1)).cuda()\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n# Placeholders used for capture\nstatic_input = torch.randn(N, D_in, device='cuda')\nstatic_target = torch.randn(N, D_out, device='cuda')\n\n# warmup\n# Uses static_input and static_target here for convenience,\n# but in a real setting, because the warmup includes optimizer.step()\n# you must use a few batches of real data.\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for i in range(3):\n        optimizer.zero_grad(set_to_none=True)\n        y_pred = model(static_input)\n        loss = loss_fn(y_pred, static_target)\n        loss.backward()\n        optimizer.step()\ntorch.cuda.current_stream().wait_stream(s)\n\n# capture\ng = torch.cuda.CUDAGraph()\n# Sets grads to None before capture, so backward() will create\n# .grad attributes with allocations from the graph's private pool\noptimizer.zero_grad(set_to_none=True)\nwith torch.cuda.graph(g):\n    static_y_pred = model(static_input)\n    static_loss = loss_fn(static_y_pred, static_target)\n    static_loss.backward()\n    optimizer.step()\n\nreal_inputs = [torch.rand_like(static_input) for _ in range(10)]\nreal_targets = [torch.rand_like(static_target) for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    # Fills the graph's input memory with new data to compute on\n    static_input.copy_(data)\n    static_target.copy_(target)\n    # replay() includes forward, backward, and step.\n    # You don't even need to call optimizer.zero_grad() between iterations\n    # because the captured backward refills static .grad tensors in place.\n    g.replay()\n    # Params have been updated. static_y_pred, static_loss, and .grad\n    # attributes hold values from computing on this iteration's data.\n\n```\n\n\n## Partial-network capture#\n\nIf some of your network is unsafe to capture (e.g., due to dynamic control flow,\ndynamic shapes, CPU syncs, or essential CPU-side logic), you can run the unsafe\npart(s) eagerly and usetorch.cuda.make_graphed_callables()to graph only\nthe capture-safe part(s).\ntorch.cuda.make_graphed_callables()\nBy default, callables returned bymake_graphed_callables()are autograd-aware, and can be used in the training loop as direct replacements\nfor the functions ornn.Modules you passed.\nmake_graphed_callables()\nnn.Module\nmake_graphed_callables()internally createsCUDAGraphobjects, runs warmup iterations, and maintains\nstatic inputs and outputs as needed.  Therefore (unlike withtorch.cuda.graph) you don\u2019t need to handle those manually.\nmake_graphed_callables()\nCUDAGraph\ntorch.cuda.graph\nIn the following example, data-dependent dynamic control flow means the\nnetwork isn\u2019t capturable end-to-end, butmake_graphed_callables()lets us capture and run graph-safe sections as graphs regardless:\nmake_graphed_callables()\n\n```python\nN, D_in, H, D_out = 640, 4096, 2048, 1024\n\nmodule1 = torch.nn.Linear(D_in, H).cuda()\nmodule2 = torch.nn.Linear(H, D_out).cuda()\nmodule3 = torch.nn.Linear(H, D_out).cuda()\n\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(chain(module1.parameters(),\n                                  module2.parameters(),\n                                  module3.parameters()),\n                            lr=0.1)\n\n# Sample inputs used for capture\n# requires_grad state of sample inputs must match\n# requires_grad state of real inputs each callable will see.\nx = torch.randn(N, D_in, device='cuda')\nh = torch.randn(N, H, device='cuda', requires_grad=True)\n\nmodule1 = torch.cuda.make_graphed_callables(module1, (x,))\nmodule2 = torch.cuda.make_graphed_callables(module2, (h,))\nmodule3 = torch.cuda.make_graphed_callables(module3, (h,))\n\nreal_inputs = [torch.rand_like(x) for _ in range(10)]\nreal_targets = [torch.randn(N, D_out, device=\"cuda\") for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    optimizer.zero_grad(set_to_none=True)\n\n    tmp = module1(data)  # forward ops run as a graph\n\n    if tmp.sum().item() > 0:\n        tmp = module2(tmp)  # forward ops run as a graph\n    else:\n        tmp = module3(tmp)  # forward ops run as a graph\n\n    loss = loss_fn(tmp, target)\n    # module2's or module3's (whichever was chosen) backward ops,\n    # as well as module1's backward ops, run as graphs\n    loss.backward()\n    optimizer.step()\n\n```\n\n\n## Usage with torch.cuda.amp#\n\nFor typical optimizers,GradScaler.stepsyncs\nthe CPU with the GPU, which is prohibited during capture. To avoid errors, either usepartial-network capture, or (if forward, loss,\nand backward are capture-safe) capture forward, loss, and backward but not the\noptimizer step:\nGradScaler.step\n\n```python\n# warmup\n# In a real setting, use a few batches of real data.\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for i in range(3):\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast():\n            y_pred = model(static_input)\n            loss = loss_fn(y_pred, static_target)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\ntorch.cuda.current_stream().wait_stream(s)\n\n# capture\ng = torch.cuda.CUDAGraph()\noptimizer.zero_grad(set_to_none=True)\nwith torch.cuda.graph(g):\n    with torch.cuda.amp.autocast():\n        static_y_pred = model(static_input)\n        static_loss = loss_fn(static_y_pred, static_target)\n    scaler.scale(static_loss).backward()\n    # don't capture scaler.step(optimizer) or scaler.update()\n\nreal_inputs = [torch.rand_like(static_input) for _ in range(10)]\nreal_targets = [torch.rand_like(static_target) for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    static_input.copy_(data)\n    static_target.copy_(target)\n    g.replay()\n    # Runs scaler.step and scaler.update eagerly\n    scaler.step(optimizer)\n    scaler.update()\n\n```\n\n\n## Usage with multiple streams#\n\nCapture mode automatically propagates to any streams that sync with a capturing stream.\nWithin capture, you may expose parallelism by issuing calls to different streams,\nbut the overall stream dependency DAG must branch out from the\ninitial capturing stream after capture begins and rejoin the initial stream\nbefore capture ends:\n\n```python\nwith torch.cuda.graph(g):\n    # at context manager entrance, torch.cuda.current_stream()\n    # is the initial capturing stream\n\n    # INCORRECT (does not branch out from or rejoin initial stream)\n    with torch.cuda.stream(s):\n        cuda_work()\n\n    # CORRECT:\n    # branches out from initial stream\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        cuda_work()\n    # rejoins initial stream before capture ends\n    torch.cuda.current_stream().wait_stream(s)\n\n```\n\nNote\nTo avoid confusion for power users looking at replays in nsight systems or nvprof:\nUnlike eager execution, the graph interprets a nontrivial stream DAG in capture\nas a hint, not a command. During replay, the graph may reorganize independent ops\nonto different streams or enqueue them in a different order (while respecting your\noriginal DAG\u2019s overall dependencies).\n\n## Usage with DistributedDataParallel#\n\nNCCL versions earlier than 2.9.6 don\u2019t allow collectives to be captured.\nYou must usepartial-network capture,\nwhich defers allreduces to happen outside graphed sections of backward.\nCallmake_graphed_callables()on graphable network sectionsbeforewrapping the network with DDP.\nmake_graphed_callables()\nNCCL versions 2.9.6 or later allow collectives in the graph.\nApproaches that capture anentire backward passare a viable option, but need three setup steps.\nDisable DDP\u2019s internal async error handling:\n\n```python\nos.environ[\"NCCL_ASYNC_ERROR_HANDLING\"] = \"0\"\ntorch.distributed.init_process_group(...)\n\n```\n\nBefore full-backward capture, DDP must be constructed in a side-stream context:\n\n```python\nwith torch.cuda.stream(s):\n    model = DistributedDataParallel(model)\n\n```\n\nYour warmup must run at least 11 DDP-enabled eager iterations before capture.\n\n## Graph memory management#\n\nA captured graph acts on the same virtual addresses every time it replays.\nIf PyTorch frees the memory, a later replay can hit an illegal memory access.\nIf PyTorch reassigns the memory to new tensors, the replay can corrupt the values\nseen by those tensors.  Therefore, the virtual addresses used by the graph must be\nreserved for the graph across replays. The PyTorch caching allocator achieves this\nby detecting when capture is underway and satisfying the capture\u2019s allocations\nfrom a graph-private memory pool. The private pool stays alive until itsCUDAGraphobject and all tensors created during capture\ngo out of scope.\nCUDAGraph\nPrivate pools are maintained automatically. By default, the allocator creates a\nseparate private pool for each capture. If you capture multiple graphs,\nthis conservative approach ensures graph replays never corrupt each other\u2019s values,\nbut sometimes needlessly wastes memory.\nTo economize the memory stashed in private pools,torch.cuda.graphandtorch.cuda.make_graphed_callables()optionally allow different\ncaptures to share the same private pool.\nIt\u2019s safe for a set of graphs to share a private pool if you know they\u2019ll always\nbe replayed in the same order they were captured,\nand never be replayed concurrently.\ntorch.cuda.graph\ntorch.cuda.make_graphed_callables()\ntorch.cuda.graph\u2019spoolargument is a hint to use a particular private pool,\nand can be used to share memory across graphs as shown:\ntorch.cuda.graph\npool\n\n```python\ng1 = torch.cuda.CUDAGraph()\ng2 = torch.cuda.CUDAGraph()\n\n# (create static inputs for g1 and g2, run warmups of their workloads...)\n\n# Captures g1\nwith torch.cuda.graph(g1):\n    static_out_1 = g1_workload(static_in_1)\n\n# Captures g2, hinting that g2 may share a memory pool with g1\nwith torch.cuda.graph(g2, pool=g1.pool()):\n    static_out_2 = g2_workload(static_in_2)\n\nstatic_in_1.copy_(real_data_1)\nstatic_in_2.copy_(real_data_2)\ng1.replay()\ng2.replay()\n\n```\n\nWithtorch.cuda.make_graphed_callables(), if you want to graph several\ncallables and you know they\u2019ll always run in the same order (and never concurrently)\npass them as a tuple in the same order they\u2019ll run in the live workload, andmake_graphed_callables()will capture their graphs using a shared\nprivate pool.\ntorch.cuda.make_graphed_callables()\nmake_graphed_callables()\nIf, in the live workload, your callables will run in an order that occasionally changes,\nor if they\u2019ll run concurrently, passing them as a tuple to a single invocation ofmake_graphed_callables()is not allowed. Instead, you must callmake_graphed_callables()separately for each one.\nmake_graphed_callables()\nmake_graphed_callables()",
  "url": "https://pytorch.org/docs/stable/notes/cuda.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
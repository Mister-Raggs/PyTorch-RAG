{
  "doc_id": "9f00baa79c2a78499b190eac60cba802",
  "source": "pytorch_docs",
  "title": "TorchInductor GPU Profiling \u2014 PyTorch 2.9 documentation",
  "text": "\n## TorchInductor GPU Profiling#\n\nCreated On: Jul 28, 2023 | Last Updated On: Jun 10, 2025\nThis section lists useful commands and workflows that can help\nyou dive into a model\u2019s performance in TorchInductor. When a model is not\nrunning as fast as expected, you may want to check individual kernels of the\nmodel. Usually, those kernels taking the majority of the\nGPU time are the most interesting ones. After that, you\nmay also want to run individual kernels directly and inspect its perf.\nPyTorch provides tools to cover everything mentioned above.\n\n## Relevant Environment Variables#\n\nYou can use the following environment variables in your analysis:\nTORCHINDUCTOR_UNIQUE_KERNEL_NAMES\nTORCHINDUCTOR_UNIQUE_KERNEL_NAMES\nBy default, TorchInductor names a Triton kernel as\u2018triton\\_\u2019. When\nthis environmental variable is enabled, inductor generates a more\nmeaningful kernel name in the trace, for example,triton_poi_fused_cat_155which contains the kernel category\n(poifor pointwise) and original ATen\noperator. This config is disabled by default to improve the chance of\ncompilation cache hit.\n\u2018triton\\_\u2019\ntriton_poi_fused_cat_155\npoi\nTORCHINDUCTOR_BENCHMARK_KERNEL\nTORCHINDUCTOR_BENCHMARK_KERNEL\nEnabling this will make inductor codegen harness to benchmark\nindividual triton kernels.\nTORCHINDUCTOR_MAX_AUTOTUNE\nTORCHINDUCTOR_MAX_AUTOTUNE\nInductor autotuner will benchmark moretriton.Configsand pick the\none with the best performance results. This will increase compilation\ntime with the hope to improve performance.\ntriton.Configs\n\n## Breakdown Model GPU Time#\n\nBelow are the steps to breakdown execution time of a model into\nindividual kernels. We takemixnet_las an example.\nmixnet_l\nRun the benchmark script for the model:\n\n```python\n   TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1\n   python -u benchmarks/dynamo/timm_models.py \u2013backend inductor \u2013amp\n   \u2013performance \u2013dashboard \u2013only mixnet_l \u2013disable-cudagraphs \u2013training\n\n```\n\nNote\nThe tool relies on kernel name to decide its category. EnablingTORCHINDUCTOR_UNIQUE_KERNEL_NAMESis crucial for that.\nTORCHINDUCTOR_UNIQUE_KERNEL_NAMES\nIn the output log, look for lines:\n\n```python\n   **Compiled module path:\n   /tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py**\n\n```\n\nWe have one line for each compiled module. If there are no extra graph\nbreaks, we would see 2 such lines in the log, one for the forward graph\nand one for the backward graph.\nFor our example command, we get the following compiled module for the\nforward and backward graphs respectively:\nForward graph compiled module\nBackward graph compiled module\nNow we can dive into the perf for each individual compiled module.\nLet\u2019s pick the one for the forward graph for illustration purposes.\nI\u2019ll name itfwd.pyfor convenience. Run it directly with the-pargument:\nfwd.py\n-p\n\n```python\n   **> python fwd.py -p**\n\n```\n\nSee the full output log in thisexample gist\nIn the output, you can notice the following:\nWe write a chrome trace file for the profile so we can load the trace and interact with it. In the log, look for lines as follows to find the path of the trace file.\nChrome trace for the profile is written to /tmp/compiled_module_profile.json\nLoading the trace into Chrome (visit chrome://tracing in the chrome browser and load the file as the UI suggested) will show UI as follows:\nYou can zoom in and out to check the profile.\nWe report the percent of GPU time regarding to the wall time by log line like:\nPercent of time when GPU is busy: 102.88%\nSometimes you may see a value larger than 100%. The reason is because PyTorch\nuses the kernel execution time with profiling enabled while using wall time\nwith profiling disabled. Profiling may distort the kernel execution time a\nbit. But overall it should not be a big deal.\nIf we run the model likedensenet121with a small batch size, we would see\nlow percent of time when GPU is busy:\ndensenet121\n\n```python\n  (Forward graph) Percent of time when GPU is busy: 32.69%\n\n```\n\nThis means the model has a lot of CPU overhead. This is consistent with\nthe fact that enabling cudagraphs improve densenet121\u2019s perf a lot.\nWe can break down the GPU time to different categories of kernels.\nIn themixnet_lexample, we see\nmixnet_l\npointwise kernel takes 28.58%\nreduction kernel takes 13.85%\npersistent reduction kernel takes 3.89%\nthe rest are cutlass/cudnn kernels for mm/conv which takes 56.57%\nThis information can be found in the summary line (last line)\nof the report for each kernel category.\nWe also call zoom into a certain category of kernels. For example,\nlet\u2019s check reduction kernels:\nWe can see an ordered table of execution time for each individual\nreduction kernel. We also see how many times a kernel is executed. This\nis helpful for a few reasons:\nIf a kernel only takes a tiny amount of time, for example, 0.1%,\nimproving it will at most bring 0.1% overall gain. It is not\nworth spending a lot of effort on it.\nFf a kernel takes 2% of time, improving it by 2x will bring in 1%\noverall gain which justifies the effort.\n\n## Benchmark Individual Triton Kernel#\n\nLet\u2019s say we want to take a closer look attriton_red_fused\\__native_batch_norm_legit_functional_16which is the\nmost expensive reduction kernel and takes 2.19% of overall wall time for\nthe forward graph.\ntriton_red_fused\\__native_batch_norm_legit_functional_16\nWe can lookup the kernel name in thefwd.py, and find comment like:\nfwd.py\n# kernel path:\n/tmp/torchinductor_shunting/jk/cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py\nI\u2019ll rename it k.py for convenience. Here is a paste for thisfile.\nk.pyis a standalone Python module containing the kernel code and its\nbenchmark.\nk.py\nRunk.pydirectly will report its execution time and bandwidth:\nk.py\nWe can check if max-autotune helps this kernel, by running:\n\n```python\n   **TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py**\n\n```\n\nWe may also temporarily add more reduction heuristics and run the script\nagain to check how that helps with the kernel.",
  "url": "https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
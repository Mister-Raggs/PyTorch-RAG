{
  "doc_id": "b76dd3c092a1610184738f2c093075fb",
  "source": "pytorch_docs",
  "title": "torch.compile has different autograd semantics \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.compilehas different autograd semantics#\n\ntorch.compile\nCreated On: Jun 26, 2025 | Last Updated On: Jun 26, 2025\nWhen you applytorch.compileto a function in your model\u2019s forward pass,\nit will automatically generate a backward pass for the compiled function.\nDuring compilation, it will trace out a graph for the backward pass that\nis used whenever autograd is invoked. We refer to the component insidetorch.compilethat is responsible for this asAOTDispatcher(sometimes known asAOTAutograd).\ntorch.compile\ntorch.compile\nAOTDispatcher\nAOTAutograd\nAs so,torch.compilebakes in details of the computation into the\ntraced-out backward graph during compilation of the function\nin the forward pass.\nHowever, in eager-mode PyTorch, the backward computation is dynamic:\noutside of the forward pass, you can wrap the call totensor.backward()ortorch.autograd.grad(...)in a context manager that may change its behavior.\ntorch.compile\ntensor.backward()\ntorch.autograd.grad(...)\nThis page documents howtorch.compile\u2019s autograd semantics differ from\neager-mode PyTorch and how to work around it.\ntorch.compile\n\n## Autocastbehavior#\n\nAutocast\ntorch.compilebakes in an assumption on if the backward pass will be\nrun under an ambient autocast context manager. By default,\nUsetorch._functorch.config.backward_pass_autocastto control that assumption; an incorrect assumption may lead to silent\nincorrectness.\ntorch.compile\ntorch._functorch.config.backward_pass_autocast\nThe options are either:\n\"same_as_forward\"(default).\nWe assume that the backward of thetorch.compile\u2019ed region\nwill be run under the same autocast context manager that the region was run\nunder (if any). Use this if your code looks like the following:\n\"same_as_forward\"\ntorch.compile\n\n```python\nwith torch.amp.autocast(...):\n    y = torch.compile(region)(x)\n    ...\n    # backward pass run under the same autocast context as the compiled region\n    z.backward()\n\n```\n\n\"off\". We assume that the backward of the torch.compile\u2019d region will\nnot be run under any autocast context managers.\nUse this if your code looks like the following:\n\"off\"\n\n```python\nwith torch.amp.autocast(...):\n    y = torch.compile(region)(x)\n    ...\n# Backward pass runs under no autocast.\nz.backward()\n\n```\n\nThere is a third option. If you settorch._functorch.config.backward_pass_autocastto a list of kwargs, we will assume the backward pass runs under an autocast context\nconstructed by those kwargs.\ntorch._functorch.config.backward_pass_autocast\nFor example, if your code looks like the following:\n\n```python\ny = torch.compile(region)(x)\n...\n# Backward pass runs under special context manager\nwith torch.amp.autocast(**kwargs):\n    z.backward()\n\n```\n\nthen settorch._functorch.config.backward_pass_autocast=kwargs.\ntorch._functorch.config.backward_pass_autocast=kwargs\nUsepatchto apply the option to a specifictorch.compilecall:\npatch\ntorch.compile\n\n```python\nwith torch.amp.autocast(...):\n    with torch._functorch.config.patch(backward_pass_autocast=\"same_as_forward\")\n    y = torch.compile(region)(x)\n    ...\n    # backward pass run under the same autocast context as the compiled region\n    z.backward()\n\n```\n",
  "url": "https://pytorch.org/docs/stable/torch.compiler_backward.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
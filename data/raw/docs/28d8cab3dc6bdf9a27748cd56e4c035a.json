{
  "doc_id": "28d8cab3dc6bdf9a27748cd56e4c035a",
  "source": "pytorch_docs",
  "title": "torch._logging \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch._logging#\n\nCreated On: Apr 24, 2023 | Last Updated On: Jun 17, 2025\nPyTorch has a configurable logging system, where different components can be\ngiven different log level settings. For instance, one component\u2019s log messages\ncan be completely disabled, while another component\u2019s log messages can be\nset to maximum verbosity.\nWarning\nThis feature is in beta and may have compatibility breaking\nchanges in the future.\nWarning\nThis feature has not been expanded to control the log messages of\nall components in PyTorch yet.\nThere are two ways to configure the logging system: through the environment variableTORCH_LOGSor the python API torch._logging.set_logs.\nTORCH_LOGS\nset_logs\n\nset_logs\nSets the log level for individual components and toggles individual log artifact types.\nThe environment variableTORCH_LOGSis a comma-separated list of[+-]<component>pairs, where<component>is a component specified below. The+prefix\nwill decrease the log level of the component, displaying more log messages while the-prefix\nwill increase the log level of the component and display fewer log messages. The default setting\nis the behavior when a component is not specified inTORCH_LOGS. In addition to components, there are\nalso artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed,\nso prefixing an artifact with+or-will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact,\nunless that artifact was specified to beoff_by_default. This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled.\nThe following components and artifacts are configurable through theTORCH_LOGSenvironment\nvariable (see torch._logging.set_logs for the python API):\nTORCH_LOGS\n[+-]<component>\n<component>\n+\n-\nTORCH_LOGS\n+\n-\noff_by_default\nTORCH_LOGS\nall\nSpecial component which configures the default log level of all components. Default:logging.WARN\nlogging.WARN\ndynamo\nThe log level for the TorchDynamo component. Default:logging.WARN\nlogging.WARN\naot\nThe log level for the AOTAutograd component. Default:logging.WARN\nlogging.WARN\ninductor\nThe log level for the TorchInductor component. Default:logging.WARN\nlogging.WARN\nyour.custom.module\nThe log level for an arbitrary unregistered module. Provide the fully qualified name and the module will be enabled. Default:logging.WARN\nlogging.WARN\nbytecode\nWhether to emit the original and generated bytecode from TorchDynamo.\nDefault:False\nFalse\naot_graphs\nWhether to emit the graphs generated by AOTAutograd. Default:False\nFalse\naot_joint_graph\nWhether to emit the joint forward-backward graph generated by AOTAutograd. Default:False\nFalse\ncompiled_autograd\nWhether to emit logs from compiled_autograd. Defaults:False\nFalse\nddp_graphs\nWhether to emit graphs generated by DDPOptimizer. Default:False\nFalse\ngraph\nWhether to emit the graph captured by TorchDynamo in tabular format.\nDefault:False\nFalse\ngraph_code\nWhether to emit the python source of the graph captured by TorchDynamo.\nDefault:False\nFalse\ngraph_breaks\nWhether to emit a message when a unique graph break is encountered during\nTorchDynamo tracing. Default:False\nFalse\nguards\nWhether to emit the guards generated by TorchDynamo for each compiled\nfunction. Default:False\nFalse\nrecompiles\nWhether to emit a guard failure reason and message every time\nTorchDynamo recompiles a function. Default:False\nFalse\noutput_code\nWhether to emit the TorchInductor output code. Default:False\nFalse\nschedule\nWhether to emit the TorchInductor schedule. Default:False\nFalse\nTORCH_LOGS=\"+dynamo,aot\"will set the log level of TorchDynamo tologging.DEBUGand  AOT tologging.INFO\nTORCH_LOGS=\"+dynamo,aot\"\nlogging.DEBUG\nlogging.INFO\nTORCH_LOGS=\"-dynamo,+inductor\"will set the log level of TorchDynamo tologging.ERRORand  TorchInductor tologging.DEBUG\nTORCH_LOGS=\"-dynamo,+inductor\"\nlogging.ERROR\nlogging.DEBUG\nTORCH_LOGS=\"aot_graphs\"will enable theaot_graphsartifact\nTORCH_LOGS=\"aot_graphs\"\naot_graphs\nTORCH_LOGS=\"+dynamo,schedule\"will enable set the log level of TorchDynamo tologging.DEBUGand enable thescheduleartifact\nTORCH_LOGS=\"+dynamo,schedule\"\nlogging.DEBUG\nschedule\nTORCH_LOGS=\"+some.random.module,schedule\"will set the log level of some.random.module tologging.DEBUGand enable thescheduleartifact\nTORCH_LOGS=\"+some.random.module,schedule\"\nlogging.DEBUG\nschedule",
  "url": "https://pytorch.org/docs/stable/logging.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
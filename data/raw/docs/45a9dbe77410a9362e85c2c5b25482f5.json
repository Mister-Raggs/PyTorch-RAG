{
  "doc_id": "45a9dbe77410a9362e85c2c5b25482f5",
  "source": "pytorch_docs",
  "title": "python.data-structure \u2014 PyTorch 2.9 documentation",
  "text": "\n## python.data-structure#\n\n\n## dictionary#\n\nNote\nTags:python.data-structure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass Dictionary(torch.nn.Module):\n    \"\"\"\n    Dictionary structures are inlined and flattened along tracing.\n    \"\"\"\n\n    def forward(self, x, y):\n        elements = {}\n        elements[\"x2\"] = x * x\n        y = y * elements[\"x2\"]\n        return {\"y\": y}\n\nexample_args = (torch.randn(3, 2), torch.tensor(4))\ntags = {\"python.data-structure\"}\nmodel = Dictionary()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"):\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, x);  x = None\n\n                 mul_1: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(y, mul);  y = mul = None\n            return (mul_1,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    mul_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## fn_with_kwargs#\n\nNote\nTags:python.data-structure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass FnWithKwargs(torch.nn.Module):\n    \"\"\"\n    Keyword arguments are not supported at the moment.\n    \"\"\"\n\n    def forward(self, pos0, tuple0, *myargs, mykw0, **mykwargs):\n        out = pos0\n        for arg in tuple0:\n            out = out * arg\n        for arg in myargs:\n            out = out * arg\n        out = out * mykw0\n        out = out * mykwargs[\"input0\"] * mykwargs[\"input1\"]\n        return out\n\nexample_args = (\n    torch.randn(4),\n    (torch.randn(4), torch.randn(4)),\n    *[torch.randn(4), torch.randn(4)]\n)\nexample_kwargs = {\n    \"mykw0\": torch.randn(4),\n    \"input0\": torch.randn(4),\n    \"input1\": torch.randn(4),\n}\ntags = {\"python.data-structure\"}\nmodel = FnWithKwargs()\n\n\ntorch.export.export(model, example_args, example_kwargs)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pos0: \"f32[4]\", tuple0_0: \"f32[4]\", tuple0_1: \"f32[4]\", myargs_0: \"f32[4]\", myargs_1: \"f32[4]\", mykw0: \"f32[4]\", input0: \"f32[4]\", input1: \"f32[4]\"):\n                 mul: \"f32[4]\" = torch.ops.aten.mul.Tensor(pos0, tuple0_0);  pos0 = tuple0_0 = None\n            mul_1: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul, tuple0_1);  mul = tuple0_1 = None\n\n                 mul_2: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_1, myargs_0);  mul_1 = myargs_0 = None\n            mul_3: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_2, myargs_1);  mul_2 = myargs_1 = None\n\n                 mul_4: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_3, mykw0);  mul_3 = mykw0 = None\n\n                 mul_5: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_4, input0);  mul_4 = input0 = None\n            mul_6: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_5, input1);  mul_5 = input1 = None\n            return (mul_6,)\n\nGraph signature:\n    # inputs\n    pos0: USER_INPUT\n    tuple0_0: USER_INPUT\n    tuple0_1: USER_INPUT\n    myargs_0: USER_INPUT\n    myargs_1: USER_INPUT\n    mykw0: USER_INPUT\n    input0: USER_INPUT\n    input1: USER_INPUT\n\n    # outputs\n    mul_6: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_contains#\n\nNote\nTags:python.data-structure,torch.dynamic-shape,python.assert\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass ListContains(torch.nn.Module):\n    \"\"\"\n    List containment relation can be checked on a dynamic shape or constants.\n    \"\"\"\n\n    def forward(self, x):\n        assert x.size(-1) in [6, 2]\n        assert x.size(0) not in [4, 5, 6]\n        assert \"monkey\" not in [\"cow\", \"pig\"]\n        return x + x\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"}\nmodel = ListContains()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_unpack#\n\nNote\nTags:python.data-structure,python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\n\nimport torch\n\nclass ListUnpack(torch.nn.Module):\n    \"\"\"\n    Lists are treated as static construct, therefore unpacking should be\n    erased after tracing.\n    \"\"\"\n\n    def forward(self, args: list[torch.Tensor]):\n        \"\"\"\n        Lists are treated as static construct, therefore unpacking should be\n        erased after tracing.\n        \"\"\"\n        x, *y = args\n        return x + y[0]\n\nexample_args = ([torch.randn(3, 2), torch.tensor(4), torch.tensor(5)],)\ntags = {\"python.control-flow\", \"python.data-structure\"}\nmodel = ListUnpack()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, args_0: \"f32[3, 2]\", args_1: \"i64[]\", args_2: \"i64[]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(args_0, args_1);  args_0 = args_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    args_0: USER_INPUT\n    args_1: USER_INPUT\n    args_2: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
  "url": "https://pytorch.org/docs/stable/generated/exportdb/python.data-structure.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
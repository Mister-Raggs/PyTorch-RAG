{
  "doc_id": "e78c1463a0f331307845fa48a840bd83",
  "source": "pytorch_docs",
  "title": "torch.func Whirlwind Tour \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.func Whirlwind Tour#\n\nCreated On: Jun 12, 2025 | Last Updated On: Jun 12, 2025\n\n## What is torch.func?#\n\ntorch.func, previously known as functorch, is a library forJAX-like composable function transforms in\nPyTorch.\nA \u201cfunction transform\u201d is a higher-order function that accepts a numerical\nfunction and returns a new function that computes a different quantity.\ntorch.func has auto-differentiation transforms (grad(f)returns a function\nthat computes the gradient off), a vectorization/batching transform\n(vmap(f)returns a function that computesfover batches of inputs),\nand others.\ngrad(f)\nf\nvmap(f)\nf\nThese function transforms can compose with each other arbitrarily. For\nexample, composingvmap(grad(f))computes a quantity called\nper-sample-gradients that stock PyTorch cannot efficiently compute today.\nvmap(grad(f))\n\n## Why composable function transforms?#\n\nThere are a number of use cases that are tricky to do in PyTorch today:\ncomputing per-sample-gradients (or other per-sample quantities)\nrunning ensembles of models on a single machine\nefficiently batching together tasks in the inner-loop of MAML\nefficiently computing Jacobians and Hessians\nefficiently computing batched Jacobians and Hessians\nComposingvmap(),grad(),vjp(), andjvp()transforms\nallows us to express the above without designing a separate subsystem for each.\nvmap()\ngrad()\nvjp()\njvp()\n\n## What are the transforms?#\n\n\n## grad()(gradient computation)#\n\ngrad()\ngrad(func)is our gradient computation transform. It returns a new function\nthat computes the gradients offunc. It assumesfuncreturns a single-element\nTensor and by default it computes the gradients of the output offuncw.r.t.\nto the first input.\ngrad(func)\nfunc\nfunc\nfunc\n\n```python\nimport torch\nfrom torch.func import grad\nx = torch.randn([])\ncos_x = grad(lambda x: torch.sin(x))(x)\nassert torch.allclose(cos_x, x.cos())\n\n# Second-order gradients\nneg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)\nassert torch.allclose(neg_sin_x, -x.sin())\n\n```\n\n\n## vmap()(auto-vectorization)#\n\nvmap()\nNote:vmap()imposes restrictions on the code that it can be used on. For more\ndetails, please seeUX Limitations.\nvmap()\nvmap(func)(*inputs)is a transform that adds a dimension to all Tensor\noperations infunc.vmap(func)returns a new function that mapsfuncover some dimension (default: 0) of each Tensor in inputs.\nvmap(func)(*inputs)\nfunc\nvmap(func)\nfunc\nvmap is useful for hiding batch dimensions: one can write a function func that\nruns on examples and then lift it to a function that can take batches of\nexamples withvmap(func), leading to a simpler modeling experience:\nvmap(func)\n\n```python\nimport torch\nfrom torch.func import vmap\nbatch_size, feature_size = 3, 5\nweights = torch.randn(feature_size, requires_grad=True)\n\ndef model(feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\nexamples = torch.randn(batch_size, feature_size)\nresult = vmap(model)(examples)\n\n```\n\nWhen composed withgrad(),vmap()can be used to compute per-sample-gradients:\ngrad()\nvmap()\n\n```python\nfrom torch.func import vmap\nbatch_size, feature_size = 3, 5\n\ndef model(weights,feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\ndef compute_loss(weights, example, target):\n    y = model(weights, example)\n    return ((y - target) ** 2).mean()  # MSELoss\n\nweights = torch.randn(feature_size, requires_grad=True)\nexamples = torch.randn(batch_size, feature_size)\ntargets = torch.randn(batch_size)\ninputs = (weights,examples, targets)\ngrad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n\n```\n\n\n## vjp()(vector-Jacobian product)#\n\nvjp()\nThevjp()transform appliesfunctoinputsand returns a new function\nthat computes the vector-Jacobian product (vjp) given somecotangentsTensors.\nvjp()\nfunc\ninputs\ncotangents\n\n```python\nfrom torch.func import vjp\n\ninputs = torch.randn(3)\nfunc = torch.sin\ncotangents = (torch.randn(3),)\n\noutputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)\n\n```\n\n\n## jvp()(Jacobian-vector product)#\n\njvp()\nThejvp()transforms computes Jacobian-vector-products and is also known as\n\u201cforward-mode AD\u201d. It is not a higher-order function unlike most other transforms,\nbut it returns the outputs offunc(inputs)as well as the jvps.\njvp()\nfunc(inputs)\n\n```python\nfrom torch.func import jvp\nx = torch.randn(5)\ny = torch.randn(5)\nf = lambda x, y: (x * y)\n_, out_tangent = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))\nassert torch.allclose(out_tangent, x + y)\n\n```\n\n\n## jacrev(),jacfwd(), andhessian()#\n\njacrev()\njacfwd()\nhessian()\nThejacrev()transform returns a new function that takes inxand returns\nthe Jacobian of the function with respect toxusing reverse-mode AD.\njacrev()\nx\nx\n\n```python\nfrom torch.func import jacrev\nx = torch.randn(5)\njacobian = jacrev(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n\n```\n\njacrev()can be composed withvmap()to produce batched jacobians:\njacrev()\nvmap()\n\n```python\nx = torch.randn(64, 5)\njacobian = vmap(jacrev(torch.sin))(x)\nassert jacobian.shape == (64, 5, 5)\n\n```\n\njacfwd()is a drop-in replacement for jacrev that computes Jacobians using\nforward-mode AD:\njacfwd()\n\n```python\nfrom torch.func import jacfwd\nx = torch.randn(5)\njacobian = jacfwd(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n\n```\n\nComposingjacrev()with itself orjacfwd()can produce hessians:\njacrev()\njacfwd()\n\n```python\ndef f(x):\n    return x.sin().sum()\n\nx = torch.randn(5)\nhessian0 = jacrev(jacrev(f))(x)\nhessian1 = jacfwd(jacrev(f))(x)\n\n```\n\nhessian()is a convenience function that combines jacfwd and jacrev:\nhessian()\n\n```python\nfrom torch.func import hessian\n\ndef f(x):\n    return x.sin().sum()\n\nx = torch.randn(5)\nhess = hessian(f)(x)\n\n```\n",
  "url": "https://pytorch.org/docs/stable/func.whirlwind_tour.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
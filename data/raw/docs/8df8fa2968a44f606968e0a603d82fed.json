{
  "doc_id": "8df8fa2968a44f606968e0a603d82fed",
  "source": "pytorch_docs",
  "title": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models \u2014 PyTorch 2.9 documentation",
  "text": "\n## AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models#\n\nCreated On: Jun 13, 2025 | Last Updated On: Aug 14, 2025\nWarning\nAOTInductor and its related features are in prototype status and are\nsubject to backwards compatibility breaking changes.\nAOTInductor is a specialized version ofTorchInductor,\ndesigned to process exported PyTorch models, optimize them, and produce shared libraries as well\nas other relevant artifacts.\nThese compiled artifacts are specifically crafted for deployment in non-Python environments,\nwhich are frequently employed for inference deployments on the server side.\nIn this tutorial, you will gain insight into the process of taking a PyTorch model, exporting it,\ncompiling it into an artifact, and conducting model predictions using C++.\n\n## Model Compilation#\n\nTo compile a model using AOTInductor, we first need to usetorch.export.export()to capture a given PyTorch model into a\ncomputational graph.torch.exportprovides soundness\nguarantees and a strict specification on the IR captured, which AOTInductor\nrelies on.\ntorch.export.export()\nWe will then usetorch._inductor.aoti_compile_and_package()to compile the\nexported program using TorchInductor, and save the compiled artifacts into one\npackage. The package is in the format of aPT2 Archive Spec.\ntorch._inductor.aoti_compile_and_package()\nNote\nIf you have a CUDA-enabled device on your machine and you installed PyTorch with CUDA support,\nthe following code will compile the model into a shared library for CUDA execution.\nOtherwise, the compiled artifact will run on CPU. For better performance during CPU inference,\nit is suggested to enable freezing by settingexportTORCHINDUCTOR_FREEZING=1before running the Python script below. The same behavior works in an environment with Intel\u00ae\nGPU as well.\nexportTORCHINDUCTOR_FREEZING=1\n\n```python\nimport os\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 16)\n        self.relu = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(16, 1)\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\nwith torch.no_grad():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = Model().to(device=device)\n    example_inputs=(torch.randn(8, 10, device=device),)\n    batch_dim = torch.export.Dim(\"batch\", min=1, max=1024)\n    # [Optional] Specify the first dimension of the input x as dynamic.\n    exported = torch.export.export(model, example_inputs, dynamic_shapes={\"x\": {0: batch_dim}})\n    # [Note] In this example we directly feed the exported module to aoti_compile_and_package.\n    # Depending on your use case, e.g. if your training platform and inference platform\n    # are different, you may choose to save the exported model using torch.export.save and\n    # then load it back using torch.export.load on your inference platform to run AOT compilation.\n    output_path = torch._inductor.aoti_compile_and_package(\n        exported,\n        # [Optional] Specify the generated shared library path. If not specified,\n        # the generated artifact is stored in your system temp directory.\n        package_path=os.path.join(os.getcwd(), \"model.pt2\"),\n    )\n\n```\n\nIn this illustrative example, theDimparameter is employed to designate the first dimension of\nthe input variable \u201cx\u201d as dynamic. Notably, the path and name of the compiled library remain unspecified,\nresulting in the shared library being stored in a temporary directory.\nTo access this path from the C++ side, we save it to a file for later retrieval within the C++ code.\nDim\n\n## Inference in Python#\n\nThere are multiple ways to deploy the compiled artifact for inference, and one of that is using Python.\nWe have provided a convenient utility API in Pythontorch._inductor.aoti_load_package()for loading\nand running the artifact, as shown in the following example:\ntorch._inductor.aoti_load_package()\n\n```python\nimport os\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = torch._inductor.aoti_load_package(os.path.join(os.getcwd(), \"model.pt2\"))\nprint(model(torch.randn(8, 10, device=device)))\n\n```\n\nThe input at inference time should have the same size, dtype, and stride as the input at export time.\n\n## Inference in C++#\n\nNext, we use the following example C++ fileinference.cppto load the compiled artifact,\nenabling us to conduct model predictions directly within a C++ environment.\ninference.cpp\n\n```python\n#include <iostream>\n#include <vector>\n\n#include <torch/torch.h>\n#include <torch/csrc/inductor/aoti_package/model_package_loader.h>\n\nint main() {\n    c10::InferenceMode mode;\n\n    torch::inductor::AOTIModelPackageLoader loader(\"model.pt2\");\n    // Assume running on CUDA\n    std::vector<torch::Tensor> inputs = {torch::randn({8, 10}, at::kCUDA)};\n    std::vector<torch::Tensor> outputs = loader.run(inputs);\n    std::cout << \"Result from the first inference:\"<< std::endl;\n    std::cout << outputs[0] << std::endl;\n\n    // The second inference uses a different batch size and it works because we\n    // specified that dimension as dynamic when compiling model.pt2.\n    std::cout << \"Result from the second inference:\"<< std::endl;\n    // Assume running on CUDA\n    std::cout << loader.run({torch::randn({1, 10}, at::kCUDA)})[0] << std::endl;\n\n    return 0;\n}\n\n```\n\nFor building the C++ file, you can make use of the providedCMakeLists.txtfile, which\nautomates the process of invokingpythonmodel.pyfor AOT compilation of the model and compilinginference.cppinto an executable binary namedaoti_example.\nCMakeLists.txt\npythonmodel.py\ninference.cpp\naoti_example\n\n```python\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\nproject(aoti_example)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(aoti_example inference.cpp model.pt2)\n\nadd_custom_command(\n    OUTPUT model.pt2\n    COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/model.py\n    DEPENDS model.py\n)\n\ntarget_link_libraries(aoti_example \"${TORCH_LIBRARIES}\")\nset_property(TARGET aoti_example PROPERTY CXX_STANDARD 17)\n\n```\n\nProvided the directory structure resembles the following, you can execute the subsequent commands\nto construct the binary. It is essential to note that theCMAKE_PREFIX_PATHvariable\nis crucial for CMake to locate the LibTorch library, and it should be set to an absolute path.\nPlease be mindful that your path may vary from the one illustrated in this example.\nCMAKE_PREFIX_PATH\n\n```python\naoti_example/\n    CMakeLists.txt\n    inference.cpp\n    model.py\n\n```\n\n\n```python\n$ mkdir build\n$ cd build\n$ CMAKE_PREFIX_PATH=/path/to/python/install/site-packages/torch/share/cmake cmake ..\n$ cmake --build . --config Release\n\n```\n\nAfter theaoti_examplebinary has been generated in thebuilddirectory, executing it will\ndisplay results akin to the following:\naoti_example\nbuild\n\n```python\n$ ./aoti_example\nResult from the first inference:\n0.4866\n0.5184\n0.4462\n0.4611\n0.4744\n0.4811\n0.4938\n0.4193\n[ CUDAFloatType{8,1} ]\nResult from the second inference:\n0.4883\n0.4703\n[ CUDAFloatType{2,1} ]\n\n```\n\n\n## Troubleshooting#\n\nBelow are some useful tools for debugging AOT Inductor.\nDebugging Tools\nTo enable runtime checks on inputs, set the environment variableAOTI_RUNTIME_CHECK_INPUTSto 1. This will raise aRuntimeErrorif the inputs to the compiled model differ in size, data type, or strides from those used during export.\nAOTI_RUNTIME_CHECK_INPUTS\nRuntimeError\n\n## API Reference#\n\nCompiles the exported program with AOTInductor, and packages it into a .pt2\nartifact specified by the input package_path. To load the package, you can\ncalltorch._inductor.aoti_load_package(package_path).\ntorch._inductor.aoti_load_package(package_path)\nAn example usage is as follows:\n\n```python\nep = torch.export.export(M(), ...)\naoti_file = torch._inductor.aoti_compile_and_package(\n    ep, package_path=\"my_package.pt2\"\n)\ncompiled_model = torch._inductor.aoti_load_package(\"my_package.pt2\")\n\n```\n\nTo compile and save multiple models into a single.pt2artifact, you can do\nthe following:\n.pt2\n\n```python\nep1 = torch.export.export(M1(), ...)\naoti_file1 = torch._inductor.aot_compile(\n    ep1, ..., options={\"aot_inductor.package\": True}\n)\nep2 = torch.export.export(M2(), ...)\naoti_file2 = torch._inductor.aot_compile(\n    ep2, ..., options={\"aot_inductor.package\": True}\n)\n\nfrom torch._inductor.package import package_aoti, load_package\n\npackage_aoti(\"my_package.pt2\", {\"model1\": aoti_file1, \"model2\": aoti_file2})\n\ncompiled_model1 = load_package(\"my_package.pt2\", \"model1\")\ncompiled_model2 = load_package(\"my_package.pt2\", \"model2\")\n\n```\n\nexported_program(ExportedProgram) \u2013 An exported program created through a call from torch.export\npackage_path(Optional[FileLike]) \u2013 Optional specified path to the generated .pt2 artifact.\ninductor_configs(Optional[dict[str,Any]]) \u2013 Optional dictionary of configs to control inductor.\nPath to the generated artifact\nstr\nLoads the model from the PT2 package.\nIf multiple models were packaged into the PT2, this will load the default\nmodel. To load a specific model, you can directly call the load API\n\n```python\nfrom torch._inductor.package import load_package\n\ncompiled_model1 = load_package(\"my_package.pt2\", \"model1\")\ncompiled_model2 = load_package(\"my_package.pt2\", \"model2\")\n\n```\n\npath(FileLike) \u2013 Path to the .pt2 package\nrun_single_threaded(bool) \u2013 Whether the model should be run without\nthread synchronization logic. This is useful to avoid conflicts with\nCUDAGraphs.\ndevice_index(int) \u2013 The index of the device to which the PT2 package is\nto be loaded. By default,device_index=-1is used, which corresponds\nto the devicecudawhen using CUDA. Passingdevice_index=1would\nload the package tocuda:1, for example.\nAOTICompiledModel",
  "url": "https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
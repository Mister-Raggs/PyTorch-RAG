{
  "doc_id": "cbdcf55def4ee4991519f7b13b42fbb4",
  "source": "pytorch_docs",
  "title": "Serialization semantics \u2014 PyTorch 2.9 documentation",
  "text": "\n## Serialization semantics#\n\nCreated On: Feb 26, 2017 | Last Updated On: Jun 23, 2025\nThis note describes how you can save and load PyTorch tensors and module states\nin Python, and how to serialize Python modules so they can be loaded in C++.\nTable of Contents\nSerialization semantics\nSaving and loading tensors\nSaving and loading tensors preserves views\nSaving and loading torch.nn.Modules\nSerialized file format fortorch.save\ntorch.save\nLayout Control\ntorch.loadwithweights_only=True\ntorch.load\nweights_only=True\nTroubleshootingweights_only\nweights_only\nGetting unsafe globals\nEnvironment Variables\nUtility functions\nConfig\n\n## Saving and loading tensors#\n\ntorch.save()andtorch.load()let you easily save and load tensors:\ntorch.save()\ntorch.load()\n\n```python\n>>> t = torch.tensor([1., 2.])\n>>> torch.save(t, 'tensor.pt')\n>>> torch.load('tensor.pt')\ntensor([1., 2.])\n\n```\n\nBy convention, PyTorch files are typically written with a \u2018.pt\u2019 or \u2018.pth\u2019 extension.\ntorch.save()andtorch.load()use Python\u2019s pickle by default,\nso you can also save multiple tensors as part of Python objects like tuples,\nlists, and dicts:\ntorch.save()\ntorch.load()\n\n```python\n>>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}\n>>> torch.save(d, 'tensor_dict.pt')\n>>> torch.load('tensor_dict.pt')\n{'a': tensor([1., 2.]), 'b': tensor([3., 4.])}\n\n```\n\nCustom data structures that include PyTorch tensors can also be saved if the\ndata structure is pickle-able.\n\n## Saving and loading tensors preserves views#\n\nSaving tensors preserves their view relationships:\n\n```python\n>>> numbers = torch.arange(1, 10)\n>>> evens = numbers[1::2]\n>>> torch.save([numbers, evens], 'tensors.pt')\n>>> loaded_numbers, loaded_evens = torch.load('tensors.pt')\n>>> loaded_evens *= 2\n>>> loaded_numbers\ntensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])\n\n```\n\nBehind the scenes, these tensors share the same \u201cstorage.\u201d SeeTensor Viewsfor more\non views and storage.\nWhen PyTorch saves tensors it saves their storage objects and tensor\nmetadata separately. This is an implementation detail that may change in the\nfuture, but it typically saves space and lets PyTorch easily\nreconstruct the view relationships between the loaded tensors. In the above\nsnippet, for example, only a single storage is written to \u2018tensors.pt\u2019.\nIn some cases, however, saving the current storage objects may be unnecessary\nand create prohibitively large files. In the following snippet a storage much\nlarger than the saved tensor is written to a file:\n\n```python\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small, 'small.pt')\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n999\n\n```\n\nInstead of saving only the five values in thesmalltensor to \u2018small.pt,\u2019\nthe 999 values in the storage it shares withlargewere saved and loaded.\nWhen saving tensors with fewer elements than their storage objects, the size of\nthe saved file can be reduced by first cloning the tensors. Cloning a tensor\nproduces a new tensor with a new storage object containing only the values\nin the tensor:\n\n```python\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small.clone(), 'small.pt')  # saves a clone of small\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n5\n\n```\n\nSince the cloned tensors are independent of each other, however, they have\nnone of the view relationships the original tensors did. If both file size and\nview relationships are important when saving tensors smaller than their\nstorage objects, then care must be taken to construct new tensors that minimize\nthe size of their storage objects but still have the desired view relationships\nbefore saving.\n\n## Saving and loading torch.nn.Modules#\n\nSee also:Tutorial: Saving and loading modules\nIn PyTorch, a module\u2019s state is frequently serialized using a \u2018state dict.\u2019\nA module\u2019s state dict contains all of its parameters and persistent buffers:\n\n```python\n>>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> list(bn.named_parameters())\n[('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),\n ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]\n\n>>> list(bn.named_buffers())\n[('running_mean', tensor([0., 0., 0.])),\n ('running_var', tensor([1., 1., 1.])),\n ('num_batches_tracked', tensor(0))]\n\n>>> bn.state_dict()\nOrderedDict([('weight', tensor([1., 1., 1.])),\n             ('bias', tensor([0., 0., 0.])),\n             ('running_mean', tensor([0., 0., 0.])),\n             ('running_var', tensor([1., 1., 1.])),\n             ('num_batches_tracked', tensor(0))])\n\n```\n\nInstead of saving a module directly, for compatibility reasons it is recommended\nto instead save only its state dict. Python modules even have a function,load_state_dict(), to restore their states from a state dict:\nload_state_dict()\n\n```python\n>>> torch.save(bn.state_dict(), 'bn.pt')\n>>> bn_state_dict = torch.load('bn.pt')\n>>> new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> new_bn.load_state_dict(bn_state_dict)\n<All keys matched successfully>\n\n```\n\nNote that the state dict is first loaded from its file withtorch.load()and the state then restored withload_state_dict().\ntorch.load()\nload_state_dict()\nEven custom modules and modules containing other modules have state dicts and\ncan use this pattern:\n\n```python\n# A module with two linear layers\n>>> class MyModule(torch.nn.Module):\n      def __init__(self):\n        super().__init__()\n        self.l0 = torch.nn.Linear(4, 2)\n        self.l1 = torch.nn.Linear(2, 1)\n\n      def forward(self, input):\n        out0 = self.l0(input)\n        out0_relu = torch.nn.functional.relu(out0)\n        return self.l1(out0_relu)\n\n>>> m = MyModule()\n>>> m.state_dict()\nOrderedDict([('l0.weight', tensor([[ 0.1400, 0.4563, -0.0271, -0.4406],\n                                   [-0.3289, 0.2827, 0.4588, 0.2031]])),\n             ('l0.bias', tensor([ 0.0300, -0.1316])),\n             ('l1.weight', tensor([[0.6533, 0.3413]])),\n             ('l1.bias', tensor([-0.1112]))])\n\n>>> torch.save(m.state_dict(), 'mymodule.pt')\n>>> m_state_dict = torch.load('mymodule.pt')\n>>> new_m = MyModule()\n>>> new_m.load_state_dict(m_state_dict)\n<All keys matched successfully>\n\n```\n\n\n## Serialized file format fortorch.save#\n\ntorch.save\nSince PyTorch 1.6.0,torch.savedefaults to returning an uncompressed ZIP64\narchive unless the user sets_use_new_zipfile_serialization=False.\ntorch.save\n_use_new_zipfile_serialization=False\nIn this archive, the files are ordered as such\n\n```python\ncheckpoint.pth\n\u251c\u2500\u2500 data.pkl\n\u251c\u2500\u2500 byteorder  # added in PyTorch 2.1.0\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 0\n\u2502   \u251c\u2500\u2500 1\n\u2502   \u251c\u2500\u2500 2\n\u2502   \u2514\u2500\u2500 \u2026\n\u2514\u2500\u2500 version\n\n```\n\ndata.pklis the result of pickling the object passed totorch.saveexcludingtorch.Storageobjects that it contains\ndata.pkl\ntorch.save\ntorch.Storage\nbyteordercontains a string with thesys.byteorderwhen saving (\u201clittle\u201d or \u201cbig\u201d)\nbyteorder\nsys.byteorder\ndata/contains all the storages in the object, where each storage is a separate file\ndata/\nversioncontains a version number at save time that can be used at load time\nversion\nWhen saving, PyTorch will ensure that the local file header of each file is padded\nto an offset that is a multiple of 64 bytes, ensuring that the offset of each file\nis 64-byte aligned.\nNote\nTensors on certain devices such as XLA are serialized as pickled numpy arrays. As\nsuch, their storages are not serialized. In these casesdata/might not exist\nin the checkpoint.\ndata/\n\n## Layout Control#\n\nThemmapargument intorch.load()allows for lazy loading of tensor storages.\nmmap\ntorch.load()\nIn addition, there are some advanced features that allow for more fine-grained\ncontrol and manipulation of atorch.savecheckpoint.\ntorch.save\ntorch.serialization.skip_data\nSaving a checkpoint withtorch.savethat includes empty space for data bytes\nto be written later.\ntorch.save\nLoading a checkpoint withtorch.loadand filling in the data bytes of tensors later.\ntorch.load\nTo inspect tensor metadata in atorch.savecheckpoint without allocating memory for storage\ndata, usetorch.loadwithin theFakeTensorModecontext manager. On top of skipping loading\nstorage data similar toskip_dataabove, it additionally tags storages with their offset within\nthe checkpoint, enabling direct checkpoint manipulation.\ntorch.save\ntorch.load\nFakeTensorMode\nskip_data\n\n```python\nimport torch.nn as nn\nfrom torch._subclasses.fake_tensor import FakeTensorMode\n\nm = nn.Linear(10, 10)\ntorch.save(m.state_dict(), \"checkpoint.pt\")\n\nwith FakeTensorMode() as mode:\n    fake_sd = torch.load(\"checkpoint.pt\")\n\nfor k, v in fake_sd.items():\n    print(f\"key={k}, dtype={v.dtype}, shape={v.shape}, stride={v.stride()}, storage_offset={v.storage_offset()}\")\n    # offset of the storage in the checkpoint\n    print(f\"key={k}, checkpoint_offset={v.untyped_storage()._checkpoint_offset}\")\n\n```\n\nFor more information,this tutorialoffers a comprehensive example of using these features to manipulate a checkpoint.\n\n## torch.loadwithweights_only=True#\n\ntorch.load\nweights_only=True\nStarting in version 2.6,torch.loadwill useweights_only=Trueif thepickle_moduleargument is not passed.\ntorch.load\nweights_only=True\npickle_module\nAs discussed in the documentation fortorch.load(),weights_only=Truerestricts\nthe unpickler used intorch.loadto only executing functions/building classes required forstate_dictsof plaintorch.Tensorsas well as some other primitive types. Further,\nunlike the defaultUnpicklerprovided by thepicklemodule, theweights_onlyUnpickler\nis not allowed to dynamically import anything during unpickling.\ntorch.load()\nweights_only=True\ntorch.load\nstate_dicts\ntorch.Tensors\nUnpickler\npickle\nweights_only\nAs mentioned above, saving a module\u2019sstate_dictis a best practice when usingtorch.save. If loading an old\ncheckpoint that contains annn.Module, we recommendweights_only=False. When loading a checkpoint that contains\ntensor subclasses, there will likely be functions/classes that need to be allowlisted, see below for further details.\nstate_dict\ntorch.save\nnn.Module\nweights_only=False\nIf theweights_onlyUnpickler encounters a function or class that is not allowlisted\nby default within the pickle file, you should see an actionable error like such\nweights_only\n\n```python\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded,\nto do so you have two options, do those steps only if you trust the source of the checkpoint.\n    1. Re-running `torch.load` with `weights_only` set to `False` will likely succeed,\n        but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n    2. Alternatively, to load with `weights_only=True` please check the recommended\n       steps in the following error message.\n       WeightsUnpickler error: Unsupported global: GLOBAL {__module__}.{__name__} was not an allowed global by\n       default. Please use `torch.serialization.add_safe_globals([{__name__}])` or the\n       `torch.serialization.safe_globals([{__name__}])` context manager to allowlist this global\n       if you trust this class/function.\n\n```\n\nPlease follow the steps in the error message and allowlist the functions or classes only if you trust them.\nTo get all GLOBALs (functions/classes) in the checkpoint that are not yet allowlisted you can usetorch.serialization.get_unsafe_globals_in_checkpoint()which will return a list of strings of the form{__module__}.{__name__}. If you trust these functions/classes, you can import them and allowlist them per\nthe error message either viatorch.serialization.add_safe_globals()or the context managertorch.serialization.safe_globals.\ntorch.serialization.get_unsafe_globals_in_checkpoint()\n{__module__}.{__name__}\ntorch.serialization.add_safe_globals()\ntorch.serialization.safe_globals\nTo access the list of user-allowlisted functions/classes you can usetorch.serialization.get_safe_globals()and\nto clear the current list seetorch.serialization.clear_safe_globals().\ntorch.serialization.get_safe_globals()\ntorch.serialization.clear_safe_globals()\n\n## Troubleshootingweights_only#\n\nweights_only\nA caveat is thattorch.serialization.get_unsafe_globals_in_checkpoint()analyzes the checkpoint statically,\nsome types might be built dynamically during the unpickling process and hence will not be reported bytorch.serialization.get_unsafe_globals_in_checkpoint(). One such example isdtypesin numpy. Innumpy<1.25after allowlisting all the functions/classes reported bytorch.serialization.get_unsafe_globals_in_checkpoint()you might see an error like\ntorch.serialization.get_unsafe_globals_in_checkpoint()\ntorch.serialization.get_unsafe_globals_in_checkpoint()\ndtypes\nnumpy<1.25\ntorch.serialization.get_unsafe_globals_in_checkpoint()\n\n```python\nWeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,\nbut got <class 'numpy.dtype[float32]'>\n\n```\n\nThis can be allowlisted via{add_}safe_globals([type(np.dtype(np.float32))]).\n{add_}safe_globals([type(np.dtype(np.float32))])\nInnumpy>=1.25you would see\nnumpy>=1.25\n\n```python\nWeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,\nbut got <class 'numpy.dtypes.Float32DType'>\n\n```\n\nThis can be allowlisted via{add_}safe_globals([np.dtypes.Float32DType]).\n{add_}safe_globals([np.dtypes.Float32DType])\nThere are two environment variables that will influence the behavior oftorch.load. These can be helpful\nif one does not have access to thetorch.loadcallsites.\ntorch.load\ntorch.load\nTORCH_FORCE_WEIGHTS_ONLY_LOAD=1will override alltorch.loadcallsites to useweights_only=True.\nTORCH_FORCE_WEIGHTS_ONLY_LOAD=1\ntorch.load\nweights_only=True\nTORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1will maketorch.loadcallsites useweights_only=Falseonlyifweights_onlywas not passed as an argument.\nTORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1\ntorch.load\nweights_only=False\nweights_only\n\n## Utility functions#\n\nThe following utility functions are related to serialization:\nRegisters callables for tagging and deserializing storage objects with an associated priority.\nTagging associates a device with a storage object at save time while deserializing moves a\nstorage object to an appropriate device at load time.taggeranddeserializerare run in the order given by theirpriorityuntil a tagger/deserializer returns a\nvalue that is notNone.\ntagger\ndeserializer\npriority\nTo override the deserialization behavior for a device in the global registry, one can register a\ntagger with a higher priority than the existing tagger.\nThis function can also be used to register a tagger and deserializer for new devices.\npriority(int) \u2013 Indicates the priority associated with the tagger and deserializer, where a lower\nvalue indicates higher priority.\ntagger(Callable[[Union[Storage,TypedStorage,UntypedStorage]],Optional[str]]) \u2013 Callable that takes in a storage object and returns its tagged device as a string\nor None.\ndeserializer(Callable[[Union[Storage,TypedStorage,UntypedStorage],str],Optional[Union[Storage,TypedStorage,UntypedStorage]]]) \u2013 Callable that takes in storage object and a device string and returns a storage\nobject on the appropriate device or None.\nNone\nExample\n\n```python\n>>> def ipu_tag(obj):\n>>>     if obj.device.type == 'ipu':\n>>>         return 'ipu'\n>>> def ipu_deserialize(obj, location):\n>>>     if location.startswith('ipu'):\n>>>         ipu = getattr(torch, \"ipu\", None)\n>>>         assert ipu is not None, \"IPU device module is not loaded\"\n>>>         assert torch.ipu.is_available(), \"ipu is not available\"\n>>>         return obj.ipu(location)\n>>> torch.serialization.register_package(11, ipu_tag, ipu_deserialize)\n\n```\n\nGet whethertorch.save()computes and writes crc32 for each record.\ntorch.save()\nDefaults toTrue.\nTrue\nbool\nSet whethertorch.save()computes and writes crc32 for each record.\ntorch.save()\nNote\nSetting this toFalsemay make unzipping of thetorch.saveoutput\nfail or warn due to corrupted CRC32. Howevertorch.loadwill be\nable to load the file.\nFalse\ntorch.save\ntorch.load\ncompute_crc32(bool) \u2013 set crc32 computation flag\nGet fallback byte order for loading files\nIf byteorder mark is not present in saved checkpoint,\nthis byte order is used as fallback.\nBy default, it\u2019s \u201cnative\u201d byte order.\nOptional[LoadEndianness]\ndefault_load_endian\nSet fallback byte order for loading files\nIf byteorder mark is not present in saved checkpoint,\nthis byte order is used as fallback.\nBy default, it\u2019s \u201cnative\u201d byte order.\nendianness\u2013 the new fallback byte order\nGet default mmap options fortorch.load()withmmap=True.\ntorch.load()\nmmap=True\nDefaults tommap.MAP_PRIVATE.\nmmap.MAP_PRIVATE\nint\ndefault_mmap_options\nContext manager or function to set default mmap options fortorch.load()withmmap=Trueto flags.\ntorch.load()\nmmap=True\nFor now, only eithermmap.MAP_PRIVATEormmap.MAP_SHAREDare supported.\nPlease open an issue if you need any other option to be added here.\nmmap.MAP_PRIVATE\nmmap.MAP_SHARED\nNote\nThis feature is currently not supported for Windows.\nflags(int) \u2013mmap.MAP_PRIVATEormmap.MAP_SHARED\nmmap.MAP_PRIVATE\nmmap.MAP_SHARED\nMarks the given globals as safe forweights_onlyload. For example, functions\nadded to this list can be called during unpickling, classes could be instantiated\nand have state set.\nweights_only\nEach item in the list can either be a function/class or a tuple of the form\n(function/class, string) where string is the full path of the function/class.\nWithin the serialized format, each function is identified with its full\npath as{__module__}.{__qualname__}. When calling this API, you can provide this\nfull path that should match the one in the checkpoint otherwise the default{fn.__module__}.{fn.__qualname__}will be used.\n{__module__}.{__qualname__}\n{fn.__module__}.{fn.__qualname__}\nsafe_globals(List[Union[Callable,Tuple[Callable,str]]]) \u2013 list of globals to mark as safe\nExample\n\n```python\n>>> import tempfile\n>>> class MyTensor(torch.Tensor):\n...     pass\n>>> t = MyTensor(torch.randn(2, 3))\n>>> with tempfile.NamedTemporaryFile() as f:\n...     torch.save(t, f.name)\n# Running `torch.load(f.name, weights_only=True)` will fail with\n# Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.\n# Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.\n...     torch.serialization.add_safe_globals([MyTensor])\n...     torch.load(f.name, weights_only=True)\n# MyTensor([[-0.5024, -1.8152, -0.5455],\n#          [-0.8234,  2.0500, -0.3657]])\n\n```\n\nClears the list of globals that are safe forweights_onlyload.\nweights_only\nReturns the list of user-added globals that are safe forweights_onlyload.\nweights_only\nlist[Union[Callable,tuple[Callable,str]]]\nReturns a list of strings of functions/classes in atorch.saveobject that are not safe forweights_only.\ntorch.save\nweights_only\nFor a given function or classf, the corresponding string will be of the form{f.__module__}.{f.__name__}.\nf\n{f.__module__}.{f.__name__}\nThis function will return any GLOBALs in the checkpoint that are not in the set marked safe\nforweights_only(either viaadd_safe_globals()orsafe_globalscontext or\nallowlisted bytorchby default).\nweights_only\nadd_safe_globals()\nsafe_globals\ntorch\nNote\nThis function will statically disassemble the pickle file in the checkpoint.\nThe implication is any classes dynamically pushed onto the stack during unpickling\nwill not be included in the output.\nf(Union[str,PathLike[str],IO[bytes]]) \u2013 File-like object or string containing the checkpoint object saved viatorch.save\ntorch.save\nA list of strings of pickle GLOBALs in the checkpoint that are not allowlisted forweights_only.\nweights_only\nlist[str]\nContext-manager that adds certain globals as safe forweights_onlyload.\nweights_only\nsafe_globals(list[Union[Callable,tuple[Callable,str]]]) \u2013 List of globals for weights_only load.\nExample\n\n```python\n>>> import tempfile\n>>> class MyTensor(torch.Tensor):\n...     pass\n>>> t = MyTensor(torch.randn(2, 3))\n>>> with tempfile.NamedTemporaryFile() as f:\n...     torch.save(t, f.name)\n# Running `torch.load(f.name, weights_only=True)` will fail with\n# Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.\n# Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.\n...     with torch.serialization.safe_globals([MyTensor]):\n...         torch.load(f.name, weights_only=True)\n# MyTensor([[-0.5024, -1.8152, -0.5455],\n#          [-0.8234,  2.0500, -0.3657]])\n>>> assert torch.serialization.get_safe_globals() == []\n\n```\n\nContext-manager that skips writing/reading storage bytes fortorch.save/torch.loadcalls.\ntorch.save\ntorch.load\nFor the save path, storages will still be saved, but the space that their bytes would usually be written to\nwill be empty space. The storage bytes can then be populated in a separate pass.\nFor the load path, tensors will be loaded per the checkpoint but their storages will not be populated with data.\nWarning\nTheskip_datacontext manager is an early prototype and is subject to change.\nskip_data\nmaterialize_fake_tensors(bool) \u2013 Whether to materialize FakeTensors during save. This is a no-op for the load path.\nExample\n\n```python\n>>> import tempfile\n>>> t = torch.randn(2, 3)\n>>> with tempfile.NamedTemporaryFile() as f:\n...     with torch.serialization.skip_data():\n...         torch.save(t, f.name)\n...     torch.load(f.name, weights_only=True)\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n```\n\n\n## Config#\n\ntorch.utils.serialization.configprovides a global config that can control the behavior oftorch.saveandtorch.load.\ntorch.utils.serialization.config\ntorch.save\ntorch.load\ntorch.utils.serialization.config.savecontains options that control the behavior oftorch.save.\ntorch.utils.serialization.config.save\ntorch.save\ncompute_crc32: whether to compute and write the zip file checksum (Default :True).\nSeeset_crc32_options().\ncompute_crc32\nTrue\nset_crc32_options()\nuse_pinned_memory_for_d2h: for storages that are on an accelerator when passed totorch.save, whether to\nmove storage to pinned memory or pageable memory on CPU withintorch.save. (Default:False(i.e. pageable))\nuse_pinned_memory_for_d2h\ntorch.save\ntorch.save\nFalse\nstorage_alignment: alignment of storages in the checkpoint duringtorch.savein bytes. (Default64)\nstorage_alignment\ntorch.save\n64\ntorch.utils.serialization.config.loadcontains options that control the behavior oftorch.load.\ntorch.utils.serialization.config.load\ntorch.load\nmmap: See the documentation formmapargument intorch.load().\nThis config will set the behavior ofmmapfortorch.loadif it is not\nalready explicitly passed to thetorch.loadcall (Default :False).\nmmap\nmmap\ntorch.load()\nmmap\ntorch.load\ntorch.load\nFalse\nendianness: Seeset_default_load_endianness().\n(Default :torch.serialization.LoadEndianness.NATIVE)\nendianness\nset_default_load_endianness()\ntorch.serialization.LoadEndianness.NATIVE\nmmap_flags: Seeset_default_mmap_options.\n(Default :MAP_PRIVATE)\nmmap_flags\nset_default_mmap_options\nMAP_PRIVATE\ncalculate_storage_offsets: If this config is set toTrue, offsets for storages will be\ncalculated rather than read via random reads when usingtorch.load(mmap=True). This minimizes\nrandom reads, which can be helpful when the file is being loaded over a network. (Default :False)\ncalculate_storage_offsets\nTrue\ntorch.load(mmap=True)\nFalse",
  "url": "https://pytorch.org/docs/stable/notes/serialization.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
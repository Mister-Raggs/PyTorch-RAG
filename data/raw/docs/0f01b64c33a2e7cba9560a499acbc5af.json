{
  "doc_id": "0f01b64c33a2e7cba9560a499acbc5af",
  "source": "pytorch_docs",
  "title": "CUDAGraph Trees \u2014 PyTorch 2.9 documentation",
  "text": "\n## CUDAGraph Trees#\n\nCreated On: May 19, 2023 | Last Updated On: Jul 30, 2025\n\n## Background#\n\n\n## CUDAGraph#\n\nFor a longer background on CUDAGraphs, readaccelerating pytorch with CUDAGraphs.\nCUDA Graphs, which made its debut in CUDA 10, let a series of CUDA kernels to be defined and encapsulated as a single unit, i.e., a graph of operations, rather than a sequence of individually-launched operations. It provides a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduces the launching overheads.\nCUDA Graphs can give large speedups, especially for models with high CPU overhead or small compute. There are a number of limitations from requiring the same kernels to be run with the same arguments and dependencies, and memory addresses.\nControl Flow is not possible\nKernels which trigger host to device syncs (such as .item()) errors\nAll input arguments to kernels are fixed to what they were recorded\nCUDA Memory addresses are fixed, however the values of the memory at those addresses can change\nNo Essential CPU ops or CPU side effects\n\n## PyTorch CUDAGraph Integration#\n\nPyTorch provides aconvenience wrapperaround CUDAGraphs that handles a couple of tricky interactions with PyTorch\u2019s caching allocator.\nThe CachingAllocator uses a separate memory pool for all the new allocations. During CUDAGraph recording, memory is accounted for, allocated, and freed exactly as during eager run. On replay, just the kernels are invoked, and there are no changes to the allocator. Subsequent to initial recording, the allocator does not know which memory is actively being used in user programs.\nUsing a separate memory pool between eager allocations and cudagraph allocations may increase the memory of your program if there is substantial memory allocated to both.\n\n## Make Graphed Callables#\n\nMake Graphed Callablesis a PyTorch Abstraction to share a single memory pool over a series of callables. Graphed Callables takes advantage of the fact that on CUDA Graph recording, memory is exactly accounted for by the caching allocator to safely share memory between separate CUDA Graph recordings. In each invocation, outputs are preserved as live memory, preventing one callable from overwriting the live memory of another. Graphed Callables can only be invoked in a single order; memory addresses from the first run are burned into the second, and so forth.\n\n## TorchDynamo Previous CUDA Graphs Integration#\n\nRunning withcudagraph_trees=Falsedoes not reuse memory across separate graph captures, which can lead to large memory regressions. Even for a model that has no graph breaks, this has issues. The forward and backward are separate graph captures, so the memory pools for forward and backward are not shared. In particular, memory for activations that are saved in the forward cannot be reclaimed in the backward.\ncudagraph_trees=False\n\n## CUDAGraph Trees Integration#\n\nLike Graph Callables, CUDA Graph Trees use a single memory pool across all graph captures. However, instead of requiring a single sequence of invocations, CUDA Graph Trees create separate trees of CUDA Graph captures. Let\u2019s take a look at an illustrative example:\n\n```python\n@torch.compile(mode=\"reduce-overhead\")\ndef foo(x):\n    # GRAPH 1\n    y = x * x * x\n    # graph break triggered here\n    if y.sum() > 0:\n        # GRAPH 2\n        z = y ** y\n    else:\n        # GRAPH 3\n        z = (y.abs() ** y.abs())\n    torch._dynamo.graph_break()\n    # GRAPH 4\n    return z * torch.rand_like(z)\n\n# the first run warms up each graph, which does things like CuBlas or Triton benchmarking\nfoo(torch.arange(0, 10, device=\"cuda\"))\n# The second run does a CUDA Graph recording, and replays it\nfoo(torch.arange(0, 10, device=\"cuda\"))\n# Finally we hit the optimized, CUDA Graph replay path\nfoo(torch.arange(0, 10, device=\"cuda\"))\n\n```\n\nIn this example, there are two separate paths that we make through the function: 1 -> 2 -> 4, or 1 -> 3 -> 4.\nWe share all of the memory in a single memory pool between separate recordings by building up a tape of CUDA Graph recordings, in this instance, 1 -> 2 -> 4. We add invariants to ensure that memory is always in the same location as it were recorded, and no live tensors exist in user programs that might be overwritten.\nSame constraints from CUDA Graphs apply: same kernels must be invoked with the same arguments (static sizes, addresses, etc)\nThe same pattern of memory must be observed between recording and replay: if a tensor output of one graph dies subsequent to another graph during recording, it must also do so during replay.\nLive memory in the CUDA pool forces a dependence between two recordings\nThese recordings can only be invoked in a single order 1 - > 2 -> 4\nAll of the memory is shared in a single memory pool, so there is no additional memory overhead compared to eager. Now, what happens if we were to hit a new path and run Graph 3?\nGraph 1 gets replayed, and then we hit Graph 3, which we have not yet recorded. On graph replays, the private memory pool is not updated, so y is not reflected in the allocator. Without care, we would overwrite it. To support reusing the same memory pool after replaying other graphs, we checkpoint the memory pool back to its state at the end of graph 1. Now that our live tensors are reflected in the caching allocator, we are safe to run a new graph.\nFirst, we would hit the optimized, CUDAGraph.replay() path that we have already recorded in graph 1. Then we would hit Graph 3. Just as before, we will need to warm up the graph once before recording. On the warmup run, the memory addresses are not fixed, so graph 4 will also fallback to the inductor, non-cudagraph invocation.\nThe second time we hit graph 3 we are warmed up and ready to record. We record graph 3 and then record graph 4 again since the input memory addresses have changed. This creates a tree of CUDA Graph recordings. A CUDA Graph Tree!\n\n```python\n  1\n / \\\\\n2   3\n \\\\   \\\\\n  4   4\n\n```\n\n\n## Input Mutation Support#\n\nInput mutation function refers to a function conducting in-place writes to an input tensor,\nas illustrated below:\n\n```python\ndef foo(x, y):\n    # mutates input x\n    x.add_(1)\n    return x + y\n\n```\n\nInput mutation functions generally lead to challenges for CUDAGraph Trees. Due to the static\nCUDA memory address requirement from CUDAGraph, for each input tensor x, CUDAGraph Trees may\nallocate a static memory address x\u2019. During execution, CUDAGraph Trees first copy the input\ntensor x to the static memory address x\u2019, and then replay the recorded CUDAGraph. For input\nmutation function, x\u2019 is in-place updated, which is not reflected on the input tensor x since\nx and x\u2019 reside on different CUDA memory addresses.\nA closer look at input mutation functions reveals that there are three types of inputs:\ninputs from eager: These tensors we assume will vary input tensor addresses from\nexecution to execution. Because cudagraphs freeze memory addresses, we need to copy these\ninputs to a static address tensor prior to graph recording and execution.\nParameters and buffers: These tensors we assume (and runtime-check) have the same tensor\naddresses on every execution. We do not need to copy over their contents because the recorded\nmemory address will be the same as the executed memory address.\nTensors which are prior outputs from CUDAGraph Trees: Because the output tensor addresses\nof a cudagraph are fixed, if we run CUDAGraph1, then run CUDAGraph2, the inputs which came from\nCUDAGraph1 into CUDAGraph2 will have a fixed memory address. These inputs, like parameters and\nbuffers, do not require copying over to a static address tensor. We check to make sure that\nthese inputs are stable at runtime, and if they\u2019re not we will re-record.\nCUDAGraph Trees support input mutation on parameters and buffers, and tensors which are prior\noutputs from CUDAGraph Trees. For mutation on inputs from eager, CUDAGraph Trees will run the\nfunction without CUDAGraph and emitskipping due to mutated inputslog. The following example\nshows CUDAGraph Trees\u2019 support for tensors which are prior outputs from CUDAGraph Trees.\n\n```python\nimport torch\n\n@torch.compile(mode=\"reduce-overhead\")\ndef foo(x):\n    return x + 1\n\n@torch.compile(mode=\"reduce-overhead\")\ndef mut(x):\n    return x.add_(2)\n\n# Enable input mutation support\ntorch._inductor.config.triton.cudagraph_support_input_mutation = True\n\nfor i in range(3):\n    torch.compiler.cudagraph_mark_step_begin()\n    inp = torch.rand([4], device=\"cuda\")\n\n    # CUDAGraph is applied since `foo` does not mutate `inp`\n    tmp = foo(inp)\n    # Although `mut` mutates `tmp`, which is an output of a CUDAGraph\n    # managed function. So CUDAGraph is still applied.\n    mut(tmp)\n\n\ntorch.compiler.cudagraph_mark_step_begin()\ninp = torch.rand([4], device=\"cuda\")\n\ntmp = foo(inp)\n# While `tmp` is a CUDAGraph Tree managed function's output, `tmp.clone()`\n# is not. So CUDAGraph is not applied to `mut` and there is a log\n# `skipping cudagraphs due to mutated inputs`\nmut(tmp.clone())\n\n```\n\nTo enable CUDAGraph Trees for a function mutating inputs from eager, please re-write\nthe function to avoid input mutation.\nNoteEnable input mutation support by settingtorch._inductor.config.cudagraph_support_input_mutation = Truefor \u201creduce-overhead\u201d mode.\n\n## Dynamic Shape Support#\n\nDynamic shapemeans that an input tensor has different shapes across function calls. Since CUDAGraph\nrequires fixed tensor addresses, CUDAGraph Trees re-record CUDAGraph for every unique\nshape of an input tensor. This leads to multiple CUDAGraphs for a single inductor graph.\nWhen there are limited shapes (e.g., batch sizes in inference), it is profitable to\nre-record CUDAGraphs. However, if input tensor shapes change frequently or even on\nevery invocation, re-recording CUDAGraph may not be profitable. Nvidia uses 64 KB of\ndevice memory per kernel launch in CUDAGraph, up until CUDA 12.4 and Driver Version 550+.\nThis memory cost can be significant with many CUDAGraph re-recordings.\nFor functions with frequently changing input tensor shapes, we suggest padding input\ntensors to a few fixed tensor shapes to still enjoy benefits from CUDAGraph. In addition,\nsettingtorch._inductor.config.triton.cudagraph_skip_dynamic_graphs=Trueallows to skip cudagraphing functions with dynamic shape inputs and only cudagraphing\nfunctions with static input tensor shapes.\n\n## NCCL Support#\n\nCUDAGraph Trees support functions with nccl operators. While CUDAGraph Trees perform per-device\nrecord for CUDAGraph, NCCL support allows cross-device communication.\n\n```python\n@torch.compile(mode=\"reduce-overhead\")\ndef func(x):\n    y = x * x\n    y = torch.distributed.all_reduce(y, op=torch.distributed.ReduceOp.SUM)\n    x = torch.nn.functional.silu(x)\n    return x * y\n\n```\n\n\n## Reasons for Skipping CUDAGraph#\n\nSince CUDAGraph has requirements such as static input tensor addresses and not supporting\nCPU operators, CUDAGraph Trees check whether a function satisfies these requirements and\nmay skip CUDAGraph when necessary. Here, we list common reasons for skipping CUDAGraph.\nInput mutation: CUDAGraph Trees skip functions that in-place mutates eager input.\nIn-place mutating parameters and buffers, or output tensors from CUDAGraph Tree managed\nfunctions are still supported. Please seeInput Mutation Supportsection for more details.\nCPU operators: Functions containing CPU operator are skipped. Please split the\nfunction into multiple functions and apply CUDAGraph Trees on functions with only GPU operators.\nMulti-device operators: A function is skipped if it contains operators on multiple\ndevices. Currently, CUDAGraph is applied on a per-device basis. Please use supported\nlibraries such as NCCL for cross-device communication. Please seeNCCL Supportsection for more details.\nFree unbacked symbols: Free unbacked symbols usually happen duringdynamic shapes.\nCUDAGraph Trees currently record a CUDAGraph for every unique input tensor shapes.\nPlease seeDynamic Shape Supportfor more details.\nCUDAGraph-unsafe custom ops: Some custom ops may include cudagraph unsafe ops, which causes cudagraph to be skipped. Please seeCUDAGraph Unsafe Custom Opsfor more details.\nIncompatible operators: CUDAGraph Trees skip a function if it contain incompatible\noperators. Please replace these operators in a function with supported operators. We\nshow an exhaustive list of incompatible operators:\n\n```python\naten._fused_moving_avg_obs_fq_helper.default\naten._fused_moving_avg_obs_fq_helper_functional.default\naten.multinomial.default\nfbgemm.dense_to_jagged.default\nfbgemm.jagged_to_padded_dense.default\nrun_and_save_rng_state\nrun_with_rng_state\naten._local_scalar_dense\naten._assert_scalar\n\n```\n\nThe following operators are incompatible whentorch.are_deterministic_algorithms_enabled().\n\n```python\naten._fused_moving_avg_obs_fq_helper.default\naten._fused_moving_avg_obs_fq_helper_functional.default\naten.multinomial.default\nfbgemm.dense_to_jagged.default\nfbgemm.jagged_to_padded_dense.default\nrun_and_save_rng_state\nrun_with_rng_state\naten._local_scalar_dense\naten._assert_scalar\n\n```\n\n\n## CUDAGraph Unsafe Custom Ops#\n\nCustom ops are assumed to be safe for CUDAGraph by default. However, some custom ops may include unsupported ops such as cpu ops. Since custom op are treated as black boxes by the compiler, users must explicitly mark these ops as unsafe for CUDAGraph by setting thetorch._C.Tag.cudagraph_unsafetag, as demonstrated in the example below. When a function contains cudagraph-unsafe custom ops, it will be skipped by CUDAGraph unlessCUDAGraph partitionis enabled.\ntorch._C.Tag.cudagraph_unsafe\n\n```python\n@torch.library.custom_op(\n    \"mylib::modify\",\n    mutates_args=(),\n    tags=(torch._C.Tag.cudagraph_unsafe,),\n)\ndef modify(pic: torch.Tensor) -> torch.Tensor:\n    pic1 = pic + 1\n    pic1_cpu = (pic1.cpu() + 1) * 2\n    return pic1_cpu.cuda() + pic\n\n@modify.register_fake\ndef _(pic):\n    return torch.empty_like(pic)\n\n```\n\n\n## CUDAGraph Partition#\n\nAs we discussed earlier, CUDAGraph does not support some ops (e.g., cpu ops) which may limit its adoption. CUDAGraph partition is a compiler solution that automatically splits off these ops, reorders ops to reduce the number of partitions, and applies CUDAGraph to each partition individually. Please settorch._inductor.config.graph_partition=Trueto enable CUDAGraph partition.\ntorch._inductor.config.graph_partition=True\nConsider the following example wherexandyare gpu inputs buty_cpuis a cpu tensor. Without graph partition, this function must be skipped due to cpu ops. With graph partition, the CPU ops are split off, and the remaining GPU ops are cudagraphified, resulting in two separate separate CUDAGraphs.\nx\ny\ny_cpu\n\n```python\ndef f(x, y):\n    x1 = x + 1\n    y1 = y + 1\n    y_cpu = y1.cpu() + 1\n    z = x @ y\n    return x1 + y1 + z + y_cpu.cuda()\n\n```\n\nCurrently, CUDAGraph partition supports splitting off the following types of ops:\nNon-GPU Ops: Popular examples include computation on cpu tensors.\nDevice Copy Ops: Data transfers between devices, such as they1.cpu()in the example above.\ny1.cpu()\nControl Flow Ops:Control flow opsare split off since they are not yet supported by CUDAGraph.\nCUDAGraph Unsafe Custom Ops: Custom ops tagged withtorch._C.Tag.cudagraph_unsafeare split off. SeeCUDAGraph Unsafe Custom Opssection for details.\ntorch._C.Tag.cudagraph_unsafe\nUnbacked Symints: Please refer toDynamic Shape Supportsection for more information.\n\n## Limitations#\n\nBecause CUDA Graph fixes memory addresses, CUDA Graphs do not have a great way of handling live tensors from a previous invocation.\nLet\u2019s say we are benchmarking running inference with the following code:\n\n```python\nimport torch\n\n@torch.compile(mode=\"reduce-overhead\")\ndef my_model(x):\n    y = torch.matmul(x, x)\n    return y\n\nx = torch.randn(10, 10, device=\"cuda\")\ny1 = my_model(x)\ny2 = my_model(x)\nprint(y1)\n# RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.\n\n```\n\nIn the Separate CUDA Graph implementation, the output from the first invocation will be overwritten by the second invocation. In CUDAGraph\nTrees, we don\u2019t want to add unintended dependencies between iterations that would cause us to not hit the hot path, nor do we want we want\nto prematurely free memory from a prior invocation. Our heuristics are in inference we start a new iteration on each invocation for\ntorch.compile, and in training we do the same so long as there is not a pending backward that has not been invoked. If those heuristics\nare wrong, you can mark the start of a new iteration withtorch.compiler.mark_step_begin(), or clone\ntensors of a prior iteration (outside of torch.compile) before you begin the next run.\n\n## Comparisons#\n\nFootguns\nSeparate CudaGraph\nCUDAGraph Trees\nMemory Can Increase\nOn each graph compilation (new sizes, etc.)\nIf you are also running non-cudagraph memory\nRecordings\nOn any new invocation of a graph\nWill re-record on any new, unique path you take through your program\nFootguns\nInvocation of one graph will overwrite prior invocation\nCannot persist memory between separate runs through your model - one training loop training, or one run of inference",
  "url": "https://pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
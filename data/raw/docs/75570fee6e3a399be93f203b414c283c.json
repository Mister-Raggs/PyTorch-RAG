{
  "doc_id": "75570fee6e3a399be93f203b414c283c",
  "source": "pytorch_docs",
  "title": "Distributed Autograd Design \u2014 PyTorch 2.9 documentation",
  "text": "\n## Distributed Autograd Design#\n\nCreated On: Nov 12, 2019 | Last Updated On: Sep 03, 2021\nThis note will present the detailed design for distributed autograd and walk\nthrough the internals of the same. Make sure you\u2019re familiar withAutograd mechanicsand theDistributed RPC Frameworkbefore\nproceeding.\n\n## Background#\n\nLet\u2019s say you have two nodes and a very simple model partitioned across two\nnodes. This can be implemented usingtorch.distributed.rpcas follows:\ntorch.distributed.rpc\n\n```python\nimport torch\nimport torch.distributed.rpc as rpc\n\ndef my_add(t1, t2):\n  return torch.add(t1, t2)\n\n# On worker 0:\nt1 = torch.rand((3, 3), requires_grad=True)\nt2 = torch.rand((3, 3), requires_grad=True)\n\n# Perform some computation remotely.\nt3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2))\n\n# Perform some computation locally based on remote result.\nt4 = torch.rand((3, 3), requires_grad=True)\nt5 = torch.mul(t3, t4)\n\n# Compute some loss.\nloss = t5.sum()\n\n```\n\nThe main motivation behind distributed autograd is to enable running a backward\npass on such distributed models with thelossthat we\u2019ve computed and\nrecord appropriate gradients for all tensors that require gradients.\nloss\n\n## Autograd recording during the forward pass#\n\nPyTorch builds the autograd graph during the forward pass and this graph is\nused to execute the backward pass. For more details seeHow autograd encodes the history.\nFor distributed autograd, we need to keep track of all RPCs during the forward\npass to ensure the backward pass is executed appropriately. For this purpose,\nwe attachsendandrecvfunctions to the autograd graph when we perform\nan RPC.\nsend\nrecv\nThesendfunction is attached to the source of the RPC and its output\nedges point to the autograd function for the input tensors of the RPC.\nThe input for this function during the backward pass is received from the\ndestination as the output of the appropriaterecvfunction.\nsend\nrecv\nTherecvfunction is attached to the destination of the RPC and its\ninputs are retrieved from operators executed on the destination using the\ninput tensors. The output gradients of this function are sent to the source\nnode to the appropriatesendfunction during the backward pass.\nrecv\nsend\nEachsend-recvpair is assigned a globally uniqueautograd_message_idto uniquely identify the pair. This is useful to look up the corresponding\nfunction on a remote node during the backward pass.\nsend-recv\nautograd_message_id\nForRRef, whenever we calltorch.distributed.rpc.RRef.to_here()we attach an appropriatesend-recvpair for the tensors involved.\ntorch.distributed.rpc.RRef.to_here()\nsend-recv\nAs an example, this is what the autograd graph for our example above would look\nlike (t5.sum() excluded for simplicity):\n\n## Distributed Autograd Context#\n\nEach forward and backward pass that uses distributed autograd is assigned a\nuniquetorch.distributed.autograd.contextand this context has a\nglobally uniqueautograd_context_id. This context is created on each node\nas needed.\ntorch.distributed.autograd.context\nautograd_context_id\nThis context serves the following purpose:\nMultiple nodes running distributed backward passes might accumulate\ngradients on the same tensor and as a result the.gradfield of the\ntensor would have gradients from a variety of distributed backward passes\nbefore we have the opportunity to run the optimizer. This is similar to\ncallingtorch.autograd.backward()multiple times locally. In order to\nprovide a way of separating out the gradients for each backward pass, the\ngradients are accumulated in thetorch.distributed.autograd.contextfor each backward pass.\n.grad\ntorch.autograd.backward()\ntorch.distributed.autograd.context\nDuring the forward pass we store thesendandrecvfunctions for\neach autograd pass in this context. This ensures we hold references to the\nappropriate nodes in the autograd graph to keep it alive. In addition to\nthis, it is easy to look up the appropriatesendandrecvfunctions\nduring the backward pass.\nsend\nrecv\nsend\nrecv\nIn general we also use this context to store some metadata for each\ndistributed autograd pass.\nFrom the user\u2019s perspective the autograd context is setup as follows:\n\n```python\nimport torch.distributed.autograd as dist_autograd\nwith dist_autograd.context() as context_id:\n  loss = model.forward()\n  dist_autograd.backward(context_id, loss)\n\n```\n\nIt is important to note that your model\u2019s forward pass must be invoked within\nthe distributed autograd context manager, as a valid context is needed in\norder to ensure that allsendandrecvfunctions are stored properly\nto run the backward pass across all participating nodes.\nsend\nrecv\n\n## Distributed Backward Pass#\n\nIn this section we outline the challenge of computing dependencies accurately\nduring a distributed backward pass and describe a couple of algorithms (with\ntradeoffs) on how we can execute a distributed backward pass.\n\n## Computing dependencies#\n\nConsider the following piece of code being run on a single machine\n\n```python\nimport torch\na = torch.rand((3, 3), requires_grad=True)\nb = torch.rand((3, 3), requires_grad=True)\nc = torch.rand((3, 3), requires_grad=True)\nd = a + b\ne = b * c\nd.sum.().backward()\n\n```\n\nThis is what the autograd graph for the code above would look like:\nThe first step the autograd engine performs as part of the backward pass is\ncomputing the number of dependencies for each node in the autograd graph. This\nhelps the autograd engine know when a node in the graph is ready for execution.\nThe numbers in brackets foradd(1)andmul(0)denote the number of\ndependencies. As you can see, this means during the backward pass theaddnode needs 1 input and themulnode doesn\u2019t need any inputs (in other\nwords doesn\u2019t need to be executed). The local autograd engine computes these\ndependencies by traversing the graph from the root nodes (din this case).\nadd(1)\nmul(0)\nadd\nmul\nd\nThe fact that certain nodes in the autograd graph might not be executed in the\nbackward pass poses a challenge for distributed autograd. Consider this piece\nof code which uses RPC.\n\n```python\nimport torch\nimport torch.distributed.rpc as rpc\n\na = torch.rand((3, 3), requires_grad=True)\nb = torch.rand((3, 3), requires_grad=True)\nc = torch.rand((3, 3), requires_grad=True)\n\nd = rpc.rpc_sync(\"worker1\", torch.add, args=(a, b))\ne = rpc.rpc_sync(\"worker1\", torch.mul, args=(b, c))\nloss = d.sum()\n\n```\n\nThe associated autograd graph for the code above would be:\nComputing dependencies of this distributed autograd graph is much more\nchallenging and requires some overhead (either in terms of computation or\nnetwork communication).\nFor performance sensitive applications we can avoid a\nlot of overhead by assuming everysendandrecvfunction are valid as\npart of the backward pass (most applications don\u2019t perform RPCs that aren\u2019t\nused). This simplifies the distributed autograd algorithm and is much more\nefficient, but at the cost that the application needs to be aware of the\nlimitations. This algorithm is called theFAST mode algorithmand is\ndescribed in detail below.\nsend\nrecv\nIn the general case it might not be necessary that everysendandrecvfunction is valid as part of the backward pass. To address this, we have\nproposed aSMART mode algorithmwhich is described in a later section.\nPlease note that currently, only theFASTmode algorithm is implemented.\nsend\nrecv\n\n## FAST mode algorithm#\n\nThe key assumption of this algorithm is that eachsendfunction has a\ndependency of 1 when we run a backward pass. In other words, we assume we\u2019ll\nreceive a gradient over RPC from another node.\nsend\nThe algorithm is as follows:\nWe start from the worker which has the roots for the backward pass\n(all roots must be local).\nLookup all thesendfunctions for the currentDistributed Autograd Context.\nsend\nCompute dependencies locally starting from the provided roots and all thesendfunctions we retrieved.\nsend\nAfter computing dependencies, kick off the local autograd engine with the\nprovided roots.\nWhen the autograd engine executes therecvfunction, therecvfunction sends the input gradients via RPC to the appropriate worker.\nEachrecvfunction knows the destination worker id since it is recorded\nas part of the forward pass. Therecvfunction also sends over theautograd_context_idandautograd_message_idto the remote host.\nrecv\nrecv\nrecv\nrecv\nautograd_context_id\nautograd_message_id\nWhen this request is received on the remote host, we use theautograd_context_idandautograd_message_idto look up the\nappropriatesendfunction.\nautograd_context_id\nautograd_message_id\nsend\nIf this is the first time a worker has received a request for the givenautograd_context_id, it will compute dependencies locally as described\nin points 1-3 above.\nautograd_context_id\nThesendfunction retrieved in 6. is then enqueued for execution on the\nlocal autograd engine for that worker.\nsend\nFinally, instead of accumulating the gradients on the.gradfield of the\nTensor, we accumulate the gradients separately perDistributed Autograd Context. The gradients are stored in aDict[Tensor,Tensor], which is basically a map from Tensor to its\nassociated gradient and this map can be retrieved using theget_gradients()API.\n.grad\nDict[Tensor,Tensor]\nget_gradients()\nAs an example the complete code with distributed autograd would be as follows:\n\n```python\nimport torch\nimport torch.distributed.autograd as dist_autograd\nimport torch.distributed.rpc as rpc\n\ndef my_add(t1, t2):\n  return torch.add(t1, t2)\n\n# On worker 0:\n\n# Setup the autograd context. Computations that take\n# part in the distributed backward pass must be within\n# the distributed autograd context manager.\nwith dist_autograd.context() as context_id:\n  t1 = torch.rand((3, 3), requires_grad=True)\n  t2 = torch.rand((3, 3), requires_grad=True)\n\n  # Perform some computation remotely.\n  t3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2))\n\n  # Perform some computation locally based on remote result.\n  t4 = torch.rand((3, 3), requires_grad=True)\n  t5 = torch.mul(t3, t4)\n\n  # Compute some loss.\n  loss = t5.sum()\n\n  # Run the backward pass.\n  dist_autograd.backward(context_id, [loss])\n\n  # Retrieve the gradients from the context.\n  dist_autograd.get_gradients(context_id)\n\n```\n\nThe distributed autograd graph with dependencies would be as follows (t5.sum() excluded for simplicity):\nTheFAST mode algorithmapplied to the above example would be as follows:\nOnWorker0we start from the rootslossandsend1to compute\ndependencies. As a resultsend1is marked with a dependency of 1 andmulonWorker0is marked with a dependency of 1.\nWorker0\nloss\nsend1\nsend1\nmul\nWorker0\nNow, we kickoff the local autograd engine onWorker0. We first execute\nthemulfunction, accumulate its output in the autograd context as the\ngradient fort4. Then, we executerecv2which sends the gradients toWorker1.\nWorker0\nmul\nt4\nrecv2\nWorker1\nSince this is the first timeWorker1has heard about this backward pass,\nit starts dependency computation and marks the dependencies forsend2,addandrecv1appropriately.\nWorker1\nsend2\nadd\nrecv1\nNext, we enqueuesend2on the local autograd engine ofWorker1, which\nin turn executesaddandrecv1.\nsend2\nWorker1\nadd\nrecv1\nWhenrecv1is executed it sends the gradients over toWorker0.\nrecv1\nWorker0\nSinceWorker0has already computed dependencies for this backward pass,\nit just enqueues and executessend1locally.\nWorker0\nsend1\nFinally, gradients fort1,t2andt4are accumulated in theDistributed Autograd Context.\nt1\nt2\nt4\n\n## SMART mode algorithm#\n\nFull details of this algorithm are still in the works, but for the general idea\nyou can refer toDistributed Autograd Algorithm Smart modesection in theRFC.\n\n## Distributed Optimizer#\n\nTheDistributedOptimizeroperates as follows:\nDistributedOptimizer\nTakes a list of remote parameters (RRef) to\noptimize. These could also be local parameters wrapped within a localRRef.\nRRef\nRRef\nTakes aOptimizerclass as the local\noptimizer to run on all distinctRRefowners.\nOptimizer\nRRef\nThe distributed optimizer creates an instance of the localOptimizeron\neach of the worker nodes and holds anRRefto them.\nOptimizer\nRRef\nWhentorch.distributed.optim.DistributedOptimizer.step()is invoked,\nthe distributed optimizer uses RPC to remotely execute all the local\noptimizers on the appropriate remote workers. A distributed autogradcontext_idmust be provided as input totorch.distributed.optim.DistributedOptimizer.step(). This is used\nby local optimizers to apply gradients stored in the corresponding\ncontext.\ntorch.distributed.optim.DistributedOptimizer.step()\ncontext_id\ntorch.distributed.optim.DistributedOptimizer.step()\nIf multiple concurrent distributed optimizers are updating the same\nparameters on a worker, these updates are serialized via a lock.\n\n## Simple end to end example#\n\nPutting it all together, the following is a simple end to end example using\ndistributed autograd and the distributed optimizer. If the code is placed into a\nfile called \u201cdist_autograd_simple.py\u201d, it can be run with the commandMASTER_ADDR=\"localhost\"MASTER_PORT=29500pythondist_autograd_simple.py:\nMASTER_ADDR=\"localhost\"MASTER_PORT=29500pythondist_autograd_simple.py\n\n```python\nimport torch\nimport torch.multiprocessing as mp\nimport torch.distributed.autograd as dist_autograd\nfrom torch.distributed import rpc\nfrom torch import optim\nfrom torch.distributed.optim import DistributedOptimizer\n\ndef random_tensor():\n    return torch.rand((3, 3), requires_grad=True)\n\ndef _run_process(rank, dst_rank, world_size):\n    name = \"worker{}\".format(rank)\n    dst_name = \"worker{}\".format(dst_rank)\n\n    # Initialize RPC.\n    rpc.init_rpc(\n        name=name,\n        rank=rank,\n        world_size=world_size\n    )\n\n    # Use a distributed autograd context.\n    with dist_autograd.context() as context_id:\n        # Forward pass (create references on remote nodes).\n        rref1 = rpc.remote(dst_name, random_tensor)\n        rref2 = rpc.remote(dst_name, random_tensor)\n        loss = rref1.to_here() + rref2.to_here()\n\n        # Backward pass (run distributed autograd).\n        dist_autograd.backward(context_id, [loss.sum()])\n\n        # Build DistributedOptimizer.\n        dist_optim = DistributedOptimizer(\n        optim.SGD,\n        [rref1, rref2],\n        lr=0.05,\n        )\n\n        # Run the distributed optimizer step.\n        dist_optim.step(context_id)\n\ndef run_process(rank, world_size):\n    dst_rank = (rank + 1) % world_size\n    _run_process(rank, dst_rank, world_size)\n    rpc.shutdown()\n\nif __name__ == '__main__':\n  # Run world_size workers\n  world_size = 2\n  mp.spawn(run_process, args=(world_size,), nprocs=world_size)\n\n```\n",
  "url": "https://pytorch.org/docs/stable/rpc/distributed_autograd.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
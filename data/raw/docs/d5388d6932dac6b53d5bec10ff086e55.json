{
  "doc_id": "d5388d6932dac6b53d5bec10ff086e55",
  "source": "pytorch_docs",
  "title": "ExportDB \u2014 PyTorch 2.9 documentation",
  "text": "\n## ExportDB#\n\nExportDB is a centralized dataset of supported and unsupported export cases.\nIt is targeted towards users who want to understand specifically what types of\ncode are supported, the subtleties of export, and how to modify their existing\ncode to be compatible with export. Note that this is not an exhaustive set of\neverything that is supported by exportdb, but it covers the\nmost common and confusing use cases that users will run into.\nIf you have a feature that you think needs a stronger guarantee from us to\nsupport in export please create an issue in the pytorch/pytorch repo with a module:export tag.\nTags\n\n## Supported#\n\n\n## assume_constant_result#\n\nNote\nTags:torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nimport torch._dynamo as torchdynamo\n\n\nclass AssumeConstantResult(torch.nn.Module):\n    \"\"\"\n    Applying `assume_constant_result` decorator to burn make non-tracable code as constant.\n    \"\"\"\n\n    @torchdynamo.assume_constant_result\n    def get_item(self, y):\n        return y.int().item()\n\n    def forward(self, x, y):\n        return x[: self.get_item(y)]\n\nexample_args = (torch.randn(3, 2), torch.tensor(4))\ntags = {\"torch.escape-hatch\"}\nmodel = AssumeConstantResult()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"):\n                 slice_1: \"f32[3, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 4);  x = None\n            return (slice_1,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    slice_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## autograd_function#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass MyAutogradFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        return x.clone()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output + 1\n\nclass AutogradFunction(torch.nn.Module):\n    \"\"\"\n    TorchDynamo does not keep track of backward() on autograd functions. We recommend to\n    use `allow_in_graph` to mitigate this problem.\n    \"\"\"\n\n    def forward(self, x):\n        return MyAutogradFunction.apply(x)\n\nexample_args = (torch.randn(3, 2),)\nmodel = AutogradFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 clone: \"f32[3, 2]\" = torch.ops.aten.clone.default(x);  x = None\n            return (clone,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    clone: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## class_method#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass ClassMethod(torch.nn.Module):\n    \"\"\"\n    Class methods are inlined during tracing.\n    \"\"\"\n\n    @classmethod\n    def method(cls, x):\n        return x + 1\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 2)\n\n    def forward(self, x):\n        x = self.linear(x)\n        return self.method(x) * self.__class__.method(x) * type(self).method(x)\n\nexample_args = (torch.randn(3, 4),)\nmodel = ClassMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, p_linear_weight: \"f32[2, 4]\", p_linear_bias: \"f32[2]\", x: \"f32[3, 4]\"):\n                 linear: \"f32[3, 2]\" = torch.ops.aten.linear.default(x, p_linear_weight, p_linear_bias);  x = p_linear_weight = p_linear_bias = None\n\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(linear, 1)\n            add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(linear, 1)\n\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(add, add_1);  add = add_1 = None\n\n                 add_2: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(linear, 1);  linear = None\n\n                 mul_1: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(mul, add_2);  mul = add_2 = None\n            return (mul_1,)\n\nGraph signature:\n    # inputs\n    p_linear_weight: PARAMETER target='linear.weight'\n    p_linear_bias: PARAMETER target='linear.bias'\n    x: USER_INPUT\n\n    # outputs\n    mul_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_class_method#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass MySubModule(torch.nn.Module):\n    def foo(self, x):\n        return x.cos()\n\n    def forward(self, x):\n        return self.foo(x)\n\nclass CondBranchClassMethod(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n\n    This example demonstrates using class method in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.subm = MySubModule()\n\n    def bar(self, x):\n        return x.sin()\n\n    def forward(self, x):\n        return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchClassMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 sin: \"f32[3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nested_function#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNestedFunction(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n    This example demonstrates using nested function in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        def true_fn(x):\n            def inner_true_fn(y):\n                return x + y\n\n            return inner_true_fn(x)\n\n        def false_fn(x):\n            def inner_false_fn(y):\n                return x - y\n\n            return inner_false_fn(x)\n\n        return cond(x.shape[0] < 10, true_fn, false_fn, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNestedFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 add: \"f32[3]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nonlocal_variables#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNonlocalVariables(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n    - both branches must take the same args, which must also match the branch args passed to cond.\n    - both branches must return a single tensor\n    - returned tensor must have the same tensor metadata, e.g. shape and dtype\n    - branch function can be free function, nested function, lambda, class methods\n    - branch function can not have closure variables\n    - no inplace mutations on inputs or global variables\n\n    This example demonstrates how to rewrite code to avoid capturing closure variables in branch functions.\n\n    The code below will not work because capturing closure variables is not supported.\n    ```\n    my_tensor_var = x + 100\n    my_primitive_var = 3.14\n\n    def true_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y + my_tensor_var + my_primitive_var\n\n    def false_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y - my_tensor_var - my_primitive_var\n\n    return cond(x.shape[0] > 5, true_fn, false_fn, [x])\n    ```\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        my_tensor_var = x + 100\n        my_primitive_var = 3.14\n\n        def true_fn(x, y, z):\n            return x + y + z\n\n        def false_fn(x, y, z):\n            return x - y - z\n\n        return cond(\n            x.shape[0] > 5,\n            true_fn,\n            false_fn,\n            [x, my_tensor_var, torch.tensor(my_primitive_var)],\n        )\n\nexample_args = (torch.randn(6),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNonlocalVariables()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, c_lifted_tensor_0: \"f32[]\", x: \"f32[6]\"):\n                 add: \"f32[6]\" = torch.ops.aten.add.Tensor(x, 100)\n\n                 lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(c_lifted_tensor_0);  c_lifted_tensor_0 = None\n            detach_: \"f32[]\" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None\n\n                 add_1: \"f32[6]\" = torch.ops.aten.add.Tensor(x, add);  x = add = None\n            add_2: \"f32[6]\" = torch.ops.aten.add.Tensor(add_1, detach_);  add_1 = detach_ = None\n            return (add_2,)\n\nGraph signature:\n    # inputs\n    c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0'\n    x: USER_INPUT\n\n    # outputs\n    add_2: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_closed_over_variable#\n\nNote\nTags:python.closure,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondClosedOverVariable(torch.nn.Module):\n    \"\"\"\n    torch.cond() supports branches closed over arbitrary variables.\n    \"\"\"\n\n    def forward(self, pred, x):\n        def true_fn(val):\n            return x * 2\n\n        def false_fn(val):\n            return x - 2\n\n        return cond(pred, true_fn, false_fn, [x + 1])\n\nexample_args = (torch.tensor(True), torch.randn(3, 2))\ntags = {\"torch.cond\", \"python.closure\"}\nmodel = CondClosedOverVariable()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pred: \"b8[]\", x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1);  add = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, (x,));  pred = true_graph_0 = false_graph_0 = x = None\n            getitem: \"f32[3, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, 2);  x = None\n                return (mul,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(x, 2);  x = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    pred: USER_INPUT\n    x: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_operands#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.export import Dim\n\nx = torch.randn(3, 2)\ny = torch.randn(2)\ndim0_x = Dim(\"dim0_x\")\n\nclass CondOperands(torch.nn.Module):\n    \"\"\"\n    The operands passed to cond() must be:\n    - a list of tensors\n    - match arguments of `true_fn` and `false_fn`\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x, y):\n        def true_fn(x, y):\n            return x + y\n\n        def false_fn(x, y):\n            return x - y\n\n        return torch.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n\nexample_args = (x, y)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nextra_inputs = (torch.randn(2, 2), torch.randn(2))\ndynamic_shapes = {\"x\": {0: dim0_x}, \"y\": None}\nmodel = CondOperands()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n             #\n            sym_size_int_1: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0)\n\n                 gt: \"Sym(s77 > 2)\" = sym_size_int_1 > 2;  sym_size_int_1 = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x, y));  gt = true_graph_0 = false_graph_0 = x = y = None\n            getitem: \"f32[s77, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         add: \"f32[s77, 2]\" = torch.ops.aten.add.Tensor(x, y);  x = y = None\n                return (add,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         sub: \"f32[s77, 2]\" = torch.ops.aten.sub.Tensor(x, y);  x = y = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {s77: VR[0, int_oo]}\n\n```\n\n\n## cond_predicate#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondPredicate(torch.nn.Module):\n    \"\"\"\n    The conditional statement (aka predicate) passed to cond() must be one of the following:\n      - torch.Tensor with a single element\n      - boolean expression\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        pred = x.dim() > 2 and x.shape[2] > 10\n\n        return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x])\n\nexample_args = (torch.randn(6, 4, 3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondPredicate()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[6, 4, 3]\"):\n                 sin: \"f32[6, 4, 3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## constrain_as_size_example#\n\nNote\nTags:torch.dynamic-value,torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass ConstrainAsSizeExample(torch.nn.Module):\n    \"\"\"\n    If the value is not known at tracing time, you can provide hint so that we\n    can trace further. Please look at torch._check and torch._check_is_size APIs.\n    torch._check_is_size is used for values that NEED to be used for constructing\n    tensor.\n    \"\"\"\n\n    def forward(self, x):\n        a = x.item()\n        torch._check_is_size(a)\n        torch._check(a <= 5)\n        return torch.zeros((a, 5))\n\n\nexample_args = (torch.tensor(4),)\ntags = {\n    \"torch.dynamic-value\",\n    \"torch.escape-hatch\",\n}\nmodel = ConstrainAsSizeExample()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"i64[]\"):\n                 item: \"Sym(u0)\" = torch.ops.aten.item.default(x);  x = None\n\n             #\n            sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item);  sym_constrain_range_for_size_default = None\n\n                 ge_1: \"Sym(u0 >= 0)\" = item >= 0\n            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\");  ge_1 = _assert_scalar_default = None\n            le_1: \"Sym(u0 <= 5)\" = item <= 5\n            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\");  le_1 = _assert_scalar_default_1 = None\n\n                 zeros: \"f32[u0, 5]\" = torch.ops.aten.zeros.default([item, 5], device = device(type='cpu'), pin_memory = False);  item = None\n            return (zeros,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    zeros: USER_OUTPUT\n\nRange constraints: {u0: VR[0, 5], u1: VR[0, 5]}\n\n```\n\n\n## constrain_as_value_example#\n\nNote\nTags:torch.dynamic-value,torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass ConstrainAsValueExample(torch.nn.Module):\n    \"\"\"\n    If the value is not known at tracing time, you can provide hint so that we\n    can trace further. Please look at torch._check and torch._check_is_size APIs.\n    torch._check is used for values that don't need to be used for constructing\n    tensor.\n    \"\"\"\n\n    def forward(self, x, y):\n        a = x.item()\n        torch._check(a >= 0)\n        torch._check(a <= 5)\n\n        if a < 6:\n            return y.sin()\n        return y.cos()\n\n\nexample_args = (torch.tensor(4), torch.randn(5, 5))\ntags = {\n    \"torch.dynamic-value\",\n    \"torch.escape-hatch\",\n}\nmodel = ConstrainAsValueExample()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"i64[]\", y: \"f32[5, 5]\"):\n                 item: \"Sym(u0)\" = torch.ops.aten.item.default(x);  x = None\n            ge_1: \"Sym(u0 >= 0)\" = item >= 0\n            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\");  ge_1 = _assert_scalar_default = None\n            le_1: \"Sym(u0 <= 5)\" = item <= 5;  item = None\n            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\");  le_1 = _assert_scalar_default_1 = None\n\n                 sin: \"f32[5, 5]\" = torch.ops.aten.sin.default(y);  y = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {u0: VR[0, 5], u1: VR[0, 5]}\n\n```\n\n\n## decorator#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport functools\n\nimport torch\n\ndef test_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        return func(*args, **kwargs) + 1\n\n    return wrapper\n\nclass Decorator(torch.nn.Module):\n    \"\"\"\n    Decorators calls are inlined into the exported function during tracing.\n    \"\"\"\n\n    @test_decorator\n    def forward(self, x, y):\n        return x + y\n\nexample_args = (torch.randn(3, 2), torch.randn(3, 2))\nmodel = Decorator()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", y: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, y);  x = y = None\n\n                 add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(add, 1);  add = None\n            return (add_1,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    add_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dictionary#\n\nNote\nTags:python.data-structure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass Dictionary(torch.nn.Module):\n    \"\"\"\n    Dictionary structures are inlined and flattened along tracing.\n    \"\"\"\n\n    def forward(self, x, y):\n        elements = {}\n        elements[\"x2\"] = x * x\n        y = y * elements[\"x2\"]\n        return {\"y\": y}\n\nexample_args = (torch.randn(3, 2), torch.tensor(4))\ntags = {\"python.data-structure\"}\nmodel = Dictionary()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"):\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, x);  x = None\n\n                 mul_1: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(y, mul);  y = mul = None\n            return (mul_1,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    mul_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_assert#\n\nNote\nTags:python.assert\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeAssert(torch.nn.Module):\n    \"\"\"\n    A basic usage of python assertion.\n    \"\"\"\n\n    def forward(self, x):\n        # assertion with error message\n        assert x.shape[0] > 2, f\"{x.shape[0]} is greater than 2\"\n        # assertion without error message\n        assert x.shape[0] > 1\n        return x\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.assert\"}\nmodel = DynamicShapeAssert()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n            return (x,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    x: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_constructor#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeConstructor(torch.nn.Module):\n    \"\"\"\n    Tensor constructors should be captured with dynamic shape inputs rather\n    than being baked in with static shape.\n    \"\"\"\n\n    def forward(self, x):\n        return torch.zeros(x.shape[0] * 2)\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\"}\nmodel = DynamicShapeConstructor()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 zeros: \"f32[6]\" = torch.ops.aten.zeros.default([6], device = device(type='cpu'), pin_memory = False)\n            return (zeros,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    zeros: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_if_guard#\n\nNote\nTags:torch.dynamic-shape,python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeIfGuard(torch.nn.Module):\n    \"\"\"\n    `if` statement with backed dynamic shape predicate will be specialized into\n    one particular branch and generate a guard. However, export will fail if the\n    the dimension is marked as dynamic shape from higher level API.\n    \"\"\"\n\n    def forward(self, x):\n        if x.shape[0] == 3:\n            return x.cos()\n\n        return x.sin()\n\nexample_args = (torch.randn(3, 2, 2),)\ntags = {\"torch.dynamic-shape\", \"python.control-flow\"}\nmodel = DynamicShapeIfGuard()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2, 2]\"):\n                 cos: \"f32[3, 2, 2]\" = torch.ops.aten.cos.default(x);  x = None\n            return (cos,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    cos: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_map#\n\nNote\nTags:torch.dynamic-shape,torch.map\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import map\n\nclass DynamicShapeMap(torch.nn.Module):\n    \"\"\"\n    functorch map() maps a function over the first tensor dimension.\n    \"\"\"\n\n    def forward(self, xs, y):\n        def body(x, y):\n            return x + y\n\n        return map(body, xs, y)\n\nexample_args = (torch.randn(3, 2), torch.randn(2))\ntags = {\"torch.dynamic-shape\", \"torch.map\"}\nmodel = DynamicShapeMap()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, xs: \"f32[3, 2]\", y: \"f32[2]\"):\n                 body_graph_0 = self.body_graph_0\n            map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y]);  body_graph_0 = xs = y = None\n            getitem: \"f32[3, 2]\" = map_impl[0];  map_impl = None\n            return (getitem,)\n\n        class body_graph_0(torch.nn.Module):\n            def forward(self, xs: \"f32[2]\", y: \"f32[2]\"):\n                         add: \"f32[2]\" = torch.ops.aten.add.Tensor(xs, y);  xs = y = None\n                return (add,)\n\nGraph signature:\n    # inputs\n    xs: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_slicing#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeSlicing(torch.nn.Module):\n    \"\"\"\n    Slices with dynamic shape arguments should be captured into the graph\n    rather than being baked in.\n    \"\"\"\n\n    def forward(self, x):\n        return x[: x.shape[0] - 2, x.shape[1] - 1 :: 2]\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\"}\nmodel = DynamicShapeSlicing()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 slice_1: \"f32[1, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 1);  x = None\n            slice_2: \"f32[1, 1]\" = torch.ops.aten.slice.Tensor(slice_1, 1, 1, 9223372036854775807, 2);  slice_1 = None\n            return (slice_2,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    slice_2: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_view#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeView(torch.nn.Module):\n    \"\"\"\n    Dynamic shapes should be propagated to view arguments instead of being\n    baked into the exported graph.\n    \"\"\"\n\n    def forward(self, x):\n        new_x_shape = x.size()[:-1] + (2, 5)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1)\n\nexample_args = (torch.randn(10, 10),)\ntags = {\"torch.dynamic-shape\"}\nmodel = DynamicShapeView()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[10, 10]\"):\n                 view: \"f32[10, 2, 5]\" = torch.ops.aten.view.default(x, [10, 2, 5]);  x = None\n\n                 permute: \"f32[10, 5, 2]\" = torch.ops.aten.permute.default(view, [0, 2, 1]);  view = None\n            return (permute,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    permute: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## fn_with_kwargs#\n\nNote\nTags:python.data-structure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass FnWithKwargs(torch.nn.Module):\n    \"\"\"\n    Keyword arguments are not supported at the moment.\n    \"\"\"\n\n    def forward(self, pos0, tuple0, *myargs, mykw0, **mykwargs):\n        out = pos0\n        for arg in tuple0:\n            out = out * arg\n        for arg in myargs:\n            out = out * arg\n        out = out * mykw0\n        out = out * mykwargs[\"input0\"] * mykwargs[\"input1\"]\n        return out\n\nexample_args = (\n    torch.randn(4),\n    (torch.randn(4), torch.randn(4)),\n    *[torch.randn(4), torch.randn(4)]\n)\nexample_kwargs = {\n    \"mykw0\": torch.randn(4),\n    \"input0\": torch.randn(4),\n    \"input1\": torch.randn(4),\n}\ntags = {\"python.data-structure\"}\nmodel = FnWithKwargs()\n\n\ntorch.export.export(model, example_args, example_kwargs)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pos0: \"f32[4]\", tuple0_0: \"f32[4]\", tuple0_1: \"f32[4]\", myargs_0: \"f32[4]\", myargs_1: \"f32[4]\", mykw0: \"f32[4]\", input0: \"f32[4]\", input1: \"f32[4]\"):\n                 mul: \"f32[4]\" = torch.ops.aten.mul.Tensor(pos0, tuple0_0);  pos0 = tuple0_0 = None\n            mul_1: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul, tuple0_1);  mul = tuple0_1 = None\n\n                 mul_2: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_1, myargs_0);  mul_1 = myargs_0 = None\n            mul_3: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_2, myargs_1);  mul_2 = myargs_1 = None\n\n                 mul_4: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_3, mykw0);  mul_3 = mykw0 = None\n\n                 mul_5: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_4, input0);  mul_4 = input0 = None\n            mul_6: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_5, input1);  mul_5 = input1 = None\n            return (mul_6,)\n\nGraph signature:\n    # inputs\n    pos0: USER_INPUT\n    tuple0_0: USER_INPUT\n    tuple0_1: USER_INPUT\n    myargs_0: USER_INPUT\n    myargs_1: USER_INPUT\n    mykw0: USER_INPUT\n    input0: USER_INPUT\n    input1: USER_INPUT\n\n    # outputs\n    mul_6: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_contains#\n\nNote\nTags:python.data-structure,torch.dynamic-shape,python.assert\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass ListContains(torch.nn.Module):\n    \"\"\"\n    List containment relation can be checked on a dynamic shape or constants.\n    \"\"\"\n\n    def forward(self, x):\n        assert x.size(-1) in [6, 2]\n        assert x.size(0) not in [4, 5, 6]\n        assert \"monkey\" not in [\"cow\", \"pig\"]\n        return x + x\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"}\nmodel = ListContains()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_unpack#\n\nNote\nTags:python.data-structure,python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\n\nimport torch\n\nclass ListUnpack(torch.nn.Module):\n    \"\"\"\n    Lists are treated as static construct, therefore unpacking should be\n    erased after tracing.\n    \"\"\"\n\n    def forward(self, args: list[torch.Tensor]):\n        \"\"\"\n        Lists are treated as static construct, therefore unpacking should be\n        erased after tracing.\n        \"\"\"\n        x, *y = args\n        return x + y[0]\n\nexample_args = ([torch.randn(3, 2), torch.tensor(4), torch.tensor(5)],)\ntags = {\"python.control-flow\", \"python.data-structure\"}\nmodel = ListUnpack()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, args_0: \"f32[3, 2]\", args_1: \"i64[]\", args_2: \"i64[]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(args_0, args_1);  args_0 = args_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    args_0: USER_INPUT\n    args_1: USER_INPUT\n    args_2: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## nested_function#\n\nNote\nTags:python.closure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass NestedFunction(torch.nn.Module):\n    \"\"\"\n    Nested functions are traced through. Side effects on global captures\n    are not supported though.\n    \"\"\"\n\n    def forward(self, a, b):\n        x = a + b\n        z = a - b\n\n        def closure(y):\n            nonlocal x\n            x += 1\n            return x * y + z\n\n        return closure(x)\n\nexample_args = (torch.randn(3, 2), torch.randn(2))\ntags = {\"python.closure\"}\nmodel = NestedFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, a: \"f32[3, 2]\", b: \"f32[2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(a, b)\n\n                 sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(a, b);  a = b = None\n\n                 add_: \"f32[3, 2]\" = torch.ops.aten.add_.Tensor(add, 1);  add = None\n\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(add_, add_);  add_ = None\n            add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(mul, sub);  mul = sub = None\n            return (add_1,)\n\nGraph signature:\n    # inputs\n    a: USER_INPUT\n    b: USER_INPUT\n\n    # outputs\n    add_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## null_context_manager#\n\nNote\nTags:python.context-manager\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport contextlib\n\nimport torch\n\nclass NullContextManager(torch.nn.Module):\n    \"\"\"\n    Null context manager in Python will be traced out.\n    \"\"\"\n\n    def forward(self, x):\n        \"\"\"\n        Null context manager in Python will be traced out.\n        \"\"\"\n        ctx = contextlib.nullcontext()\n        with ctx:\n            return x.sin() + x.cos()\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.context-manager\"}\nmodel = NullContextManager()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 sin: \"f32[3, 2]\" = torch.ops.aten.sin.default(x)\n            cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(x);  x = None\n            add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(sin, cos);  sin = cos = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## pytree_flatten#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.utils import _pytree as pytree\n\nclass PytreeFlatten(torch.nn.Module):\n    \"\"\"\n    Pytree from PyTorch can be captured by TorchDynamo.\n    \"\"\"\n\n    def forward(self, x):\n        y, _spec = pytree.tree_flatten(x)\n        return y[0] + 1\n\nexample_args = ({1: torch.randn(3, 2), 2: torch.randn(3, 2)},),\nmodel = PytreeFlatten()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x_0_1: \"f32[3, 2]\", x_0_2: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x_0_1, 1);  x_0_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x_0_1: USER_INPUT\n    x_0_2: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## scalar_output#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.export import Dim\n\nx = torch.randn(3, 2)\ndim1_x = Dim(\"dim1_x\")\n\nclass ScalarOutput(torch.nn.Module):\n    \"\"\"\n    Returning scalar values from the graph is supported, in addition to Tensor\n    outputs. Symbolic shapes are captured and rank is specialized.\n    \"\"\"\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x):\n        return x.shape[1] + 1\n\nexample_args = (x,)\ntags = {\"torch.dynamic-shape\"}\ndynamic_shapes = {\"x\": {1: dim1_x}}\nmodel = ScalarOutput()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, s27]\"):\n             #\n            sym_size_int_1: \"Sym(s27)\" = torch.ops.aten.sym_size.int(x, 1);  x = None\n\n                 add: \"Sym(s27 + 1)\" = sym_size_int_1 + 1;  sym_size_int_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {s27: VR[0, int_oo]}\n\n```\n\n\n## specialized_attribute#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nfrom enum import Enum\n\nimport torch\n\nclass Animal(Enum):\n    COW = \"moo\"\n\nclass SpecializedAttribute(torch.nn.Module):\n    \"\"\"\n    Model attributes are specialized.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.a = \"moo\"\n        self.b = 4\n\n    def forward(self, x):\n        if self.a == Animal.COW.value:\n            return x * x + self.b\n        else:\n            raise ValueError(\"bad\")\n\nexample_args = (torch.randn(3, 2),)\nmodel = SpecializedAttribute()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, x);  x = None\n            add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(mul, 4);  mul = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## static_for_loop#\n\nNote\nTags:python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass StaticForLoop(torch.nn.Module):\n    \"\"\"\n    A for loop with constant number of iterations should be unrolled in the exported graph.\n    \"\"\"\n\n    def forward(self, x):\n        # constant\n        ret = [i + x for i in range(10)]\n        return ret\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.control-flow\"}\nmodel = StaticForLoop()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 0)\n            add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1)\n            add_2: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 2)\n            add_3: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 3)\n            add_4: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4)\n            add_5: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 5)\n            add_6: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 6)\n            add_7: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 7)\n            add_8: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 8)\n            add_9: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 9);  x = None\n            return (add, add_1, add_2, add_3, add_4, add_5, add_6, add_7, add_8, add_9)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n    add_1: USER_OUTPUT\n    add_2: USER_OUTPUT\n    add_3: USER_OUTPUT\n    add_4: USER_OUTPUT\n    add_5: USER_OUTPUT\n    add_6: USER_OUTPUT\n    add_7: USER_OUTPUT\n    add_8: USER_OUTPUT\n    add_9: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## static_if#\n\nNote\nTags:python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass StaticIf(torch.nn.Module):\n    \"\"\"\n    `if` statement with static predicate value should be traced through with the\n    taken branch.\n    \"\"\"\n\n    def forward(self, x):\n        if len(x.shape) == 3:\n            return x + torch.ones(1, 1, 1)\n\n        return x\n\nexample_args = (torch.randn(3, 2, 2),)\ntags = {\"python.control-flow\"}\nmodel = StaticIf()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2, 2]\"):\n                 ones: \"f32[1, 1, 1]\" = torch.ops.aten.ones.default([1, 1, 1], device = device(type='cpu'), pin_memory = False)\n            add: \"f32[3, 2, 2]\" = torch.ops.aten.add.Tensor(x, ones);  x = ones = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## tensor_setattr#\n\nNote\nTags:python.builtin\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass TensorSetattr(torch.nn.Module):\n    \"\"\"\n    setattr() call onto tensors is not supported.\n    \"\"\"\n    def forward(self, x, attr):\n        setattr(x, attr, torch.randn(3, 2))\n        return x + 4\n\nexample_args = (torch.randn(3, 2), \"attr\")\ntags = {\"python.builtin\"}\nmodel = TensorSetattr()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", attr):\n                 randn: \"f32[3, 2]\" = torch.ops.aten.randn.default([3, 2], device = device(type='cpu'), pin_memory = False);  randn = None\n\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    attr: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## type_reflection_method#\n\nNote\nTags:python.builtin\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass A:\n    @classmethod\n    def func(cls, x):\n        return 1 + x\n\nclass TypeReflectionMethod(torch.nn.Module):\n    \"\"\"\n    type() calls on custom objects followed by attribute accesses are not allowed\n    due to its overly dynamic nature.\n    \"\"\"\n\n    def forward(self, x):\n        a = A()\n        return type(a).func(x)\n\n\nexample_args = (torch.randn(3, 4),)\ntags = {\"python.builtin\"}\nmodel = TypeReflectionMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 4]\"):\n                 add: \"f32[3, 4]\" = torch.ops.aten.add.Tensor(x, 1);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## user_input_mutation#\n\nNote\nTags:torch.mutation\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass UserInputMutation(torch.nn.Module):\n    \"\"\"\n    Directly mutate user input in forward\n    \"\"\"\n\n    def forward(self, x):\n        x.mul_(2)\n        return x.cos()\n\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.mutation\"}\nmodel = UserInputMutation()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 mul_: \"f32[3, 2]\" = torch.ops.aten.mul_.Tensor(x, 2);  x = None\n\n                 cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(mul_);  mul_ = None\n            return (cos,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    cos: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## Not Supported Yet#\n\n\n## dynamic_shape_round#\n\nNote\nTags:python.builtin,torch.dynamic-shape\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch._export.db.case import SupportLevel\nfrom torch.export import Dim\n\nclass DynamicShapeRound(torch.nn.Module):\n    \"\"\"\n    Calling round on dynamic shapes is not supported.\n    \"\"\"\n\n    def forward(self, x):\n        return x[: round(x.shape[0] / 2)]\n\nx = torch.randn(3, 2)\ndim0_x = Dim(\"dim0_x\")\nexample_args = (x,)\ntags = {\"torch.dynamic-shape\", \"python.builtin\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\ndynamic_shapes = {\"x\": {0: dim0_x}}\nmodel = DynamicShapeRound()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nUnsupported: Constraints violated (dim0_x)! For more information, run with TORCH_LOGS=\"+dynamic\".\n\n```\n\n\n## model_attr_mutation#\n\nNote\nTags:python.object-model\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nfrom torch._export.db.case import SupportLevel\n\n\nclass ModelAttrMutation(torch.nn.Module):\n    \"\"\"\n    Attribute mutation is not supported.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.attr_list = [torch.randn(3, 2), torch.randn(3, 2)]\n\n    def recreate_list(self):\n        return [torch.zeros(3, 2), torch.zeros(3, 2)]\n\n    def forward(self, x):\n        self.attr_list = self.recreate_list()\n        return x.sum() + self.attr_list[0].sum()\n\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.object-model\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\nmodel = ModelAttrMutation()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nAssertionError: Mutating module attribute attr_list during export.\n\n```\n\n\n## optional_input#\n\nNote\nTags:python.object-model\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nfrom torch._export.db.case import SupportLevel\n\n\nclass OptionalInput(torch.nn.Module):\n    \"\"\"\n    Tracing through optional input is not supported yet\n    \"\"\"\n\n    def forward(self, x, y=torch.randn(2, 3)):\n        if y is not None:\n            return x + y\n        return x\n\n\nexample_args = (torch.randn(2, 3),)\ntags = {\"python.object-model\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\nmodel = OptionalInput()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nUnsupported: Tracing through optional input is not supported yet\n\n```\n\n\n## unsupported_operator#\n\nNote\nTags:torch.operator\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nfrom torch._export.db.case import SupportLevel\n\n\nclass TorchSymMin(torch.nn.Module):\n    \"\"\"\n    torch.sym_min operator is not supported in export.\n    \"\"\"\n\n    def forward(self, x):\n        return x.sum() + torch.sym_min(x.size(0), 100)\n\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.operator\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\nmodel = TorchSymMin()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nUnsupported: torch.* op returned non-Tensor\n\n```\n",
  "url": "https://pytorch.org/docs/stable/generated/exportdb/index.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
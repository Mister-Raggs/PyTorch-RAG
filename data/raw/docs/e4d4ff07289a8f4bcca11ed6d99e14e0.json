{
  "doc_id": "e4d4ff07289a8f4bcca11ed6d99e14e0",
  "source": "pytorch_docs",
  "title": "Dealing with Recompilations \u2014 PyTorch 2.9 documentation",
  "text": "\n## Dealing with Recompilations#\n\nCreated On: Jul 29, 2025 | Last Updated On: Jul 29, 2025\nRecompilations are necessary fortorch.compilesoundness, but can result in significantly increased compile time.\nThus, minimizing recompilations while preserving soundness is essential for reducing compile time.\ntorch.compile\nYou can view recompilations and their reasons using tlparse orTORCH_LOGS=recompiles.\nTORCH_LOGS=recompiles\n\n## Is Dynamic Shapes Enabled?#\n\nIn the below example, we recompile due to mismatched shapes:\n\n```python\n@torch.compile\ndef fn(x):\n    return x + 1\nfn(torch.ones(3))\nfn(torch.ones(4))\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/2479206322.py:1\n    triggered by the following guard failure(s):\n    - 0/0: tensor 'x' size mismatch at index 0. expected 3, actual 4\n\n```\n\n\n```python\ntensor([2., 2., 2., 2.])\n\n```\n\nMake sure that the dynamic option oftorch.compileis not set toFalse.\nThe default option,dynamic=None, will only attempt dynamic shapes after the first compilation.\nYou can setdynamic=Trueto upfront compile as dynamic as possible:\ntorch.compile\nFalse\ndynamic=None\ndynamic=True\n\n```python\n@torch.compile(dynamic=True)\ndef gn(x):\n    return x + 1\ngn(torch.ones(3))\ngn(torch.ones(4))\n\n```\n\n\n```python\ntensor([2., 2., 2., 2.])\n\n```\n\nFor more information on dynamic shapes, including dealing with errors/recompilations due to\ndynamic shapes, seethe dynamic shapes manual.\n\n## Wrapping Constants with Tensors#\n\nBy default,int/floatvariables are treated as constants and are guarded on their exact value.\nIn the below example, we have a recompilation for each function call.\nint\nfloat\n\n```python\n@torch.compile\ndef fn(x, c):\n    return x + c\nfor i in range(5):\n    fn(torch.ones(i), 0.5 + i)\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/3647755280.py:1\n    triggered by the following guard failure(s):\n    - 2/0: c == 0.5                                                 # return x + c  # mp/ipykernel_507/3647755280.py:3 in fn\nRecompiling function fn in /tmp/ipykernel_507/3647755280.py:1\n    triggered by the following guard failure(s):\n    - 2/1: tensor 'x' size mismatch at index 0. expected 1, actual 2\n    - 2/0: c == 0.5                                                 # return x + c  # mp/ipykernel_507/3647755280.py:3 in fn\n\n```\n\nIn particular, for LR schedulers, initializing with a constant can lead to recompilations:\n\n```python\nmod = torch.nn.Linear(3, 3)\nopt = torch.optim.Adam(mod.parameters(), lr=0.01)\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, 0.9)\n@torch.compile\ndef gn(inp):\n    opt.zero_grad(True)\n    out = mod(inp).sum()\n    out.backward()\n    opt.step()\n    sched.step()\nfor i in range(5):\n    gn(torch.ones(3, 3))\n\n```\n\n\n```python\nProfiler function <class 'torch.autograd.profiler.record_function'> will be ignored\nRecompiling function step in /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/optim/adam.py:213\n    triggered by the following guard failure(s):\n    - 7/0: self.param_groups[0]['lr'] == 0.01                       # for group in self.param_groups:  # optim/adam.py:228 in step\n\n```\n\nIn both examples, we can wrapfloatvariables in tensors in order to prevent recompilations.\nfloat\n\n```python\n# first example\nfor i in range(5):\n    fn(torch.ones(i), torch.tensor(0.5 + i))\n# second example\nopt = torch.optim.Adam(mod.parameters(), lr=torch.tensor(0.01))\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, torch.tensor(0.9))\nfor i in range(5):\n    gn(torch.ones(3, 3))\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/3647755280.py:1\n    triggered by the following guard failure(s):\n    - 0/0: tensor 'x' size mismatch at index 0. expected 0, actual 1\nRecompiling function fn in /tmp/ipykernel_507/3647755280.py:1\n    triggered by the following guard failure(s):\n    - 0/1: tensor 'x' size mismatch at index 0. expected 1, actual 2\n    - 0/0: tensor 'x' size mismatch at index 0. expected 0, actual 2\n\n```\n\n\n## Changing the Cache Size Limit#\n\nThere is a limit to how many times a function can be recompiled,\ndetermined bytorch._dynamo.config.cache_size_limitandtorch._dynamo.config.accumulated_cache_size_limit(The exact difference between these 2 values is detailed intorch/_dynamo/cache_size.py).\nIf the Dynamo cache limit is hit, then all future compilation attemptswill result in the function being skipped (run eagerly).\nDynamo will still attempt to use previously compiled bytecode for future function calls, if the guards pass.\nNote that in the case of a recompilation limit hit,all nested function calls WILL be skipped(Dynamo will try to use previously compiled bytecode for the nested functions).\nDynamo will also issue a warning containing the affected function and which limit was hit.\nIn the example below, each function call results in a recompile attempt.\nWhen we hit the cache size limit (by default, 8), we stop attempting to recompile.\n(Note that we setdynamic=Falsefor demonstration purposes to force recompilation every time).\ntorch._dynamo.config.cache_size_limit\ntorch._dynamo.config.accumulated_cache_size_limit\ntorch/_dynamo/cache_size.py\ndynamic=False\n\n```python\n@torch.compile(dynamic=False)\ndef fn(x):\n    return x + 1\nfor i in range(1, 10):\n    # recompile every time due to dynamic=False\n    fn(torch.ones(i))\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 2\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 3\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 3\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 4\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 4\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 4\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 5\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 5\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 5\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 5\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/4: tensor 'x' size mismatch at index 0. expected 5, actual 6\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 6\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 6\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 6\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 6\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/5: tensor 'x' size mismatch at index 0. expected 6, actual 7\n    - 8/4: tensor 'x' size mismatch at index 0. expected 5, actual 7\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 7\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 7\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 7\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 7\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/6: tensor 'x' size mismatch at index 0. expected 7, actual 8\n    - 8/5: tensor 'x' size mismatch at index 0. expected 6, actual 8\n    - 8/4: tensor 'x' size mismatch at index 0. expected 5, actual 8\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 8\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 8\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 8\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 8\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/7: tensor 'x' size mismatch at index 0. expected 8, actual 9\n    - 8/6: tensor 'x' size mismatch at index 0. expected 7, actual 9\n    - 8/5: tensor 'x' size mismatch at index 0. expected 6, actual 9\n    - 8/4: tensor 'x' size mismatch at index 0. expected 5, actual 9\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 9\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 9\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 9\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 9\ntorch._dynamo hit config.recompile_limit (8)\n   function: 'fn' (/tmp/ipykernel_507/3054308037.py:1)\n   last reason: 8/7: tensor 'x' size mismatch at index 0. expected 8, actual 9\nTo log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\nTo diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n\n```\n\nIf you know that the number of recompilations has a reasonable constant upper bound, you can raise the cache size limit.\nIf the cost of recompilation outweighs the benefit of compilation, then you can consider lowering the cache size limit.\n\n```python\ntorch._dynamo.config.cache_size_limit = 16\n@torch.compile(dynamic=False)\ndef gn(x):\n    return x + 1\nfor i in range(1, 10):\n    gn(torch.ones(i))\n\n```\n\n\n```python\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 2\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 3\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 3\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 4\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 4\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 4\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 5\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 5\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 5\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 5\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/4: tensor 'x' size mismatch at index 0. expected 5, actual 6\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 6\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 6\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 6\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 6\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/5: tensor 'x' size mismatch at index 0. expected 6, actual 7\n    - 9/4: tensor 'x' size mismatch at index 0. expected 5, actual 7\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 7\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 7\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 7\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 7\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/6: tensor 'x' size mismatch at index 0. expected 7, actual 8\n    - 9/5: tensor 'x' size mismatch at index 0. expected 6, actual 8\n    - 9/4: tensor 'x' size mismatch at index 0. expected 5, actual 8\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 8\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 8\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 8\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 8\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/7: tensor 'x' size mismatch at index 0. expected 8, actual 9\n    - 9/6: tensor 'x' size mismatch at index 0. expected 7, actual 9\n    - 9/5: tensor 'x' size mismatch at index 0. expected 6, actual 9\n    - 9/4: tensor 'x' size mismatch at index 0. expected 5, actual 9\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 9\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 9\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 9\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 9\n\n```\n\n\n## Graph Breaking to Reduce Recompilation Costs#\n\nIf a large graph is recompiling and causing high compile time, you can intentionally introduce\na graph break in order to reduce recompilation costs, at the expense of introducing a performance hit.\n\n```python\ndef very_large_function(x):\n    return x + 1\n\n@torch.compile(dynamic=False)\ndef fn(x, c):\n    y = very_large_function(x)  # recompiled every time\n    return y + c\n\nfor i in range(1, 5):\n    fn(torch.ones(3), i)\n\n@torch.compile(dynamic=False)\ndef gn(x, c):\n    y = very_large_function(x)  # compiled only once\n    torch._dynamo.graph_break()\n    return y + c  # recompiled every time\n\nfor i in range(1, 5):\n    gn(torch.ones(3), i)\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/2876112129.py:4\n    triggered by the following guard failure(s):\n    - 10/0: c == 1                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\nRecompiling function fn in /tmp/ipykernel_507/2876112129.py:4\n    triggered by the following guard failure(s):\n    - 10/1: c == 2                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\n    - 10/0: c == 1                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\nRecompiling function fn in /tmp/ipykernel_507/2876112129.py:4\n    triggered by the following guard failure(s):\n    - 10/2: c == 3                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\n    - 10/1: c == 2                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\n    - 10/0: c == 1                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\nRecompiling function torch_dynamo_resume_in_gn_at_15 in /tmp/ipykernel_507/2876112129.py:15\n    triggered by the following guard failure(s):\n    - 12/0: c == 1                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\nRecompiling function torch_dynamo_resume_in_gn_at_15 in /tmp/ipykernel_507/2876112129.py:15\n    triggered by the following guard failure(s):\n    - 12/1: c == 2                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\n    - 12/0: c == 1                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\nRecompiling function torch_dynamo_resume_in_gn_at_15 in /tmp/ipykernel_507/2876112129.py:15\n    triggered by the following guard failure(s):\n    - 12/2: c == 3                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\n    - 12/1: c == 2                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\n    - 12/0: c == 1                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\n\n```\n",
  "url": "https://pytorch.org/docs/stable/compile/programming_model.recompilation.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
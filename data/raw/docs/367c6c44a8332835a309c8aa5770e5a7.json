{
  "doc_id": "367c6c44a8332835a309c8aa5770e5a7",
  "source": "pytorch_docs",
  "title": "torch.nn.functional \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.nn.functional#\n\nCreated On: Jun 11, 2019 | Last Updated On: Mar 25, 2024\n\n## Convolution functions#\n\nconv1d\n\nconv1d\nApplies a 1D convolution over an input signal composed of several input planes.\nconv2d\n\nconv2d\nApplies a 2D convolution over an input image composed of several input planes.\nconv3d\n\nconv3d\nApplies a 3D convolution over an input image composed of several input planes.\nconv_transpose1d\n\nconv_transpose1d\nApplies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \"deconvolution\".\nconv_transpose2d\n\nconv_transpose2d\nApplies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\".\nconv_transpose3d\n\nconv_transpose3d\nApplies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\"\nunfold\n\nunfold\nExtract sliding local blocks from a batched input tensor.\nfold\n\nfold\nCombine an array of sliding local blocks into a large containing tensor.\n\n## Pooling functions#\n\navg_pool1d\n\navg_pool1d\nApplies a 1D average pooling over an input signal composed of several input planes.\navg_pool2d\n\navg_pool2d\nApplies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.\navg_pool3d\n\navg_pool3d\nApplies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.\nmax_pool1d\n\nmax_pool1d\nApplies a 1D max pooling over an input signal composed of several input planes.\nmax_pool2d\n\nmax_pool2d\nApplies a 2D max pooling over an input signal composed of several input planes.\nmax_pool3d\n\nmax_pool3d\nApplies a 3D max pooling over an input signal composed of several input planes.\nmax_unpool1d\n\nmax_unpool1d\nCompute a partial inverse ofMaxPool1d.\nMaxPool1d\nmax_unpool2d\n\nmax_unpool2d\nCompute a partial inverse ofMaxPool2d.\nMaxPool2d\nmax_unpool3d\n\nmax_unpool3d\nCompute a partial inverse ofMaxPool3d.\nMaxPool3d\nlp_pool1d\n\nlp_pool1d\nApply a 1D power-average pooling over an input signal composed of several input planes.\nlp_pool2d\n\nlp_pool2d\nApply a 2D power-average pooling over an input signal composed of several input planes.\nlp_pool3d\n\nlp_pool3d\nApply a 3D power-average pooling over an input signal composed of several input planes.\nadaptive_max_pool1d\n\nadaptive_max_pool1d\nApplies a 1D adaptive max pooling over an input signal composed of several input planes.\nadaptive_max_pool2d\n\nadaptive_max_pool2d\nApplies a 2D adaptive max pooling over an input signal composed of several input planes.\nadaptive_max_pool3d\n\nadaptive_max_pool3d\nApplies a 3D adaptive max pooling over an input signal composed of several input planes.\nadaptive_avg_pool1d\n\nadaptive_avg_pool1d\nApplies a 1D adaptive average pooling over an input signal composed of several input planes.\nadaptive_avg_pool2d\n\nadaptive_avg_pool2d\nApply a 2D adaptive average pooling over an input signal composed of several input planes.\nadaptive_avg_pool3d\n\nadaptive_avg_pool3d\nApply a 3D adaptive average pooling over an input signal composed of several input planes.\nfractional_max_pool2d\n\nfractional_max_pool2d\nApplies 2D fractional max pooling over an input signal composed of several input planes.\nfractional_max_pool3d\n\nfractional_max_pool3d\nApplies 3D fractional max pooling over an input signal composed of several input planes.\n\n## Attention Mechanisms#\n\nThetorch.nn.attention.biasmodule contains attention_biases that are designed to be used with\nscaled_dot_product_attention.\ntorch.nn.attention.bias\nscaled_dot_product_attention\n\nscaled_dot_product_attention\nscaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n\n## Non-linear activation functions#\n\nthreshold\n\nthreshold\nApply a threshold to each element of the input Tensor.\nthreshold_\n\nthreshold_\nIn-place version ofthreshold().\nthreshold()\nrelu\n\nrelu\nApplies the rectified linear unit function element-wise.\nrelu_\n\nrelu_\nIn-place version ofrelu().\nrelu()\nhardtanh\n\nhardtanh\nApplies the HardTanh function element-wise.\nhardtanh_\n\nhardtanh_\nIn-place version ofhardtanh().\nhardtanh()\nhardswish\n\nhardswish\nApply hardswish function, element-wise.\nrelu6\n\nrelu6\nApplies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).\nelu\n\nelu\nApply the Exponential Linear Unit (ELU) function element-wise.\nelu_\n\nelu_\nIn-place version ofelu().\nelu()\nselu\n\nselu\nApplies element-wise,SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717andscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.\ncelu\n\ncelu\nApplies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).\nleaky_relu\n\nleaky_relu\nApplies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)\nleaky_relu_\n\nleaky_relu_\nIn-place version ofleaky_relu().\nleaky_relu()\nprelu\n\nprelu\nApplies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.\nrrelu\n\nrrelu\nRandomized leaky ReLU.\nrrelu_\n\nrrelu_\nIn-place version ofrrelu().\nrrelu()\nglu\n\nglu\nThe gated linear unit.\ngelu\n\ngelu\nWhen the approximate argument is 'none', it applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)\nlogsigmoid\n\nlogsigmoid\nApplies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)\nhardshrink\n\nhardshrink\nApplies the hard shrinkage function element-wise\ntanhshrink\n\ntanhshrink\nApplies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)\nsoftsign\n\nsoftsign\nApplies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b\nsoftplus\n\nsoftplus\nApplies element-wise, the functionSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).\nsoftmin\n\nsoftmin\nApply a softmin function.\nsoftmax\n\nsoftmax\nApply a softmax function.\nsoftshrink\n\nsoftshrink\nApplies the soft shrinkage function elementwise\ngumbel_softmax\n\ngumbel_softmax\nSample from the Gumbel-Softmax distribution (Link 1Link 2) and optionally discretize.\nlog_softmax\n\nlog_softmax\nApply a softmax followed by a logarithm.\ntanh\n\ntanh\nApplies element-wise,Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b\nsigmoid\n\nsigmoid\nApplies the element-wise functionSigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b\nhardsigmoid\n\nhardsigmoid\nApply the Hardsigmoid function element-wise.\nsilu\n\nsilu\nApply the Sigmoid Linear Unit (SiLU) function, element-wise.\nmish\n\nmish\nApply the Mish function, element-wise.\nbatch_norm\n\nbatch_norm\nApply Batch Normalization for each channel across a batch of data.\ngroup_norm\n\ngroup_norm\nApply Group Normalization for last certain number of dimensions.\ninstance_norm\n\ninstance_norm\nApply Instance Normalization independently for each channel in every data sample within a batch.\nlayer_norm\n\nlayer_norm\nApply Layer Normalization for last certain number of dimensions.\nlocal_response_norm\n\nlocal_response_norm\nApply local response normalization over an input signal.\nrms_norm\n\nrms_norm\nApply Root Mean Square Layer Normalization.\nnormalize\n\nnormalize\nPerformLpL_pLp\u200bnormalization of inputs over specified dimension.\n\n## Linear functions#\n\nlinear\n\nlinear\nApplies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b.\nbilinear\n\nbilinear\nApplies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b\n\n## Dropout functions#\n\ndropout\n\ndropout\nDuring training, randomly zeroes some elements of the input tensor with probabilityp.\np\nalpha_dropout\n\nalpha_dropout\nApply alpha dropout to the input.\nfeature_alpha_dropout\n\nfeature_alpha_dropout\nRandomly masks out entire channels (a channel is a feature map).\ndropout1d\n\ndropout1d\nRandomly zero out entire channels (a channel is a 1D feature map).\ndropout2d\n\ndropout2d\nRandomly zero out entire channels (a channel is a 2D feature map).\ndropout3d\n\ndropout3d\nRandomly zero out entire channels (a channel is a 3D feature map).\n\n## Sparse functions#\n\nembedding\n\nembedding\nGenerate a simple lookup table that looks up embeddings in a fixed dictionary and size.\nembedding_bag\n\nembedding_bag\nCompute sums, means or maxes ofbagsof embeddings.\none_hot\n\none_hot\nTakes LongTensor with index values of shape(*)and returns a tensor of shape(*,num_classes)that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.\n(*)\n(*,num_classes)\n\n## Distance functions#\n\npairwise_distance\n\npairwise_distance\nSeetorch.nn.PairwiseDistancefor details\ntorch.nn.PairwiseDistance\ncosine_similarity\n\ncosine_similarity\nReturns cosine similarity betweenx1andx2, computed along dim.\nx1\nx2\npdist\n\npdist\nComputes the p-norm distance between every pair of row vectors in the input.\n\n## Loss functions#\n\nbinary_cross_entropy\n\nbinary_cross_entropy\nCompute Binary Cross Entropy between the target and input probabilities.\nbinary_cross_entropy_with_logits\n\nbinary_cross_entropy_with_logits\nCompute Binary Cross Entropy between target and input logits.\npoisson_nll_loss\n\npoisson_nll_loss\nCompute the Poisson negative log likelihood loss.\ncosine_embedding_loss\n\ncosine_embedding_loss\nCompute the cosine embedding loss.\ncross_entropy\n\ncross_entropy\nCompute the cross entropy loss between input logits and target.\nctc_loss\n\nctc_loss\nCompute the Connectionist Temporal Classification loss.\ngaussian_nll_loss\n\ngaussian_nll_loss\nCompute the Gaussian negative log likelihood loss.\nhinge_embedding_loss\n\nhinge_embedding_loss\nCompute the hinge embedding loss.\nkl_div\n\nkl_div\nCompute the KL Divergence loss.\nl1_loss\n\nl1_loss\nCompute the L1 loss, with optional weighting.\nmse_loss\n\nmse_loss\nCompute the element-wise mean squared error, with optional weighting.\nmargin_ranking_loss\n\nmargin_ranking_loss\nCompute the margin ranking loss.\nmultilabel_margin_loss\n\nmultilabel_margin_loss\nCompute the multilabel margin loss.\nmultilabel_soft_margin_loss\n\nmultilabel_soft_margin_loss\nCompute the multilabel soft margin loss.\nmulti_margin_loss\n\nmulti_margin_loss\nCompute the multi margin loss, with optional weighting.\nnll_loss\n\nnll_loss\nCompute the negative log likelihood loss.\nhuber_loss\n\nhuber_loss\nCompute the Huber loss, with optional weighting.\nsmooth_l1_loss\n\nsmooth_l1_loss\nCompute the Smooth L1 loss.\nsoft_margin_loss\n\nsoft_margin_loss\nCompute the soft margin loss.\ntriplet_margin_loss\n\ntriplet_margin_loss\nCompute the triplet loss between given input tensors and a margin greater than 0.\ntriplet_margin_with_distance_loss\n\ntriplet_margin_with_distance_loss\nCompute the triplet margin loss for input tensors using a custom distance function.\n\n## Vision functions#\n\npixel_shuffle\n\npixel_shuffle\nRearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor.\nupscale_factor\npixel_unshuffle\n\npixel_unshuffle\nReverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor.\nPixelShuffle\ndownscale_factor\npad\n\npad\nPads tensor.\ninterpolate\n\ninterpolate\nDown/up samples the input.\nupsample\n\nupsample\nUpsample input.\nupsample_nearest\n\nupsample_nearest\nUpsamples the input, using nearest neighbours' pixel values.\nupsample_bilinear\n\nupsample_bilinear\nUpsamples the input, using bilinear upsampling.\ngrid_sample\n\ngrid_sample\nCompute grid sample.\naffine_grid\n\naffine_grid\nGenerate 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.\ntheta\n\n## DataParallel functions (multi-GPU, distributed)#\n\n\n## data_parallel#\n\ntorch.nn.parallel.data_parallel\ntorch.nn.parallel.data_parallel\nEvaluate module(input) in parallel across the GPUs given in device_ids.",
  "url": "https://pytorch.org/docs/stable/nn.functional.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
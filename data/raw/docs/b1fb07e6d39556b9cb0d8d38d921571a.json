{
  "doc_id": "b1fb07e6d39556b9cb0d8d38d921571a",
  "source": "pytorch_docs",
  "title": "CUDA Environment Variables \u2014 PyTorch 2.9 documentation",
  "text": "\n## CUDA Environment Variables#\n\nCreated On: Feb 15, 2024 | Last Updated On: Feb 15, 2024\nFor more information on CUDA runtime environment variables, seeCUDA Environment Variables.\nPyTorch Environment Variables\nVariable\nDescription\nPYTORCH_NO_CUDA_MEMORY_CACHING\nPYTORCH_NO_CUDA_MEMORY_CACHING\nIf set to1, disables caching of memory allocations in CUDA. This can be useful for debugging.\n1\nPYTORCH_CUDA_ALLOC_CONF\nPYTORCH_CUDA_ALLOC_CONF\nFor a more in depth explanation of this environment variable, seeMemory management.\nPYTORCH_NVML_BASED_CUDA_CHECK\nPYTORCH_NVML_BASED_CUDA_CHECK\nIf set to1, before importing PyTorch modules that check if CUDA is available, PyTorch will use NVML to check if the CUDA driver is functional instead of using the CUDA runtime. This can be helpful if forked processes fail with a CUDA initialization error.\n1\nTORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\nTORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\nThe cache limit for the cuDNN v8 API. This is used to limit the memory used by the cuDNN v8 API. The default value is 10000, which roughly corresponds to 2GiB assuming 200KiB per ExecutionPlan. Set to0for no limit or a negative value for no caching.\n0\nTORCH_CUDNN_V8_API_DISABLED\nTORCH_CUDNN_V8_API_DISABLED\nIf set to1, disables the cuDNN v8 API. And will fall back to the cuDNN v7 API.\n1\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE\nIf set to1, forces TF32 enablement, overridesset_float32_matmul_precisionsetting.\n1\nset_float32_matmul_precision\nTORCH_NCCL_USE_COMM_NONBLOCKING\nTORCH_NCCL_USE_COMM_NONBLOCKING\nIf set to1, enables non-blocking error handling in NCCL.\n1\nTORCH_NCCL_AVOID_RECORD_STREAMS\nTORCH_NCCL_AVOID_RECORD_STREAMS\nIf set to0, enables fallback to record streams-based synchronization behavior in NCCL.\n0\nTORCH_CUDNN_V8_API_DEBUG\nTORCH_CUDNN_V8_API_DEBUG\nIf set to1, sanity check whether cuDNN V8 is being used.\n1\nCUDA Runtime and Libraries Environment Variables\nVariable\nDescription\nCUDA_VISIBLE_DEVICES\nCUDA_VISIBLE_DEVICES\nComma-separated list of GPU device IDs that should be made available to CUDA runtime. If set to-1, no GPUs are made available.\n-1\nCUDA_LAUNCH_BLOCKING\nCUDA_LAUNCH_BLOCKING\nIf set to1, makes CUDA calls synchronous. This can be useful for debugging.\n1\nCUBLAS_WORKSPACE_CONFIG\nCUBLAS_WORKSPACE_CONFIG\nThis environment variable is used to set the workspace configuration for cuBLAS per allocation. The format is:[SIZE]:[COUNT].\nAs an example, the default workspace size per allocation isCUBLAS_WORKSPACE_CONFIG=:4096:2:16:8which specifies a total size of2*4096+8*16KiB.\nTo force cuBLAS to avoid using workspaces, setCUBLAS_WORKSPACE_CONFIG=:0:0.\n:[SIZE]:[COUNT]\nCUBLAS_WORKSPACE_CONFIG=:4096:2:16:8\n2*4096+8*16KiB\nCUBLAS_WORKSPACE_CONFIG=:0:0\nCUDNN_CONV_WSCAP_DBG\nCUDNN_CONV_WSCAP_DBG\nSimilar toCUBLAS_WORKSPACE_CONFIG, this environment variable is used to set the workspace configuration for cuDNN per allocation.\nCUBLAS_WORKSPACE_CONFIG\nCUBLASLT_WORKSPACE_SIZE\nCUBLASLT_WORKSPACE_SIZE\nSimilar toCUBLAS_WORKSPACE_CONFIG, this environment variable is used to set the workspace size for cuBLASLT.\nCUBLAS_WORKSPACE_CONFIG\nCUDNN_ERRATA_JSON_FILE\nCUDNN_ERRATA_JSON_FILE\nCan be set to a file path for an errata filter that can be passed to cuDNN to avoid specific engine configs, used primarily for debugging or to hardcode autotuning.\nNVIDIA_TF32_OVERRIDE\nNVIDIA_TF32_OVERRIDE\nIf set to0, disables TF32 globally across all kernels, overriding all PyTorch settings.\n0",
  "url": "https://pytorch.org/docs/stable/cuda_environment_variables.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "ea54a74a40205038c5f72f785962eb7a",
  "source": "pytorch_docs",
  "title": "Windows FAQ \u2014 PyTorch 2.9 documentation",
  "text": "\n## Windows FAQ#\n\nCreated On: Apr 23, 2018 | Last Updated On: May 20, 2025\n\n## Building from source#\n\n\n## Include optional components#\n\nThere are two supported components for Windows PyTorch:\nMKL and MAGMA. Here are the steps to build with them.\n\n```python\nREM Make sure you have 7z and curl installed.\n\nREM Download MKL files\ncurl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O\n7z x -aoa mkl_2020.2.254.7z -omkl\n\nREM Download MAGMA files\nREM version available:\nREM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release)\nREM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release)\nREM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\nREM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\nset \"CUDA_PREFIX=cuda102\"\nset \"CONFIG=release\"\nset \"HOST=https://s3.amazonaws.com/ossci-windows\"\ncurl -k \"%HOST%/magma_2.5.4_%CUDA_PREFIX%_%CONFIG%.7z\" -o magma.7z\n7z x -aoa magma.7z -omagma\n\nREM Setting essential environment variables\nset \"CMAKE_INCLUDE_PATH=%cd%\\mkl\\include\"\nset \"LIB=%cd%\\mkl\\lib;%LIB%\"\nset \"MAGMA_HOME=%cd%\\magma\"\n\n```\n\n\n## Speeding CUDA build for Windows#\n\nVisual Studio doesn\u2019t support parallel custom task currently.\nAs an alternative, we can useNinjato parallelize CUDA\nbuild tasks. It can be used by typing only a few lines of code.\nNinja\n\n```python\nREM Let's install ninja first.\npip install ninja\n\nREM Set it as the cmake generator\nset CMAKE_GENERATOR=Ninja\n\n```\n\n\n## One key install script#\n\nYou can take a look atthis set of scripts.\nIt will lead the way for you.\n\n## Extension#\n\n\n## CFFI Extension#\n\nThe support for CFFI Extension is very experimental. You must specify\nadditionallibrariesinExtensionobject to make it build on\nWindows.\nlibraries\nExtension\n\n```python\nffi = create_extension(\n    '_ext.my_lib',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_compile_args=[\"-std=c99\"],\n    libraries=['ATen', '_C'] # Append cuda libraries when necessary, like cudart\n)\n\n```\n\n\n## Cpp Extension#\n\nThis type of extension has better support compared with\nthe previous one. However, it still needs some manual\nconfiguration. First, you should open thex86_x64 Cross Tools Command Prompt for VS 2017.\nAnd then, you can start your compiling process.\n\n## Installation#\n\n\n## Package not found in win-32 channel.#\n\n\n```python\nSolving environment: failed\n\nPackagesNotFoundError: The following packages are not available from current channels:\n\n- pytorch\n\nCurrent channels:\n- https://repo.continuum.io/pkgs/main/win-32\n- https://repo.continuum.io/pkgs/main/noarch\n- https://repo.continuum.io/pkgs/free/win-32\n- https://repo.continuum.io/pkgs/free/noarch\n- https://repo.continuum.io/pkgs/r/win-32\n- https://repo.continuum.io/pkgs/r/noarch\n- https://repo.continuum.io/pkgs/pro/win-32\n- https://repo.continuum.io/pkgs/pro/noarch\n- https://repo.continuum.io/pkgs/msys2/win-32\n- https://repo.continuum.io/pkgs/msys2/noarch\n\n```\n\nPyTorch doesn\u2019t work on 32-bit system. Please use Windows and\nPython 64-bit version.\n\n## Import error#\n\n\n```python\nfrom torch._C import *\n\nImportError: DLL load failed: The specified module could not be found.\n\n```\n\nThe problem is caused by the missing of the essential files.\nFor the wheels package, since we didn\u2019t pack some libraries and VS2017\nredistributable files in, please make sure you install them manually.\nTheVS 2017 redistributable installercan be downloaded.\nAnd you should also pay attention to your installation of Numpy. Make sure it\nuses MKL instead of OpenBLAS. You may type in the following command.\n\n```python\npip install numpy mkl intel-openmp mkl_fft\n\n```\n\n\n## Usage (multiprocessing)#\n\n\n## Multiprocessing error without if-clause protection#\n\n\n```python\nRuntimeError:\n       An attempt has been made to start a new process before the\n       current process has finished its bootstrapping phase.\n\n   This probably means that you are not using fork to start your\n   child processes and you have forgotten to use the proper idiom\n   in the main module:\n\n       if __name__ == '__main__':\n           freeze_support()\n           ...\n\n   The \"freeze_support()\" line can be omitted if the program\n   is not going to be frozen to produce an executable.\n\n```\n\nThe implementation ofmultiprocessingis different on Windows, which\nusesspawninstead offork. So we have to wrap the code with an\nif-clause to protect the code from executing multiple times. Refactor\nyour code into the following structure.\nmultiprocessing\nspawn\nfork\n\n```python\nimport torch\n\ndef main()\n    for i, data in enumerate(dataloader):\n        # do something here\n\nif __name__ == '__main__':\n    main()\n\n```\n\n\n## Multiprocessing error \u201cBroken pipe\u201d#\n\n\n```python\nForkingPickler(file, protocol).dump(obj)\n\nBrokenPipeError: [Errno 32] Broken pipe\n\n```\n\nThis issue happens when the child process ends before the parent process\nfinishes sending data. There may be something wrong with your code. You\ncan debug your code by reducing thenum_workerofDataLoaderto zero and see if the issue persists.\nnum_worker\nDataLoader\n\n## Multiprocessing error \u201cdriver shut down\u201d#\n\n\n```python\nCouldn\u2019t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\\lib\\TH\\THAllocator.c:154\n\n[windows] driver shut down\n\n```\n\nPlease update your graphics driver. If this persists, this may be that your\ngraphics card is too old or the calculation is too heavy for your card. Please\nupdate the TDR settings according to thispost.\n\n## CUDA IPC operations#\n\n\n```python\nTHCudaCheck FAIL file=torch\\csrc\\generic\\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS\n\n```\n\nThey are not supported on Windows. Something like doing multiprocessing on CUDA\ntensors cannot succeed, there are two alternatives for this.\n1. Don\u2019t usemultiprocessing. Set thenum_workerofDataLoaderto zero.\nmultiprocessing\nnum_worker\nDataLoader\n2. Share CPU tensors instead. Make sure your customDataSetreturns CPU tensors.\nDataSet",
  "url": "https://pytorch.org/docs/stable/notes/windows.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "091700cb6225093ea147183f0c5c3693",
  "source": "pytorch_docs",
  "title": "Probability distributions - torch.distributions \u2014 PyTorch 2.9 documentation",
  "text": "\n## Probability distributions - torch.distributions#\n\nCreated On: Oct 19, 2017 | Last Updated On: Jun 13, 2025\nThedistributionspackage contains parameterizable probability distributions\nand sampling functions. This allows the construction of stochastic computation\ngraphs and stochastic gradient estimators for optimization. This package\ngenerally follows the design of theTensorFlow Distributionspackage.\ndistributions\nIt is not possible to directly backpropagate through random samples. However,\nthere are two main methods for creating surrogate functions that can be\nbackpropagated through. These are the score function estimator/likelihood ratio\nestimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly\nseen as the basis for policy gradient methods in reinforcement learning, and the\npathwise derivative estimator is commonly seen in the reparameterization trick\nin variational autoencoders. Whilst the score function only requires the value\nof samplesf(x)f(x)f(x), the pathwise derivative requires the derivativef\u2032(x)f'(x)f\u2032(x). The next sections discuss these two in a reinforcement learning\nexample. For more details seeGradient Estimation Using Stochastic Computation Graphs.\n\n## Score function#\n\nWhen the probability density function is differentiable with respect to its\nparameters, we only needsample()andlog_prob()to implement REINFORCE:\nsample()\nlog_prob()\nwhere\u03b8\\theta\u03b8are the parameters,\u03b1\\alpha\u03b1is the learning rate,rrris the reward andp(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s))p(a\u2223\u03c0\u03b8(s))is the probability of\ntaking actionaaain statesssgiven policy\u03c0\u03b8\\pi^\\theta\u03c0\u03b8.\nIn practice we would sample an action from the output of a network, apply this\naction in an environment, and then uselog_probto construct an equivalent\nloss function. Note that we use a negative because optimizers use gradient\ndescent, whilst the rule above assumes gradient ascent. With a categorical\npolicy, the code for implementing REINFORCE would be as follows:\nlog_prob\n\n```python\nprobs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()\n\n```\n\n\n## Pathwise derivative#\n\nThe other way to implement these stochastic/policy gradients would be to use the\nreparameterization trick from thersample()method, where the\nparameterized random variable can be constructed via a parameterized\ndeterministic function of a parameter-free random variable. The reparameterized\nsample therefore becomes differentiable. The code for implementing the pathwise\nderivative would be as follows:\nrsample()\n\n```python\nparams = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()\n\n```\n\n\n## Distribution#\n\nBases:object\nobject\nDistribution is the abstract base class for probability distributions.\nbatch_shape(torch.Size) \u2013 The shape over which parameters are batched.\nevent_shape(torch.Size) \u2013 The shape of a single sample (without batching).\nvalidate_args(bool,optional) \u2013 Whether to validate arguments. Default: None.\nReturns a dictionary from argument names toConstraintobjects that\nshould be satisfied by each argument of this distribution. Args that\nare not tensors need not appear in this dict.\nConstraint\nReturns the shape over which parameters are batched.\nReturns the cumulative density/mass function evaluated atvalue.\nvalue(Tensor) \u2013\nTensor\nReturns entropy of distribution, batched over batch_shape.\nTensor of shape batch_shape.\nTensor\nReturns tensor containing all values supported by a discrete\ndistribution. The result will enumerate over dimension 0, so the shape\nof the result will be(cardinality,) + batch_shape + event_shape(whereevent_shape = ()for univariate distributions).\nNote that this enumerates over all batched tensors in lock-step[[0, 0], [1, 1], \u2026]. Withexpand=False, enumeration happens\nalong dim 0, but with the remaining batch dimensions being\nsingleton dimensions,[[0], [1], ...\nTo iterate over the full Cartesian product useitertools.product(m.enumerate_support()).\nexpand(bool) \u2013 whether to expand the support over the\nbatch dims to match the distribution\u2019sbatch_shape.\nTensor iterating over dimension 0.\nTensor\nReturns the shape of a single sample (without batching).\nReturns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded tobatch_shape. This method callsexpandon\nthe distribution\u2019s parameters. As such, this does not allocate new\nmemory for the expanded distribution instance. Additionally,\nthis does not repeat any args checking or parameter broadcasting in__init__.py, when an instance is first created.\nexpand\nbatch_shape(torch.Size) \u2013 the desired expanded size.\n_instance\u2013 new instance provided by subclasses that\nneed to override.expand.\nNew distribution instance with batch dimensions expanded tobatch_size.\nReturns the inverse cumulative density/mass function evaluated atvalue.\nvalue(Tensor) \u2013\nTensor\nReturns the log of the probability density/mass function evaluated atvalue.\nvalue(Tensor) \u2013\nTensor\nReturns the mean of the distribution.\nReturns the mode of the distribution.\nReturns perplexity of distribution, batched over batch_shape.\nTensor of shape batch_shape.\nTensor\nGenerates a sample_shape shaped reparameterized sample or sample_shape\nshaped batch of reparameterized samples if the distribution parameters\nare batched.\nTensor\nGenerates a sample_shape shaped sample or sample_shape shaped batch of\nsamples if the distribution parameters are batched.\nTensor\nGenerates n samples or n batches of samples if the distribution\nparameters are batched.\nTensor\nSets whether validation is enabled or disabled.\nThe default behavior mimics Python\u2019sassertstatement: validation\nis on by default, but is disabled if Python is run in optimized mode\n(viapython-O). Validation may be expensive, so you may want to\ndisable it once a model is working.\nassert\npython-O\nvalue(bool) \u2013 Whether to enable validation.\nReturns the standard deviation of the distribution.\nReturns aConstraintobject\nrepresenting this distribution\u2019s support.\nConstraint\nReturns the variance of the distribution.\n\n## ExponentialFamily#\n\nBases:Distribution\nDistribution\nExponentialFamily is the abstract base class for probability distributions belonging to an\nexponential family, whose probability mass/density function has the form is defined below\nwhere\u03b8\\theta\u03b8denotes the natural parameters,t(x)t(x)t(x)denotes the sufficient statistic,F(\u03b8)F(\\theta)F(\u03b8)is the log normalizer function for a given family andk(x)k(x)k(x)is the carrier\nmeasure.\nNote\nThis class is an intermediary between theDistributionclass and distributions which belong\nto an exponential family mainly to check the correctness of the.entropy()and analytic KL\ndivergence methods. We use this class to compute the entropy and KL divergence using the AD\nframework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and\nCross-entropies of Exponential Families).\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\n## Bernoulli#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Bernoulli distribution parameterized byprobsorlogits(but not both).\nprobs\nlogits\nSamples are binary (0 or 1). They take the value1with probabilitypand0with probability1 - p.\nExample:\n\n```python\n>>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n\n```\n\nprobs(Number,Tensor) \u2013 the probability of sampling1\nlogits(Number,Tensor) \u2013 the log-odds of sampling1\nvalidate_args(bool,optional) \u2013 whether to validate arguments, None by default\n\n## Beta#\n\nBases:ExponentialFamily\nExponentialFamily\nBeta distribution parameterized byconcentration1andconcentration0.\nconcentration1\nconcentration0\nExample:\n\n```python\n>>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n\n```\n\nconcentration1(floatorTensor) \u2013 1st concentration parameter of the distribution\n(often referred to as alpha)\nconcentration0(floatorTensor) \u2013 2nd concentration parameter of the distribution\n(often referred to as beta)\nTensor\n\n## Binomial#\n\nBases:Distribution\nDistribution\nCreates a Binomial distribution parameterized bytotal_countand\neitherprobsorlogits(but not both).total_countmust be\nbroadcastable withprobs/logits.\ntotal_count\nprobs\nlogits\ntotal_count\nprobs\nlogits\nExample:\n\n```python\n>>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n\n```\n\ntotal_count(intorTensor) \u2013 number of Bernoulli trials\nprobs(Tensor) \u2013 Event probabilities\nlogits(Tensor) \u2013 Event log-odds\n_DependentProperty\n\n## Categorical#\n\nBases:Distribution\nDistribution\nCreates a categorical distribution parameterized by eitherprobsorlogits(but not both).\nprobs\nlogits\nNote\nIt is equivalent to the distribution thattorch.multinomial()samples from.\ntorch.multinomial()\nSamples are integers from{0,\u2026,K\u22121}\\{0, \\ldots, K-1\\}{0,\u2026,K\u22121}whereKisprobs.size(-1).\nprobs.size(-1)\nIfprobsis 1-dimensional with length-K, each element is the relative probability\nof sampling the class at that index.\nIfprobsis N-dimensional, the first N-1 dimensions are treated as a batch of\nrelative probability vectors.\nNote\nTheprobsargument must be non-negative, finite and have a non-zero sum,\nand it will be normalized to sum to 1 along the last dimension.probswill return this normalized value.\nThelogitsargument will be interpreted as unnormalized log probabilities\nand can therefore be any real number. It will likewise be normalized so that\nthe resulting probabilities sum to 1 along the last dimension.logitswill return this normalized value.\nprobs\nlogits\nSee also:torch.multinomial()\ntorch.multinomial()\nExample:\n\n```python\n>>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n\n```\n\nprobs(Tensor) \u2013 event probabilities\nlogits(Tensor) \u2013 event log probabilities (unnormalized)\n_DependentProperty\n\n## Cauchy#\n\nBases:Distribution\nDistribution\nSamples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\nindependent normally distributed random variables with means0follows a\nCauchy distribution.\nExample:\n\n```python\n>>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n\n```\n\nloc(floatorTensor) \u2013 mode or median of the distribution.\nscale(floatorTensor) \u2013 half width at half maximum.\nTensor\n\n## Chi2#\n\nBases:Gamma\nGamma\nCreates a Chi-squared distribution parameterized by shape parameterdf.\nThis is exactly equivalent toGamma(alpha=0.5*df,beta=0.5)\ndf\nGamma(alpha=0.5*df,beta=0.5)\nExample:\n\n```python\n>>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n\n```\n\ndf(floatorTensor) \u2013 shape parameter of the distribution\n\n## ContinuousBernoulli#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a continuous Bernoulli distribution parameterized byprobsorlogits(but not both).\nprobs\nlogits\nThe distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to\nlog-odds, but the same names are used due to the similarity with the\nBernoulli. See [1] for more details.\nExample:\n\n```python\n>>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])\n\n```\n\nprobs(Number,Tensor) \u2013 (0,1) valued parameters\nlogits(Number,Tensor) \u2013 real valued parameters whose sigmoid matches \u2018probs\u2019\n[1] The continuous Bernoulli: fixing a pervasive error in variational\nautoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.https://arxiv.org/abs/1907.06845\nTensor\n\n## Dirichlet#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Dirichlet distribution parameterized by concentrationconcentration.\nconcentration\nExample:\n\n```python\n>>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentration [0.5, 0.5]\ntensor([ 0.1046,  0.8954])\n\n```\n\nconcentration(Tensor) \u2013 concentration parameter of the distribution\n(often referred to as alpha)\nTensor\n\n## Exponential#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Exponential distribution parameterized byrate.\nrate\nExample:\n\n```python\n>>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n\n```\n\nrate(floatorTensor) \u2013 rate = 1 / scale of the distribution\nTensor\n\n## FisherSnedecor#\n\nBases:Distribution\nDistribution\nCreates a Fisher-Snedecor distribution parameterized bydf1anddf2.\ndf1\ndf2\nExample:\n\n```python\n>>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n\n```\n\ndf1(floatorTensor) \u2013 degrees of freedom parameter 1\ndf2(floatorTensor) \u2013 degrees of freedom parameter 2\nTensor\n\n## Gamma#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Gamma distribution parameterized by shapeconcentrationandrate.\nconcentration\nrate\nExample:\n\n```python\n>>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n\n```\n\nconcentration(floatorTensor) \u2013 shape parameter of the distribution\n(often referred to as alpha)\nrate(floatorTensor) \u2013 rate parameter of the distribution\n(often referred to as beta), rate = 1 / scale\nTensor\n\n## GeneralizedPareto#\n\nBases:Distribution\nDistribution\nCreates a Generalized Pareto distribution parameterized byloc,scale, andconcentration.\nloc\nscale\nconcentration\nThe Generalized Pareto distribution is a family of continuous probability distributions on the real line.\nSpecial cases include Exponential (whenloc= 0,concentration= 0), Pareto (whenconcentration> 0,loc=scale/concentration), and Uniform (whenconcentration= -1).\nloc\nconcentration\nconcentration\nloc\nscale\nconcentration\nconcentration\nThis distribution is often used to model the tails of other distributions. This implementation is based on the\nimplementation in TensorFlow Probability.\nExample:\n\n```python\n>>> m = GeneralizedPareto(torch.tensor([0.1]), torch.tensor([2.0]), torch.tensor([0.4]))\n>>> m.sample()  # sample from a Generalized Pareto distribution with loc=0.1, scale=2.0, and concentration=0.4\ntensor([ 1.5623])\n\n```\n\nloc(floatorTensor) \u2013 Location parameter of the distribution\nscale(floatorTensor) \u2013 Scale parameter of the distribution\nconcentration(floatorTensor) \u2013 Concentration parameter of the distribution\n_DependentProperty\n\n## Geometric#\n\nBases:Distribution\nDistribution\nCreates a Geometric distribution parameterized byprobs,\nwhereprobsis the probability of success of Bernoulli trials.\nprobs\nprobs\nNote\ntorch.distributions.geometric.Geometric()(k+1)(k+1)(k+1)-th trial is the first success\nhence draws samples in{0,1,\u2026}\\{0, 1, \\ldots\\}{0,1,\u2026}, whereastorch.Tensor.geometric_()k-th trial is the first success hence draws samples in{1,2,\u2026}\\{1, 2, \\ldots\\}{1,2,\u2026}.\ntorch.distributions.geometric.Geometric()\ntorch.Tensor.geometric_()\nExample:\n\n```python\n>>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n\n```\n\nprobs(Number,Tensor) \u2013 the probability of sampling1. Must be in range (0, 1]\nlogits(Number,Tensor) \u2013 the log-odds of sampling1.\n\n## Gumbel#\n\nBases:TransformedDistribution\nTransformedDistribution\nSamples from a Gumbel Distribution.\nExamples:\n\n```python\n>>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n\n```\n\nloc(floatorTensor) \u2013 Location parameter of the distribution\nscale(floatorTensor) \u2013 Scale parameter of the distribution\n\n## HalfCauchy#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a half-Cauchy distribution parameterized byscalewhere:\n\n```python\nX ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n\n```\n\nExample:\n\n```python\n>>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n\n```\n\nscale(floatorTensor) \u2013 scale of the full Cauchy distribution\n\n## HalfNormal#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a half-normal distribution parameterized byscalewhere:\n\n```python\nX ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n\n```\n\nExample:\n\n```python\n>>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n\n```\n\nscale(floatorTensor) \u2013 scale of the full Normal distribution\n\n## Independent#\n\nBases:Distribution,Generic[D]\nDistribution\nGeneric\nD\nReinterprets some of the batch dims of a distribution as event dims.\nThis is mainly useful for changing the shape of the result oflog_prob(). For example to create a diagonal Normal distribution with\nthe same shape as a Multivariate Normal distribution (so they are\ninterchangeable), you can:\nlog_prob()\n\n```python\n>>> from torch.distributions.multivariate_normal import MultivariateNormal\n>>> from torch.distributions.normal import Normal\n>>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size([]), torch.Size([3])]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size([3]), torch.Size([])]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size([]), torch.Size([3])]\n\n```\n\nbase_distribution(torch.distributions.distribution.Distribution) \u2013 a\nbase distribution\nreinterpreted_batch_ndims(int) \u2013 the number of batch dims to\nreinterpret as event dims\nTensor\nTensor\n_DependentProperty\n\n## InverseGamma#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates an inverse gamma distribution parameterized byconcentrationandratewhere:\nconcentration\nrate\n\n```python\nX ~ Gamma(concentration, rate)\nY = 1 / X ~ InverseGamma(concentration, rate)\n\n```\n\nExample:\n\n```python\n>>> m = InverseGamma(torch.tensor([2.0]), torch.tensor([3.0]))\n>>> m.sample()\ntensor([ 1.2953])\n\n```\n\nconcentration(floatorTensor) \u2013 shape parameter of the distribution\n(often referred to as alpha)\nrate(floatorTensor) \u2013 rate = 1 / scale of the distribution\n(often referred to as beta)\n\n## Kumaraswamy#\n\nBases:TransformedDistribution\nTransformedDistribution\nSamples from a Kumaraswamy distribution.\nExample:\n\n```python\n>>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])\n\n```\n\nconcentration1(floatorTensor) \u2013 1st concentration parameter of the distribution\n(often referred to as alpha)\nconcentration0(floatorTensor) \u2013 2nd concentration parameter of the distribution\n(often referred to as beta)\n\n## LKJCholesky#\n\nBases:Distribution\nDistribution\nLKJ distribution for lower Cholesky factor of correlation matrices.\nThe distribution is controlled byconcentrationparameter\u03b7\\eta\u03b7to make the probability of the correlation matrixMMMgenerated from\na Cholesky factor proportional todet\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1}det(M)\u03b7\u22121. Because of that,\nwhenconcentration==1, we have a uniform distribution over Cholesky\nfactors of correlation matrices:\nconcentration\nconcentration==1\n\n```python\nL ~ LKJCholesky(dim, concentration)\nX = L @ L' ~ LKJCorr(dim, concentration)\n\n```\n\nNote that this distribution samples the\nCholesky factor of correlation matrices and not the correlation matrices\nthemselves and thereby differs slightly from the derivations in [1] for\ntheLKJCorrdistribution. For sampling, this uses the Onion method from\n[1] Section 3.\nExample:\n\n```python\n>>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])\n\n```\n\ndimension(dim) \u2013 dimension of the matrices\nconcentration(floatorTensor) \u2013 concentration/shape parameter of the\ndistribution (often referred to as eta)\nReferences\n[1]Generating random correlation matrices based on vines and extended onion method(2009),\nDaniel Lewandowski, Dorota Kurowicka, Harry Joe.\nJournal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008\n\n## Laplace#\n\nBases:Distribution\nDistribution\nCreates a Laplace distribution parameterized bylocandscale.\nloc\nscale\nExample:\n\n```python\n>>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n\n```\n\nloc(floatorTensor) \u2013 mean of the distribution\nscale(floatorTensor) \u2013 scale of the distribution\nTensor\n\n## LogNormal#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a log-normal distribution parameterized bylocandscalewhere:\nloc\nscale\n\n```python\nX ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n\n```\n\nExample:\n\n```python\n>>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n\n```\n\nloc(floatorTensor) \u2013 mean of log of distribution\nscale(floatorTensor) \u2013 standard deviation of log of the distribution\n\n## LowRankMultivariateNormal#\n\nBases:Distribution\nDistribution\nCreates a multivariate normal distribution with covariance matrix having a low-rank form\nparameterized bycov_factorandcov_diag:\ncov_factor\ncov_diag\n\n```python\ncovariance_matrix = cov_factor @ cov_factor.T + cov_diag\n\n```\n\nExample\n\n```python\n>>> m = LowRankMultivariateNormal(\n...     torch.zeros(2), torch.tensor([[1.0], [0.0]]), torch.ones(2)\n... )\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n\n```\n\nloc(Tensor) \u2013 mean of the distribution with shapebatch_shape + event_shape\ncov_factor(Tensor) \u2013 factor part of low-rank form of covariance matrix with shapebatch_shape + event_shape + (rank,)\ncov_diag(Tensor) \u2013 diagonal part of low-rank form of covariance matrix with shapebatch_shape + event_shape\nNote\nThe computation for determinant and inverse of covariance matrix is avoided whencov_factor.shape[1] << cov_factor.shape[0]thanks toWoodbury matrix identityandmatrix determinant lemma.\nThanks to these formulas, we just need to compute the determinant and inverse of\nthe small size \u201ccapacitance\u201d matrix:\n\n```python\ncapacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n\n```\n\nTensor\n\n## MixtureSameFamily#\n\nBases:Distribution\nDistribution\nTheMixtureSameFamilydistribution implements a (batch of) mixture\ndistribution where all component are from different parameterizations of\nthe same distribution type. It is parameterized by aCategorical\u201cselecting distribution\u201d (overkcomponent) and a component\ndistribution, i.e., aDistributionwith a rightmost batch shape\n(equal to[k]) which indexes each (batch of) component.\nExamples:\n\n```python\n>>> # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n>>> # weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n>>> # Construct Gaussian Mixture Model in 2D consisting of 5 equally\n>>> # weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n...          torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n>>> # Construct a batch of 3 Gaussian Mixture Models in 2D each\n>>> # consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n...         torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n```\n\nmixture_distribution(Categorical) \u2013torch.distributions.Categorical-like\ninstance. Manages the probability of selecting component.\nThe number of categories must match the rightmost batch\ndimension of thecomponent_distribution. Must have either\nscalarbatch_shapeorbatch_shapematchingcomponent_distribution.batch_shape[:-1]\ncomponent_distribution(Distribution) \u2013torch.distributions.Distribution-like\ninstance. Right-most batch dimension indexes component.\n_DependentProperty\n\n## Multinomial#\n\nBases:Distribution\nDistribution\nCreates a Multinomial distribution parameterized bytotal_countand\neitherprobsorlogits(but not both). The innermost dimension ofprobsindexes over categories. All other dimensions index over batches.\ntotal_count\nprobs\nlogits\nprobs\nNote thattotal_countneed not be specified if onlylog_prob()is\ncalled (see example below)\ntotal_count\nlog_prob()\nNote\nTheprobsargument must be non-negative, finite and have a non-zero sum,\nand it will be normalized to sum to 1 along the last dimension.probswill return this normalized value.\nThelogitsargument will be interpreted as unnormalized log probabilities\nand can therefore be any real number. It will likewise be normalized so that\nthe resulting probabilities sum to 1 along the last dimension.logitswill return this normalized value.\nprobs\nlogits\nsample()requires a single sharedtotal_countfor all\nparameters and samples.\nsample()\nlog_prob()allows differenttotal_countfor each parameter and\nsample.\nlog_prob()\nExample:\n\n```python\n>>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n\n```\n\ntotal_count(int) \u2013 number of trials\nprobs(Tensor) \u2013 event probabilities\nlogits(Tensor) \u2013 event log probabilities (unnormalized)\n_DependentProperty\n\n## MultivariateNormal#\n\nBases:Distribution\nDistribution\nCreates a multivariate normal (also called Gaussian) distribution\nparameterized by a mean vector and a covariance matrix.\nThe multivariate normal distribution can be parameterized either\nin terms of a positive definite covariance matrix\u03a3\\mathbf{\\Sigma}\u03a3or a positive definite precision matrix\u03a3\u22121\\mathbf{\\Sigma}^{-1}\u03a3\u22121or a lower-triangular matrixL\\mathbf{L}Lwith positive-valued\ndiagonal entries, such that\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\u03a3=LL\u22a4. This triangular matrix\ncan be obtained via e.g. Cholesky decomposition of the covariance.\nExample\n\n```python\n>>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n\n```\n\nloc(Tensor) \u2013 mean of the distribution\ncovariance_matrix(Tensor) \u2013 positive-definite covariance matrix\nprecision_matrix(Tensor) \u2013 positive-definite precision matrix\nscale_tril(Tensor) \u2013 lower-triangular factor of covariance, with positive-valued diagonal\nNote\nOnly one ofcovariance_matrixorprecision_matrixorscale_trilcan be specified.\ncovariance_matrix\nprecision_matrix\nscale_tril\nUsingscale_trilwill be more efficient: all computations internally\nare based onscale_tril. Ifcovariance_matrixorprecision_matrixis passed instead, it is only used to compute\nthe corresponding lower triangular matrices using a Cholesky decomposition.\nscale_tril\nscale_tril\ncovariance_matrix\nprecision_matrix\nTensor\n\n## NegativeBinomial#\n\nBases:Distribution\nDistribution\nCreates a Negative Binomial distribution, i.e. distribution\nof the number of successful independent and identical Bernoulli trials\nbeforetotal_countfailures are achieved. The probability\nof success of each Bernoulli trial isprobs.\ntotal_count\nprobs\ntotal_count(floatorTensor) \u2013 non-negative number of negative Bernoulli\ntrials to stop, although the distribution is still valid for real\nvalued count\nprobs(Tensor) \u2013 Event probabilities of success in the half open interval [0, 1)\nlogits(Tensor) \u2013 Event log-odds for probabilities of success\n\n## Normal#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a normal (also called Gaussian) distribution parameterized bylocandscale.\nloc\nscale\nExample:\n\n```python\n>>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n\n```\n\nloc(floatorTensor) \u2013 mean of the distribution (often referred to as mu)\nscale(floatorTensor) \u2013 standard deviation of the distribution\n(often referred to as sigma)\nTensor\n\n## OneHotCategorical#\n\nBases:Distribution\nDistribution\nCreates a one-hot categorical distribution parameterized byprobsorlogits.\nprobs\nlogits\nSamples are one-hot coded vectors of sizeprobs.size(-1).\nprobs.size(-1)\nNote\nTheprobsargument must be non-negative, finite and have a non-zero sum,\nand it will be normalized to sum to 1 along the last dimension.probswill return this normalized value.\nThelogitsargument will be interpreted as unnormalized log probabilities\nand can therefore be any real number. It will likewise be normalized so that\nthe resulting probabilities sum to 1 along the last dimension.logitswill return this normalized value.\nprobs\nlogits\nSee also:torch.distributions.Categorical()for specifications ofprobsandlogits.\ntorch.distributions.Categorical()\nprobs\nlogits\nExample:\n\n```python\n>>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n\n```\n\nprobs(Tensor) \u2013 event probabilities\nlogits(Tensor) \u2013 event log probabilities (unnormalized)\n\n## Pareto#\n\nBases:TransformedDistribution\nTransformedDistribution\nSamples from a Pareto Type 1 distribution.\nExample:\n\n```python\n>>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n\n```\n\nscale(floatorTensor) \u2013 Scale parameter of the distribution\nalpha(floatorTensor) \u2013 Shape parameter of the distribution\nTensor\nPareto\n_DependentProperty\n\n## Poisson#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Poisson distribution parameterized byrate, the rate parameter.\nrate\nSamples are nonnegative integers, with a pmf given by\nExample:\n\n```python\n>>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])\n\n```\n\nrate(Number,Tensor) \u2013 the rate parameter\n\n## RelaxedBernoulli#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a RelaxedBernoulli distribution, parametrized bytemperature, and eitherprobsorlogits(but not both). This is a relaxed version of theBernoullidistribution,\nso the values are in (0, 1), and has reparametrizable samples.\ntemperature\nprobs\nlogits\nExample:\n\n```python\n>>> m = RelaxedBernoulli(torch.tensor([2.2]),\n...                      torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n\n```\n\ntemperature(Tensor) \u2013 relaxation temperature\nprobs(Number,Tensor) \u2013 the probability of sampling1\nlogits(Number,Tensor) \u2013 the log-odds of sampling1\n\n## LogitRelaxedBernoulli#\n\nBases:Distribution\nDistribution\nCreates a LogitRelaxedBernoulli distribution parameterized byprobsorlogits(but not both), which is the logit of a RelaxedBernoulli\ndistribution.\nprobs\nlogits\nSamples are logits of values in (0, 1). See [1] for more details.\ntemperature(Tensor) \u2013 relaxation temperature\nprobs(Number,Tensor) \u2013 the probability of sampling1\nlogits(Number,Tensor) \u2013 the log-odds of sampling1\n[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random\nVariables (Maddison et al., 2017)\n[2] Categorical Reparametrization with Gumbel-Softmax\n(Jang et al., 2017)\nTensor\n\n## RelaxedOneHotCategorical#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a RelaxedOneHotCategorical distribution parametrized bytemperature, and eitherprobsorlogits.\nThis is a relaxed version of theOneHotCategoricaldistribution, so\nits samples are on simplex, and are reparametrizable.\ntemperature\nprobs\nlogits\nOneHotCategorical\nExample:\n\n```python\n>>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n...                              torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n\n```\n\ntemperature(Tensor) \u2013 relaxation temperature\nprobs(Tensor) \u2013 event probabilities\nlogits(Tensor) \u2013 unnormalized log probability for each event\n\n## StudentT#\n\nBases:Distribution\nDistribution\nCreates a Student\u2019s t-distribution parameterized by degree of\nfreedomdf, meanlocand scalescale.\ndf\nloc\nscale\nExample:\n\n```python\n>>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n\n```\n\ndf(floatorTensor) \u2013 degrees of freedom\nloc(floatorTensor) \u2013 mean of the distribution\nscale(floatorTensor) \u2013 scale of the distribution\nTensor\n\n## TransformedDistribution#\n\nBases:Distribution\nDistribution\nExtension of the Distribution class, which applies a sequence of Transforms\nto a base distribution.  Let f be the composition of transforms applied:\n\n```python\nX ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n\n```\n\nNote that the.event_shapeof aTransformedDistributionis the\nmaximum shape of its base distribution and its transforms, since transforms\ncan introduce correlations among events.\n.event_shape\nTransformedDistribution\nAn example for the usage ofTransformedDistributionwould be:\nTransformedDistribution\n\n```python\n# Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n\n```\n\nFor more examples, please look at the implementations ofGumbel,HalfCauchy,HalfNormal,LogNormal,Pareto,Weibull,RelaxedBernoulliandRelaxedOneHotCategorical\nGumbel\nHalfCauchy\nHalfNormal\nLogNormal\nPareto\nWeibull\nRelaxedBernoulli\nRelaxedOneHotCategorical\nComputes the cumulative distribution function by inverting the\ntransform(s) and computing the score of the base distribution.\nComputes the inverse cumulative distribution function using\ntransform(s) and computing the score of the base distribution.\nScores the sample by inverting the transform(s) and computing the score\nusing the score of the base distribution and the log abs det jacobian.\nGenerates a sample_shape shaped reparameterized sample or sample_shape\nshaped batch of reparameterized samples if the distribution parameters\nare batched. Samples first from base distribution and appliestransform()for every transform in the list.\nTensor\nGenerates a sample_shape shaped sample or sample_shape shaped batch of\nsamples if the distribution parameters are batched. Samples first from\nbase distribution and appliestransform()for every transform in the\nlist.\n_DependentProperty\n\n## Uniform#\n\nBases:Distribution\nDistribution\nGenerates uniformly distributed random samples from the half-open interval[low,high).\n[low,high)\nExample:\n\n```python\n>>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n\n```\n\nlow(floatorTensor) \u2013 lower range (inclusive).\nhigh(floatorTensor) \u2013 upper range (exclusive).\nTensor\n_DependentProperty\n\n## VonMises#\n\nBases:Distribution\nDistribution\nA circular von Mises distribution.\nThis implementation uses polar coordinates. Thelocandvalueargs\ncan be any real number (to facilitate unconstrained optimization), but are\ninterpreted as angles modulo 2 pi.\nloc\nvalue\n\n```python\n>>> m = VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n\n```\n\nloc(torch.Tensor) \u2013 an angle in radians.\nconcentration(torch.Tensor) \u2013 concentration parameter\nThe provided mean is the circular one.\nThe sampling algorithm for the von Mises distribution is based on the\nfollowing paper: D.J. Best and N.I. Fisher, \u201cEfficient simulation of the\nvon Mises distribution.\u201d Applied Statistics (1979): 152-157.\nSampling is always done in double precision internally to avoid a hang\nin _rejection_sample() for small values of the concentration, which\nstarts to happen for single precision around 1e-4 (see issue #88443).\nThe provided variance is the circular one.\n\n## Weibull#\n\nBases:TransformedDistribution\nTransformedDistribution\nSamples from a two-parameter Weibull distribution.\nExample\n\n```python\n>>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n\n```\n\nscale(floatorTensor) \u2013 Scale parameter of distribution (lambda).\nconcentration(floatorTensor) \u2013 Concentration parameter of distribution (k/shape).\nvalidate_args(bool,optional) \u2013 Whether to validate arguments. Default: None.\n\n## Wishart#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Wishart distribution parameterized by a symmetric positive definite matrix\u03a3\\Sigma\u03a3,\nor its Cholesky decomposition\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\u03a3=LL\u22a4\nExample\n\n```python\n>>> m = Wishart(torch.Tensor([2]), covariance_matrix=torch.eye(2))\n>>> m.sample()  # Wishart distributed with mean=`df * I` and\n>>> # variance(x_ij)=`df` for i != j and variance(x_ij)=`2 * df` for i == j\n\n```\n\ndf(floatorTensor) \u2013 real-valued parameter larger than the (dimension of Square matrix) - 1\ncovariance_matrix(Tensor) \u2013 positive-definite covariance matrix\nprecision_matrix(Tensor) \u2013 positive-definite precision matrix\nscale_tril(Tensor) \u2013 lower-triangular factor of covariance, with positive-valued diagonal\nNote\nOnly one ofcovariance_matrixorprecision_matrixorscale_trilcan be specified.\nUsingscale_trilwill be more efficient: all computations internally\nare based onscale_tril. Ifcovariance_matrixorprecision_matrixis passed instead, it is only used to compute\nthe corresponding lower triangular matrices using a Cholesky decomposition.\n\u2018torch.distributions.LKJCholesky\u2019 is a restricted Wishart distribution.[1]\ncovariance_matrix\nprecision_matrix\nscale_tril\nscale_tril\nscale_tril\ncovariance_matrix\nprecision_matrix\nReferences\n[1] Wang, Z., Wu, Y. and Chu, H., 2018.On equivalence of the LKJ distribution and the restricted Wishart distribution.\n[2] Sawyer, S., 2007.Wishart Distributions and Inverse-Wishart Sampling.\n[3] Anderson, T. W., 2003.An Introduction to Multivariate Statistical Analysis (3rd ed.).\n[4] Odell, P. L. & Feiveson, A. H., 1966.A Numerical Procedure to Generate a SampleCovariance Matrix. JASA, 61(313):199-203.\n[5] Ku, Y.-C. & Bloomfield, P., 2010.Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX.\nWarning\nIn some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples.\nSeveral tries to correct singular samples are performed by default, but it may end up returning\nsingular matrix samples. Singular samples may return-infvalues in.log_prob().\nIn those cases, the user should validate the samples and either fix the value ofdfor adjustmax_try_correctionvalue for argument in.rsampleaccordingly.\nTensor\n\n## KLDivergence#\n\nKLDivergence\nCompute Kullback-Leibler divergenceKL(p\u2225q)KL(p \\| q)KL(p\u2225q)between two distributions.\np(Distribution) \u2013 ADistributionobject.\nDistribution\nq(Distribution) \u2013 ADistributionobject.\nDistribution\nA batch of KL divergences of shapebatch_shape.\nTensor\nNotImplementedError\u2013 If the distribution types have not been registered viaregister_kl().\nregister_kl()\nBernoulliandBernoulli\nBernoulli\nBernoulli\nBernoulliandPoisson\nBernoulli\nPoisson\nBetaandBeta\nBeta\nBeta\nBetaandContinuousBernoulli\nBeta\nContinuousBernoulli\nBetaandExponential\nBeta\nExponential\nBetaandGamma\nBeta\nGamma\nBetaandNormal\nBeta\nNormal\nBetaandPareto\nBeta\nPareto\nBetaandUniform\nBeta\nUniform\nBinomialandBinomial\nBinomial\nBinomial\nCategoricalandCategorical\nCategorical\nCategorical\nCauchyandCauchy\nCauchy\nCauchy\nContinuousBernoulliandContinuousBernoulli\nContinuousBernoulli\nContinuousBernoulli\nContinuousBernoulliandExponential\nContinuousBernoulli\nExponential\nContinuousBernoulliandNormal\nContinuousBernoulli\nNormal\nContinuousBernoulliandPareto\nContinuousBernoulli\nPareto\nContinuousBernoulliandUniform\nContinuousBernoulli\nUniform\nDirichletandDirichlet\nDirichlet\nDirichlet\nExponentialandBeta\nExponential\nBeta\nExponentialandContinuousBernoulli\nExponential\nContinuousBernoulli\nExponentialandExponential\nExponential\nExponential\nExponentialandGamma\nExponential\nGamma\nExponentialandGumbel\nExponential\nGumbel\nExponentialandNormal\nExponential\nNormal\nExponentialandPareto\nExponential\nPareto\nExponentialandUniform\nExponential\nUniform\nExponentialFamilyandExponentialFamily\nExponentialFamily\nExponentialFamily\nGammaandBeta\nGamma\nBeta\nGammaandContinuousBernoulli\nGamma\nContinuousBernoulli\nGammaandExponential\nGamma\nExponential\nGammaandGamma\nGamma\nGamma\nGammaandGumbel\nGamma\nGumbel\nGammaandNormal\nGamma\nNormal\nGammaandPareto\nGamma\nPareto\nGammaandUniform\nGamma\nUniform\nGeometricandGeometric\nGeometric\nGeometric\nGumbelandBeta\nGumbel\nBeta\nGumbelandContinuousBernoulli\nGumbel\nContinuousBernoulli\nGumbelandExponential\nGumbel\nExponential\nGumbelandGamma\nGumbel\nGamma\nGumbelandGumbel\nGumbel\nGumbel\nGumbelandNormal\nGumbel\nNormal\nGumbelandPareto\nGumbel\nPareto\nGumbelandUniform\nGumbel\nUniform\nHalfNormalandHalfNormal\nHalfNormal\nHalfNormal\nIndependentandIndependent\nIndependent\nIndependent\nLaplaceandBeta\nLaplace\nBeta\nLaplaceandContinuousBernoulli\nLaplace\nContinuousBernoulli\nLaplaceandExponential\nLaplace\nExponential\nLaplaceandGamma\nLaplace\nGamma\nLaplaceandLaplace\nLaplace\nLaplace\nLaplaceandNormal\nLaplace\nNormal\nLaplaceandPareto\nLaplace\nPareto\nLaplaceandUniform\nLaplace\nUniform\nLowRankMultivariateNormalandLowRankMultivariateNormal\nLowRankMultivariateNormal\nLowRankMultivariateNormal\nLowRankMultivariateNormalandMultivariateNormal\nLowRankMultivariateNormal\nMultivariateNormal\nMultivariateNormalandLowRankMultivariateNormal\nMultivariateNormal\nLowRankMultivariateNormal\nMultivariateNormalandMultivariateNormal\nMultivariateNormal\nMultivariateNormal\nNormalandBeta\nNormal\nBeta\nNormalandContinuousBernoulli\nNormal\nContinuousBernoulli\nNormalandExponential\nNormal\nExponential\nNormalandGamma\nNormal\nGamma\nNormalandGumbel\nNormal\nGumbel\nNormalandLaplace\nNormal\nLaplace\nNormalandNormal\nNormal\nNormal\nNormalandPareto\nNormal\nPareto\nNormalandUniform\nNormal\nUniform\nOneHotCategoricalandOneHotCategorical\nOneHotCategorical\nOneHotCategorical\nParetoandBeta\nPareto\nBeta\nParetoandContinuousBernoulli\nPareto\nContinuousBernoulli\nParetoandExponential\nPareto\nExponential\nParetoandGamma\nPareto\nGamma\nParetoandNormal\nPareto\nNormal\nParetoandPareto\nPareto\nPareto\nParetoandUniform\nPareto\nUniform\nPoissonandBernoulli\nPoisson\nBernoulli\nPoissonandBinomial\nPoisson\nBinomial\nPoissonandPoisson\nPoisson\nPoisson\nTransformedDistributionandTransformedDistribution\nTransformedDistribution\nTransformedDistribution\nUniformandBeta\nUniform\nBeta\nUniformandContinuousBernoulli\nUniform\nContinuousBernoulli\nUniformandExponential\nUniform\nExponential\nUniformandGamma\nUniform\nGamma\nUniformandGumbel\nUniform\nGumbel\nUniformandNormal\nUniform\nNormal\nUniformandPareto\nUniform\nPareto\nUniformandUniform\nUniform\nUniform\nDecorator to register a pairwise function withkl_divergence().\nUsage:\nkl_divergence()\n\n```python\n@register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n\n```\n\nLookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, aRuntimeWarningis raised. For example to\nresolve the ambiguous situation:\n\n```python\n@register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n\n```\n\nyou should register a third most-specific implementation, e.g.:\n\n```python\nregister_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n\n```\n\ntype_p(type) \u2013 A subclass ofDistribution.\nDistribution\ntype_q(type) \u2013 A subclass ofDistribution.\nDistribution\n\n## Transforms#\n\nTransforms\nTransform via the mappingy=\u2223x\u2223y = |x|y=\u2223x\u2223.\nTransform via the pointwise affine mappingy=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times xy=loc+scale\u00d7x.\nloc(Tensororfloat) \u2013 Location parameter.\nscale(Tensororfloat) \u2013 Scale parameter.\nevent_dim(int) \u2013 Optional size ofevent_shape. This should be zero\nfor univariate random variables, 1 for distributions over vectors,\n2 for distributions over matrices, etc.\nTransform functor that applies a sequence of transformstseqcomponent-wise to each submatrix atdim, of lengthlengths[dim],\nin a way compatible withtorch.cat().\ntorch.cat()\nExample:\n\n```python\nx0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)\nx = torch.cat([x0, x0], dim=0)\nt0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])\nt = CatTransform([t0, t0], dim=0, lengths=[20, 20])\ny = t(x)\n\n```\n\nComposes multiple transforms in a chain.\nThe transforms being composed are responsible for caching.\nparts(list ofTransform) \u2013 A list of transforms to compose.\nTransform\ncache_size(int) \u2013 Size of cache. If zero, no caching is done. If one,\nthe latest single value is cached. Only 0 and 1 are supported.\nTransforms an unconstrained real vectorxxxwith lengthD\u2217(D\u22121)/2D*(D-1)/2D\u2217(D\u22121)/2into the\nCholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower\ntriangular matrix with positive diagonals and unit Euclidean norm for each row.\nThe transform is processed as follows:\nFirst we convert x into a lower triangular matrix in row order.\nFor each rowXiX_iXi\u200bof the lower triangular part, we apply asignedversion of\nclassStickBreakingTransformto transformXiX_iXi\u200binto a\nunit Euclidean length vector using the following steps:\n- Scales into the interval(\u22121,1)(-1, 1)(\u22121,1)domain:ri=tanh\u2061(Xi)r_i = \\tanh(X_i)ri\u200b=tanh(Xi\u200b).\n- Transforms into an unsigned domain:zi=ri2z_i = r_i^2zi\u200b=ri2\u200b.\n- Appliessi=StickBreakingTransform(zi)s_i = StickBreakingTransform(z_i)si\u200b=StickBreakingTransform(zi\u200b).\n- Transforms back into signed domain:yi=sign(ri)\u2217siy_i = sign(r_i) * \\sqrt{s_i}yi\u200b=sign(ri\u200b)\u2217si\u200b\u200b.\nStickBreakingTransform\nTransform via the cumulative distribution function of a probability distribution.\ndistribution(Distribution) \u2013 Distribution whose cumulative distribution function to use for\nthe transformation.\nExample:\n\n```python\n# Construct a Gaussian copula from a multivariate normal.\nbase_dist = MultivariateNormal(\n    loc=torch.zeros(2),\n    scale_tril=LKJCholesky(2).sample(),\n)\ntransform = CumulativeDistributionTransform(Normal(0, 1))\ncopula = TransformedDistribution(base_dist, [transform])\n\n```\n\nTransform via the mappingy=exp\u2061(x)y = \\exp(x)y=exp(x).\nWrapper around another transform to treatreinterpreted_batch_ndims-many extra of the right most dimensions as\ndependent. This has no effect on the forward or backward transforms, but\ndoes sum outreinterpreted_batch_ndims-many of the rightmost dimensions\ninlog_abs_det_jacobian().\nreinterpreted_batch_ndims\nreinterpreted_batch_ndims\nlog_abs_det_jacobian()\nbase_transform(Transform) \u2013 A base transform.\nTransform\nreinterpreted_batch_ndims(int) \u2013 The number of extra rightmost\ndimensions to treat as dependent.\nTransform from unconstrained matrices to lower-triangular matrices with\nnonnegative diagonal entries.\nThis is useful for parameterizing positive definite matrices in terms of\ntheir Cholesky factorization.\nTransform from unconstrained matrices to positive-definite matrices.\nTransform via the mappingy=xexponenty = x^{\\text{exponent}}y=xexponent.\nUnit Jacobian transform to reshape the rightmost part of a tensor.\nNote thatin_shapeandout_shapemust have the same number of\nelements, just as fortorch.Tensor.reshape().\nin_shape\nout_shape\ntorch.Tensor.reshape()\nin_shape(torch.Size) \u2013 The input event shape.\nout_shape(torch.Size) \u2013 The output event shape.\ncache_size(int) \u2013 Size of cache. If zero, no caching is done. If one,\nthe latest single value is cached. Only 0 and 1 are supported. (Default 0.)\nTransform via the mappingy=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)}y=1+exp(\u2212x)1\u200bandx=logit(y)x = \\text{logit}(y)x=logit(y).\nTransform via the mappingSoftplus(x)=log\u2061(1+exp\u2061(x))\\text{Softplus}(x) = \\log(1 + \\exp(x))Softplus(x)=log(1+exp(x)).\nThe implementation reverts to the linear function whenx>20x > 20x>20.\nTransform via the mappingy=tanh\u2061(x)y = \\tanh(x)y=tanh(x).\nIt is equivalent to\n\n```python\nComposeTransform(\n    [\n        AffineTransform(0.0, 2.0),\n        SigmoidTransform(),\n        AffineTransform(-1.0, 2.0),\n    ]\n)\n\n```\n\nHowever this might not be numerically stable, thus it is recommended to useTanhTransforminstead.\nNote that one should usecache_size=1when it comes toNaN/Infvalues.\nTransform from unconstrained space to the simplex viay=exp\u2061(x)y = \\exp(x)y=exp(x)then\nnormalizing.\nThis is not bijective and cannot be used for HMC. However this acts mostly\ncoordinate-wise (except for the final normalization), and thus is\nappropriate for coordinate-wise optimization algorithms.\nTransform functor that applies a sequence of transformstseqcomponent-wise to each submatrix atdimin a way compatible withtorch.stack().\ntorch.stack()\nExample:\n\n```python\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)\nt = StackTransform([ExpTransform(), identity_transform], dim=1)\ny = t(x)\n\n```\n\nTransform from unconstrained space to the simplex of one additional\ndimension via a stick-breaking process.\nThis transform arises as an iterated sigmoid transform in a stick-breaking\nconstruction of theDirichletdistribution: the first logit is\ntransformed via sigmoid to the first probability and the probability of\neverything else, and then the process recurses.\nThis is bijective and appropriate for use in HMC; however it mixes\ncoordinates together and is less appropriate for optimization.\nAbstract class for invertable transformations with computable log\ndet jacobians. They are primarily used intorch.distributions.TransformedDistribution.\ntorch.distributions.TransformedDistribution\nCaching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values\nsince the autograd graph may be reversed. For example while the following\nworks with or without caching:\n\n```python\ny = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n\n```\n\nHowever the following will error when caching due to dependency reversal:\n\n```python\ny = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n\n```\n\nDerived classes should implement one or both of_call()or_inverse(). Derived classes that setbijective=Trueshould also\nimplementlog_abs_det_jacobian().\n_call()\n_inverse()\nlog_abs_det_jacobian()\ncache_size(int) \u2013 Size of cache. If zero, no caching is done. If one,\nthe latest single value is cached. Only 0 and 1 are supported.\ndomain(Constraint) \u2013 The constraint representing valid inputs to this transform.\nConstraint\ncodomain(Constraint) \u2013 The constraint representing valid outputs to this transform\nwhich are inputs to the inverse transform.\nConstraint\nbijective(bool) \u2013 Whether this transform is bijective. A transformtis bijective ifft.inv(t(x))==xandt(t.inv(y))==yfor everyxin the domain andyin\nthe codomain. Transforms that are not bijective should at least\nmaintain the weaker pseudoinverse propertiest(t.inv(t(x))==t(x)andt.inv(t(t.inv(y)))==t.inv(y).\nt\nt.inv(t(x))==x\nt(t.inv(y))==y\nx\ny\nt(t.inv(t(x))==t(x)\nt.inv(t(t.inv(y)))==t.inv(y)\nsign(intorTensor) \u2013 For bijective univariate transforms, this\nshould be +1 or -1 depending on whether transform is monotone\nincreasing or decreasing.\nReturns the inverseTransformof this transform.\nThis should satisfyt.inv.invist.\nTransform\nt.inv.invist\nReturns the sign of the determinant of the Jacobian, if applicable.\nIn general this only makes sense for bijective transforms.\nComputes the log det jacobianlog |dy/dx|given input and output.\nInfers the shape of the forward computation, given the input shape.\nDefaults to preserving shape.\nInfers the shapes of the inverse computation, given the output shape.\nDefaults to preserving shape.\n\n## Constraints#\n\nConstraints\nAbstract base class for constraints.\nA constraint object represents a region over which a variable is valid,\ne.g. within which a variable can be optimized.\nis_discrete(bool) \u2013 Whether constrained space is discrete.\nDefaults to False.\nevent_dim(int) \u2013 Number of rightmost dimensions that together define\nan event. Thecheck()method will remove this many dimensions\nwhen computing validity.\ncheck()\nReturns a byte tensor ofsample_shape+batch_shapeindicating\nwhether each event in value satisfies this constraint.\nsample_shape+batch_shape\nalias of_Cat\n_Cat\nalias of_DependentProperty\n_DependentProperty\nalias of_GreaterThan\n_GreaterThan\nalias of_GreaterThanEq\n_GreaterThanEq\nalias of_IndependentConstraint\n_IndependentConstraint\nalias of_IntegerInterval\n_IntegerInterval\nalias of_Interval\n_Interval\nalias of_HalfOpenInterval\n_HalfOpenInterval\nChecks ifconstraintis a_Dependentobject.\nconstraint\n_Dependent\nconstraint\u2013 AConstraintobject.\nConstraint\nTrue ifconstraintcan be refined to the type_Dependent, False otherwise.\nconstraint\n_Dependent\nbool\nbool\nExamples\n\n```python\n>>> import torch\n>>> from torch.distributions import Bernoulli\n>>> from torch.distributions.constraints import is_dependent\n\n```\n\n\n```python\n>>> dist = Bernoulli(probs=torch.tensor([0.6], requires_grad=True))\n>>> constraint1 = dist.arg_constraints[\"probs\"]\n>>> constraint2 = dist.arg_constraints[\"logits\"]\n\n```\n\n\n```python\n>>> for constraint in [constraint1, constraint2]:\n>>>     if is_dependent(constraint):\n>>>         continue\n\n```\n\nalias of_LessThan\n_LessThan\nConstraint for theMixtureSameFamilydistribution that adds back the rightmost batch dimension before\nperforming the validity check with the component distribution\nconstraint.\nMixtureSameFamily\nbase_constraint\u2013 TheConstraintobject of\nthe component distribution of\ntheMixtureSameFamilydistribution.\nConstraint\nMixtureSameFamily\nCheck validity ofvalueas a possible outcome of sampling\ntheMixtureSameFamilydistribution.\nvalue\nMixtureSameFamily\nalias of_Multinomial\n_Multinomial\nalias of_Stack\n_Stack\n\n## ConstraintRegistry#\n\nConstraintRegistry\nPyTorch provides two globalConstraintRegistryobjects that linkConstraintobjects toTransformobjects. These objects both\ninput constraints and return transforms, but they have different guarantees on\nbijectivity.\nConstraintRegistry\nConstraint\nTransform\nbiject_to(constraint)looks up a bijectiveTransformfromconstraints.realto the givenconstraint. The returned transform is guaranteed to have.bijective=Trueand should implement.log_abs_det_jacobian().\nbiject_to(constraint)\nTransform\nconstraints.real\nconstraint\n.bijective=True\n.log_abs_det_jacobian()\ntransform_to(constraint)looks up a not-necessarily bijectiveTransformfromconstraints.realto the givenconstraint. The returned transform is not guaranteed to\nimplement.log_abs_det_jacobian().\ntransform_to(constraint)\nTransform\nconstraints.real\nconstraint\n.log_abs_det_jacobian()\nThetransform_to()registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s.arg_constraintsdict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:\ntransform_to()\n.arg_constraints\n\n```python\nloc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints[\"scale\"])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()\n\n```\n\nThebiject_to()registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained.supportare\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:\nbiject_to()\n.support\n\n```python\ndist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()\n\n```\n\nNote\nAn example wheretransform_toandbiject_todiffer isconstraints.simplex:transform_to(constraints.simplex)returns aSoftmaxTransformthat simply\nexponentiates and normalizes its inputs; this is a cheap and mostly\ncoordinate-wise operation appropriate for algorithms like SVI. In\ncontrast,biject_to(constraints.simplex)returns aStickBreakingTransformthat\nbijects its input down to a one-fewer-dimensional space; this a more\nexpensive less numerically stable transform but is needed for algorithms\nlike HMC.\ntransform_to\nbiject_to\nconstraints.simplex\ntransform_to(constraints.simplex)\nSoftmaxTransform\nbiject_to(constraints.simplex)\nStickBreakingTransform\nThebiject_toandtransform_toobjects can be extended by user-defined\nconstraints and transforms using their.register()method either as a\nfunction on singleton constraints:\nbiject_to\ntransform_to\n.register()\n\n```python\ntransform_to.register(my_constraint, my_transform)\n\n```\n\nor as a decorator on parameterized constraints:\n\n```python\n@transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)\n\n```\n\nYou can create your own registry by creating a newConstraintRegistryobject.\nConstraintRegistry\nRegistry to link constraints to transforms.\nRegisters aConstraintsubclass in this registry. Usage:\nConstraint\n\n```python\n@my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n\n```\n\nconstraint(subclass ofConstraint) \u2013 A subclass ofConstraint, or\na singleton object of the desired class.\nConstraint\nConstraint\nfactory(Callable) \u2013 A callable that inputs a constraint object and returns\naTransformobject.\nTransform",
  "url": "https://pytorch.org/docs/stable/distributions.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
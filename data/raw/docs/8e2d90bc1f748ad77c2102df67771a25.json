{
  "doc_id": "8e2d90bc1f748ad77c2102df67771a25",
  "source": "pytorch_docs",
  "title": "Dynamo Core Concepts \u2014 PyTorch 2.9 documentation",
  "text": "\n## Dynamo Core Concepts#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nSummary:\nDynamo,torch.compile\u2019s frontend, performstracingto capture the semantics of a Python function\n(and its nested function calls) into a linear sequence of operations (the \u201c(FX) graph\u201d),\nresidual bytecode, and \u201cguards\u201d (a list of conditions under which the graph and bytecode are valid).\ntorch.compile\nUnsupported Python features lead tograph breaks, where Dynamo compiles a partial graph acquired from tracing,\nthen runs the unsupported code, then resumes tracing.\nGraph breaks may lead to slowness in torch.compile and prevent backend optimization opportunities.\nIf you\u2019re not seeing the performance you expect, then check for graph breaks.\n\n## Dynamo Tracing#\n\ntorch.compile\u2019s frontend (Dynamo) is a custom Python bytecode interpreter designed to allow graph compilation\nin PyTorch programs while retaining the full flexibility of Python. Given a function to be compiled, Dynamo\ninterprets Python bytecode to extract sequences of PyTorch operations into 1 or more FX graphs that may be further optimized by a backend.\ntorch.compile\n\nFor example, for the functionfin the above diagram, Dynamo produces:\nf\na singleFX graphthat takes in the original input plus some additional inputs required by the function.\nPython bytecodethat can be used as a drop-in replacement forf. In our example, the bytecode retrieves\nthe additional inputs and passes it to the graph and also contains unoptimizable Python side effects (the list append)\nf\nguardsthat specify the conditions under which the graph and bytecode are valid. Unless otherwise specified,\nthe graph produced by Dynamo specializes on the shapes of input Tensors.\n\n## Graph Breaks#\n\nDynamo traces your code and attempts to capture your PyTorch code into a single computation graph of PyTorch\noperators (FX graph). However, this is not always possible. When encountering code that can\u2019t be traced, a \u201cgraph break\u201d occurs.\nIn the defaulttorch.compilesettings, a graph break involves compiling the FX graph that has been determined so far,\nrunning the unsupported code in regular Python, then resuming tracing after the unsupported code with a new FX graph.\ntorch.compile\nGraph breaks are a feature that allows Dynamo to run over arbitrary Python code and carve out functional subgraphs that can each be individually optimized.\nHowever, it is possible for graph breaks to lead to unexpected slowness intorch.compile.\nIf you\u2019re not getting the speedups you expect, we recommend checking for graph breaks and removing them.\ntorch.compile\nGraph breaks may occur on things like:\nData-dependent if-statements\nMany Python built-in functions\nC functions\nBelow is an example of a graph break due to calling an unsupported operationtorch.save:\ntorch.save\n\n```python\n@torch.compile\ndef f(x):\n   y = x ** 2  / 2\n   torch.save(y, \"foo.pt\")  # torch.save is an unsupported operation\n   z = y ** 3 / 6\n   return z\n\nx = torch.randn(3)\nprint(f(x))\n\n```\n\n\n```python\ntensor([6.3085e-03, 8.2592e-01, 5.1903e-08])\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_265/215272159.py:4\nGraph Break Reason: Attempted to call function marked as skipped\n  Explanation: Dynamo developers have intentionally marked that the function `save` in file `/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/serialization.py` should not be traced.\n  Hint: Avoid calling the function `save`.\n  Hint: Apply `@torch._dynamo.dont_skip_tracing` to the function `save` to force tracing into the function. More graph breaks may occur as a result of attempting to trace into the function.\n  Hint: Please file an issue to PyTorch.\n\n  Developer debug context: module: torch.serialization, qualname: save, skip reason: <missing reason>\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_265/215272159.py\", line 9, in <module>\n    print(f(x))\n  File \"/tmp/ipykernel_265/215272159.py\", line 4, in f\n    torch.save(y, \"foo.pt\")  # torch.save is an unsupported operation\n\n```\n\nThe semantics oftorch.compile(f)(x)are roughly this:\ntorch.compile(f)(x)\n\n```python\ndef compiled_f_semantics(x):\n   y = torch.compile(g, fullgraph=True)(x)\n   torch.save(y, \"foo.pt\")\n   z = torch.compile(h, fullgraph=True)(x)\n   return z\n\ndef g(x):\n    return x ** 2  / 2\n\ndef h(x):\n    return y ** 3 / 6\n\n```\n\n\n## Guards#\n\ntorch.compilemakes some assumptions about runtime values as we trace through code. During tracing, we generate \u201cguards\u201d,\nwhich are runtime checks for these assumptions. Guards are run in future calls to the compiled function to determine if we\ncan reuse previously compiled code. Examples of runtime checks are constant values, types, and object IDs.\ntorch.compile\nBelow is an example of generated guards. TheTENSOR_MATCHguard checks for the input\u2019s type, device, dtype, shape, etc.\nTENSOR_MATCH\n\n```python\n@torch.compile\ndef fn(x):\n    return x + 1\n\nprint(fn(torch.ones(3, 3)))\n\n```\n\n\n```python\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n```\n\n\n```python\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3, 3], stride=[3, 1])  # return x + 1  # mp/ipykernel_265/1068332425.py:3 in fn\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x + 1  # mp/ipykernel_265/1068332425.py:3 in fn\n\nGuard eval latency = 155.78 us\n\n```\n\n\n## Recompilations#\n\nIf the guards fail for every instance of previously compiled code, thentorch.compilemust \u201crecompile\u201d the function,\nrequiring the original code to be traced again. In the example below, recompilation is necessary because the guard checking the tensor argument\u2019s shape failed.\ntorch.compile\n\n```python\n@torch.compile\ndef fn(x):\n    return x + 1\n\nprint(fn(torch.ones(3, 3)))\nprint(fn(torch.ones(4, 4)))\n\n```\n\n\n```python\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_265/420870727.py:1\n    triggered by the following guard failure(s):\n    - 3/0: tensor 'x' size mismatch at index 0. expected 3, actual 4\n\n```\n\n\n## Dynamic Shapes#\n\ntorch.compileinitially assumes tensor shapes are static/constant and guards based on these assumptions. By using \u201cdynamic shapes,\u201d\nwe can gettorch.compileto produce compiled code that can accept tensor inputs with different shapes - we avoid recompiling every time shapes differ.\nBy default, automatic dynamic shapes are enabled intorch.compile(dynamic=None)- if compilation fails due to shape mismatch,\nrecompilation is attempted with dynamic shapes. Dynamic shapes can also be fully enabled (dynamic=True) or disabled (dynamic=False).\ntorch.compile\ntorch.compile\ntorch.compile(dynamic=None)\ndynamic=True\ndynamic=False\nBelow, we enable dynamic shapes and note that we no longer need to recompile.\n\n```python\n@torch.compile(dynamic=True)\ndef fn(x):\n    return x + 1\n\nprint(fn(torch.ones(3, 3)))\nprint(fn(torch.ones(4, 4)))\n\n```\n\n\n```python\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n```\n\n\n```python\ncreate_env\ncreate_symbol s77 = 3 for L['x'].size()[0] [2, int_oo] return x + 1  # mp/ipykernel_265/1458103805.py:3 in fn (_dynamo/variables/builder.py:3508 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\"\ncreate_symbol s77 duck sized L['x'].size()[1]\neval False == False [statically known]\neval False == False [statically known]\nproduce_guards\ntrack_symint L['x'].size()[0] s77 None\ntrack_symint L['x'].size()[1] s77 None\ntrack_symint L['x'].stride()[0] s77 None\ntrack_symint L['x'].stride()[1] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].stride()[1] == 1\nSkipping guard L['x'].storage_offset() == 0\n\n```\n\nFor more information on dynamic shapes, seeThe dynamic shapes manual.",
  "url": "https://pytorch.org/docs/stable/compile/programming_model.dynamo_core_concepts.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
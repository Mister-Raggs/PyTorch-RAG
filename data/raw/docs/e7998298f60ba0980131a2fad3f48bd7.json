{
  "doc_id": "e7998298f60ba0980131a2fad3f48bd7",
  "source": "pytorch_docs",
  "title": "torch.backends \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.backends#\n\nCreated On: Sep 16, 2020 | Last Updated On: Aug 26, 2025\ntorch.backendscontrols the behavior of various backends that PyTorch supports.\ntorch.backends\nThese backends include:\ntorch.backends.cpu\ntorch.backends.cpu\ntorch.backends.cuda\ntorch.backends.cuda\ntorch.backends.cudnn\ntorch.backends.cudnn\ntorch.backends.cusparselt\ntorch.backends.cusparselt\ntorch.backends.mha\ntorch.backends.mha\ntorch.backends.mps\ntorch.backends.mps\ntorch.backends.mkl\ntorch.backends.mkl\ntorch.backends.mkldnn\ntorch.backends.mkldnn\ntorch.backends.nnpack\ntorch.backends.nnpack\ntorch.backends.openmp\ntorch.backends.openmp\ntorch.backends.opt_einsum\ntorch.backends.opt_einsum\ntorch.backends.xeon\ntorch.backends.xeon\n\n## torch.backends.cpu#\n\nReturn cpu capability as a string value.\nPossible values:\n- \u201cDEFAULT\u201d\n- \u201cVSX\u201d\n- \u201cZ VECTOR\u201d\n- \u201cNO AVX\u201d\n- \u201cAVX2\u201d\n- \u201cAVX512\u201d\n- \u201cSVE256\u201d\nstr\n\n## torch.backends.cuda#\n\nReturn whether PyTorch is built with CUDA support.\nNote that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run on a machine with working CUDA drivers and devices, we would be able to use it.\nAboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. allow_tf32 is going to be deprecated. SeeTensorFloat-32 (TF32) on Ampere (and later) devices.\nbool\nAboolthat controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.\nbool\nAboolthat controls whether reduced precision reductions are allowed with bf16 GEMMs.\nbool\ncufft_plan_cachecontains the cuFFT plan caches for each CUDA device.\nQuery a specific devicei\u2019s cache viatorch.backends.cuda.cufft_plan_cache[i].\ncufft_plan_cache\nA readonlyintthat shows the number of plans currently in a cuFFT plan cache.\nint\nAintthat controls the capacity of a cuFFT plan cache.\nint\nClears a cuFFT plan cache.\nOverride the library PyTorch uses for BLAS operations. Choose between cuBLAS, cuBLASLt, and CK [ROCm-only].\nWarning\nThis flag is experimental and subject to change.\nWhen PyTorch runs a CUDA BLAS operation it defaults to cuBLAS even if both cuBLAS and cuBLASLt are available.\nFor PyTorch built for ROCm, hipBLAS, hipBLASLt, and CK may offer different performance.\nThis flag (astr) allows overriding which BLAS library to use.\nstr\nIf\u201ccublas\u201dis set then cuBLAS will be used wherever possible.\nIf\u201ccublaslt\u201dis set then cuBLASLt will be used wherever possible.\nIf\u201cck\u201dis set then CK will be used wherever possible.\nIf\u201cdefault\u201d(the default) is set then heuristics will be used to pick between the other options.\nWhen no input is given, this function returns the currently preferred library.\nUser may use the environment variable TORCH_BLAS_PREFER_CUBLASLT=1 to set the preferred library to cuBLASLt\nglobally.\nThis flag only sets the initial value of the preferred library and the preferred library\nmay still be overridden by this function call later in your script.\nNote: When a library is preferred other libraries may still be used if the preferred library\ndoesn\u2019t implement the operation(s) called.\nThis flag may achieve better performance if PyTorch\u2019s library selection is incorrect\nfor your application\u2019s inputs.\n_BlasBackend\n[ROCm-only]\nOverride the backend PyTorch uses in ROCm environments for Flash Attention. Choose between AOTriton and CK\nWarning\nThis flag is experimental and subject to change.\nWhen Flash Attention is enabled and desired, PyTorch defaults to using AOTriton as the backend.\nThis flag (astr) allows users to override this backend to use composable_kernel\nstr\nIf\u201cdefault\u201dis set then the default backend will be used wherever possible. Currently AOTriton.\nIf\u201caotriton\u201dis set then AOTriton will be used wherever possible.\nIf\u201cck\u201dis set then CK will be used wherever possible.\nWhen no input is given, this function returns the currently preferred library.\nUser may use the environment variable TORCH_ROCM_FA_PREFER_CK=1 to set the preferred library to CK\nglobally.\nNote: When a library is preferred other libraries may still be used if the preferred library\ndoesn\u2019t implement the operation(s) called.\nThis flag may achieve better performance if PyTorch\u2019s library selection is incorrect\nfor your application\u2019s inputs.\n_ROCmFABackend\nOverride the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for CUDA linear algebra operations.\nWarning\nThis flag is experimental and subject to change.\nWhen PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries,\nand if both are available it decides which to use with a heuristic.\nThis flag (astr) allows overriding those heuristics.\nstr\nIf\u201ccusolver\u201dis set then cuSOLVER will be used wherever possible.\nIf\u201cmagma\u201dis set then MAGMA will be used wherever possible.\nIf\u201cdefault\u201d(the default) is set then heuristics will be used to pick between\ncuSOLVER and MAGMA if both are available.\nWhen no input is given, this function returns the currently preferred library.\nUser may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER\nglobally.\nThis flag only sets the initial value of the preferred library and the preferred library\nmay still be overridden by this function call later in your script.\nNote: When a library is preferred other libraries may still be used if the preferred library\ndoesn\u2019t implement the operation(s) called.\nThis flag may achieve better performance if PyTorch\u2019s heuristic library selection is incorrect\nfor your application\u2019s inputs.\nCurrently supported linalg operators:\ntorch.linalg.inv()\ntorch.linalg.inv()\ntorch.linalg.inv_ex()\ntorch.linalg.inv_ex()\ntorch.linalg.cholesky()\ntorch.linalg.cholesky()\ntorch.linalg.cholesky_ex()\ntorch.linalg.cholesky_ex()\ntorch.cholesky_solve()\ntorch.cholesky_solve()\ntorch.cholesky_inverse()\ntorch.cholesky_inverse()\ntorch.linalg.lu_factor()\ntorch.linalg.lu_factor()\ntorch.linalg.lu()\ntorch.linalg.lu()\ntorch.linalg.lu_solve()\ntorch.linalg.lu_solve()\ntorch.linalg.qr()\ntorch.linalg.qr()\ntorch.linalg.eigh()\ntorch.linalg.eigh()\ntorch.linalg.eighvals()\ntorch.linalg.eighvals()\ntorch.linalg.svd()\ntorch.linalg.svd()\ntorch.linalg.svdvals()\ntorch.linalg.svdvals()\n_LinalgBackend\nWarning\nThis flag is beta and subject to change.\nReturns whether flash scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables memory efficient scaled dot product attention.\nWarning\nThis flag is beta and subject to change.\nReturns whether memory efficient scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables flash scaled dot product attention.\nWarning\nThis flag is beta and subject to change.\nReturns whether math scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables math scaled dot product attention.\nWarning\nThis flag is beta and subject to change.\nReturns whether fp16/bf16 reduction in math scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables fp16/bf16 reduction in math scaled dot product attention.\nWarning\nThis flag is beta and subject to change.\nReturns whether cuDNN scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables cuDNN scaled dot product attention.\nCheck if PyTorch was built with FlashAttention for scaled_dot_product_attention.\nTrue if FlashAttention is built and available; otherwise, False.\nbool\nNote\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False\nin non-CUDA environments.\nCheck if FlashAttention can be utilized in scaled_dot_product_attention.\nparams(_SDPAParams) \u2013 An instance of SDPAParams containing the tensors for query,\nkey, value, an optional attention mask, dropout rate, and\na flag indicating if the attention is causal.\ndebug(bool) \u2013 Whether to logging.warn debug information as to why FlashAttention could not be run.\nDefaults to False.\nTrue if FlashAttention can be used with the given parameters; otherwise, False.\nbool\nNote\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False\nin non-CUDA environments.\nCheck if efficient_attention can be utilized in scaled_dot_product_attention.\nparams(_SDPAParams) \u2013 An instance of SDPAParams containing the tensors for query,\nkey, value, an optional attention mask, dropout rate, and\na flag indicating if the attention is causal.\ndebug(bool) \u2013 Whether to logging.warn with information as to why efficient_attention could not be run.\nDefaults to False.\nTrue if efficient_attention can be used with the given parameters; otherwise, False.\nbool\nNote\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False\nin non-CUDA environments.\nCheck if cudnn_attention can be utilized in scaled_dot_product_attention.\nparams(_SDPAParams) \u2013 An instance of SDPAParams containing the tensors for query,\nkey, value, an optional attention mask, dropout rate, and\na flag indicating if the attention is causal.\ndebug(bool) \u2013 Whether to logging.warn with information as to why cuDNN attention could not be run.\nDefaults to False.\nTrue if cuDNN can be used with the given parameters; otherwise, False.\nbool\nNote\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False\nin non-CUDA environments.\nWarning\nThis flag is beta and subject to change.\nThis context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention.\nUpon exiting the context manager, the previous state of the flags will be restored.\n\n## torch.backends.cudnn#\n\nReturn the version of cuDNN.\nReturn a bool indicating if CUDNN is currently available.\nAboolthat controls whether cuDNN is enabled.\nbool\nAboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. allow_tf32 is going to be deprecated. SeeTensorFloat-32 (TF32) on Ampere (and later) devices.\nbool\nAboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().\nbool\ntorch.are_deterministic_algorithms_enabled()\ntorch.use_deterministic_algorithms()\nAboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.\nbool\nAintthat specifies the maximum number of cuDNN convolution algorithms to try whentorch.backends.cudnn.benchmarkis True. Setbenchmark_limitto zero to try every\navailable algorithm. Note that this setting only affects convolutions dispatched via the\ncuDNN v8 API.\nint\n\n## torch.backends.cusparselt#\n\nReturn the version of cuSPARSELt\nOptional[int]\nReturn a bool indicating if cuSPARSELt is currently available.\nbool\n\n## torch.backends.mha#\n\nReturns whether fast path for TransformerEncoder and MultiHeadAttention\nis enabled, orTrueif jit is scripting.\nTrue\nNote\nThe fastpath might not be run even ifget_fastpath_enabledreturnsTrueunless all conditions on inputs are met.\nget_fastpath_enabled\nTrue\nbool\nSets whether fast path is enabled\n\n## torch.backends.miopen#\n\nAboolthat, if True, causes MIOpen to use Immediate Mode\n(https://rocm.docs.amd.com/projects/MIOpen/en/latest/how-to/find-and-immediate.html).\nbool\n\n## torch.backends.mps#\n\nReturn a bool indicating if MPS is currently available.\nbool\nReturn whether PyTorch is built with MPS support.\nNote that this doesn\u2019t necessarily mean MPS is available; just that\nif this PyTorch binary were run a machine with working MPS drivers\nand devices, we would be able to use it.\nbool\n\n## torch.backends.mkl#\n\nReturn whether PyTorch is built with MKL support.\nOn-demand oneMKL verbosing functionality.\nTo make it easier to debug performance issues, oneMKL can dump verbose\nmessages containing execution information like duration while executing\nthe kernel. The verbosing functionality can be invoked via an environment\nvariable namedMKL_VERBOSE. However, this methodology dumps messages in\nall steps. Those are a large amount of verbose messages. Moreover, for\ninvestigating the performance issues, generally taking verbose messages\nfor one single iteration is enough. This on-demand verbosing functionality\nmakes it possible to control scope for verbose message dumping. In the\nfollowing example, verbose messages will be dumped out for the second\ninference only.\n\n```python\nimport torch\n\nmodel(data)\nwith torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):\n    model(data)\n\n```\n\nlevel\u2013 Verbose level\n-VERBOSE_OFF: Disable verbosing\n-VERBOSE_ON:  Enable verbosing\nVERBOSE_OFF\nVERBOSE_ON\n\n## torch.backends.mkldnn#\n\nOn-demand oneDNN (former MKL-DNN) verbosing functionality.\nTo make it easier to debug performance issues, oneDNN can dump verbose\nmessages containing information like kernel size, input data size and\nexecution duration while executing the kernel. The verbosing functionality\ncan be invoked via an environment variable namedDNNL_VERBOSE. However,\nthis methodology dumps messages in all steps. Those are a large amount of\nverbose messages. Moreover, for investigating the performance issues,\ngenerally taking verbose messages for one single iteration is enough.\nThis on-demand verbosing functionality makes it possible to control scope\nfor verbose message dumping. In the following example, verbose messages\nwill be dumped out for the second inference only.\n\n```python\nimport torch\n\nmodel(data)\nwith torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):\n    model(data)\n\n```\n\nlevel\u2013 Verbose level\n-VERBOSE_OFF: Disable verbosing\n-VERBOSE_ON:  Enable verbosing\n-VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation\nVERBOSE_OFF\nVERBOSE_ON\nVERBOSE_ON_CREATION\n\n## torch.backends.nnpack#\n\nReturn whether PyTorch is built with NNPACK support.\nContext manager for setting if nnpack is enabled globally\nSet if nnpack is enabled globally\n\n## torch.backends.openmp#\n\nReturn whether PyTorch is built with OpenMP support.\n\n## torch.backends.opt_einsum#\n\nReturn a bool indicating if opt_einsum is currently available.\nYou must install opt-einsum in order for torch to automatically optimize einsum. To\nmake opt-einsum available, you can install it along with torch:pipinstalltorch[opt-einsum]or by itself:pipinstallopt-einsum. If the package is installed, torch will import\nit automatically and use it accordingly. Use this function to check whether opt-einsum\nwas installed and properly imported by torch.\npipinstalltorch[opt-einsum]\npipinstallopt-einsum\nbool\nReturn the opt_einsum package if opt_einsum is currently available, else None.\nAny\nAboolthat controls whether opt_einsum is enabled (Trueby default). If so,\ntorch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)\nif available to calculate an optimal path of contraction for faster performance.\nbool\nTrue\nIf opt_einsum is not available, torch.einsum will fall back to the default contraction path\nof left to right.\nAstrthat specifies which strategies to try whentorch.backends.opt_einsum.enabledisTrue. By default, torch.einsum will try the \u201cauto\u201d strategy, but the \u201cgreedy\u201d and \u201coptimal\u201d\nstrategies are also supported. Note that the \u201coptimal\u201d strategy is factorial on the number of\ninputs as it tries all possible paths. See more details in opt_einsum\u2019s docs\n(https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).\nstr\ntorch.backends.opt_einsum.enabled\nTrue\n\n## torch.backends.xeon#\n",
  "url": "https://pytorch.org/docs/stable/backends.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
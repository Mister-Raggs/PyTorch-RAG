{
  "doc_id": "9ba806bf717056232c967724a3be3da2",
  "source": "pytorch_docs",
  "title": "UX Limitations \u2014 PyTorch 2.9 documentation",
  "text": "\n## UX Limitations#\n\nCreated On: Jun 12, 2025 | Last Updated On: Jun 12, 2025\ntorch.func, likeJAX, has restrictions around\nwhat can be transformed. In general, JAX\u2019s limitations are that transforms\nonly work with pure functions: that is, functions where the output is completely\ndetermined by the input and that do not involve side effects (like mutation).\nWe have a similar guarantee: our transforms work well with pure functions.\nHowever, we do support certain in-place operations. On one hand, writing code\ncompatible with function transforms may involve changing how you write PyTorch\ncode, on the other hand, you may find that our transforms let you express things\nthat were previously difficult to express in PyTorch.\n\n## General limitations#\n\nAll torch.func transforms share a limitation in that a function should not\nassign to global variables. Instead, all outputs to a function must be returned\nfrom the function. This restriction comes from how torch.func is implemented:\neach transform wraps Tensor inputs in special torch.func Tensor subclasses\nthat facilitate the transform.\nSo, instead of the following:\n\n```python\nimport torch\nfrom torch.func import grad\n\n# Don't do this\nintermediate = None\n\ndef f(x):\n  global intermediate\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z\n\nx = torch.randn([])\ngrad_x = grad(f)(x)\n\n```\n\nPlease rewritefto returnintermediate:\nf\nintermediate\n\n```python\ndef f(x):\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z, intermediate\n\ngrad_x, intermediate = grad(f, has_aux=True)(x)\n\n```\n\n\n## torch.autograd APIs#\n\nIf you are trying to use atorch.autogradAPI liketorch.autograd.gradortorch.autograd.backwardinside of a function being transformed byvmap()or one of torch.func\u2019s AD transforms (vjp(),jvp(),jacrev(),jacfwd()), the transform may not be able to transform over it.\nIf it is unable to do so, you\u2019ll receive an error message.\ntorch.autograd\ntorch.autograd.grad\ntorch.autograd.backward\nvmap()\nvjp()\njvp()\njacrev()\njacfwd()\nThis is a fundamental design limitation in how PyTorch\u2019s AD support is implemented\nand the reason why we designed the torch.func library. Please instead use the torch.func\nequivalents of thetorch.autogradAPIs:\ntorch.autograd\ntorch.autograd.grad,Tensor.backward->torch.func.vjportorch.func.grad\ntorch.autograd.grad\nTensor.backward\ntorch.func.vjp\ntorch.func.grad\ntorch.autograd.functional.jvp->torch.func.jvp\ntorch.autograd.functional.jvp\ntorch.func.jvp\ntorch.autograd.functional.jacobian->torch.func.jacrevortorch.func.jacfwd\ntorch.autograd.functional.jacobian\ntorch.func.jacrev\ntorch.func.jacfwd\ntorch.autograd.functional.hessian->torch.func.hessian\ntorch.autograd.functional.hessian\ntorch.func.hessian\n\n## vmap limitations#\n\nNote\nvmap()is our most restrictive transform.\nThe grad-related transforms (grad(),vjp(),jvp()) do not\nhave these limitations.jacfwd()(andhessian(), which is\nimplemented withjacfwd()) is a composition ofvmap()andjvp()so it also has these limitations.\nvmap()\ngrad()\nvjp()\njvp()\njacfwd()\nhessian()\njacfwd()\nvmap()\njvp()\nvmap(func)is a transform that returns a function that mapsfuncover\nsome new dimension of each input Tensor. The mental model for vmap is that it is\nlike running a for-loop: for pure functions (i.e. in the absence of side\neffects),vmap(f)(x)is equivalent to:\nvmap(func)\nfunc\nvmap(f)(x)\n\n```python\ntorch.stack([f(x_i) for x_i in x.unbind(0)])\n\n```\n\n\n## Mutation: Arbitrary mutation of Python data structures#\n\nIn the presence of side effects,vmap()no longer acts like it is running\na for-loop. For example, the following function:\nvmap()\n\n```python\ndef f(x, list):\n  list.pop()\n  print(\"hello!\")\n  return x.sum(0)\n\nx = torch.randn(3, 1)\nlst = [0, 1, 2, 3]\n\nresult = vmap(f, in_dims=(0, None))(x, lst)\n\n```\n\nwill print \u201chello!\u201d once and pop only one element fromlst.\nlst\nvmap()executesfa single time, so all side effects only happen once.\nvmap()\nf\nThis is a consequence of how vmap is implemented. torch.func has a special,\ninternal BatchedTensor class.vmap(f)(*inputs)takes all Tensor inputs,\nturns them into BatchedTensors, and callsf(*batched_tensor_inputs).\nBatchedTensor overrides the PyTorch API to produce batched (i.e. vectorized)\nbehavior for each PyTorch operator.\nvmap(f)(*inputs)\nf(*batched_tensor_inputs)\n\n## Mutation: in-place PyTorch Operations#\n\nYou might be here due to receiving an error about vmap-incompatible in-place\noperations.vmap()will raise an error if it encounters an unsupported PyTorch\nin-place operation and it will succeed otherwise. Unsupported operations\nare those that would cause a Tensor with more elements to be written to a\nTensor with fewer elements. Here\u2019s an example of how this can occur:\nvmap()\n\n```python\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(1)\ny = torch.randn(3, 1)  # When vmapped over, looks like it has shape [1]\n\n# Raises an error because `x` has fewer elements than `y`.\nvmap(f, in_dims=(None, 0))(x, y)\n\n```\n\nxis a Tensor with one element,yis a Tensor with three elements.x+yhas three elements (due to broadcasting), but attempting to write\nthree elements back intox, which only has one element, raises an error\ndue to attempting to write three elements into a Tensor with a single element.\nx\ny\nx+y\nx\nThere is no problem if the Tensor being written to is batched undervmap()(i.e. it is being vmapped over).\nvmap()\n\n```python\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(3, 1)\ny = torch.randn(3, 1)\nexpected = x + y\n\n# Does not raise an error because x is being vmapped over.\nvmap(f, in_dims=(0, 0))(x, y)\nassert torch.allclose(x, expected)\n\n```\n\nOne common fix for this is to replace calls to factory functions with\ntheir \u201cnew_*\u201d equivalent. For example:\nReplacetorch.zeros()withTensor.new_zeros()\ntorch.zeros()\nTensor.new_zeros()\nReplacetorch.empty()withTensor.new_empty()\ntorch.empty()\nTensor.new_empty()\nTo see why this helps, consider the following.\n\n```python\ndef diag_embed(vec):\n  assert vec.dim() == 1\n  result = torch.zeros(vec.shape[0], vec.shape[0])\n  result.diagonal().copy_(vec)\n  return result\n\nvecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\n\n# RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ...\nvmap(diag_embed)(vecs)\n\n```\n\nInside ofvmap(),resultis a Tensor of shape [3, 3].\nHowever, althoughveclooks like it has shape [3],vecactually has\nunderlying shape [2, 3].\nIt is not possible to copyvecintoresult.diagonal(), which has\nshape [3], because it has too many elements.\nvmap()\nresult\nvec\nvec\nvec\nresult.diagonal()\n\n```python\ndef diag_embed(vec):\n  assert vec.dim() == 1\n  result = vec.new_zeros(vec.shape[0], vec.shape[0])\n  result.diagonal().copy_(vec)\n  return result\n\nvecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\nvmap(diag_embed)(vecs)\n\n```\n\nReplacingtorch.zeros()withTensor.new_zeros()makes it so thatresulthas an underlying Tensor of shape [2, 3, 3], so it is now possible\nto copyvec, which has underlying shape [2, 3], intoresult.diagonal().\ntorch.zeros()\nTensor.new_zeros()\nresult\nvec\nresult.diagonal()\n\n## Mutation: out= PyTorch Operations#\n\nvmap()doesn\u2019t support theout=keyword argument in PyTorch operations.\nIt will error out gracefully if it encounters that in your code.\nvmap()\nout=\nThis is not a fundamental limitation; we could theoretically support this in the\nfuture but we have chosen not to for now.\n\n## Data-dependent Python control flow#\n\nWe don\u2019t yet supportvmapover data-dependent control flow. Data-dependent\ncontrol flow is when the condition of an if-statement, while-loop, or\nfor-loop is a Tensor that is beingvmap\u2019ed over. For example, the\nfollowing will raise an error message:\nvmap\nvmap\n\n```python\ndef relu(x):\n  if x > 0:\n    return x\n  return 0\n\nx = torch.randn(3)\nvmap(relu)(x)\n\n```\n\nHowever, any control flow that is not dependent on the values invmap\u2019ed\ntensors will work:\nvmap\n\n```python\ndef custom_dot(x):\n  if x.dim() == 1:\n    return torch.dot(x, x)\n  return (x * x).sum()\n\nx = torch.randn(3)\nvmap(custom_dot)(x)\n\n```\n\nJAX supports transforming overdata-dependent control flowusing special control flow operators (e.g.jax.lax.cond,jax.lax.while_loop).\nWe\u2019re investigating adding equivalents of those to PyTorch.\njax.lax.cond\njax.lax.while_loop\n\n## Data-dependent operations (.item())#\n\nWe do not (and will not) support vmap over a user-defined function that calls.item()on a Tensor. For example, the following will raise an error message:\n.item()\n\n```python\ndef f(x):\n  return x.item()\n\nx = torch.randn(3)\nvmap(f)(x)\n\n```\n\nPlease try to rewrite your code to not use.item()calls.\n.item()\nYou may also encounter an error message about using.item()but you might\nnot have used it. In those cases, it is possible that PyTorch internally is\ncalling.item()\u2013 please file an issue on GitHub and we\u2019ll fix\nPyTorch internals.\n.item()\n.item()\n\n## Dynamic shape operations (nonzero and friends)#\n\nvmap(f)requires thatfapplied to every \u201cexample\u201d in your input\nreturns a Tensor with the same shape. Operations such astorch.nonzero,torch.is_nonzeroare not supported and will error as a result.\nvmap(f)\nf\ntorch.nonzero\ntorch.is_nonzero\nTo see why, consider the following example:\n\n```python\nxs = torch.tensor([[0, 1, 2], [0, 0, 3]])\nvmap(torch.nonzero)(xs)\n\n```\n\ntorch.nonzero(xs[0])returns a Tensor of shape 2;\nbuttorch.nonzero(xs[1])returns a Tensor of shape 1.\nWe are unable to construct a single Tensor as an output;\nthe output would need to be a ragged Tensor (and PyTorch does not yet have\nthe concept of a ragged Tensor).\ntorch.nonzero(xs[0])\ntorch.nonzero(xs[1])\n\n## Randomness#\n\nThe user\u2019s intention when calling a random operation can be unclear. Specifically, some users may want\nthe random behavior to be the same across batches while others may want it to differ across batches.\nTo address this,vmaptakes a randomness flag.\nvmap\nThe flag can only be passed to vmap and can take on 3 values, \u201cerror,\u201d \u201cdifferent,\u201d or \u201csame,\u201d defaulting\nto error. Under \u201cerror\u201d mode, any call to a random function will produce an error asking the user to use\none of the other two flags based on their use case.\nUnder \u201cdifferent\u201d randomness, elements in a batch produce different random values. For instance,\n\n```python\ndef add_noise(x):\n  y = torch.randn(())  # y will be different across the batch\n  return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"different\")(x)  # we get 3 different values\n\n```\n\nUnder \u201csame\u201d randomness, elements in a batch produce same random values. For instance,\n\n```python\ndef add_noise(x):\n  y = torch.randn(())  # y will be the same across the batch\n  return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"same\")(x)  # we get the same value, repeated 3 times\n\n```\n\nWarning\nOur system only determine the randomness behavior of PyTorch operators and cannot control the\nbehavior of other libraries, like numpy. This is similar to JAX\u2019s limitations with their solutions\nNote\nMultiple vmap calls using either type of supported randomness will not produce\nthe same results. Like with standard PyTorch, a user can get randomness reproducibility through\neither usingtorch.manual_seed()outside of vmap or by using generators.\ntorch.manual_seed()\nNote\nFinally, our randomness differs from JAX because we aren\u2019t using a stateless PRNG, in part because PyTorch\ndoesn\u2019t have full support for a stateless PRNG. Instead, we\u2019ve introduced a flag system to allow for the\nmost common forms of randomness that we see. If your use case does not fit these forms of randomness, please\nfile an issue.",
  "url": "https://pytorch.org/docs/stable/func.ux_limitations.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
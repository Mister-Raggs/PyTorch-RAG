{
  "doc_id": "e4f12efc5570705d431331e6fb639203",
  "source": "pytorch_docs",
  "title": "torch.xpu \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.xpu#\n\nCreated On: Feb 01, 2024 | Last Updated On: Jun 06, 2025\nThis package introduces support for the XPU backend, specifically tailored for\nIntel GPU optimization.\nThis package is lazily initialized, so you can always import it, and useis_available()to determine if your system supports XPU.\nis_available()\nStreamContext\n\nStreamContext\nContext-manager that selects a given stream.\ncurrent_device\n\ncurrent_device\nReturn the index of a currently selected device.\ncurrent_stream\n\ncurrent_stream\nReturn the currently selectedStreamfor a given device.\nStream\ndevice\n\ndevice\nContext-manager that changes the selected device.\ndevice_count\n\ndevice_count\nReturn the number of XPU device available.\ndevice_of\n\ndevice_of\nContext-manager that changes the current device to that of given object.\nget_arch_list\n\nget_arch_list\nReturn list XPU architectures this library was compiled for.\nget_device_capability\n\nget_device_capability\nGet the xpu capability of a device.\nget_device_name\n\nget_device_name\nGet the name of a device.\nget_device_properties\n\nget_device_properties\nGet the properties of a device.\nget_gencode_flags\n\nget_gencode_flags\nReturn XPU AOT(ahead-of-time) build flags this library was compiled with.\nget_stream_from_external\n\nget_stream_from_external\nReturn aStreamfrom an external SYCL queue.\nStream\ninit\n\ninit\nInitialize PyTorch's XPU state.\nis_available\n\nis_available\nReturn a bool indicating if XPU is currently available.\nis_initialized\n\nis_initialized\nReturn whether PyTorch's XPU state has been initialized.\nset_device\n\nset_device\nSet the current device.\nset_stream\n\nset_stream\nSet the current stream.This is a wrapper API to set the stream.\nstream\n\nstream\nWrap around the Context-manager StreamContext that selects a given stream.\nsynchronize\n\nsynchronize\nWait for all kernels in all streams on a XPU device to complete.\n\n## Random Number Generator#\n\nget_rng_state\n\nget_rng_state\nReturn the random number generator state of the specified GPU as a ByteTensor.\nget_rng_state_all\n\nget_rng_state_all\nReturn a list of ByteTensor representing the random number states of all devices.\ninitial_seed\n\ninitial_seed\nReturn the current random seed of the current GPU.\nmanual_seed\n\nmanual_seed\nSet the seed for generating random numbers for the current GPU.\nmanual_seed_all\n\nmanual_seed_all\nSet the seed for generating random numbers on all GPUs.\nseed\n\nseed\nSet the seed for generating random numbers to a random number for the current GPU.\nseed_all\n\nseed_all\nSet the seed for generating random numbers to a random number on all GPUs.\nset_rng_state\n\nset_rng_state\nSet the random number generator state of the specified GPU.\nset_rng_state_all\n\nset_rng_state_all\nSet the random number generator state of all devices.\n\n## Streams and events#\n\nEvent\n\nEvent\nWrapper around a XPU event.\nStream\n\nStream\nWrapper around a XPU stream.\n\n## Memory management#\n\nempty_cache\n\nempty_cache\nRelease all unoccupied cached memory currently held by the caching allocator so that those can be used in other XPU application.\nmax_memory_allocated\n\nmax_memory_allocated\nReturn the maximum GPU memory occupied by tensors in bytes for a given device.\nmax_memory_reserved\n\nmax_memory_reserved\nReturn the maximum GPU memory managed by the caching allocator in bytes for a given device.\nmem_get_info\n\nmem_get_info\nReturn the global free and total GPU memory for a given device.\nmemory_allocated\n\nmemory_allocated\nReturn the current GPU memory occupied by tensors in bytes for a given device.\nmemory_reserved\n\nmemory_reserved\nReturn the current GPU memory managed by the caching allocator in bytes for a given device.\nmemory_stats\n\nmemory_stats\nReturn a dictionary of XPU memory allocator statistics for a given device.\nmemory_stats_as_nested_dict\n\nmemory_stats_as_nested_dict\nReturn the result ofmemory_stats()as a nested dictionary.\nmemory_stats()\nreset_accumulated_memory_stats\n\nreset_accumulated_memory_stats\nReset the \"accumulated\" (historical) stats tracked by the XPU memory allocator.\nreset_peak_memory_stats\n\nreset_peak_memory_stats\nReset the \"peak\" stats tracked by the XPU memory allocator.",
  "url": "https://pytorch.org/docs/stable/xpu.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
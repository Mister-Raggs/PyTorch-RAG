{
  "doc_id": "743f87dddc2fa8aef40c021b798a1368",
  "source": "pytorch_docs",
  "title": "torch.fx.experimental \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.fx.experimental#\n\nCreated On: Feb 07, 2024 | Last Updated On: Jun 12, 2025\nWarning\nThese APIs are experimental and subject to change without notice.\n\n## torch.fx.experimental.symbolic_shapes#\n\nShapeEnv\n\nShapeEnv\n\nDimDynamic\n\nDimDynamic\nControls how to perform symbol allocation for a dimension.\nStrictMinMaxConstraint\n\nStrictMinMaxConstraint\nFor clients: the size at this dimension must be within 'vr' (which specifies a lower and upper bound, inclusive-inclusive) AND it must be non-negative and should not be 0 or 1 (but see NB below).\nRelaxedUnspecConstraint\n\nRelaxedUnspecConstraint\nFor clients: no explicit constraint; constraint is whatever is implicitly inferred by guards from tracing.\nEqualityConstraint\n\nEqualityConstraint\nRepresent and decide various kinds of equality constraints between input sources.\nSymbolicContext\n\nSymbolicContext\nData structure specifying how we should create symbols increate_symbolic_sizes_strides_storage_offset; e.g., should they be static or dynamic.\ncreate_symbolic_sizes_strides_storage_offset\nStatelessSymbolicContext\n\nStatelessSymbolicContext\nCreate symbols increate_symbolic_sizes_strides_storage_offsetvia a symbolic_context determination as given byDimDynamicandDimConstraint.\ncreate_symbolic_sizes_strides_storage_offset\nDimDynamic\nDimConstraint\nStatefulSymbolicContext\n\nStatefulSymbolicContext\nCreate symbols increate_symbolic_sizes_strides_storage_offsetvia a symbolic_context determination as given by a cache of Source:Symbol.\ncreate_symbolic_sizes_strides_storage_offset\nSubclassSymbolicContext\n\nSubclassSymbolicContext\nThe correct symbolic context for a given inner tensor of a traceable tensor subclass may differ from that of the outer symbolic context.\nDimConstraints\n\nDimConstraints\nCustom solver for a system of constraints on symbolic dimensions.\nShapeEnvSettings\n\nShapeEnvSettings\nEncapsulates all shape env settings that could potentially affect FakeTensor dispatch.\nConvertIntKey\n\nConvertIntKey\n\nCallMethodKey\n\nCallMethodKey\n\nPropagateUnbackedSymInts\n\nPropagateUnbackedSymInts\n\nDivideByKey\n\nDivideByKey\n\nInnerTensorKey\n\nInnerTensorKey\n\nSpecialization\n\nSpecialization\nThis class is used in multi-graph compilation contexts where we generate multiple specialized graphs and dispatch to the appropriate one at runtime.\nhint_int\n\nhint_int\nRetrieve the hint for an int (based on the underlying real values as observed at runtime).\nis_concrete_int\n\nis_concrete_int\nUtility to check if underlying object in SymInt is concrete value.\nis_concrete_bool\n\nis_concrete_bool\nUtility to check if underlying object in SymBool is concrete value.\nis_concrete_float\n\nis_concrete_float\nUtility to check if underlying object in SymInt is concrete value.\nhas_free_symbols\n\nhas_free_symbols\nFaster version of bool(free_symbols(val))\nhas_free_unbacked_symbols\n\nhas_free_unbacked_symbols\nFaster version of bool(free_unbacked_symbols(val))\nguard_or_true\n\nguard_or_true\nTry to guard a, if data dependent error encountered just return true.\nguard_or_false\n\nguard_or_false\nTry to guard a, if data dependent error encountered just return false.\nguard_size_oblivious\n\nguard_size_oblivious\nPerform a guard on a symbolic boolean expression in a size oblivious way.\nsym_and\n\nsym_and\nand, but for symbolic expressions, without bool casting.\nsym_eq\n\nsym_eq\nLike ==, but when run on list/tuple, it will recursively test equality and use sym_and to join the results together, without guarding.\nsym_or\n\nsym_or\nor, but for symbolic expressions, without bool casting.\nconstrain_range\n\nconstrain_range\nApplies a constraint that the passed in SymInt must lie between min-max inclusive-inclusive, WITHOUT introducing a guard on the SymInt (meaning that it can be used on unbacked SymInts).\nconstrain_unify\n\nconstrain_unify\nGiven two SymInts, constrain them so that they must be equal.\ncanonicalize_bool_expr\n\ncanonicalize_bool_expr\nCanonicalize a boolean expression by transforming it into a lt / le inequality and moving all the non-constant terms to the rhs.\nstatically_known_true\n\nstatically_known_true\nReturns True if x can be simplified to a constant and is true.\nstatically_known_false\n\nstatically_known_false\nReturns True if x can be simplified to a constant and is False.\nhas_static_value\n\nhas_static_value\nUser-code friendly utility to check if a value is static or dynamic.\nlru_cache\n\nlru_cache\n\ncheck_consistent\n\ncheck_consistent\nTest that two \"meta\" values (typically either Tensor or SymInt) have the same values, e.g., after retracing.\ncompute_unbacked_bindings\n\ncompute_unbacked_bindings\nAfter having run fake tensor propagation and producing example_value result, traverse example_value looking for freshly bound unbacked symbols and record their paths for later.\nrebind_unbacked\n\nrebind_unbacked\nSuppose we are retracing a pre-existing FX graph that previously had fake tensor propagation (and therefore unbacked SymInts).\nresolve_unbacked_bindings\n\nresolve_unbacked_bindings\nWhen we do fake tensor prop, we oftentimes will allocate new unbacked symints.\nis_accessor_node\n\nis_accessor_node\nHelper function to determine if a node is trying to access a symbolic integer such as size, stride, offset or item.\n\n## torch.fx.experimental.proxy_tensor#\n\nmake_fx\n\nmake_fx\nGiven a function f, return a new function which when executed with valid arguments to f, returns an FX GraphModule representing the set of operations that were executed during the course of execution.\nhandle_sym_dispatch\n\nhandle_sym_dispatch\nCall into the currently active proxy tracing mode to do a SymInt/SymFloat/SymBool dispatch trace on a function that operates on these arguments.\nget_proxy_mode\n\nget_proxy_mode\nCurrent the currently active proxy tracing mode, or None if we are not currently tracing.\nmaybe_enable_thunkify\n\nmaybe_enable_thunkify\nWithin this context manager, if you are doing make_fx tracing, we will thunkify all SymNode compute and avoid tracing it into the graph unless it is actually needed.\nmaybe_disable_thunkify\n\nmaybe_disable_thunkify\nWithin a context, disable thunkification.",
  "url": "https://pytorch.org/docs/stable/fx.experimental.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
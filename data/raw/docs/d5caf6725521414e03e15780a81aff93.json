{
  "doc_id": "d5caf6725521414e03e15780a81aff93",
  "source": "pytorch_docs",
  "title": "MKLDNN backend \u2014 PyTorch 2.9 documentation",
  "text": "\n## MKLDNN backend#\n\nCreated On: May 10, 2025 | Last Updated On: Jul 17, 2025\nMKLDNN is an open-source cross-platform performance library of basic building blocks\nfor deep learning applications.\n\n```python\n# The flag below controls whether enable MKLDNN backend in Pytorch.\ntorch.backends.mkldnn.enabled = True\n\n```\n\nUsers can disable MKLDNN backend by:\n\n```python\ntorch.backends.mkldnn.enabled = False\n\n```\n\n\n## Bfloat16 (BF16) on MKLDNN backend#\n\nStarting in PyTorch 2.9, there is a set of APIs to control the internal computation precision\nforfloat32operators.\n\n```python\n# The flag below controls the internal computation precision for mkldnn matmul. Default ieee is float32.\ntorch.backends.mkldnn.matmul.fp32_precision = \"ieee\"\n\n# The flag below controls the internal computation precision for mkldnn conv. Default ieee is float32.\ntorch.backends.mkldnn.conv.fp32_precision = \"ieee\"\n\n# The flag below controls the internal computation precision for mkldnn rnn. Default ieee is float32.\ntorch.backends.mkldnn.rnn.fp32_precision = \"ieee\"\n\n```\n\nNote that besides matmuls and convolutions themselves, functions and nn modules that internally uses\nmatmuls or convolutions are also affected. These includetorch.nn.Linear,torch.nn._ConvNd,torch.cdist(),torch.tensordot(),torch.nn.functional.affine_grid()andtorch.nn.functional.grid_sample(),torch.nn.AdaptiveLogSoftmaxWithLoss,torch.nn.GRUandtorch.nn.LSTM.\ntorch.nn.Linear\ntorch.nn._ConvNd\ntorch.cdist()\ntorch.tensordot()\ntorch.nn.functional.affine_grid()\ntorch.nn.functional.grid_sample()\ntorch.nn.AdaptiveLogSoftmaxWithLoss\ntorch.nn.GRU\ntorch.nn.LSTM\nTo get an idea of the precision and speed, see the example code and benchmark data (on SPR) below:\n\n```python\ntorch.manual_seed(0)\na_full = torch.randn(10240, 10240, dtype=torch.double)\nb_full = torch.randn(10240, 10240, dtype=torch.double)\nab_full = a_full @ b_full\nmean = ab_full.abs().mean()  # 80.7451\n\na = a_full.float()\nb = b_full.float()\n\n# Do matmul at BF16 mode.\ntorch.backends.mkldnn.matmul.fp32_precision = 'bf16'\nab_bf16 = a @ b  # expected speedup with BF16 dot-product acceleration\nerror = (ab_bf16 - ab_full).abs().max()  # 1.3704\nrelative_error = error / mean  # 0.0170\nprint(error, relative_error)\n\n# Do matmul at TF32 mode.\ntorch.backends.mkldnn.matmul.fp32_precision = 'tf32'\nab_tf32 = a @ b  # expected speedup with TF32 dot-product acceleration\nerror = (ab_tf32 - ab_full).abs().max()  # 0.0004\nrelative_error = error / mean  # 0.00000552\nprint(error, relative_error)\n\n# Do matmul FP32 mode.\ntorch.backends.mkldnn.matmul.fp32_precision = 'ieee'\nab_fp32 = a @ b\nerror = (ab_fp32 - ab_full).abs().max()  # 0.0003\nrelative_error = error / mean  # 0.00000317\nprint(error, relative_error)\n\n```\n\nFrom the above example, we can see that with BF16, the speed is ~7x faster on SPR, and that\nrelative error compared to double precision is approximately 2 orders of magnitude larger.\nIf full FP32 precision is needed, users can disable BF16 by:\n\n```python\ntorch.backends.mkldnn.matmul.fp32_precision = 'ieee'\ntorch.backends.mkldnn.conv.fp32_precision = 'ieee'\ntorch.backends.mkldnn.rnn.fp32_precision = 'ieee'\n\n```\n\nTo toggle the BF16 flags off in C++, you can do\n\n```python\nat::globalContext().setFloat32Precision(\"ieee\", \"mkldnn\", \"matmul\");\nat::globalContext().setFloat32Precision(\"ieee\", \"mkldnn\", \"conv\");\nat::globalContext().setFloat32Precision(\"ieee\", \"mkldnn\", \"rnn\");\n\n```\n\nWe can override a generic setting for a specific operator or backend if the fp32_precision is set toieee.\n\n```python\ntorch.backends.fp32_precision = \"bf16\"\ntorch.backends.mkldnn.fp32_precision = \"ieee\"\ntorch.backends.mkldnn.matmul.fp32_precision = \"ieee\"\n\n```\n\nFor such case, bothtorch.backends.mkldnn.fp32_precisionandtorch.backends.mkldnn.matmul.fp32_precisionis overridden to bf16.",
  "url": "https://pytorch.org/docs/stable/notes/mkldnn.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "a401b7059899bbd849a2c183df9c7876",
  "source": "pytorch_docs",
  "title": "torch.utils.data \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.utils.data#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jun 13, 2025\nAt the heart of PyTorch data loading utility is thetorch.utils.data.DataLoaderclass. It represents a Python iterable over a dataset, with support for\ntorch.utils.data.DataLoader\nmap-style and iterable-style datasets,\ncustomizing data loading order,\nautomatic batching,\nsingle- and multi-process data loading,\nautomatic memory pinning.\nThese options are configured by the constructor arguments of aDataLoader, which has signature:\nDataLoader\n\n```python\nDataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)\n\n```\n\nThe sections below describe in details the effects and usages of these options.\n\n## Dataset Types#\n\nThe most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets:\nDataLoader\ndataset\nMap-style datasets,\nIterable-style datasets.\n\n## Map-style datasets#\n\nA map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples.\n__getitem__()\n__len__()\nFor example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk.\ndataset[idx]\nidx\nSeeDatasetfor more details.\nDataset\n\n## Iterable-style datasets#\n\nAn iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data.\nIterableDataset\n__iter__()\nFor example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time.\niter(dataset)\nSeeIterableDatasetfor more details.\nIterableDataset\nNote\nWhen using aIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this.\nIterableDataset\nIterableDataset\n\n## Data Loading Order andSampler#\n\nSampler\nForiterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time).\nThe rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets. E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD.\ntorch.utils.data.Sampler\nSampler\nA sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch.\nshuffle\nDataLoader\nsampler\nSampler\nA customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this.\nSampler\nbatch_sampler\nbatch_size\ndrop_last\nNote\nNeithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex.\nsampler\nbatch_sampler\n\n## Loading Batched and Non-Batched Data#\n\nDataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last,batch_sampler, andcollate_fn(which has a default function).\nDataLoader\nbatch_size\ndrop_last\nbatch_sampler\ncollate_fn\n\n## Automatic batching (default)#\n\nThis is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first).\nWhenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time.\nbatch_size\n1\nNone\nbatch_size\ndrop_last\nbatch_sampler\nNote\nThebatch_sizeanddrop_lastarguments essentially are used\nto construct abatch_samplerfromsampler. For map-style\ndatasets, thesampleris either provided by user or constructed\nbased on theshuffleargument. For iterable-style datasets, thesampleris a dummy infinite one. Seethis sectionon more details on\nsamplers.\nbatch_size\ndrop_last\nbatch_sampler\nsampler\nsampler\nshuffle\nsampler\nNote\nWhen fetching fromiterable-style datasetswithmulti-processingthedrop_lastargument drops the last non-full batch of each worker\u2019s dataset replica.\ndrop_last\nAfter fetching a list of samples using the indices from sampler, the function\npassed as thecollate_fnargument is used to collate lists of samples\ninto batches.\ncollate_fn\nIn this case, loading from a map-style dataset is roughly equivalent with:\n\n```python\nfor indices in batch_sampler:\n    yield collate_fn([dataset[i] for i in indices])\n\n```\n\nand loading from an iterable-style dataset is roughly equivalent with:\n\n```python\ndataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])\n\n```\n\nA customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn.\ncollate_fn\ncollate_fn\n\n## Disable automatic batching#\n\nIn certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples. Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject.\ncollate_fn\ndataset\nWhen bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument.\nbatch_size\nbatch_sampler\nNone\nbatch_sampler\nNone\ndataset\ncollate_fn\nWhen automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.\ncollate_fn\nIn this case, loading from a map-style dataset is roughly equivalent with:\n\n```python\nfor index in sampler:\n    yield collate_fn(dataset[index])\n\n```\n\nand loading from an iterable-style dataset is roughly equivalent with:\n\n```python\nfor data in iter(dataset):\n    yield collate_fn(data)\n\n```\n\nSeethis sectionon more aboutcollate_fn.\ncollate_fn\n\n## Working withcollate_fn#\n\ncollate_fn\nThe use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled.\ncollate_fn\nWhen automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors.\ncollate_fn\ncollate_fn\nWhen automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes the behavior of the defaultcollate_fn(default_collate()).\ncollate_fn\ncollate_fn\ndefault_collate()\nFor instance, if each data sample consists of a 3-channel image and an integral\nclass label, i.e., each element of the dataset returns a tuple(image,class_index), the defaultcollate_fncollates a list of\nsuch tuples into a single tuple of a batched image tensor and a batched class\nlabel Tensor. In particular, the defaultcollate_fnhas the following\nproperties:\n(image,class_index)\ncollate_fn\ncollate_fn\nIt always prepends a new dimension as the batch dimension.\nIt automatically converts NumPy arrays and Python numerical values into\nPyTorch Tensors.\nIt preserves the data structure, e.g., if each sample is a dictionary, it\noutputs a dictionary with the same set of keys but batched Tensors as values\n(or lists if the values can not be converted into Tensors). Same\nforlists,tuples,namedtuples, etc.\nlist\ntuple\nnamedtuple\nUsers may use customizedcollate_fnto achieve custom batching, e.g.,\ncollating along a dimension other than the first, padding sequences of\nvarious lengths, or adding support for custom data types.\ncollate_fn\nIf you run into a situation where the outputs ofDataLoaderhave dimensions or type that is different from your expectation, you may\nwant to check yourcollate_fn.\nDataLoader\ncollate_fn\n\n## Single- and Multi-process Data Loading#\n\nADataLoaderuses single-process data loading by\ndefault.\nDataLoader\nWithin a Python process, theGlobal Interpreter Lock (GIL)prevents true fully parallelizing Python code across threads. To avoid blocking\ncomputation code with data loading, PyTorch provides an easy switch to perform\nmulti-process data loading by simply setting the argumentnum_workersto a positive integer.\nnum_workers\n\n## Single-process data loading (default)#\n\nIn this mode, data fetching is done in the same process aDataLoaderis initialized. Therefore, data loading\nmay block computing. However, this mode may be preferred when resource(s) used\nfor sharing data among processes (e.g., shared memory, file descriptors) is\nlimited, or when the entire dataset is small and can be loaded entirely in\nmemory. Additionally, single-process loading often shows more readable error\ntraces and thus is useful for debugging.\nDataLoader\n\n## Multi-process data loading#\n\nSetting the argumentnum_workersas a positive integer will\nturn on multi-process data loading with the specified number of loader worker\nprocesses.\nnum_workers\nWarning\nAfter several iterations, the loader worker processes will consume\nthe same amount of CPU memory as the parent process for all Python\nobjects in the parent process which are accessed from the worker\nprocesses. This can be problematic if the Dataset contains a lot of\ndata (e.g., you are loading a very large list of filenames at Dataset\nconstruction time) and/or you are using a lot of workers (overall\nmemory usage isnumberofworkers*sizeofparentprocess). The\nsimplest workaround is to replace Python objects with non-refcounted\nrepresentations such as Pandas, Numpy or PyArrow objects. Check outissue #13246for more details on why this occurs and example code for how to\nworkaround these problems.\nnumberofworkers*sizeofparentprocess\nIn this mode, each time an iterator of aDataLoaderis created (e.g., when you callenumerate(dataloader)),num_workersworker processes are created. At this point, thedataset,collate_fn, andworker_init_fnare passed to each\nworker, where they are used to initialize, and fetch data. This means that\ndataset access together with its internal IO, transforms\n(includingcollate_fn) runs in the worker process.\nDataLoader\nenumerate(dataloader)\nnum_workers\ndataset\ncollate_fn\nworker_init_fn\ncollate_fn\ntorch.utils.data.get_worker_info()returns various useful information\nin a worker process (including the worker id, dataset replica, initial seed,\netc.), and returnsNonein main process. Users may use this function in\ndataset code and/orworker_init_fnto individually configure each\ndataset replica, and to determine whether the code is running in a worker\nprocess. For example, this can be particularly helpful in sharding the dataset.\ntorch.utils.data.get_worker_info()\nNone\nworker_init_fn\nFor map-style datasets, the main process generates the indices usingsamplerand sends them to the workers. So any shuffle randomization is\ndone in the main process which guides loading by assigning indices to load.\nsampler\nFor iterable-style datasets, since each worker process gets a replica of thedatasetobject, naive multi-process loading will often result in\nduplicated data. Usingtorch.utils.data.get_worker_info()and/orworker_init_fn, users may configure each replica independently. (SeeIterableDatasetdocumentations for how to achieve\nthis. ) For similar reasons, in multi-process loading, thedrop_lastargument drops the last non-full batch of each worker\u2019s iterable-style dataset\nreplica.\ndataset\ntorch.utils.data.get_worker_info()\nworker_init_fn\nIterableDataset\ndrop_last\nWorkers are shut down once the end of the iteration is reached, or when the\niterator becomes garbage collected.\nWarning\nIt is generally not recommended to return CUDA tensors in multi-process\nloading because of many subtleties in using CUDA and sharing CUDA tensors in\nmultiprocessing (seeCUDA in multiprocessing). Instead, we recommend\nusingautomatic memory pinning(i.e., settingpin_memory=True), which enables fast data transfer to CUDA-enabled\nGPUs.\npin_memory=True\nSince workers rely on Pythonmultiprocessing, worker launch behavior is\ndifferent on Windows compared to Unix.\nmultiprocessing\nOn Unix,fork()is the defaultmultiprocessingstart method.\nUsingfork(), child workers typically can access thedatasetand\nPython argument functions directly through the cloned address space.\nfork()\nmultiprocessing\nfork()\ndataset\nOn Windows or MacOS,spawn()is the defaultmultiprocessingstart method.\nUsingspawn(), another interpreter is launched which runs your main script,\nfollowed by the internal worker function that receives thedataset,collate_fnand other arguments throughpickleserialization.\nspawn()\nmultiprocessing\nspawn()\ndataset\ncollate_fn\npickle\nThis separate serialization means that you should take two steps to ensure you\nare compatible with Windows while using multi-process data loading:\nWrap most of you main script\u2019s code withinif__name__=='__main__':block,\nto make sure it doesn\u2019t run again (most likely generating error) when each worker\nprocess is launched. You can place your dataset andDataLoaderinstance creation logic here, as it doesn\u2019t need to be re-executed in workers.\nif__name__=='__main__':\nDataLoader\nMake sure that any customcollate_fn,worker_init_fnordatasetcode is declared as top level definitions, outside of the__main__check. This ensures that they are available in worker processes.\n(this is needed since functions are pickled as references only, notbytecode.)\ncollate_fn\nworker_init_fn\ndataset\n__main__\nbytecode\nBy default, each worker will have its PyTorch seed set tobase_seed+worker_id,\nwherebase_seedis a long generated by main process using its RNG (thereby,\nconsuming a RNG state mandatorily) or a specifiedgenerator. However, seeds for other\nlibraries may be duplicated upon initializing workers, causing each worker to return\nidentical random numbers. (Seethis sectionin FAQ.).\nbase_seed+worker_id\nbase_seed\ngenerator\nInworker_init_fn, you may access the PyTorch seed set for each worker\nwith eithertorch.utils.data.get_worker_info().seedortorch.initial_seed(), and use it to seed other libraries before data\nloading.\nworker_init_fn\ntorch.utils.data.get_worker_info().seed\ntorch.initial_seed()\n\n## Memory Pinning#\n\nHost to GPU copies are much faster when they originate from pinned (page-locked)\nmemory. SeeUse pinned memory buffersfor more details on when and how to use\npinned memory generally.\nFor data loading, passingpin_memory=Trueto aDataLoaderwill automatically put the fetched data\nTensors in pinned memory, and thus enables faster data transfer to CUDA-enabled\nGPUs.\npin_memory=True\nDataLoader\nThe default memory pinning logic only recognizes Tensors and maps and iterables\ncontaining Tensors. By default, if the pinning logic sees a batch that is a\ncustom type (which will occur if you have acollate_fnthat returns a\ncustom batch type), or if each element of your batch is a custom type, the\npinning logic will not recognize them, and it will return that batch (or those\nelements) without pinning the memory. To enable memory pinning for custom\nbatch or data type(s), define apin_memory()method on your custom\ntype(s).\ncollate_fn\npin_memory()\nSee the example below.\nExample:\n\n```python\nclass SimpleCustomBatch:\n    def __init__(self, data):\n        transposed_data = list(zip(*data))\n        self.inp = torch.stack(transposed_data[0], 0)\n        self.tgt = torch.stack(transposed_data[1], 0)\n\n    # custom memory pinning method on custom type\n    def pin_memory(self):\n        self.inp = self.inp.pin_memory()\n        self.tgt = self.tgt.pin_memory()\n        return self\n\ndef collate_wrapper(batch):\n    return SimpleCustomBatch(batch)\n\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ndataset = TensorDataset(inps, tgts)\n\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                    pin_memory=True)\n\nfor batch_ndx, sample in enumerate(loader):\n    print(sample.inp.is_pinned())\n    print(sample.tgt.is_pinned())\n\n```\n\nData loader combines a dataset and a sampler, and provides an iterable over the given dataset.\nTheDataLoadersupports both map-style and\niterable-style datasets with single- or multi-process loading, customizing\nloading order and optional automatic batching (collation) and memory pinning.\nDataLoader\nSeetorch.utils.datadocumentation page for more details.\ntorch.utils.data\ndataset(Dataset) \u2013 dataset from which to load the data.\nbatch_size(int,optional) \u2013 how many samples per batch to load\n(default:1).\n1\nshuffle(bool,optional) \u2013 set toTrueto have the data reshuffled\nat every epoch (default:False).\nTrue\nFalse\nsampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified.\nIterable\n__len__\nshuffle\nbatch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last.\nsampler\nbatch_size\nshuffle\nsampler\ndrop_last\nnum_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0)\n0\n0\ncollate_fn(Callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset.\npin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto device/CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below.\nTrue\ncollate_fn\ndrop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False)\nTrue\nFalse\nFalse\ntimeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0)\n0\nworker_init_fn(Callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None)\nNone\n[0,num_workers-1]\nNone\nmultiprocessing_context(strormultiprocessing.context.BaseContext,optional) \u2013 IfNone, the defaultmultiprocessing context# noqa: D401\nof your operating system will\nbe used. (default:None)\nNone\nNone\ngenerator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None)\nNone\nbase_seed\nNone\nprefetch_factor(int,optional,keyword-only arg) \u2013 Number of batches loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers batches prefetched across all workers. (default value depends\non the set value for num_workers. If value of num_workers=0 default isNone.\nOtherwise, if value ofnum_workers>0default is2).\n2\nNone\nnum_workers>0\n2\npersistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shut down\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False)\nTrue\nFalse\npin_memory_device(str,optional) \u2013 Deprecated, the currentacceleratorwill be used as the device ifpin_memory=True.\npin_memory=True\nin_order(bool,optional) \u2013 IfFalse, the data loader will not enforce that batches\nare returned in a first-in, first-out order. Only applies whennum_workers>0. (default:True)\nFalse\nnum_workers>0\nTrue\nWarning\nIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch.\nspawn\nworker_init_fn\nWarning\nlen(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data.\nlen(dataloader)\ndataset\nIterableDataset\nlen(dataset)/batch_size\ndrop_last\ndataset\nHowever, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general.\ndrop_last\nSeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading.\nIterableDataset\nWarning\nSeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions.\nWarning\nSettingin_ordertoFalsecan harm reproducibility and may lead to a skewed data\ndistribution being fed to the trainer in cases with imbalanced data.\nAn abstract class representing aDataset.\nDataset\nAll datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Subclasses could also\noptionally implement__getitems__(), for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.\n__getitem__()\n__len__()\nSampler\nDataLoader\n__getitems__()\nNote\nDataLoaderby default constructs an index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.\nDataLoader\nAn iterable Dataset.\nAll datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.\nAll subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.\n__iter__()\nWhen a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior.\nDataLoader\nDataLoader\nnum_workers>0\nget_worker_info()\n__iter__()\nDataLoader\nworker_init_fn\nExample 1: splitting workload across all workers in__iter__():\n__iter__()\n\n```python\n>>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n\n>>> # Multi-process loading with two worker processes\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=12)))\n[tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n\n```\n\nExample 2: splitting workload across all workers usingworker_init_fn:\nworker_init_fn\n\n```python\n>>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         return iter(range(self.start, self.end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n>>>\n>>> # Directly doing multi-process loading yields duplicate data\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 3, 4, 4, 5, 5, 6, 6]\n\n>>> # Define a `worker_init_fn` that configures each dataset copy differently\n>>> def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n...\n\n>>> # Mult-process loading with the custom `worker_init_fn`\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=12, worker_init_fn=worker_init_fn)))\n[3, 4, 5, 6]\n\n```\n\nDataset wrapping tensors.\nEach sample will be retrieved by indexing tensors along the first dimension.\n*tensors(Tensor) \u2013 tensors that have the same size of the first dimension.\nDataset as a stacking of multiple datasets.\nThis class is useful to assemble different parts of complex input data, given as datasets.\nExample\n\n```python\n>>> images = ImageDataset()\n>>> texts = TextDataset()\n>>> tuple_stack = StackDataset(images, texts)\n>>> tuple_stack[0] == (images[0], texts[0])\n>>> dict_stack = StackDataset(image=images, text=texts)\n>>> dict_stack[0] == {\"image\": images[0], \"text\": texts[0]}\n\n```\n\n*args(Dataset) \u2013 Datasets for stacking returned as tuple.\n**kwargs(Dataset) \u2013 Datasets for stacking returned as dict.\nDataset as a concatenation of multiple datasets.\nThis class is useful to assemble different existing datasets.\ndatasets(sequence) \u2013 List of datasets to be concatenated\nDataset for chaining multipleIterableDatasets.\nIterableDataset\nThis class is useful to assemble different existing dataset streams. The\nchaining operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient.\ndatasets(iterableofIterableDataset) \u2013 datasets to be chained together\nSubset of a dataset at specified indices.\ndataset(Dataset) \u2013 The whole Dataset\nindices(sequence) \u2013 Indices in the whole set selected for subset\nGeneral collate function that handles collection type of element within each batch.\nThe function also opens function registry to deal with specific element types.default_collate_fn_mapprovides default collate functions for tensors, numpy arrays, numbers and strings.\nbatch\u2013 a single batch to be collated\ncollate_fn_map(Optional[dict[Union[type,tuple[type,...]],Callable]]) \u2013 Optional dictionary mapping from element type to the corresponding collate function.\nIf the element type isn\u2019t present in this dictionary,\nthis function will go through each key of the dictionary in the insertion order to\ninvoke the corresponding collate function if the element type is a subclass of the key.\nExamples\n\n```python\n>>> def collate_tensor_fn(batch, *, collate_fn_map):\n...     # Extend this function to handle batch of tensors\n...     return torch.stack(batch, 0)\n>>> def custom_collate(batch):\n...     collate_map = {torch.Tensor: collate_tensor_fn}\n...     return collate(batch, collate_fn_map=collate_map)\n>>> # Extend `default_collate` by in-place modifying `default_collate_fn_map`\n>>> default_collate_fn_map.update({torch.Tensor: collate_tensor_fn})\n\n```\n\nNote\nEach collate function requires a positional argument for batch and a keyword argument\nfor the dictionary of collate functions ascollate_fn_map.\nTake in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\nThe exact output type can be atorch.Tensor, aSequenceoftorch.Tensor, a\nCollection oftorch.Tensor, or left unchanged, depending on the input type.\nThis is used as the default function for collation whenbatch_sizeorbatch_sampleris defined inDataLoader.\ntorch.Tensor\ntorch.Tensor\ntorch.Tensor\nDataLoader\nHere is the general input type (based on the type of the element within the batch) to output type mapping:\ntorch.Tensor->torch.Tensor(with an added outer dimension batch size)\ntorch.Tensor\ntorch.Tensor\nNumPy Arrays ->torch.Tensor\ntorch.Tensor\nfloat->torch.Tensor\ntorch.Tensor\nint->torch.Tensor\ntorch.Tensor\nstr->str(unchanged)\nbytes->bytes(unchanged)\nMapping[K, V_i]->Mapping[K, default_collate([V_1, V_2, \u2026])]\nNamedTuple[V1_i, V2_i, \u2026]->NamedTuple[default_collate([V1_1, V1_2, \u2026]),\ndefault_collate([V2_1, V2_2, \u2026]), \u2026]\nSequence[V1_i, V2_i, \u2026]->Sequence[default_collate([V1_1, V1_2, \u2026]),\ndefault_collate([V2_1, V2_2, \u2026]), \u2026]\nbatch\u2013 a single batch to be collated\nExamples\n\n```python\n>>> # Example with a batch of `int`s:\n>>> default_collate([0, 1, 2, 3])\ntensor([0, 1, 2, 3])\n>>> # Example with a batch of `str`s:\n>>> default_collate([\"a\", \"b\", \"c\"])\n['a', 'b', 'c']\n>>> # Example with `Map` inside the batch:\n>>> default_collate([{\"A\": 0, \"B\": 1}, {\"A\": 100, \"B\": 100}])\n{'A': tensor([  0, 100]), 'B': tensor([  1, 100])}\n>>> # Example with `NamedTuple` inside the batch:\n>>> Point = namedtuple(\"Point\", [\"x\", \"y\"])\n>>> default_collate([Point(0, 0), Point(1, 1)])\nPoint(x=tensor([0, 1]), y=tensor([0, 1]))\n>>> # Example with `Tuple` inside the batch:\n>>> default_collate([(0, 1), (2, 3)])\n[tensor([0, 2]), tensor([1, 3])]\n>>> # Example with `List` inside the batch:\n>>> default_collate([[0, 1], [2, 3]])\n[tensor([0, 2]), tensor([1, 3])]\n>>> # Two options to extend `default_collate` to handle specific type\n>>> # Option 1: Write custom collate function and invoke `default_collate`\n>>> def custom_collate(batch):\n...     elem = batch[0]\n...     if isinstance(elem, CustomType):  # Some custom condition\n...         return ...\n...     else:  # Fall back to `default_collate`\n...         return default_collate(batch)\n>>> # Option 2: In-place modify `default_collate_fn_map`\n>>> def collate_customtype_fn(batch, *, collate_fn_map=None):\n...     return ...\n>>> default_collate_fn_map.update(CustomType, collate_customtype_fn)\n>>> default_collate(batch)  # Handle `CustomType` automatically\n\n```\n\nConvert each NumPy array element into atorch.Tensor.\ntorch.Tensor\nIf the input is aSequence,Collection, orMapping, it tries to convert each element inside to atorch.Tensor.\nIf the input is not an NumPy array, it is left unchanged.\nThis is used as the default function for collation when bothbatch_samplerandbatch_sizeare NOT defined inDataLoader.\ntorch.Tensor\nDataLoader\nThe general input type to output type mapping is similar to that\nofdefault_collate(). See the description there for more details.\ndefault_collate()\ndata\u2013 a single data point to be converted\nExamples\n\n```python\n>>> # Example with `int`\n>>> default_convert(0)\n0\n>>> # Example with NumPy array\n>>> default_convert(np.array([0, 1]))\ntensor([0, 1])\n>>> # Example with NamedTuple\n>>> Point = namedtuple(\"Point\", [\"x\", \"y\"])\n>>> default_convert(Point(0, 0))\nPoint(x=0, y=0)\n>>> default_convert(Point(np.array(0), np.array(0)))\nPoint(x=tensor(0), y=tensor(0))\n>>> # Example with List\n>>> default_convert([np.array([0, 1]), np.array([2, 3])])\n[tensor([0, 1]), tensor([2, 3])]\n\n```\n\nReturns the information about the currentDataLoaderiterator worker process.\nDataLoader\nWhen called in a worker, this returns an object guaranteed to have the\nfollowing attributes:\nid: the current worker id.\nid\nnum_workers: the total number of workers.\nnum_workers\nseed: the random seed set for the current worker. This value is\ndetermined by main process RNG and the worker id. SeeDataLoader\u2019s documentation for more details.\nseed\nDataLoader\ndataset: the copy of the dataset object inthisprocess. Note\nthat this will be a different object in a different process than the one\nin the main process.\ndataset\nWhen called in the main process, this returnsNone.\nNone\nNote\nWhen used in aworker_init_fnpassed over toDataLoader, this method can be useful to\nset up each worker process differently, for instance, usingworker_idto configure thedatasetobject to only read a specific fraction of a\nsharded dataset, or useseedto seed other libraries used in dataset\ncode.\nworker_init_fn\nDataLoader\nworker_id\ndataset\nseed\nOptional[WorkerInfo]\nRandomly split a dataset into non-overlapping new datasets of given lengths.\nIf a list of fractions that sum up to 1 is given,\nthe lengths will be computed automatically as\nfloor(frac * len(dataset)) for each fraction provided.\nAfter computing the lengths, if there are any remainders, 1 count will be\ndistributed in round-robin fashion to the lengths\nuntil there are no remainders left.\nOptionally fix the generator for reproducible results, e.g.:\nExample\n\n```python\n>>> generator1 = torch.Generator().manual_seed(42)\n>>> generator2 = torch.Generator().manual_seed(42)\n>>> random_split(range(10), [3, 7], generator=generator1)\n>>> random_split(range(30), [0.3, 0.3, 0.4], generator=generator2)\n\n```\n\ndataset(Dataset) \u2013 Dataset to be split\nlengths(sequence) \u2013 lengths or fractions of splits to be produced\ngenerator(Generator) \u2013 Generator used for the random permutation.\nlist[torch.utils.data.dataset.Subset[~_T]]\nBase class for all Samplers.\nEvery Sampler subclass has to provide an__iter__()method, providing a\nway to iterate over indices or lists of indices (batches) of dataset elements,\nand may provide a__len__()method that returns the length of the returned iterators.\n__iter__()\n__len__()\ndata_source(Dataset) \u2013 This argument is not used and will be removed in 2.2.0.\nYou may still have custom implementation that utilizes it.\nExample\n\n```python\n>>> class AccedingSequenceLengthSampler(Sampler[int]):\n>>>     def __init__(self, data: List[str]) -> None:\n>>>         self.data = data\n>>>\n>>>     def __len__(self) -> int:\n>>>         return len(self.data)\n>>>\n>>>     def __iter__(self) -> Iterator[int]:\n>>>         sizes = torch.tensor([len(x) for x in self.data])\n>>>         yield from torch.argsort(sizes).tolist()\n>>>\n>>> class AccedingSequenceLengthBatchSampler(Sampler[List[int]]):\n>>>     def __init__(self, data: List[str], batch_size: int) -> None:\n>>>         self.data = data\n>>>         self.batch_size = batch_size\n>>>\n>>>     def __len__(self) -> int:\n>>>         return (len(self.data) + self.batch_size - 1) // self.batch_size\n>>>\n>>>     def __iter__(self) -> Iterator[List[int]]:\n>>>         sizes = torch.tensor([len(x) for x in self.data])\n>>>         for batch in torch.chunk(torch.argsort(sizes), len(self)):\n>>>             yield batch.tolist()\n\n```\n\nNote\nThe__len__()method isn\u2019t strictly required byDataLoader, but is expected in any\ncalculation involving the length of aDataLoader.\n__len__()\nDataLoader\nDataLoader\nSamples elements sequentially, always in the same order.\ndata_source(Dataset) \u2013 dataset to sample from\nSamples elements randomly. If without replacement, then sample from a shuffled dataset.\nIf with replacement, then user can specifynum_samplesto draw.\nnum_samples\ndata_source(Dataset) \u2013 dataset to sample from\nreplacement(bool) \u2013 samples are drawn on-demand with replacement ifTrue, default=``False``\nTrue\nnum_samples(int) \u2013 number of samples to draw, default=`len(dataset)`.\ngenerator(Generator) \u2013 Generator used in sampling.\nSamples elements randomly from a given list of indices, without replacement.\nindices(sequence) \u2013 a sequence of indices\ngenerator(Generator) \u2013 Generator used in sampling.\nSamples elements from[0,..,len(weights)-1]with given probabilities (weights).\n[0,..,len(weights)-1]\nweights(sequence) \u2013 a sequence of weights, not necessary summing up to one\nnum_samples(int) \u2013 number of samples to draw\nreplacement(bool) \u2013 ifTrue, samples are drawn with replacement.\nIf not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row.\nTrue\ngenerator(Generator) \u2013 Generator used in sampling.\nExample\n\n```python\n>>> list(\n...     WeightedRandomSampler(\n...         [0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True\n...     )\n... )\n[4, 4, 1, 4, 5]\n>>> list(\n...     WeightedRandomSampler(\n...         [0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False\n...     )\n... )\n[0, 1, 4, 3, 2]\n\n```\n\nWraps another sampler to yield a mini-batch of indices.\nsampler(SamplerorIterable) \u2013 Base sampler. Can be any iterable object\nbatch_size(int) \u2013 Size of mini-batch.\ndrop_last(bool) \u2013 IfTrue, the sampler will drop the last batch if\nits size would be less thanbatch_size\nTrue\nbatch_size\nExample\n\n```python\n>>> list(\n...     BatchSampler(\n...         SequentialSampler(range(10)), batch_size=3, drop_last=False\n...     )\n... )\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n>>> list(\n...     BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True)\n... )\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n\n```\n\nSampler that restricts data loading to a subset of the dataset.\nIt is especially useful in conjunction withtorch.nn.parallel.DistributedDataParallel. In such a case, each\nprocess can pass aDistributedSamplerinstance as aDataLoadersampler, and load a subset of the\noriginal dataset that is exclusive to it.\ntorch.nn.parallel.DistributedDataParallel\nDistributedSampler\nDataLoader\nNote\nDataset is assumed to be of constant size and that any instance of it always\nreturns the same elements in the same order.\ndataset(Dataset) \u2013 Dataset used for sampling.\nnum_replicas(int,optional) \u2013 Number of processes participating in\ndistributed training. By default,world_sizeis retrieved from the\ncurrent distributed group.\nworld_size\nrank(int,optional) \u2013 Rank of the current process withinnum_replicas.\nBy default,rankis retrieved from the current distributed\ngroup.\nnum_replicas\nrank\nshuffle(bool,optional) \u2013 IfTrue(default), sampler will shuffle the\nindices.\nTrue\nseed(int,optional) \u2013 random seed used to shuffle the sampler ifshuffle=True. This number should be identical across all\nprocesses in the distributed group. Default:0.\nshuffle=True\n0\ndrop_last(bool,optional) \u2013 ifTrue, then the sampler will drop the\ntail of the data to make it evenly divisible across the number of\nreplicas. IfFalse, the sampler will add extra indices to make\nthe data evenly divisible across the replicas. Default:False.\nTrue\nFalse\nFalse\nWarning\nIn distributed mode, calling theset_epoch()method at\nthe beginning of each epochbeforecreating theDataLoaderiterator\nis necessary to make shuffling work properly across multiple epochs. Otherwise,\nthe same ordering will be always used.\nset_epoch()\nDataLoader\nExample:\n\n```python\n>>> sampler = DistributedSampler(dataset) if is_distributed else None\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\n...                     sampler=sampler)\n>>> for epoch in range(start_epoch, n_epochs):\n...     if is_distributed:\n...         sampler.set_epoch(epoch)\n...     train(loader)\n\n```\n",
  "url": "https://pytorch.org/docs/stable/data.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
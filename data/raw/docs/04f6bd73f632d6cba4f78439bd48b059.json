{
  "doc_id": "04f6bd73f632d6cba4f78439bd48b059",
  "source": "pytorch_docs",
  "title": "Extending PyTorch \u2014 PyTorch 2.9 documentation",
  "text": "\n## Extending PyTorch#\n\nCreated On: Jan 16, 2017 | Last Updated On: May 07, 2025\nIn this note we\u2019ll cover ways of extendingtorch.nn,torch.autograd,torch, and writing custom C++ extensions.\ntorch.nn\ntorch.autograd\ntorch\n\n## Adding new operators#\n\nPyTorch offers a large library of operators that work on Tensors (e.g.torch.add(),torch.sum(), etc). However, you may wish to bring a new custom operation to PyTorch\nand have it behave like PyTorch\u2019s built-in operators. In order to do so, you must\nregister the custom operation with PyTorch via the Pythontorch.libraryor C++ TORCH_LIBRARY\nAPIs.\ntorch.add()\ntorch.sum()\nPlease seePyTorch Custom Operators Landing Pagefor more details.\n\n## Extendingtorch.autograd#\n\ntorch.autograd\nAdding operations toautogradrequires implementing a newFunctionsubclass for each operation. Recall that Functions\nare whatautograduses to encode the operation history and compute\ngradients.\nautograd\nFunction\nautograd\nThe first part of this doc is focused on backward mode AD as it is the most widely used\nfeature. A section at the end discusses the extensions for forward mode AD.\n\n## When to use#\n\nIn general, implement a custom function if you want to perform computations in your model\nthat are not differentiable or rely on non-PyTorch libraries (e.g., NumPy), but\nstill wish for your operation to chain with other ops and work with the autograd engine.\nIn some situations, custom functions can also be used to improve performance and\nmemory usage: If you implemented your forward and backward passes using aC++ extension,\nyou can wrap them inFunctionto interface with the autograd\nengine. If you\u2019d like to reduce the number of buffers saved for the backward pass,\ncustom functions can be used to combine ops together.\nFunction\n\n## When not to use#\n\nIf you can already write your function in terms of PyTorch\u2019s built-in ops, its\nbackward graph is (most likely) already able to be recorded by autograd. In this case, you do\nnot need to implement the backward function yourself. Consider using a plain\nold Python function.\nIf you need to maintain state, i.e., trainable parameters, you should (also) use a\ncustom module. See the section below for more information on extendingtorch.nn.\ntorch.nn\nIf you\u2019d like to alter the gradients during the backward pass or perform a side\neffect, consider registering atensororModulehook.\n\n## How to use#\n\nTake the following steps:\n1. SubclassFunctionand implement theforward(),\n(optional)setup_context()andbackward()methods.\n2. Call the proper methods on thectxargument.\n3. Declare whether your function supportsdouble backward.\n4. Validate whether your gradients are correct using gradcheck.\nFunction\nforward()\nsetup_context()\nbackward()\nStep 1:After subclassingFunction, you\u2019ll need to define 3 methods:\nFunction\nforward()is the code that performs the operation. It can take\nas many arguments as you want, with some of them being optional, if you\nspecify the default values. All kinds of Python objects are accepted here.Tensorarguments that track history (i.e., withrequires_grad=True) will be converted to ones that don\u2019t track history\nbefore the call, and their use will be registered in the graph. Note that this\nlogic won\u2019t traverse lists/dicts/any other data structures and will only\nconsider tensors that are direct arguments to the call. You can\nreturn either a singleTensoroutput, or atupleof\ntensors if there are multiple outputs. Also, please refer to the\ndocs ofFunctionto find descriptions of useful methods that can be\ncalled only fromforward().\nforward()\nTensor\nrequires_grad=True\nTensor\ntuple\nFunction\nforward()\nsetup_context()(optional). One can either write a \u201ccombined\u201dforward()that\naccepts actxobject or (as of PyTorch 2.0) a separateforward()that does\nnot acceptctxand asetup_context()method where thectxmodification happens.\nTheforward()should have the compute andsetup_context()should\nonly be responsible for thectxmodification (and not have any compute).\nIn general the separateforward()andsetup_context()is closer to how\nPyTorch native operations work and therefore more composable with various PyTorch subsystems.\nSeeCombined or separate forward() and setup_context()for more details.\nsetup_context()\nforward()\nctx\nforward()\nctx\nsetup_context()\nctx\nforward()\nsetup_context()\nctx\nforward()\nsetup_context()\nbackward()(orvjp()) defines the gradient formula.\nIt will be given as manyTensorarguments as there were outputs, with each\nof them representing gradient w.r.t. that output. It is important NEVER to modify\nthese in-place. It should return as many tensors as there\nwere inputs, with each of them containing the gradient w.r.t. its\ncorresponding input. If your inputs didn\u2019t require gradient\n(needs_input_gradis a tuple of booleans indicating\nwhether each input needs gradient computation), or were non-Tensorobjects, you can returnpython:None. Also, if you have optional\narguments toforward()you can return more gradients than there\nwere inputs, as long as they\u2019re allNone.\nbackward()\nvjp()\nTensor\nneeds_input_grad\nTensor\npython:None\nforward()\nNone\nStep 2:It is your responsibility to use the functions inctxproperly in order to ensure that the newFunctionworks properly with\nthe autograd engine.\nctx\nFunction\nsave_for_backward()should be\nused to save any tensors needed for the backward pass (as opposed to\ndirectly onctx). You cannot usesave_for_backwardfor non-tensors;\nyou should store those directly onctx.\nsave_for_backward()\nctx\nsave_for_backward\nctx\nSaving tensors viasave_for_backward:\n1. Allows the autograd engine to clear\nthem as soon as the backward computation of theautograd.Functioncompletes.\n(If a tensor is stored directly onctxit will unnecessarily remain alive for the lifetime of the autograd graph \u2013\ntypically until the end of the iteration.)\n2. Helps avoid certain reference cycles, (e.g., since the tensor\noutput of theautograd.Functionitself keeps a reference to the ctx).\n3. Is important for compatibility with\nfeatures like activation checkpointing and offloading that rely ontorch.autograd.graph.saved_tensors_hooks.\nsave_for_backward\nautograd.Function\nctx\nautograd.Function\ntorch.autograd.graph.saved_tensors_hooks\nIf tensors that are neither inputs nor outputs are saved for backward yourFunctionmay not support double backward (see step 3).\nFunction\nmark_dirty()must be used to\nmark any input that is modified inplace by the forward function.\nmark_dirty()\nmark_non_differentiable()must\nbe used to tell the engine if an output is not differentiable. By\ndefault all output tensors that are of differentiable type will be set\nto require gradient. Tensors of non-differentiable type (i.e., integral types)\nare never marked as requiring gradients.\nmark_non_differentiable()\nset_materialize_grads()can be\nused to tell the autograd engine to optimize gradient computations in the cases where\nthe output does not depend on the input by not materializing grad tensors given to backward\nfunction. That is, if set to False, None object in Python or \u201cundefined tensor\u201d (tensor x for\nwhich x.defined() is False) in C++ will not be converted to a tensor filled with zeros prior\nto calling backward, and so your code will need to handle such objects as if they were\ntensors filled with zeros. The default value of this setting is True.\nset_materialize_grads()\nStep 3:If yourFunctiondoes not support double backward\nyou should explicitly declare this by decorating backward with theonce_differentiable(). With this decorator, attempts to\nperform double backward through your function will produce an error.\nSee our double backward tutorial for more information on double backward.\nFunction\nonce_differentiable()\nStep 4:It is recommended that you usetorch.autograd.gradcheck()to check whether your backward function correctly computes gradients of the\nforward by computing the Jacobian matrix using your backward function and\ncomparing the value element-wise with the Jacobian computed numerically using\nfinite-differencing.\ntorch.autograd.gradcheck()\n\n## Example#\n\nBelow you can find code for aLinearfunction, with\nadditional comments:\nLinear\n\n```python\n# Inherit from Function\nclass LinearFunction(Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(input, weight, bias):\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    # inputs is a Tuple of all of the inputs passed to forward.\n    # output is the output of the forward().\n    def setup_context(ctx, inputs, output):\n        input, weight, bias = inputs\n        ctx.save_for_backward(input, weight, bias)\n\n    # This function has only a single output, so it gets only one gradient\n    @staticmethod\n    def backward(ctx, grad_output):\n        # This is a pattern that is very convenient - at the top of backward\n        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n        # None. Thanks to the fact that additional trailing Nones are\n        # ignored, the return statement is simple even when the function has\n        # optional inputs.\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        # These needs_input_grad checks are optional and there only to\n        # improve efficiency. If you want to make your code simpler, you can\n        # skip them. Returning gradients for inputs that don't require it is\n        # not an error.\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias\n\n```\n\nNow, to make it easier to use these custom ops, we recommend either aliasing\nthem or wrapping them in a function. Wrapping in a function lets us support\ndefault arguments and keyword arguments:\n\n```python\n# Option 1: alias\nlinear = LinearFunction.apply\n\n# Option 2: wrap in a function, to support default args and keyword args.\ndef linear(input, weight, bias=None):\n    return LinearFunction.apply(input, weight, bias)\n\n```\n\nHere, we give an additional example of a function that is parametrized by\nnon-Tensor arguments:\n\n```python\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        # ctx is a context object that can be used to stash information\n        # for backward computation\n        tensor, constant = inputs\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None\n\n```\n\nAnd here, we optimize the above example by calling set_materialize_grads(False):\n\n```python\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        tensor, constant = inputs\n        ctx.set_materialize_grads(False)\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Here we must handle None grad_output tensor. In this case we\n        # can skip unnecessary computations and just return None.\n        if grad_output is None:\n            return None, None\n\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None\n\n```\n\nIf you need any \u201cintermediate\u201d Tensors computed inforward()to be saved,\neither they must be returned as outputs, or combineforwardandsetup_context()(seeCombined or separate forward() and setup_context()).\nNote that this means if you want gradients to flow through those intermediate values, you\nneed to define the gradient formula for them (see alsothe double backward tutorial):\nforward()\nforward\nsetup_context()\n\n```python\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        # We wish to save dx for backward. In order to do so, it must\n        # be returned as an output.\n        dx = 3 * x ** 2\n        result = x ** 3\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`,\n        # which is grad_dx * 6 * x.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\n# Wrap MyCube in a function so that it is clearer what the output is\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result\n\n```\n\nNote\nInputs tobackward, i.e.,grad_output, can also be tensors that\ntrack history. So ifbackwardis implemented with differentiable\noperations, (e.g., invocation of another customFunction), higher order derivatives will work.\nIn this case, the tensors saved withsave_for_backwardcan also be used\nin the backward and have gradients flowing back but tensors saved in thectxwon\u2019t have gradients flowing back for them.\nIf you need gradients to flow back for a Tensor saved in thectx, you should\nmake it an output of the customFunctionand save it withsave_for_backward.\nbackward\ngrad_output\nbackward\nFunction\nsave_for_backward\nctx\nctx\nFunction\nsave_for_backward\nYou probably want to check if the backward method you implemented actually\ncomputes the derivatives of your function. It is possible by comparing with\nnumerical approximations using small finite differences:\n\n```python\nfrom torch.autograd import gradcheck\n\n# gradcheck takes a tuple of tensors as input, check if your gradient\n# evaluated with these tensors are close enough to numerical\n# approximations and returns True if they all verify this condition.\ninput = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\ntest = gradcheck(linear, input, eps=1e-6, atol=1e-4)\nprint(test)\n\n```\n\nSeeNumerical gradient checkingfor more details on finite-difference gradient comparisons.\nIf your function is used in higher order derivatives (differentiating the backward pass) you\ncan use thegradgradcheckfunction from the same package to check higher order derivatives.\ngradgradcheck\n\n## Combined or separateforward()andsetup_context()#\n\nforward()\nsetup_context()\nThere are two main ways to defineFunction. Either:\nFunction\ndefine aforward()that combines the forward compute logic withsetup_context()\nforward()\nsetup_context()\n(as of PyTorch 2.0) define a separateforward()andsetup_context()\nforward()\nsetup_context()\nWe recommend the second option (separateforward()andsetup_context())\nbecause that is closer to how PyTorch native operations are implemented and it composes\nwithtorch.functransforms. However, we plan to support both approaches going forward;\ncombiningforward()withsetup_context(): leads to more flexibility since\nyou are able to save intermediates without returning them as output.\nforward()\nsetup_context()\ntorch.func\nforward()\nsetup_context()\nPlease see the previous section for how to defineFunctionwith separateforward()andsetup_context().\nFunction\nforward()\nsetup_context()\nHere is an example of how to define aFunctionwith combinedforward()andsetup_context():\nFunction\nforward()\nsetup_context()\n\n```python\nclass LinearFunction(Function):\n    @staticmethod\n    # ctx is the first argument to forward\n    def forward(ctx, input, weight, bias=None):\n        # The forward pass can use ctx.\n        ctx.save_for_backward(input, weight, bias)\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias\n\n```\n\n\n## Forward mode AD#\n\nOverriding the forward mode AD formula has a very similar API with some different subtleties.\nYou can implement thejvp()function.\njvp()\nIt will be given as manyTensorarguments as there were inputs, with each\nof them representing gradient w.r.t. that input. It should return as many tensors as there\nwere outputs, with each of them containing the gradient w.r.t. its corresponding output.\nThejvp()will be called just after theforward()method, before theapply()returns.\nTensor\njvp()\nforward()\napply()\njvp()has a few subtle differences with thebackward()function:\njvp()\nbackward()\nYou can use thectxto pass any data from theforward()to thejvp()function.\nIf that state will not be needed for thebackward(),\nyou can explicitly free it by doingdelctx.fooat the end of thejvp()function.\nforward()\njvp()\nbackward()\ndelctx.foo\njvp()\nThe implementation ofjvp()must be backward differentiable or explicitly check that\nnone of the given forward mode gradient hasrequires_gradset.\njvp()\nrequires_grad\nThejvp()function must match the view/inplace behavior offorward().\nFor example, if theith input is modified inplace, then theith gradient must be updated inplace.\nSimilarly, if thejth output is a view of thekth input. Then the returnedjth output gradient must be\na view of the givenkth input gradient.\njvp()\nforward()\ni\ni\nj\nk\nj\nk\nBecause the user cannot specify which gradient needs to be computed, thejvp()function should\nalways compute gradients for all the outputs.\njvp()\nThe forward mode gradients do respect the flag set byset_materialize_grads()and you can getNoneinput gradients when this is disabled.\nset_materialize_grads()\n\n## torch.functransforms and/ortorch.vmap()#\n\ntorch.func\ntorch.vmap()\nPlease seeExtending torch.func with autograd.Functionfor details.\n\n## Extendingtorch.nn#\n\ntorch.nn\nnnexports two kinds of interfaces - modules and their functional\nversions. You can extend it in both ways, but we recommend using modules for\nall kinds of layers, that hold any parameters or buffers, and recommend using\na functional form parameter-less operations like activation functions, pooling,\netc.\nnn\nAdding a functional version of an operation is already fully covered in the\nsection above.\n\n## Adding aModule#\n\nModule\nSincennheavily utilizesautograd, adding a newModulerequires implementing aFunctionthat performs the operation and can compute the gradient. From now on let\u2019s\nassume that we want to implement aLinearmodule and we have the function\nimplemented as in the listing above. There\u2019s very little code required to\nadd this. Now, there are two functions that need to be implemented:\nnn\nautograd\nModule\nFunction\nLinear\n__init__(optional) - takes in arguments such as kernel sizes, numbers\nof features, etc. and initializes parameters and buffers.\n__init__\nforward()- instantiates aFunctionand\nuses it to perform the operation. It\u2019s very similar to a functional wrapper\nshown above.\nforward()\nFunction\nThis is how aLinearmodule can be implemented:\nLinear\n\n```python\nclass Linear(nn.Module):\n    def __init__(self, input_features, output_features, bias=True):\n        super().__init__()\n        self.input_features = input_features\n        self.output_features = output_features\n\n        # nn.Parameter is a special kind of Tensor, that will get\n        # automatically registered as Module's parameter once it's assigned\n        # as an attribute. Parameters and buffers need to be registered, or\n        # they won't appear in .parameters() (doesn't apply to buffers), and\n        # won't be converted when e.g. .cuda() is called. You can use\n        # .register_buffer() to register buffers.\n        # nn.Parameters require gradients by default.\n        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(output_features))\n        else:\n            # You should always register all possible parameters, but the\n            # optional ones can be None if you want.\n            self.register_parameter('bias', None)\n\n        # Not a very smart way to initialize weights\n        nn.init.uniform_(self.weight, -0.1, 0.1)\n        if self.bias is not None:\n            nn.init.uniform_(self.bias, -0.1, 0.1)\n\n    def forward(self, input):\n        # See the autograd section for explanation of what happens here.\n        return LinearFunction.apply(input, self.weight, self.bias)\n\n    def extra_repr(self):\n        # (Optional)Set the extra information about this module. You can test\n        # it by printing an object of this class.\n        return 'input_features={}, output_features={}, bias={}'.format(\n            self.input_features, self.output_features, self.bias is not None\n        )\n\n```\n\n\n## ExtendingtorchPython API#\n\ntorch\nYou can create custom types that emulateTensorby defining a custom\nclass with methods that matchTensor. But what if you want to be able\nto pass these types to functions liketorch.add()in the top-leveltorchnamespace that acceptTensoroperands?\nTensor\nTensor\ntorch.add()\ntorch\nTensor\nIf your custom Python type defines a method named__torch_function__, PyTorch\nwill invoke your__torch_function__implementation when an instance of your\ncustom class is passed to a function in thetorchnamespace. This makes\nit possible to define custom implementations for any of the functions in thetorchnamespace which your__torch_function__implementation can call,\nallowing your users to make use of your custom type with existing PyTorch\nworkflows that they have already written forTensor. This works with\n\u201cduck\u201d types that are unrelated toTensoras well as user-defined\nsubclasses ofTensor.\n__torch_function__\n__torch_function__\ntorch\ntorch\n__torch_function__\nTensor\nTensor\nTensor\n\n## Extendingtorchwith aTensor-like type#\n\ntorch\nTensor\nNote\nThis functionality is inspired by the NumPy__array_function__protocol. Seethe NumPy documentationandNEP-0018for\nmore details.\n__array_function__\nTo make this concrete, let\u2019s begin with a simple example that illustrates the\nAPI dispatch mechanism. We\u2019ll create a custom type that represents a 2D scalar\ntensor, parametrized by the orderNand value along the diagonal entries,value:\nN\nvalue\n\n```python\nclass ScalarTensor(object):\n   def __init__(self, N, value):\n       self._N = N\n       self._value = value\n\n   def __repr__(self):\n       return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n   def tensor(self):\n       return self._value * torch.eye(self._N)\n\n```\n\nThis first iteration of the design isn\u2019t very useful. The main functionality ofScalarTensoris to provide a more compact string representation of a scalar\ntensor than in the base tensor class:\nScalarTensor\n\n```python\n>>> d = ScalarTensor(5, 2)\n>>> d\nScalarTensor(N=5, value=2)\n>>> d.tensor()\ntensor([[2., 0., 0., 0., 0.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 0., 0.],\n        [0., 0., 0., 2., 0.],\n        [0., 0., 0., 0., 2.]])\n\n```\n\nIf we try to use this object with thetorchAPI, we will run\ninto issues:\ntorch\n\n```python\n>>> import torch\n>>> torch.mean(d)\nTypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor\n\n```\n\nAdding a__torch_function__implementation toScalarTensormakes it\npossible for the above operation to succeed. Let\u2019s re-do our implementation,\nthis time adding a__torch_function__implementation:\n__torch_function__\nScalarTensor\n__torch_function__\n\n```python\nHANDLED_FUNCTIONS = {}\nclass ScalarTensor(object):\n    def __init__(self, N, value):\n        self._N = N\n        self._value = value\n\n    def __repr__(self):\n        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n    def tensor(self):\n        return self._value * torch.eye(self._N)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n            return NotImplemented\n        return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\n```\n\nThe__torch_function__method takes four arguments:func, a reference\nto the torch API function that is being overridden,types, the list of\ntypes of Tensor-likes that implement__torch_function__,args, the\ntuple of arguments passed to the function, andkwargs, the dict of keyword\narguments passed to the function. It uses a global dispatch table namedHANDLED_FUNCTIONSto store custom implementations. The keys of this\ndictionary are functions in thetorchnamespace and the values are\nimplementations forScalarTensor.\n__torch_function__\nfunc\ntypes\n__torch_function__\nargs\nkwargs\nHANDLED_FUNCTIONS\ntorch\nScalarTensor\nNote\nUsing a global dispatch table is not a mandated part of the__torch_function__API, it is just a useful design pattern for\nstructuring your override implementations.\n__torch_function__\nThis class definition isn\u2019t quite enough to maketorch.meando the right\nthing when we pass it aScalarTensor\u2013 we also need to define an\nimplementation fortorch.meanforScalarTensoroperands and add the\nimplementation to theHANDLED_FUNCTIONSdispatch table dictionary. One way\nof doing this is to define a decorator:\ntorch.mean\nScalarTensor\ntorch.mean\nScalarTensor\nHANDLED_FUNCTIONS\n\n```python\nimport functools\ndef implements(torch_function):\n    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n    def decorator(func):\n        functools.update_wrapper(func, torch_function)\n        HANDLED_FUNCTIONS[torch_function] = func\n        return func\n    return decorator\n\n```\n\nwhich can be applied to the implementation of our override:\n\n```python\n@implements(torch.mean)\ndef mean(input):\n    return float(input._value) / input._N\n\n```\n\nWith this change we can now usetorch.meanwithScalarTensor:\ntorch.mean\nScalarTensor\n\n```python\n>>> d = ScalarTensor(5, 2)\n>>> torch.mean(d)\n0.4\n\n```\n\nOf coursetorch.meanis an example of the simplest kind of function to\noverride since it only takes one operand. We can use the same machinery to\noverride a function that takes more than one operand, any one of which might be\na tensor or tensor-like that defines__torch_function__, for example fortorch.add():\ntorch.mean\n__torch_function__\ntorch.add()\n\n```python\ndef ensure_tensor(data):\n    if isinstance(data, ScalarTensor):\n        return data.tensor()\n    return torch.as_tensor(data)\n\n@implements(torch.add)\ndef add(input, other):\n   try:\n       if input._N == other._N:\n           return ScalarTensor(input._N, input._value + other._value)\n       else:\n           raise ValueError(\"Shape mismatch!\")\n   except AttributeError:\n       return torch.add(ensure_tensor(input), ensure_tensor(other))\n\n```\n\nThis version has a fast path for when both operands areScalarTensorinstances and also a slower path which degrades to converting the data to\ntensors when either operand is not aScalarTensor. That makes the override\nfunction correctly when either operand is aScalarTensoror a regularTensor:\nScalarTensor\nScalarTensor\nScalarTensor\nTensor\n\n```python\n>>> s = ScalarTensor(2, 2)\n>>> torch.add(s, s)\nScalarTensor(N=2, value=4)\n>>> t = torch.tensor([[1, 1,], [1, 1]])\n>>> torch.add(s, t)\ntensor([[3., 1.],\n        [1., 3.]])\n\n```\n\nNote that our implementation ofadddoes not takealphaoroutas\nkeyword arguments liketorch.add()does:\nadd\nalpha\nout\ntorch.add()\n\n```python\n>>> torch.add(s, s, alpha=2)\nTypeError: add() got an unexpected keyword argument 'alpha'\n\n```\n\nFor speed and flexibility the__torch_function__dispatch mechanism does not\ncheck that the signature of an override function matches the signature of the\nfunction being overridden in thetorchAPI. For some applications ignoring\noptional arguments would be fine but to ensure full compatibility withTensor, user implementations of torch API functions should take care to\nexactly emulate the API of the function that is being overridden.\n__torch_function__\ntorch\nTensor\nFunctions in thetorchAPI that do not have explicit overrides will\nreturnNotImplementedfrom__torch_function__. If all operands with__torch_function__defined on them returnNotImplemented, PyTorch will\nraise aTypeError. This means that most of the time operations that do not\nhave explicit overrides for a type will raise aTypeErrorwhen an instance\nof such a type is passed:\ntorch\nNotImplemented\n__torch_function__\n__torch_function__\nNotImplemented\nTypeError\nTypeError\n\n```python\n>>> torch.mul(s, 3)\nTypeError: no implementation found for 'torch.mul' on types that\nimplement __torch_function__: [ScalarTensor]\n\n```\n\nIn practice this means that if you would like to implement your overrides using\na__torch_function__implementation along these lines, you will need to\nexplicitly implement the fulltorchAPI or the entire subset of the API\nthat you care about for your use case. This may be a tall order as the fulltorchAPI is quite extensive.\n__torch_function__\ntorch\ntorch\nAnother option is to not returnNotImplementedfor operations that are not\nhandled but to instead pass aTensorto the originaltorchfunction when no override is available. For example, if we change our\nimplementation of__torch_function__forScalarTensorto the one below:\nNotImplemented\nTensor\ntorch\n__torch_function__\nScalarTensor\n\n```python\n@classmethod\ndef __torch_function__(cls, func, types, args=(), kwargs=None):\n    if kwargs is None:\n        kwargs = {}\n    if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n        args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n        return func(*args, **kwargs)\n    return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\n```\n\nThentorch.mul()will work correctly, although the return type will always\nbe aTensorrather than aScalarTensor, even if both operands\nareScalarTensorinstances:\ntorch.mul()\nTensor\nScalarTensor\nScalarTensor\n\n```python\n>>> s = ScalarTensor(2, 2)\n>>> torch.mul(s, s)\ntensor([[4., 0.],\n        [0., 4.]])\n\n```\n\nAlso see theMetadataTensorexample below for another variation on this\npattern but instead always returns aMetadataTensorto propagate metadata\nthrough operations in thetorchAPI.\nMetadataTensor\nMetadataTensor\ntorch\nThe__torch_function__protocol is designed for full coverage of the API,\npartial coverage may lead to undesirable results, in particular, certain\nfunctions raising aTypeError. This is especially true for subclasses,\nwhere all three oftorch.add,torch.Tensor.__add__andtorch.Tensor.addmust be covered, even if they return exactly the same result. Failing to do\nthis may also lead to infinite recursion. If one requires the implementation\nof a function fromtorch.Tensorsubclasses, they must usesuper().__torch_function__inside their implementation.\n__torch_function__\nTypeError\ntorch.Tensor\nsuper().__torch_function__\n\n## Subclassingtorch.Tensor#\n\ntorch.Tensor\nAs of version 1.7.0, methods ontorch.Tensorand functions in publictorch.*namespaces applied ontorch.Tensorsubclasses\nwill return subclass instances instead oftorch.Tensorinstances:\ntorch.Tensor\ntorch.*\ntorch.Tensor\ntorch.Tensor\n\n```python\n>>> class SubTensor(torch.Tensor):\n...     pass\n>>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__\n'SubTensor'\n>>> type(torch.add(SubTensor([0]), torch.tensor([1]))).__name__\n'SubTensor'\n\n```\n\nIf multiple subclasses exist, the lowest one in the hierarchy will be chosen by\ndefault. If there is no unique way to determine such a case, then aTypeErroris raised:\nTypeError\n\n```python\n>>> type(torch.add(SubTensor2([0]), SubTensor([1]))).__name__\n'SubTensor2'\n>>> type(torch.add(SubTensor2([0]), torch.tensor([1]))).__name__\n'SubTensor2'\n>>> torch.add(SubTensor([0]), OtherSubTensor([1]))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor]\n\n```\n\nIf one wishes to have a global override for all tensor methods, one can use__torch_function__. Here is an example that logs all function/method\ncalls:\n__torch_function__\n\n```python\nclass LoggingTensor(torch.Tensor):\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion\n        if func is not torch.Tensor.__repr__:\n            logging.info(f\"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}\")\n        if kwargs is None:\n            kwargs = {}\n        return super().__torch_function__(func, types, args, kwargs)\n\n```\n\nHowever, if one instead wishes to override a method on the Tensor subclass,\nthere one can do so either by directly overriding the method (by defining\nit for a subclass), or by using__torch_function__and matching withfunc.\n__torch_function__\nfunc\nOne should be careful within__torch_function__for subclasses to always\ncallsuper().__torch_function__(func,...)instead offuncdirectly,\nas was the case before version 1.7.0. Failing to do this may causefuncto recurse back into__torch_function__and therefore cause infinite\nrecursion.\n__torch_function__\nsuper().__torch_function__(func,...)\nfunc\nfunc\n__torch_function__\n\n## Extendingtorchwith aTensorwrapper type#\n\ntorch\nTensor\nAnother useful case is a type that wraps aTensor, either as an\nattribute or via subclassing. Below we implement a special case of this sort of\ntype, aMetadataTensorthat attaches a dictionary of metadata to aTensorthat is propagated throughtorchoperations. Since this\nis a generic sort of wrapping for the fulltorchAPI, we do not need to\nindividually implement each override so we can make the__torch_function__implementation more permissive about what operations are allowed:\nTensor\nMetadataTensor\nTensor\ntorch\ntorch\n__torch_function__\n\n```python\nclass MetadataTensor(object):\n    def __init__(self, data, metadata=None, **kwargs):\n        self._t = torch.as_tensor(data, **kwargs)\n        self._metadata = metadata\n\n    def __repr__(self):\n        return \"Metadata:\\n{}\\n\\ndata:\\n{}\".format(self._metadata, self._t)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))\n        args = [getattr(a, '_t', a) for a in args]\n        assert len(metadatas) > 0\n        ret = func(*args, **kwargs)\n        return MetadataTensor(ret, metadata=metadatas[0])\n\n```\n\nThis simple implementation won\u2019t necessarily work with every function in thetorchAPI but it is good enough to capture most common operations:\ntorch\n\n```python\n>>> metadata = {'owner': 'Ministry of Silly Walks'}\n>>> m = MetadataTensor([[1, 2], [3, 4]], metadata=metadata)\n>>> t = torch.tensor([[1, 2], [1, 2]])\n>>> torch.add(t, m)\nMetadata:\n{'owner': 'Ministry of Silly Walks'}\n\ndata:\ntensor([[2, 4],\n        [4, 6]])\n>>> torch.mul(t, m)\nMetadata:\n{'owner': 'Ministry of Silly Walks'}\n\ndata:\ntensor([[1, 4],\n        [3, 8]])\n\n```\n\n\n## Operations on multiple types that define__torch_function__#\n\n__torch_function__\nIt is possible to use the torch API with multiple distinct types that each have\na__torch_function__implementation, but special care must be taken. In such\na case the rules are:\n__torch_function__\nThe dispatch operation gathers all distinct implementations of__torch_function__for each operand and calls them in order: subclasses\nbefore superclasses, and otherwise left to right in the operator expression.\n__torch_function__\nIf any value other thanNotImplementedis returned, that value is\nreturned as the result. Implementations can register that they do not\nimplement an operation by returningNotImplemented.\nNotImplemented\nNotImplemented\nIf all of the__torch_function__implementations returnNotImplemented, PyTorch raises aTypeError.\n__torch_function__\nNotImplemented\nTypeError\n\n## Testing Coverage of Overrides for the PyTorch API#\n\nOne troublesome aspect of implementing__torch_function__is that if some\noperations do and others do not have overrides, users will at best see an\ninconsistent experience, or at worst will see errors raised at runtime when they\nuse a function that does not have an override. To ease this process, PyTorch\nprovides a developer-facing API for ensuring full support for__torch_function__overrides. This API is private and may be subject to\nchanges without warning in the future.\n__torch_function__\n__torch_function__\nFirst, to get a listing of all overridable functions, usetorch.overrides._get_overridable_functions. This returns a dictionary whose\nkeys are namespaces in thePyTorchPython API and whose values are a list of\nfunctions in that namespace that can be overridden. For example, let\u2019s print the\nnames of the first 5 functions intorch.nn.functionalthat can be\noverridden:\ntorch.overrides._get_overridable_functions\nPyTorch\ntorch.nn.functional\n\n```python\n>>> from torch.overrides import get_overridable_functions\n>>> func_dict = get_overridable_functions()\n>>> nn_funcs = func_dict[torch.nn.functional]\n>>> print([f.__name__ for f in nn_funcs[:5])\n['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d',\n 'adaptive_max_pool1d', 'adaptive_max_pool1d_with_indices']\n\n```\n\nThis listing of functions makes it possible to iterate over all overridable\nfunctions, however in practice this is not enough to write tests for all of\nthese functions without laboriously and manually copying the signature of each\nfunction for each test. To ease this process, thetorch.overrides._get_testing_overridesfunction returns a dictionary mapping\noverridable functions in thePyTorchAPI to dummy lambda functions that have\nthe same signature as the original function but unconditionally return -1. These\nfunctions are most useful to use withinspectto analyze the function\nsignature of the originalPyTorchfunction:\ntorch.overrides._get_testing_overrides\nPyTorch\ninspect\nPyTorch\n\n```python\n>>> import inspect\n>>> from torch.overrides import get_testing_overrides\n>>> override_dict = get_testing_overrides()\n>>> dummy_add = override_dict[torch.add]\n>>> inspect.signature(dummy_add)\n<Signature (input, other, out=None)>\n\n```\n\nFinally,torch.overrides.get_ignored_functionsreturns a tuple of functions\nthat explicitly cannot be overridden by__torch_function__. This list can be\nuseful to confirm that a function that isn\u2019t present in the dictionary returned\nbyget_overridable_functionscannot be overridden.\ntorch.overrides.get_ignored_functions\n__torch_function__\nget_overridable_functions\n\n## Extendingtorchnative API#\n\ntorch\nWhile__torch_function__allows one to effectively extend PyTorch\u2019s pure Python\ncomponents\u2019 behavior, it does not allow one to extend the parts of\nPyTorch implemented in C++. To that end, aTensorsubclass can also\ndefine__torch_dispatch__which will be able to override the behavior at the\nC++ level.\n__torch_function__\nTensor\n__torch_dispatch__\nTo effectively use this feature, it is important to know how the native part of\nPyTorch is implemented. The most important component there is what we call the\n\u201cdispatcher\u201d (the best description can be found in thisblog posteven though it is slightly outdated). As\nhinted by its name, it is responsible for calling the right backend\nfunction for a specific call of a function. For example, when callingtorch.add(a,b), the dispatcher will inspect both arguments, figure out which\n\u201cfeature\u201d (autograd, autocast, functionalization, etc) and which \u201cbackend\u201d (CPU,\nCUDA, MPS, etc) should be used for this specific call and finally call all the\nright kernels.\nA very common thing done by a kernel is to \u201credispatch\u201d. For example, when running your\nneural network on GPU with autocast, the first call will be the autocast kernel that\nwill handle any potential autocast logic and redispatch down. The next feature in line\nwill be autograd that will properly create the autograd graph and then redispatch down.\nFinally, we reach the backend kernel for CUDA which will launch the right CUDA kernel\nand return the final result. On the way out, autograd will attach the graph to the\noutput and, finally, autocast will have a chance to do any update it needs on exit.\ntorch.add(a,b)\nOne configuration of the dispatcher is the order in which all these feature and backend keys are called. The latest list and their order can be found inDispatchKey.hinside theDispatchKeyenum. For the purpose of extending torch, the important subset of the ordering for this discussion is:\nDispatchKey.h\nDispatchKey\nvmap -> Autocast -> Autograd -> ZeroTensor -> Neg/Conj -> Functionalize -> Python -> Backends\nThe most important key for the purpose of this discussion isPythonas every Tensor subclass with the__torch_dispatch__method defined will call into this feature. It is from there that the user-defined method is called and where the behavior can be overwritten arbitrarily. From there, calling the providedfuncagain will perform a \u201credispatch\u201d.\nPython\n__torch_dispatch__\nfunc\nSome important implications of this implementation are:\nThis code runs \u201cbelow all features\u201d. It is thus only responsible, like a regular backend, for generating the output value of each Tensor (and can, and should, ignore all advanced features like autograd, autocast, etc).\nIf any high level feature implements a given function without redispatching, it will never reach thePythonkey and so the__torch_dispatch__callback will never be triggered. This happens in particular for CompositeImplicitAutograd functions which are evaluated at the Autograd level without redispatching. This is because a CompositeImplicitAutograd function specifies its autograd formula by implicitly calling other native ops, so at the Autograd level, the function is decomposed into its native ops and those are evaluated instead.\nPython\n__torch_dispatch__\nWhen calling back to Python and when wrapping the results, the same conversions are used as the regular PyTorch Python/C++ binding. In particular, some objects cannot be represented in Python and need special handling (undefined Tensors for example become None).\nOur native functions are lazily populated astorch.ops.{namespace}.{func_name}.{overload_name}as callable Python objects to enable easily interacting with them from Python. Thefuncobject given to__torch_dispatch__is always an entry from this namespace. This namespace can be used to directly call native ops and bypass the usual Python API and binding code.\ntorch.ops.{namespace}.{func_name}.{overload_name}\nfunc\n__torch_dispatch__\nIn a similar way where__torch_function__is able to interpose on all of torch\u2019s Python API and Tensor methods,__torch_dispatch__is able to intercept all calls into the aten native API. Note that all methods on Tensors are converted into function calls before entering the dispatcher and thus will appear as function calls here:torch.add(a,2)anda+2will lead to exactly the same aten call.\nMost of these functions are defined innative_functions.yamlwhich specifies the properties of these functions as well as their backend implementation. Their implementation alongside specified features are then automatically registered via codegen.\nSome more exotic functions or features are also registered in other places in the C++ codebase or in user-defined C++ extensions.\n__torch_function__\n__torch_dispatch__\ntorch.add(a,2)\na+2\nnative_functions.yaml\nIt is also possible to addnewnative functions usingtorch.library. This Python feature allows defining and/or adding new implementations to native functions. This can be used to add missing kernels, replace existing ones or define brand new native functions.\ntorch.library\nYou can find many examples of__torch_dispatch__-based subclasses in thesubclass zoorepo.\n__torch_dispatch__\n\n## __torch_dispatch__calling convention#\n\n__torch_dispatch__\n\n```python\n@classmethod\ndef __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n    pass\n\n```\n\nWhen a user calls an operator with inputs that have__torch_dispatch__, that call\nmay be forwarded to the__torch_dispatch__. args and kwargs get normalized before\nthe call to__torch_dispatch__, that is:\n__torch_dispatch__\n__torch_dispatch__\n__torch_dispatch__\nthekwargsconsist of keyword-only arguments in the operator\u2019s schema.\nIf a kwarg is equal to its default value (in the schema), it will not be passed.\nkwargs\ntheargsconsists of all other arguments, no matter how they were passed\nto the operator (positional vs keyword).\nIf an arg is equal to its default value, and\nit is the right-most positional arg or all the args to the right of it\nare not passed, it will not be passed.\nargs\n\n## Extending alltorchAPI with Modes#\n\ntorch\nUnfortunately, there are functions that do not take Tensor inputs. This means that the subclass approach described above cannot be used to override the behavior of all of PyTorch\u2019s functions. Also, if the use case requires to intercept every function call, changing every Tensor to be a subclass can be overly intrusive.\nTo address this use case, we introduced the concept of \u201cMode\u201d. These exist for__torch_function__and__torch_dispatch__overrides, are created by subclassing respectivelytorch.overrides.TorchFunctionModeandtorch.utils._python_dispatch.TorchDispatchMode, and are used as a context manager.\n__torch_function__\n__torch_dispatch__\ntorch.overrides.TorchFunctionMode\ntorch.utils._python_dispatch.TorchDispatchMode\nTo simplify the description of how it interacts with subclasses and other modes, whenever the context manager for a mode is entered, every function behaves as if there was an extra Tensor argument at the beginning of the argument list with the mode as a subclass.\nThis means in particular that all modes handlers will be called before any subclass handler and that modes corresponding to the inner context manager will always run first.\nIt is also important to note that within a given mode handler, this specific mode is disabled and can be re-enabled manually by doingwithself:.\nwithself:\nHere is an example that shows logging modes of each type:\n\n```python\nimport torch\nfrom torch.overrides import TorchFunctionMode, resolve_name\nfrom torch.utils._python_dispatch import TorchDispatchMode\n\nclass FunctionLog(TorchFunctionMode):\n    def __torch_function__(self, func, types, args, kwargs=None):\n        print(f\"Function Log: {resolve_name(func)}(*{args}, **{kwargs})\")\n        return func(*args, **(kwargs or {}))\n\nclass DispatchLog(TorchDispatchMode):\n    def __torch_dispatch__(self, func, types, args, kwargs=None):\n        print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n        return func(*args, **(kwargs or {}))\n\ndef f():\n    a = torch.rand(10, requires_grad=True)\n    b = a * 2\n    b.sum().backward()\n\nprint(\"TorchFunctionMode logging:\")\nwith FunctionLog():\n    f()\n\nprint(\"TorchDispatchMode logging:\")\nwith DispatchLog():\n    f()\n\n```\n\nWhich prints the following, with extra comments:\n\n```python\nTorchFunctionMode logging:\nFunction Log: torch.rand(*(10,), **{'requires_grad': True})\nFunction Log: torch.Tensor.mul(*(tensor([0.7164, 0.9897, 0.1745, 0.9336, 0.4287, 0.7989, 0.2169, 0.7474, 0.5624,\n        0.5970], requires_grad=True), 2), **None)\nFunction Log: torch.Tensor.sum(*(tensor([1.4328, 1.9794, 0.3490, 1.8671, 0.8573, 1.5977, 0.4338, 1.4948, 1.1249,\n        1.1939], grad_fn=<MulBackward0>),), **None)\n# Note that at the python level, we only see the call to backward but not what happens in the autograd engine.\nFunction Log: torch.Tensor.backward(*(tensor(12.3307, grad_fn=<SumBackward0>),), **{'gradient': None, 'retain_graph': None, 'create_graph': False, 'inputs': None})\n\nTorchDispatchMode logging:\n# Here the requires_grad flag from autograd is removed while default arguments were populated.\nDispatch Log: aten.rand.default(*([10],), **{'device': device(type='cpu'), 'pin_memory': False})\nDispatch Log: aten.mul.Tensor(*(tensor([0.2151, 0.6018, 0.8415, 0.9060, 0.2974, 0.7708, 0.6668, 0.0352, 0.7948,\n        0.6023], requires_grad=True), 2), **{})\nDispatch Log: aten.sum.default(*(tensor([0.4303, 1.2036, 1.6831, 1.8120, 0.5949, 1.5416, 1.3335, 0.0705, 1.5897,\n        1.2046], grad_fn=<MulBackward0>),), **{})\n# Here we don't see the call to backward itself, but its constituents. Starting here with the factory function that creates the initial gradient.\nDispatch Log: aten.ones_like.default(*(tensor(11.4637, grad_fn=<SumBackward0>),), **{'pin_memory': False, 'memory_format': torch.preserve_format})\n# This is the backward of the sum\nDispatch Log: aten.expand.default(*(tensor(1.), [10]), **{})\nDispatch Log: aten.mul.Tensor(*(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 2), **{})\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})\n\n```\n",
  "url": "https://pytorch.org/docs/stable/notes/extending.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
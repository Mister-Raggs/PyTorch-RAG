{
  "doc_id": "80891cc0bb58f9bd7e92c5eef692b014",
  "source": "pytorch_docs",
  "title": "torch.distributed.tensor \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.distributed.tensor#\n\nCreated On: Jun 13, 2025 | Last Updated On: Aug 23, 2025\nNote\ntorch.distributed.tensoris currently in alpha state and under\ndevelopment, we are committing backward compatibility for the most APIs listed\nin the doc, but there might be API changes if necessary.\ntorch.distributed.tensor\n\n## PyTorch DTensor (Distributed Tensor)#\n\nPyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed\nlogic, including sharded storage, operator computation and collective communications across devices/hosts.DTensorcould be used to build different parallelism solutions and support sharded state_dict representation\nwhen working with multi-dimensional sharding.\nDTensor\nPlease see examples from the PyTorch native parallelism solutions that are built on top ofDTensor:\nDTensor\nTensor Parallel\nFSDP2\nDTensorfollows the SPMD (single program, multiple data) programming model to empower users to\nwrite distributed program as if it\u2019s asingle-device program with the same convergence property. It\nprovides a uniform tensor sharding layout (DTensor Layout) through specifying theDeviceMeshandPlacement:\nDTensor\nDeviceMesh\nPlacement\nDeviceMeshrepresents the device topology and the communicators of the cluster using\nan n-dimensional array.\nDeviceMesh\nPlacementdescribes the sharding layout of the logical tensor on theDeviceMesh.\nDTensor supports three types of placements:Shard,ReplicateandPartial.\nPlacement\nDeviceMesh\nShard\nReplicate\nPartial\n\n## DTensor Class APIs#\n\nDTensoris atorch.Tensorsubclass. This means once aDTensoris created, it could be\nused in very similar way totorch.Tensor, including running different types of PyTorch operators as if\nrunning them in a single device, allowing proper distributed computation for PyTorch operators.\nDTensor\ntorch.Tensor\nDTensor\ntorch.Tensor\nIn addition to existingtorch.Tensormethods, it also offers a set of additional methods to interact withtorch.Tensor,redistributethe DTensor Layout to a new DTensor, get the full tensor content\non all devices, etc.\ntorch.Tensor\ntorch.Tensor\nredistribute\nDTensor(Distributed Tensor) is a subclass oftorch.Tensorthat provides single-device like\nabstraction to program with multi-devicetorch.Tensor. It describes the distributed tensor sharding\nlayout (DTensor Layout) through theDeviceMeshand following types ofPlacement:\nDTensor\ntorch.Tensor\ntorch.Tensor\nDeviceMesh\nPlacement\nShard: Tensor sharded on the tensor dimensiondimon the devices of theDeviceMeshdimension\nShard\ndim\nDeviceMesh\nReplicate: Tensor replicated on the devices of theDeviceMeshdimension\nReplicate\nDeviceMesh\nPartial: Tensor is pending reduction on the devices of theDeviceMeshdimension\nPartial\nDeviceMesh\nWhen calling PyTorch operators,DTensoroverrides the PyTorch operators to perform sharded computation and issue\ncommunications whenever necessary. Along with the operator computation,DTensorwill transform or propagate the\nplacements (DTensor Layout) properly (based on the operator semantic itself) and generate newDTensoroutputs.\nDTensor\nDTensor\nDTensor\nTo ensure numerical correctness of theDTensorsharded computation when calling PyTorch operators,DTensorrequires every Tensor argument of the operator be DTensor.\nDTensor\nDTensor\nNote\nDirectly using the Tensor subclass constructor here is not the recommended way to create aDTensor(i.e. it does not handle autograd correctly hence is not the public API). Please refer to thecreate_dtensorsection to see how to create aDTensor.\nDTensor\nDTensor\nDTensor\nReturn a list of ChunkStorageMetadata, which is a dataclass that describes the size/offset of the local shard/replica\non current rank. For DTensor, each rank will have a single local shard/replica, so the returned list usually only\nhas one element.\nThis dunder method is primariy used for distributed checkpoint purpose.\nA List[ChunkStorageMetadata] object that represents the shard size/offset on the current rank.\nChunkStorageMetadata\nCreate aDTensorfrom a local torch.Tensor on each rank\naccording to thedevice_meshandplacementsspecified.\nDTensor\ndevice_mesh\nplacements\nlocal_tensor(torch.Tensor) \u2013 local torch.Tensor on each rank.\ndevice_mesh(DeviceMesh, optional) \u2013 DeviceMesh to place the\ntensor, if not specified, must be called under a DeviceMesh\ncontext manager, default: None\nDeviceMesh\nplacements(List[Placement], optional) \u2013 the placements that\ndescribes how to place the local torch.Tensor on DeviceMesh, must\nhave the same number of elements asdevice_mesh.ndim.\nPlacement\ndevice_mesh.ndim\nrun_check(bool,optional) \u2013 at a cost of extra communications, perform\nsanity check across ranks to check each local tensor\u2019s meta information\nto ensure correctness. If haveReplicateinplacements, the\ndata on first rank of the device mesh dimension will be broadcasted\nto other ranks. default: False\nReplicate\nplacements\nshape(torch.Size,optional) \u2013 A List of int which specifies the size of\nDTensor which build on top oflocal_tensor. Note this needs to be\nprovided if the shape oflocal_tensorare different across the ranks.\nIf not provided,shapewill be computed assuming the given distributed\ntensor is evenly sharded across ranks. default: None\nlocal_tensor\nshape\nstride(tuple,optional) \u2013 A List of int which specifies the stride of DTensor.\nIf not provided,stridewill be computed assuming the given distributed\ntensor is evenly sharded across ranks. default: None\nstride\nADTensorobject\nDTensor\nDTensor\nNote\nWhenrun_check=False, it is the user\u2019s responsibility to ensure the\nlocal tensor passed in is correct across ranks (i.e. the tensor is sharded for\ntheShard(dim)placement or replicated for theReplicate()placement).\nIf not, the behavior of the created DTensor is undefined.\nrun_check=False\nShard(dim)\nReplicate()\nNote\nfrom_localis differentiable, therequires_gradof the createdDTensorobject will depend on iflocal_tensorrequires_grad or not.\nfrom_local\nReturn the full tensor of this DTensor. It will perform necessary collectives\nto gather the local tensors from other ranks in its DeviceMesh and concatenate\nthem together. It\u2019s a syntactic sugar of the following code:\ndtensor.redistribute(placements=[Replicate()]*mesh.ndim).to_local()\ndtensor.redistribute(placements=[Replicate()]*mesh.ndim).to_local()\ngrad_placements(List[Placement], optional) \u2013 the placements describes\nthe future layout of any gradient layout of the full Tensor returned from this\nfunction.full_tensorconverts DTensor to a full torch.Tensor and the returned torch.tensor\nmight not be used as the original replicated DTensor layout later in the code. This\nargument is the hint that user can give to autograd in case the gradient\nlayout of the returned tensor does not match the original replicated DTensor layout.\nIf not specified, we will assume the gradient layout of the full tensor be replicated.\nPlacement\nAtorch.Tensorobject that represents the full tensor of this DTensor.\ntorch.Tensor\nTensor\nNote\nfull_tensoris differentiable.\nfull_tensor\nredistributeperforms necessary collective operations that redistribute the current\nDTensor from its current placements to a new placements, or from its current DeviceMesh\nto a new DeviceMesh. i.e. we can turn a Sharded DTensor to a Replicated DTensor by\nspecifying a Replicate placement for each dimension of the DeviceMesh.\nredistribute\nWhen redistributing from current to the new placements on one device mesh dimension, we\nwill perform the following operations including communication collective or local operation:\nShard(dim)->Replicate():all_gather\nShard(dim)\nReplicate()\nall_gather\nShard(src_dim)->Shard(dst_dim):all_to_all\nShard(src_dim)\nShard(dst_dim)\nall_to_all\nReplicate()->Shard(dim): local chunking (i.e.torch.chunk)\nReplicate()\nShard(dim)\ntorch.chunk\nPartial()->Replicate():all_reduce\nPartial()\nReplicate()\nall_reduce\nPartial()->Shard(dim):reduce_scatter\nPartial()\nShard(dim)\nreduce_scatter\nredistributewould correctly figure out the necessary redistribute steps for DTensors\nthat are created either on 1-D or N-D DeviceMesh.\nredistribute\ndevice_mesh(DeviceMesh, optional) \u2013 DeviceMesh to place the\nDTensor. If not specified, it would use the current DTensor\u2019s DeviceMesh.\ndefault: None\nDeviceMesh\nplacements(List[Placement], optional) \u2013 the new placements that\ndescribes how to place the DTensor into the DeviceMesh, must\nhave the same number of elements asdevice_mesh.ndim.\ndefault: replicate on all mesh dimensions\nPlacement\ndevice_mesh.ndim\nasync_op(bool,optional) \u2013 whether to perform the DTensor redistribute operation\nasynchronously or not. Default: False\nforward_dtype(torch.dtype,optional) \u2013 the local tensor datatype can be converted toforward_dtypebefore redistributing the local tensor in its forward.\nThe result DTensor will be inforward_dtypeDefault: None.\nforward_dtype\nforward_dtype\nbackward_dtype(torch.dtype,optional) \u2013 the local tensor datatype can be converted tobackward_dtypebefore redistributing the local tensor in its backward.\nThe result DTensor gradient would be converted back to the current DTensor dtype. Default: None\nbackward_dtype\nADTensorobject\nDTensor\nDTensor\nNote\nredistributeis differentiable, which means user do not need to worry about\nthe backward formula of the redistribute operation.\nredistribute\nNote\nredistributecurrently only supports redistributing DTensor on the same DeviceMesh,\nPlease file an issue if you need to redistribute DTensor to different DeviceMesh.\nredistribute\nGet the local tensor of this DTensor on its current rank. For sharding it returns\na local shard of the logical tensor view, for replication it returns the replica on\nits current rank.\ngrad_placements(List[Placement], optional) \u2013 the placements describes\nthe future layout of any gradient layout of the Tensor returned from this\nfunction.to_localconverts DTensor to local tensor and the returned local tensor\nmight not be used as the original DTensor layout later in the code. This\nargument is the hint that user can give to autograd in case the gradient\nlayout of the returned tensor does not match the original DTensor layout.\nIf not specified, we will assume the gradient layout remains the same\nas the original DTensor and use that for gradient computation.\nPlacement\nAtorch.TensororAsyncCollectiveTensorobject. it represents the\nlocal tensor on its current rank. When anAsyncCollectiveTensorobject is returned,\nit means the local tensor is not ready yet (i.e. communication is not finished). In this\ncase, user needs to callwaitto wait the local tensor to be ready.\ntorch.Tensor\nAsyncCollectiveTensor\nAsyncCollectiveTensor\nwait\nTensor\nNote\nto_localis differentiable, therequires_gradof the local tensor returned\nwill depend on if theDTensorrequires_grad or not.\nto_local\nrequires_grad\nTheDeviceMeshattribute that associates with this DTensor object.\nDeviceMesh\nNote\ndevice_meshis a read-only property, it can not be set.\ndevice_mesh\nThe placements attribute of this DTensor that describes the layout of this\nDTensor on the its DeviceMesh.\nNote\nplacementsis a read-only property, it can not be set.\nplacements\n\n## DeviceMesh as the distributed communicator#\n\nDeviceMeshwas built from DTensor as the abstraction to describe cluster\u2019s device topology and represent\nmulti-dimensional communicators (on top ofProcessGroup). To see the details of how to create/use a DeviceMesh,\nplease refer to theDeviceMesh recipe.\nDeviceMesh\nProcessGroup\n\n## DTensor Placement Types#\n\nDTensor supports the following types ofPlacementon eachDeviceMeshdimension:\nPlacement\nDeviceMesh\nTheShard(dim)placement describes the DTensor sharding on tensor dimensiondimover a correspondingDeviceMeshdimension, where each rank on the\nDeviceMesh dimension only holds a shard/piece of the global Tensor. TheShard(dim)placement follows thetorch.chunk(dim)semantic, where the\nlast few shards on the DeviceMesh dimension might be empty when the tensor dimension\nis not evenly divisible on the DeviceMesh dimension. TheShardplacement can be\nused by all DTensor APIs (i.e. distribute_tensor, from_local, etc.)\nShard(dim)\ndim\nDeviceMesh\nShard(dim)\ntorch.chunk(dim)\nShard\ndim(int) \u2013 The tensor dimension that describes the DTensor is sharded over its\ncorresponding DeviceMesh dimension.\nWarning\nsharding on a tensor dimension where the tensor dimension size is not\nevenly divisible on a DeviceMesh dimension is currently experimental and subject to change.\nTheReplicate()placement describes the DTensor replicating on a correspondingDeviceMeshdimension, where each rank on the DeviceMesh dimension holds a\nreplica of the global Tensor. TheReplicateplacement can be used by all\nDTensor APIs (i.e.distribute_tensor,DTensor.from_local, etc.)\nReplicate()\nDeviceMesh\nReplicate\ndistribute_tensor\nDTensor.from_local\nThePartial(reduce_op)placement describes the DTensor that is pending\nreduction on a specifiedDeviceMeshdimension, where each rank on the\nDeviceMesh dimension holds the partial value of the global Tensor. User can\nredistribute thePartialDTensor to aReplicateorShard(dim)placement on the specifiedDeviceMeshdimension usingredistribute,\nwhich would trigger necessary communication operations under the hood (i.e.allreduce,reduce_scatter).\nPartial(reduce_op)\nDeviceMesh\nPartial\nReplicate\nShard(dim)\nDeviceMesh\nredistribute\nallreduce\nreduce_scatter\nreduce_op(str,optional) \u2013 The reduction op to be used for the partial DTensor\nto produce Replicated/Sharded DTensor. Only element-wise reduction operations\nare supported, including: \u201csum\u201d, \u201cavg\u201d, \u201cproduct\u201d, \u201cmax\u201d, \u201cmin\u201d, default: \u201csum\u201d.\nNote\nThePartialplacement can be generated as a result of the DTensor operators,\nand can only be used by theDTensor.from_localAPI.\nPartial\nDTensor.from_local\nThe base class for the Placement type, where it describes how a DTensor is placed onto theDeviceMesh.PlacementandDeviceMeshtogether could describe the DTensor Layout.\nIt is the base class of the three main DTensor Placement types:Shard,Replicate,\nandPartial.\nDeviceMesh\nPlacement\nDeviceMesh\nShard\nReplicate\nPartial\nThis class is not meant to be used directly, mainly served as a typing stub.\nbool\nbool\nbool\n\n## Different ways to create a DTensor#\n\nDTensor\ndistribute_tensor()creates aDTensorfrom a logical or \u201cglobal\u201dtorch.Tensoron\neach rank. This could be used to shard the leaftorch.Tensors (i.e. model parameters/buffers\nand inputs).\ndistribute_tensor()\nDTensor\ntorch.Tensor\ntorch.Tensor\nDTensor.from_local()creates aDTensorfrom a localtorch.Tensoron each rank, which can\nbe used to createDTensorfrom a non-leaftorch.Tensors (i.e. intermediate activation\ntensors during forward/backward).\nDTensor.from_local()\nDTensor\ntorch.Tensor\nDTensor\ntorch.Tensor\nDTensor provides dedicated tensor factory functions (e.g.empty(),ones(),randn(), etc.)\nto allow differentDTensorcreations by directly specifying theDeviceMeshandPlacement. Compare todistribute_tensor(), this could directly materializing the sharded memory\non device, instead of performing sharding after initializing the logical Tensor memory.\nempty()\nones()\nrandn()\nDTensor\nDeviceMesh\nPlacement\ndistribute_tensor()\n\n## Create DTensor from a logical torch.Tensor#\n\nThe SPMD (single program, multiple data) programming model intorch.distributedlaunches multiple processes\n(i.e. viatorchrun) to execute the same program, this means that the model inside the program would be\ninitialized on different processes first (i.e. the model might be initialized on CPU, or meta device, or directly\non GPU if enough memory).\ntorch.distributed\ntorchrun\nDTensoroffers adistribute_tensor()API that could shard the model weights or Tensors toDTensors,\nwhere it would create a DTensor from the \u201clogical\u201d Tensor on each process. This would empower the createdDTensors to comply with the single device semantic, which is critical fornumerical correctness.\nDTensor\ndistribute_tensor()\nDTensor\nDTensor\nDistribute a leaftorch.Tensor(i.e. nn.Parameter/buffers) to thedevice_meshaccording\nto theplacementsspecified. The rank ofdevice_meshandplacementsmust be the\nsame. Thetensorto distribute is the logical or \u201cglobal\u201d tensor, and the API would use\nthetensorfrom first rank of the DeviceMesh dimension as the source of truth to preserve\nthe single-device semantic. If you want to construct a DTensor in the middle of the Autograd\ncomputation, please useDTensor.from_local()instead.\ntorch.Tensor\ndevice_mesh\nplacements\ndevice_mesh\nplacements\ntensor\ntensor\nDTensor.from_local()\ntensor(torch.Tensor) \u2013 torch.Tensor to be distributed. Note that if you\nwant to shard a tensor on a dimension that is not evenly divisible by\nthe number of devices in that mesh dimension, we usetorch.chunksemantic to shard the tensor and scatter the shards. The uneven sharding\nbehavior is experimental and subject to change.\ntorch.chunk\ndevice_mesh(DeviceMesh, optional) \u2013 DeviceMesh to distribute the\ntensor, if not specified, must be called under a DeviceMesh context\nmanager, default: None\nDeviceMesh\nplacements(List[Placement], optional) \u2013 the placements that\ndescribes how to place the tensor on DeviceMesh, must have the same\nnumber of elements asdevice_mesh.ndim. If not specified, we will\nby default replicate the tensor across thedevice_meshfrom the\nfirst rank of each dimension of thedevice_mesh.\nPlacement\ndevice_mesh.ndim\ndevice_mesh\nsrc_data_rank(int,optional) \u2013 the rank of the source data for the logical/global tensor, it is\nused bydistribute_tensor()to scatter/broadcast the shards/replicas to other ranks.\nBy default, we usegroup_rank=0on each DeviceMesh dimension as the source data to preserve\nthe single-device semantic. If passingNoneexplicitly,distribute_tensor()simply uses\nits local data instead of trying to preserve the single-device semantic via scatter/broadcast.\nDefault: 0\ndistribute_tensor()\ngroup_rank=0\nNone\ndistribute_tensor()\nADTensororXLAShardedTensorobject.\nDTensor\nXLAShardedTensor\nDTensor\nNote\nWhen initialize the DeviceMesh with thexladevice_type,distribute_tensorreturnXLAShardedTensorinstead. seethis issuefor more details. The XLA integration is experimental and subject to change.\nxla\ndistribute_tensor\nAlong withdistribute_tensor(), DTensor also offers adistribute_module()API to allow easier\nsharding on thenn.Modulelevel\ndistribute_tensor()\ndistribute_module()\nnn.Module\nThis function expose three functions to control the parameters/inputs/outputs of the module:\n1. To perform sharding on the module before runtime execution by specifying thepartition_fn(i.e. allow user to convert Module parameters toDTensorparameters according to thepartition_fnspecified).\n2. To control the inputs or outputs of the module during runtime execution by\nspecifying theinput_fnandoutput_fn. (i.e. convert the input toDTensor, convert the output back totorch.Tensor)\npartition_fn\nDTensor\ninput_fn\noutput_fn\nDTensor\ntorch.Tensor\nmodule(nn.Module) \u2013 user module to be partitioned.\nnn.Module\ndevice_mesh(DeviceMesh) \u2013 the device mesh to place the module.\nDeviceMesh\npartition_fn(Callable) \u2013 the function to partition parameters (i.e. shard certain\nparameters across thedevice_mesh). Ifpartition_fnis not specified,\nby default we replicate all module parameters ofmoduleacross the mesh.\ndevice_mesh\npartition_fn\nmodule\ninput_fn(Callable) \u2013 specify the input distribution, i.e. could control how the\ninput of the module is sharded.input_fnwill be installed as a moduleforward_pre_hook(pre forward hook).\ninput_fn\nforward_pre_hook\noutput_fn(Callable) \u2013 specify the output distribution, i.e. could control how the\noutput is sharded, or convert it back to torch.Tensor.output_fnwill be\ninstalled as a moduleforward_hook(post forward hook).\noutput_fn\nforward_hook\nA module that contains parameters/buffers that are allDTensors.\nDTensor\nModule\n\nNote\nWhen initialize the DeviceMesh with thexladevice_type,distribute_modulereturn nn.Module with PyTorch/XLA SPMD annotated parameters. Seethis issuefor more details. The XLA integration is experimental and subject to change.\nxla\ndistribute_module\n\n## DTensor Factory Functions#\n\nDTensor also provides dedicated tensor factory functions to allow creatingDTensordirectly\nusing torch.Tensor like factory function APIs (i.e. torch.ones, torch.empty, etc), by additionally\nspecifying theDeviceMeshandPlacementfor theDTensorcreated:\nDTensor\nDeviceMesh\nPlacement\nDTensor\nReturns aDTensorfilled with the scalar value 0.\nDTensor\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: zeros(1,2,3..) or zeros([1,2,3..]) or zeros((1,2,3..))\nDTensor\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returnedDTensor.\nDefault:torch.strided.\ntorch.layout\nDTensor\ntorch.strided\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled with the scalar value 1, with the shape defined\nby the variable argumentsize.\nDTensor\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))\nDTensor\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returned DTensor.\nDefault:torch.strided.\ntorch.layout\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled with uninitialized data. The shape of theDTensoris defined by the variable argumentsize.\nDTensor\nDTensor\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: empty(1,2,3..) or empty([1,2,3..]) or empty((1,2,3..))\nDTensor\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).        layout (torch.layout, optional): the desired layout of returnedDTensor.\nDefault:torch.strided.\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\ntorch.layout\nDTensor\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled withfill_valueaccording todevice_meshandplacements, with the shape defined by the argumentsize.\nDTensor\nfill_value\ndevice_mesh\nplacements\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))\nDTensor\nfill_value(Scalar) \u2013 the value to fill the output tensor with.\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returned DTensor.\nDefault:torch.strided.\ntorch.layout\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks.\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled with random numbers from a uniform distribution\non the interval[0,1). The shape of the tensor is defined by the variable\nargumentsize.\nDTensor\n[0,1)\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))\nDTensor\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returned DTensor.\nDefault:torch.strided.\ntorch.layout\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks.\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled with random numbers from a normal distribution\nwith mean 0 and variance 1. The shape of the tensor is defined by the variable\nargumentsize.\nDTensor\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))\nDTensor\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returned DTensor.\nDefault:torch.strided.\ntorch.layout\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks.\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\n\n## Random Operations#\n\nDTensor provides distributed RNG functionality to ensure that random operations on sharded tensors get unique values, and random operations on replicated tensors get the same values. This system requires that all participating\nranks (e.g. SPMD ranks) start out using the same generator state before each dtensor random operation is performed,\nand if this is true, it ensures they all end up at the same state after each dtensor random operation completes. There is no communication performed during random operations to synchronize RNG states.\nOperators that accept ageneratorkwarg will utilize the user-passed generator, if passed, or the default generator for the device otherwise. Whichever generator is used, it will be advanced after the DTensor operation.  It is valid to use the same generator for both DTensor and non-DTensor operations, but care must be taken to ensure the non-DTensor operations advance the generator state equally on all ranks if so.\ngenerator\nWhen using DTensor together with Pipeline Parallelism, ranks for each pipeline stage should use a distinct seed, and ranks within a pipeline stage should use the same seed.\nDTensor\u2019s RNG infra is based on the philox based RNG algorithm, and supports any philox based backend (cuda, and other cuda-like devices), but unfortunately does not yet support the CPU backend.\n\n## Debugging#\n\n\n## Logging#\n\nWhen launching the program, you can turn on additional logging using theTORCH_LOGSenvironment variable fromtorch._logging:\nTORCH_LOGS\nTORCH_LOGS=+dtensorwill displaylogging.DEBUGmessages and all levels above it.\nTORCH_LOGS=+dtensor\nlogging.DEBUG\nTORCH_LOGS=dtensorwill displaylogging.INFOmessages and above.\nTORCH_LOGS=dtensor\nlogging.INFO\nTORCH_LOGS=-dtensorwill displaylogging.WARNINGmessages and above.\nTORCH_LOGS=-dtensor\nlogging.WARNING\n\n## Debugging Tools#\n\nTo debug the program that applied DTensor, and understand more details about what collectives happened under the\nhood, DTensor provides aCommDebugMode:\nCommDebugMode\nCommDebugModeis a context manager that counts the number of\nfunctional collectives within its context. It does this using aTorchDispatchMode.\nCommDebugMode\nTorchDispatchMode\nNote\nNot all collectives are supported yet.\nExample usage\n\n```python\nmod = ...\ncomm_mode = CommDebugMode()\nwith comm_mode:\n    mod.sum().backward()\nprint(comm_mode.get_comm_counts())\n\n```\n\nGenerates detailed table displaying operations and collective tracing information\non a module level. Amount of information is dependent on noise_level\nprints module-level collective counts\nprints dTensor operations not included in trivial operations, module information\nprints operations not included in trivial operations\nprints all operations\nCreates json file used to build browser visual\n0. prints module-level collective counts\n1. prints dTensor operations not included in trivial operations\n2. prints operations not included in trivial operations\n3. prints all operations\nReturns the communication counts as a dictionary.\nThe communication counts as a dictionary.\nDict[Any,int]\ndict[str,dict[str,Any]]\ndict[str,dict[str,Any]]\nint\nAlternative to console CommDebugMode output, writes to file specified by the user\nTo visualize the sharding of a DTensor that have less than 3 dimensions, DTensor providesvisualize_sharding():\nvisualize_sharding()\nVisualizes sharding in the terminal forDTensorthat are 1D or 2D.\nDTensor\nNote\nThis requires thetabulatepackage, orrichandmatplotlib.\nNo sharding info will be printed for empty tensors\ntabulate\nrich\nmatplotlib\n\n## Experimental Features#\n\nDTensoralso provides a set of experimental features. These features are either in prototyping stage, or the basic\nfunctionality is done and but looking for user feedbacks. Please submit a issue to PyTorch if you have feedbacks to\nthese features.\nDTensor\ncontext_parallelis an experimental API to enable context\nparallelism (CP). This API performs two actions: 1) patch the SDPA\n(torch.nn.functional.scaled_dot_product_attention) with the CP-enabled\none, 2) shardbuffersalong the sequence dimension and each rank will\npreserve the corresponding shard accordingmesh.\ncontext_parallel\ntorch.nn.functional.scaled_dot_product_attention\nbuffers\nmesh\nmesh(DeviceMesh) \u2013 the device mesh for the context parallelism.\nDeviceMesh\nbuffers(Optional[List[torch.Tensor]]) \u2013 buffers that the usage depend\non the sequence dimension. Examples are input batch, labels and\npositional embedding buffers. These buffers must be sharded along\nthe sequence dimension to ensure the accuracy. The sharding will\nhappen in-place, the buffer\u2019s shape will change within the context.\nThe buffers will be restored after the context finishes.no_restore_bufferscan be used to specify which buffers don\u2019t\nneed to be restored. Note thatbuffersshould not contain any\nnn.Parameter.\nno_restore_buffers\nbuffers\nbuffer_seq_dims(Optional[List[int]]) \u2013 the sequence dimensions ofbuffers.\nbuffers\nno_restore_buffers(Optional[Set[torch.Tensor]]) \u2013 buffers in these set\nwon\u2019t be restored after the context exits. This set must be a subset\nofbuffers. If the buffers won\u2019t be used after the context exits,\nthese buffers can be put in this list to avoid extra restore time.\nbuffers\nGenerator[None, None, None]\nWarning\ntorch.distributed.tensor.experimental.context_parallelis a\nprototype feature in PyTorch. The API is subject to change.\nlocal_map()is an experimental API that allows users to passDTensors\nto a function that is written to be applied ontorch.Tensors. It is done by extracting\nthe local components ofDTensor, call the function, and wrap the outputs toDTensoraccording to theout_placements.\nlocal_map()\nDTensor\ntorch.Tensor\nDTensor\nDTensor\nout_placements\nfunc(Callable) \u2013 the function to be applied on each local shard ofDTensors.\nDTensor\nout_placements(Union[PlacementType, Tuple[PlacementType, \u2026]]) \u2013 the desired placements of theDTensors infunc\u2019s flattened output.\nIf the flattenedoutputis a single value, theout_placementsshould be\nof typePlacementType. Otherwise if the flattenedoutputhas multiple\nvalues, theout_placementsshould be a tuple ofPlacementTypevalues 1:1\nmapping to the flattenedoutput.\nBesides, forTensoroutput, we usePlacementTypeas its\nplacements (aTuple[Placement]value). For non-Tensor output, thePlacementTypeshould beNone.\nNote that the only exception is when noDTensorargument is passed\nin. In this case, even ifout_placementsis notNone, the result function\nshould ignore the desired placements because the function is not running withDTensors.\nDTensor\nfunc\noutput\nout_placements\noutput\nout_placements\noutput\nTensor\nDTensor\nDTensor\nin_placements(Tuple[PlacementType, \u2026], optional) \u2013 the required placements of theDTensors in the flattened inputs offunc.\nIfin_placementsis specified,local_map()would examine whether the\nplacements of eachDTensorargument is the same as the required\nplacements or not. If the placements are not the same andredistribute_inputsisFalse, an exception will be raised. Otherwise ifredistribute_inputsisTrue, the argument will be first redistributed to\nthe required sharding placements before passing its local tensor tofunc.\nThe only exception is when required placements are notNoneand the\nargument is atorch.Tensor. In this case, the placements examination\nwill be skipped and the argument will be directly passed tofunc.\nIfin_placementsisNone, no placements examination will be performed.\nDefault: None\nDTensor\nfunc\nin_placements\nlocal_map()\nDTensor\nredistribute_inputs\nFalse\nredistribute_inputs\nTrue\nfunc\nNone\ntorch.Tensor\nfunc\nin_placements\nNone\nin_grad_placements(Tuple[PlacementType, \u2026], optional) \u2013 the placements hint of theDTensors gradient corresponds\nto the flattened input DTensor. This argument is the hint that user\ncan give toto_local()in case the gradient layout of the\nlocal tensor input does not match itsDTensorinput layout.\nIf not specified, we will assume the gradient layout of the local\ntensor input remains the same as the originalDTensorinput\nand use that for gradient computation. Default: None.\nDTensor\nto_local()\nDTensor\nDTensor\ndevice_mesh(DeviceMesh, optional) \u2013 the device mesh that the outputDTensors are placed on. If not\nspecified, this will be inferred from the first inputDTensor\u2019s device\nmesh. Default: None.\nDeviceMesh\nDTensor\nDTensor\nredistribute_inputs(bool,optional) \u2013 the bool value indicating whether to reshard the inputDTensors when\ntheir placements are different from the required input placements. If this\nvalue isFalseand someDTensorinput has a different placement,\nan exception will be raised. Default: False.\nDTensor\nFalse\nDTensor\nACallablethat appliesfuncto each local shard of the inputDTensorand returns aDTensorconstructed from the return value offunc.\nCallable\nfunc\nDTensor\nDTensor\nfunc\nAssertionError\u2013 For any non-DTensor output, we require its corresponding\n    output placement inout_placementsbe None. An AssertionError will be raised\n    if this is not the case.\nout_placements\nValueError\u2013 Ifredistribute_inputs=Falsebut the inputDTensorneeds\n    a redistribution according toin_placements.\nredistribute_inputs=False\nDTensor\nin_placements\nExample\n\n```python\n>>> def mm_allreduce_forward(device_mesh, W, X):\n>>>     partial_sum_tensor = torch.mm(W, X)\n>>>     reduced_tensor = funcol.all_reduce(partial_sum_tensor, \"sum\", device_mesh)\n>>>     return reduced_tensor\n>>>\n>>> W = torch.randn(12, 8, requires_grad=False)\n>>> X = torch.randn(8, 16, requires_grad=False)\n>>> Y = torch.mm(W, X)\n>>> row_wise = [Shard(0)]  # row-wise sharding placements on 1-d mesh\n>>> col_wise = [Shard(1)]  # col-wise sharding placements on 1-d mesh\n>>>\n>>> # local_mm_allreduce_forward is the function wrapped with DTensor/Tensor conversion\n>>> local_mm_allreduce_forward = local_map(\n>>>     mm_allreduce_forward,\n>>>     out_placements=[Replicate()],\n>>>     in_placements=[col_wise, row_wise],\n>>>     device_mesh=device_mesh,\n>>> )\n>>>\n>>> W_dt = distribute_tensor(\n...     W, device_mesh, (col_wise)\n... )  # col-wisely sharded W tensor\n>>> X_dt = distribute_tensor(\n...     X, device_mesh, (row_wise)\n... )  # row-wisely sharded X tensor\n>>> Y_dt = local_mm_allreduce_forward(\n...     device_mesh, W_dt, X_dt\n... )  # apply local_mm_allreduce_forward to DTensors\n\n```\n\nNote\nThis API is currently experimental and subject to change\nregister_sharding()is an experimental API that allows users to register sharding\nstrategies for an operator when the tensor inputs and outputs are DTensor.\nIt can be useful when: (1) there doesn\u2019t exist a default sharding strategy forop,\ne.g. whenopis a custom operator that is not supported byDTensor; (2)\nwhen users would like to overwrite default sharding strategies of existing operators.\nregister_sharding()\nop\nop\nDTensor\nop(Union[OpOverload,List[OpOverload]]) \u2013 An op or a list of ops to register the customized sharding function.\nA function decorator which can be used to wrap a function that defines the sharding\nstrategy for the operator specified inop. The defined sharding strategy will be\nregistered to DTensor and will override the default sharding strategy if DTensor has\nalready implemented the operator. The customized sharding function takes the same inputs\nas the original op (except that if an arg is atorch.Tensor, it will be\nreplaced by a tensor-like object that DTensor uses internally). The function should\nreturn a sequence of 2-tuples, each specifying acceptable output placements and its\ncorresponding input placements.\nop\ntorch.Tensor\nExample\n\n```python\n>>> @register_sharding(aten._softmax.default)\n>>> def custom_softmax_sharding(x, dim, half_to_float):\n>>>     softmax_dim = dim if dim >= 0 else dim + x.ndim\n>>>     acceptable_shardings = []\n>>>\n>>>     all_replicate = ([Replicate()], [Replicate(), None, None])\n>>>     acceptable_shardings.append(all_replicate)\n>>>\n>>>     for sharding_dim in range(x.ndim):\n>>>         if sharding_dim != softmax_dim:\n>>>             all_sharded = (\n>>>                 [Shard(sharding_dim)],\n>>>                 [Shard(sharding_dim), None, None],\n>>>             )\n>>>             acceptable_shardings.append(all_sharded)\n>>>\n>>>     return acceptable_shardings\n\n```\n\nNote\nThis API is currently experimental and subject to change",
  "url": "https://pytorch.org/docs/stable/distributed.tensor.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "0fabed75411f0f8d73798cf429948dc7",
  "source": "pytorch_docs",
  "title": "Dynamic Shapes \u2014 PyTorch 2.9 documentation",
  "text": "\n## Dynamic Shapes#\n\nCreated On: May 19, 2023 | Last Updated On: Jun 10, 2025\nCode:symbolic_shapes.py\nSee also:The dynamic shapes manual\n\n## Motivation#\n\nDeep learning compilers commonly only work for static shapes, that is to say, they produced compiled programs which only work for a single specific configuration of input shapes, and must recompile if any input shape changes. This assumption works great for the majority of commonly run deep learning models today, but there are a few situations where it is insufficient:\nSome dimensions, such as batch size or sequence length, may vary. For example, an inference service performing adaptive batching will execute inference requests with varying batch sizes depending on how many requests it received within its batching window. We may also want to consider padding out variable size sequences only to the maximum sequence length within a batch, which may vary from batch-to-batch.\nSome models exhibit data-dependent output shapes, that is to say, the size of their outputs and intermediates may depend on the actual input data which may vary across runs. For example, detection models may first generate a variable number of potential bounding boxes before running a more expensive image recognition model to identify if the subject is in a bounding box. The number of bounding boxes is data dependent.\nOne particularly important case of data-dependent shapes occurs when dealing with sparse representations, such as sparse tensors, jagged tensors, and graph neural networks. In all of these cases, the amount of data to be processed depends on the sparse structure of the problem, which will typically vary in a data-dependent way.\nIn supporting dynamic shapes, we chose not to support dynamic rank programs, e.g., programs whose inputs tensors change in dimensionality, as this pattern rarely occurs in real-world deep learning programs, and it avoids the need to reason inductively over symbolic lists of shapes.\n\n## Abridged public API#\n\nThe default dynamic behavior in PyTorch 2.1 is:\nPT2 assumes everything is static by default\nIf we recompile because a size changed, we will instead attempt to recompile\nthat size as being dynamic (sizes that have changed are likely to change in\nthe future). This generalization may fail (e.g., because user code does a\nconditional branch on the size in question or missing dynamic shapes support\nin PT2). If you are trying to understand why PT2 has overspecialized some\ncode, run withTORCH_LOGS=dynamicand look for \u201ceval\u201d entries that say\nwhen guards are added and why.\nTORCH_LOGS=dynamic\nIf you know ahead of time something will be dynamic, you can skip the first\nrecompile withtorch._dynamo.mark_dynamic(tensor,dim). If you know ahead of time\ntheminandmaxvalue this dimension can take, you can specifytorch._dynamo.mark_dynamic(tensor,dim,min=min,max=max)\ntorch._dynamo.mark_dynamic(tensor,dim)\nmin\nmax\ntorch._dynamo.mark_dynamic(tensor,dim,min=min,max=max)\nIf you saytorch.compile(dynamic=False), we will turn off automatic\ndynamic shapes on recompiles and always recompile for each distinct size.\nConversely, if you saytorch.compile(dynamic=True), we will try to make\neverything as dynamic as possible. This is mostly useful for small\noperators; if you try it on a big model it will (1) probably crash PT2 and (2) run slow for no good reason.\ntorch.compile(dynamic=False)\ntorch.compile(dynamic=True)\nYou can whitelist specific sources to be marked as dynamic using theTORCH_COMPILE_DYNAMIC_SOURCESenvironment variable or by settingtorch.compiler.config.dynamic_sources. This is particularly useful for large\nmodels with graph breaks, as you can maintain dynamism across graph breaks since\nsource names stay consistent. You can also use this to mark integers as dynamic.\nThe format is a comma-delimited list of source names, e.g.,\"L['x'],L['y']\".\nYou can also use regexes, e.g.,\"L\\['x.*'\\],L\\['y.*'\\]\").\nThis whitelist takes precedence over other flags likedynamic=False,force_nn_module_property_static_shapes, andforce_parameter_static_shapes.\nTORCH_COMPILE_DYNAMIC_SOURCES\ntorch.compiler.config.dynamic_sources\n\"L['x'],L['y']\"\n\"L\\['x.*'\\],L\\['y.*'\\]\")\ndynamic=False\nforce_nn_module_property_static_shapes\nforce_parameter_static_shapes\nSometimes it can be cumbersome to find the right inputs to mark as dynamic. If\nyou\u2019re willing to take a performance hit for the first batch, one other affordable\noption we have are the eager_then_compile stances which derive dynamism for you.\nSeetorch.compiler.set_stancefor more details.\n\n## The Guard Model#\n\nWhen considering how to add support for dynamic shapes to TorchDynamo and TorchInductor, we made a major design decision: in order to reuse decompositions and other preexisting code written in Python/C++ targeting the PyTorch API, we must be able to trace through dynamic shapes. Unlike a fully symbolic system which might capture both branches of a conditional, we always pick one branch and specialize our trace under the assumption that we only use this trace when we would have made the same choice for that branch in the future. To do this, we maintain a \u201chint\u201d for every symbolic size saying what its concrete value is at compile time (as TorchDynamo is a just-in-time compiler, it always knows what the actual input sizes are.) When we perform a condition on a tensor, we simply consult the hint to find out which branch to take.\nThis greatly simplifies the symbolic shape formulas we produce, but means we have a much more involved system for managing guards. Consider, for example, the following program:\n\n```python\ndef f(x, y):\n    z = torch.cat([x, y])\n    if z.size(0) > 2:\n        return z.mul(2)\n    else:\n        return z.add(2)\n\n```\n\nThe final IR we will compile with TorchInductor will either betorch.cat([x,y]).add(2)ortorch.cat([x,y]).mul(2)(with the condition flattened away), but to determine which branch we are in, we would need to know the size ofz, an intermediate. Because TorchDynamo must know upfront if a compiled trace is valid (we do not support bailouts, like some JIT compilers), we must be able to reducez.size(0)as an expression in terms of the inputs,x.size(0)+y.size(0). This is done by writing meta functions for all operators in PyTorch which can propagate size information to the output of a tensor without actually performing computation on the node.\ntorch.cat([x,y]).add(2)\ntorch.cat([x,y]).mul(2)\nz\nz.size(0)\nx.size(0)+y.size(0)\n\n## Overall architecture#\n\nSymbolic shapes workflow:\nWhen we start compiling a frame in Dynamo, we allocate a ShapeEnv (attached to FakeTensorMode) which keeps track of symbolic shapes state.\nWe allocate symbolic sizes for tensors on entry (what is static or dynamic is a policy decision, with some knobs).\nWe propagate the symbolic sizes through operators, maintaining both (1) FX IR so that we can faithfully export symbolic compute, and (2) Sympy expressions representing the size vars, so we can reason about them.\nWhen we condition on symbolic sizes, either in Dynamo tracing or in Inductor optimization, we add guards based on the conditional. These can be induced from both Python and C++.\nThese guards can induce further simplifications on symbolic variables. For example, if you asserts0==4, we can now replace all occurrences ofs0with4.\ns0==4\ns0\n4\nWhen we\u2019re done tracing and optimizing, we install all of these guards with the compiled code; the compiled code is only reusable if all the guards evaluate true.\nImportant files:\nC++ SymInt API:c10/core/SymInt.h,SymFloat.h,SymBool.h\nc10/core/SymInt.h\nSymFloat.h\nSymBool.h\nPython SymInt API:torch/__init__.py(look forSymInt/SymFloat/SymBool)\ntorch/__init__.py\nSymInt/SymFloat/SymBool\nC++ plumbing:c10/core/SymNodeImpl.h,torch/csrc/utils/python_symnode.h,torch/csrc/jit/python/init.cpp\nc10/core/SymNodeImpl.h\ntorch/csrc/utils/python_symnode.h\ntorch/csrc/jit/python/init.cpp\nPython infrastructure:torch/fx/experimental/symbolic_shapes.py\ntorch/fx/experimental/symbolic_shapes.py\nOther important files:torch/_subclasses/fake_tensor.py,torch/_meta_registrations.py, decomps, PrimTorch refs\ntorch/_subclasses/fake_tensor.py\ntorch/_meta_registrations.py\n\n## Abridged internal API#\n\nUnderstanding the Python class hierarchy:\nSymInt/SymFloat/SymBool: these are user-visible classes that simulate their int/float/bool counterparts. If you add two SymInts, we give you a new SymInt that symbolically tracks that the integer addition had occurred.\nSymNode: this is the internal structure (accessible via e.g.,symint.node) which holds the actual symbolic tracking info. SymNode is type erased; this makes it more convenient to represent mixed-type operations. Note that technically you don\u2019t have to call into Python SymNode from SymInt; for example, XLA\u2019s C++SymNodeImplwould take the place of SymNode.\nsymint.node\nSymNodeImpl\nShapeEnv: per-compile context state which keeps track of all the free symbols and guards we have accumulated so far. Every SymNode records its ShapeEnv (but not vice versa; SymNodes only get used if they participate in a guard).\nC++ is fairly similar:\nc10::SymInt/SymFloat/SymBool: user-visible classes that simulate int/float/bool.\nc10::SymNode/SymNodeImpl: analogous to SymNode\nThere is no ShapeEnv in C++; for ease of debugging, the entire symbolic reasoning apparatus is in Python.\nWhen you write code that is traceable withmake_fx, it must be able to deal with SymInt/SymFloat/SymBool flowing through it.The dynamic shapes manualgives some guidance for how to do this.\nmake_fx\n\n## DimDynamic policy#\n\nSymbolic reasoning:\nValue ranges\nSympy usage notes\nConstraints\nDimDynamic/Constraint\n\n## Unbacked SymInts#\n\nTo resolve control flow, we check the hint, aka actual value, of a symbolic integer to determine which branch to go. However, in some cases, we may not have a hint: so-called unbacked symbolic integers arise when a size variable emerges from a data-dependent operation like.nonzero()or.item(). It is illegal to perform control flow on these symbolic integers, so we must graph break on these operations.\n.nonzero()\n.item()\nNaively implemented, this is too restrictive: most PyTorch programs will immediately fail if you try to do anything with unbacked symbolic integers. Here are the most important enhancements to make this actually work:\nOn tensor creation, PyTorch precomputes a lot of data about a tensor; for example, if you useempty_stridedto create a tensor, we will eagerly sort the strides and determine if the tensor is non-overlapping and dense. Sorts produce a lot of guards. However, it is more common to produce a tensor directly with a higher-level API likeempty, which is guaranteed to produce a non-overlapping and dense tensor. We modified PyTorch to avoid needlessly recomputing these properties.\nempty_strided\nempty\nEven if nontrivial compute is needed, sometimes a property is never actually queried at all. Making these precomputed properties lazy allows us to avoid guarding on an unbacked symbolic integer unless it is actually needed.\nThe data in an integer tensor is generally not known to be non-negative. However, we provide an APIconstrain_rangewhereby a user can specify that a size is bounded above and below by known limits.\nconstrain_range\nSimilar to the dynamic APIs, there are corresponding unbacked APIs: namely you can use mark_unbacked instead ofmark_dynamicandTORCH_COMPILE_UNBACKED_SOURCESinstead ofTORCH_COMPILE_DYNAMIC_SOURCESto tell the compiler to mark an input as unbacked.\nmark_dynamic\nTORCH_COMPILE_UNBACKED_SOURCES\nTORCH_COMPILE_DYNAMIC_SOURCES\nIn future versions of PT2 (beyond PT2.1), we will extend our reasoning system\nto infer that an unbacked symbolic integer is size-like based on usage. For\nexample, if you pass the result of an.item()call to a factory function\nliketorch.empty, we will automatically infer that the result is a size\n(because if it was not, it would fail.) This assumption would get validated\nat runtime, raising an error if it was not fulfilled.\n.item()\ntorch.empty",
  "url": "https://pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
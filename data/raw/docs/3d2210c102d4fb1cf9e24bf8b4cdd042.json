{
  "doc_id": "3d2210c102d4fb1cf9e24bf8b4cdd042",
  "source": "pytorch_docs",
  "title": "Reporting Issues \u2014 PyTorch 2.9 documentation",
  "text": "\n## Reporting Issues#\n\nCreated On: Jul 29, 2025 | Last Updated On: Jul 29, 2025\nIf the provided workarounds were not enough to gettorch.compileworking,\nthen you should consider reporting the issue to PyTorch.\nBut there are a few things that you can do to make our lives significantly easier.\ntorch.compile\n\n## Ablation#\n\nCheck which component of thetorch.compilestack is the one causing the issue using thebackend=option fortorch.compile.\nIn particular, try:\ntorch.compile\nbackend=\ntorch.compile\ntorch.compile(fn,backend=\"eager\"), which only runs TorchDynamo, the graph capture component oftorch.compile.\ntorch.compile(fn,backend=\"eager\")\ntorch.compile\ntorch.compile(fn,backend=\"aot_eager\"), which runs TorchDynamo and AOTAutograd, which additionally generates the backward graph during compilation.\ntorch.compile(fn,backend=\"aot_eager\")\ntorch.compile(fn,backend=\"aot_eager_decomp_partition\"), which runs TorchDynamo and AOTAutograd with operator decompositions/partitions.\ntorch.compile(fn,backend=\"aot_eager_decomp_partition\")\ntorch.compile(fn,backend=\"inductor\"), which runs TorchDynamo, AOTAutograd, and TorchInductor, the backend ML compiler that generates compiled kernels.\ntorch.compile(fn,backend=\"inductor\")\nIf you only fail with the Inductor backend, you can additionally test various Inductor modes:\ntorch.compile(fn,backend=\"inductor\",mode=\"default\")\ntorch.compile(fn,backend=\"inductor\",mode=\"default\")\ntorch.compile(fn,backend=\"inductor\",mode=\"reduce-overhead\")\ntorch.compile(fn,backend=\"inductor\",mode=\"reduce-overhead\")\ntorch.compile(fn,backend=\"inductor\",mode=\"max-autotune\")\ntorch.compile(fn,backend=\"inductor\",mode=\"max-autotune\")\nYou can also check if dynamic shapes is causing issues with any backend:\ntorch.compile(fn,dynamic=True)(always use dynamic shapes)\ntorch.compile(fn,dynamic=True)\ntorch.compile(fn,dynamic=False)(never use dynamic shapes)\ntorch.compile(fn,dynamic=False)\ntorch.compile(fn,dynamic=None)(automatic dynamic shapes)\ntorch.compile(fn,dynamic=None)\n\n## Bisecting#\n\nDid you try on the latest nightly? Did something work in the past but now no longer works?\nCan you bisect to determine the first nightly where your issue occurs?\nBisecting is especially helpful for performance, accuracy, or compile time regressions,\nwhere it is not immediately obvious where the problem originates from.\n\n## Creating a reproducer#\n\nCreating reproducers is a lot of work, and it is perfectly fine if you do not have the time to do it.\nHowever, if you are a motivated user unfamiliar with the internals oftorch.compile,\ncreating a standalone reproducer can have a huge impact on our ability to fix the bug.\nWithout a reproducer, your bug report must contain enough information for us to identify the root cause of the problem and write a reproducer from scratch.\ntorch.compile\nHere\u2019s a list of useful reproducers, ranked from most to least preferred:\nSelf-contained, small reproducer:A script with no external dependencies, under 100 lines of code, that reproduces the problem when run.\nSelf-contained, large reproducer:Even if it\u2019s large, being self-contained is a huge advantage!\nNon-self-contained reproducer with manageable dependencies:For example, if you can reproduce the problem by running a script afterpipinstalltransformers,\nthat\u2019s manageable. We can likely run it and investigate.\npipinstalltransformers\nNon-self-contained reproducer requiring substantial setup:This might involve downloading datasets,\nmultiple environment setup steps, or specific system library versions requiring a Docker image.\nThe more complex the setup, the harder it is for us to recreate the environment.\nNote\nDocker simplifies setup but complicates changes to the environment, so it\u2019s not a perfect solution, though we\u2019ll use it if necessary.\nIf possible, try to make your reproducer single-process, as those are easier to debug than a multi-process reproducer.\nAdditionally, below is a non-exhaustive list of aspects to check in your\nissue that you can attempt to replicate in your reproducer:\nAutograd. Did you have tensor inputs withrequires_grad=True? Did you callbackward()on the output?\nrequires_grad=True\nbackward()\nDynamic shapes. Did you setdynamic=True? Or did you run the test code multiple times with varying shapes?\ndynamic=True\nCustom operators. Is there a custom operator involved in the real workflow?\nCan you replicate some of its important characteristics using the Python custom operator API?\nConfiguration. Did you set all the same configuration?\nThis includestorch._dynamo.configandtorch._inductor.configsettings,\nas well as arguments totorch.compilelikebackend/mode.\ntorch._dynamo.config\ntorch._inductor.config\ntorch.compile\nbackend\nmode\nContext managers. Did you replicate any active context managers?\nThis could betorch.no_grad, automatic mixed precision,TorchFunctionMode/TorchDispatchMode,\nactivation checkpointing, compiled autograd etc.\ntorch.no_grad\nTorchFunctionMode\nTorchDispatchMode\nTensor subclasses. Is there a tensor subclass involved?",
  "url": "https://pytorch.org/docs/stable/compile/programming_model.reporting_issues.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
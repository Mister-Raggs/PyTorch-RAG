{
  "doc_id": "56c92329acaf36c44a4ad86f6cb7190b",
  "source": "pytorch_docs",
  "title": "torch.export Programming Model \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.export Programming Model#\n\nCreated On: Dec 18, 2024 | Last Updated On: Jul 16, 2025\nThis document aims to explain the behaviors and capabilities oftorch.export.export(). It is intended to help build your intuition\nfor howtorch.export.export()handles code.\ntorch.export.export()\ntorch.export.export()\n\n## Basics of Tracing#\n\ntorch.export.export()captures a graph representing your model by\ntracing its execution on \u201cexample\u201d inputs and recording the PyTorch operations\nand conditions observed along the traced path. This graph can then be run\non different inputs as long as they satisfy the same conditions.\ntorch.export.export()\nThe basic output oftorch.export.export()is a single graph of PyTorch\noperations, with associated metadata. The exact format of this output is\ncovered in theexport IR spec.\ntorch.export.export()\n\n## Strict vs. Non-Strict Tracing#\n\ntorch.export.export()provides two modes of tracing.\ntorch.export.export()\nInnon-strict mode, we trace through the program using the normal Python\ninterpreter. Your code executes exactly as it would in eager mode; the only\ndifference is that all Tensors are replaced byfake Tensors,which have shapes and other forms of metadata but no data, wrapped inProxy objectsthat record all\noperations on them into a graph. We also captureconditions on Tensor shapesthat guard the correctness of the generated code.\nInstrict mode, we first trace through the program usingTorchDynamo, a Python bytecode\nanalysis engine. TorchDynamo does not actually execute your Python code.\nInstead, it symbolically analyzes it and builds a graph based on the results.\nOn the one hand, this analysis allowstorch.export.export()to provide\nadditional guarantees on Python-level safety (beyond capturing conditions on\nTensor shapes, as in non-strict mode). On the other hand, not all Python\nfeatures are supported by this analysis.\ntorch.export.export()\nAlthough currently the default mode of tracing is strict,we strongly\nrecommend using non-strict, which will soon become the default.\nFor most models, conditions on Tensor shapes are enough for soundness, and\nthe additional guarantees on Python-level safety have no impact; at the same\ntime, the possibility of hitting unsupported Python features in TorchDynamo\npresents an unnecessary risk.\nIn the rest of this document we assume we are tracing innon-strict mode;\nin particular, we assume thatall Python features are supported.\n\n## Values: Static vs. Dynamic#\n\nA key concept in understanding the behavior oftorch.export.export()is\nthe difference betweenstaticanddynamicvalues.\ntorch.export.export()\n\n## Static Values#\n\nAstaticvalue is a value that isfixed at export time and cannot change\nbetween executions of the exported program. When the value is encountered\nduring tracing, we treat it as a constant and hard-code it into the graph.\nWhen an operation is performed (e.g.x+y) and all inputs are static,\nthe output of the operation is directly hard-coded into the graph and the\noperation does not show up (i.e. it gets \u201cconstant-folded\u201d).\nx+y\nWhen a value has been hard-coded into the graph, we say that the graph has\nbeenspecializedto that value. For example:\n\n```python\nimport torch\n\nclass MyMod(torch.nn.Module):\n    def forward(self, x, y):\n        z = y + 7\n        return x + z\n\nm = torch.export.export(MyMod(), (torch.randn(1), 3))\nprint(m.graph_module.code)\n\n\"\"\"\ndef forward(self, arg0_1, arg1_1):\n    add = torch.ops.aten.add.Tensor(arg0_1, 10);  arg0_1 = None\n    return (add,)\n\n\"\"\"\n\n```\n\nHere, we provide3as the traced value fory; it is treated as a static\nvalue and added to7, burning in the static value10in the graph.\n3\ny\n7\n10\n\n## Dynamic Values#\n\nAdynamicvalue is one thatcan change from run to run. It behaves just\nlike a \u201cnormal\u201d function argument: you can pass different inputs and expect\nyour function to do the right thing.\n\n## Which values are static vs. dynamic?#\n\nWhether a value is static or dynamic depends on its type:\nFor Tensor:\nTensordatais treated as dynamic.\nTensorshapescan be treated by the system as static or dynamic.\nBy default, shapes of all input Tensors are considered static.\nThe user can override this behavior for any input Tensor by specifying\nadynamic shapefor it.\nTensors that are part of module state, i.e., parameters and buffers,\nalways have static shapes.\nOther forms of Tensormetadata(e.g.device,dtype) are static.\ndevice\ndtype\nPythonprimitives(int,float,bool,str,None) are static.\nint\nfloat\nbool\nstr\nNone\nThere are dynamic variants for some primitive types (SymInt,SymFloat,SymBool). Typically users do not have to deal with them.\nSymInt\nSymFloat\nSymBool\nUsers can specify integer inputs as dynamic by specifying\nadynamic shapefor it.\nFor Pythonstandard containers(list,tuple,dict,namedtuple):\nlist\ntuple\ndict\nnamedtuple\nThe structure (i.e., length forlistandtuplevalues, and key\nsequence fordictandnamedtuplevalues) is static.\nlist\ntuple\ndict\nnamedtuple\nThe contained elements have these rules applied to them recursively\n(basically thePyTreescheme)\nwith leaves that are either Tensor or primitive types.\nOtherclasses(including data classes) can be registered with PyTree\n(see below), and follow the same rules as the standard containers.\n\n## Input types#\n\nInputs will be treated as either static or dynamic, based on their type\n(as explained above).\nA static input will get hard-coded into the graph, and passing a different\nvalue at run time will result in an error. Recall that these are mostly\nvalues of primitive types.\nA dynamic input behaves like a \u201cnormal\u201d function input. Recall that these\nare mostly values of Tensor types.\nBy default, the types of inputs you can use for your program are:\nTensor\nPython primitives (int,float,bool,str,None)\nint\nfloat\nbool\nstr\nNone\nPython standard containers (list,tuple,dict,namedtuple)\nlist\ntuple\ndict\nnamedtuple\n\n## Custom Input Types (PyTree)#\n\nIn addition, you can also define your own (custom) class and use it as an\ninput type, but you will need to register such a class as a PyTree.\nHere\u2019s an example of using an utility to register a dataclass that is used as\nan input type.\n\n```python\n@dataclass\nclass Input:\n    f: torch.Tensor\n    p: torch.Tensor\n\nimport torch.utils._pytree as pytree\npytree.register_dataclass(Input)\n\nclass M(torch.nn.Module):\n    def forward(self, x: Input):\n        return x.f + 1\n\ntorch.export.export(M(), (Input(f=torch.ones(10, 4), p=torch.zeros(10, 4)),))\n\n```\n\n\n## Optional input types#\n\nFor optional inputs to the program that are not passed in,torch.export.export()will specialize to their default values. As a\nresult, the exported program will require users to explicitly pass in all\narguments, and will lose the defaulting behavior. For example:\ntorch.export.export()\n\n```python\nclass M(torch.nn.Module):\n    def forward(self, x, y=None):\n        if y is not None:\n            return y * x\n        return x + x\n\n# Optional input is passed in\nep = torch.export.export(M(), (torch.randn(3, 3), torch.randn(3, 3)))\nprint(ep)\n\"\"\"\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 3]\", y: \"f32[3, 3]\"):\n            # File: /data/users/angelayi/pytorch/moo.py:15 in forward, code: return y * x\n            mul: \"f32[3, 3]\" = torch.ops.aten.mul.Tensor(y, x);  y = x = None\n            return (mul,)\n\"\"\"\n\n# Optional input is not passed in\nep = torch.export.export(M(), (torch.randn(3, 3),))\nprint(ep)\n\"\"\"\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 3]\", y):\n            # File: /data/users/angelayi/pytorch/moo.py:16 in forward, code: return x + x\n            add: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\"\"\"\n\n```\n\n\n## Control Flow: Static vs. Dynamic#\n\nControl flow is supported bytorch.export.export(). The behavior of\ncontrol flow depends on whether the value you are branching on is static or\ndynamic.\ntorch.export.export()\n\n## Static Control Flow#\n\nPython control flow over static values is supported transparently. (Recall\nthat static values include static shapes, so control flow over static shapes\nis also covered by this case.)\nAs mentioned above, we \u201cburn in\u201d static values, so the exported graph will\nnever see any control flow over static values.\nIn the case of anifstatement, we will continue tracing the branch taken\nat export time. In the case of afororwhilestatement, we will continue\ntracing by unrolling the loop.\nif\nfor\nwhile\n\n## Dynamic Control Flow: Shape-Dependent vs. Data-Dependent#\n\nWhen the value involved in a control flow is dynamic, it could depend on\ndynamic shapes or dynamic data. Given that the compiler traces with\ninformation on shapes rather than data, the implications on the programming\nmodel are different in these cases.\nWhen the value involved in a control flow is adynamic shape,\nin most caseswe will also know the concrete value of the dynamic shape\nduring tracing: see the following section for more details on how the\ncompiler tracks this information.\nIn these cases we say that the control flow is shape-dependent.We use the\nconcrete value of the dynamic shape to evaluate the conditionto eitherTrueorFalseand continue tracing (as discussed above), additionally\nemitting a guard corresponding to the condition just evaluated.\nTrue\nFalse\nOtherwise the control flow is considered data-dependent. We cannot evaluate\nthe condition to eitherTrueorFalse, so cannot continue tracing and have to\nraise an error at export time. See next section.\nTrue\nFalse\nData-dependent control flow over dynamic values is supported, but you must\nuse one of PyTorch\u2019s explicit operatorsto continue tracing. Using Python\ncontrol flow statements over dynamic values is not permitted, because the\ncompiler cannot evaluate the conditions necessary to continue tracing and\nthus an error must be raised at export time.\nWe provideoperators to express general conditionals and loops over dynamic\nvalues, e.g.,torch.cond,torch.map. Note that you only need to use these\nif you truly wantdata-dependent control flow.\ntorch.cond\ntorch.map\nHere\u2019s an example of anifstatement on a data-dependent condition,x.sum()>0, wherexis an input Tensor, rewritten usingtorch.cond.\nInstead of having to decide which branch to trace, now both branches are\ntraced.\nif\nx.sum()>0\nx\ntorch.cond\n\n```python\nclass M_old(torch.nn.Module):\n    def forward(self, x):\n        if x.sum() > 0:\n            return x.sin()\n        else:\n            return x.cos()\n\nclass M_new(torch.nn.Module):\n    def forward(self, x):\n        return torch.cond(\n            pred=x.sum() > 0,\n            true_fn=lambda x: x.sin(),\n            false_fn=lambda x: x.cos(),\n            operands=(x,),\n        )\n\n```\n\nA special case of data-dependent control flow is where it involves adata-dependent dynamic shape:\ntypically, the shape of some intermediate Tensor that depends on input data\nrather than on input shapes (thus not shape-dependent). Instead of using a\ncontrol flow operator, in this case you can provide an assertion that decides\nwhether the condition isTrueorFalse. Given such an assertion, we can\ncontinue tracing, emitting a guard as above.\nTrue\nFalse\nWe provideoperators to express assertions on dynamic shapes, e.g.,torch._check. Note that you only need to use this when there is control\nflow on data-dependent dynamic shapes.\ntorch._check\nHere\u2019s an example of anifstatement on a condition involving a\ndata-dependent dynamic shape,nz.shape[0]>0, wherenzis the result of\ncallingtorch.nonzero(), an operator whose output shape depends on input\ndata. Instead of rewriting it, you can add an assertion usingtorch._checkto effectively decide which branch to trace.\nif\nnz.shape[0]>0\nnz\ntorch.nonzero()\ntorch._check\n\n```python\nclass M_old(torch.nn.Module):\n    def forward(self, x):\n        nz = x.nonzero()\n        if nz.shape[0] > 0:\n            return x.sin()\n        else:\n            return x.cos()\n\nclass M_new(torch.nn.Module):\n    def forward(self, x):\n        nz = x.nonzero()\n        torch._check(nz.shape[0] > 0)\n        if nz.shape[0] > 0:\n            return x.sin()\n        else:\n            return x.cos()\n\n```\n\n\n## Basics of Symbolic Shapes#\n\nDuring tracing, dynamic Tensor shapes and conditions over them are encoded as\n\u201csymbolic expressions.\u201d (In contrast, static Tensor shapes and conditions\nover them are simplyintandboolvalues.)\nint\nbool\nAsymbolis like a variable; it describes a dynamic Tensor shape.\nAs tracing proceeds, shapes of intermediate Tensors may be described by more\ngeneral expressions, typically involving integer arithmetic operators. This\nis becausefor most PyTorch operators, shapes of output Tensors can be\ndescribed as functions of shapes of input Tensors. For example, the shape of\nthe output oftorch.cat()is the sum of the shapes of its inputs.\ntorch.cat()\nMoreover, as we encounter control flow in the program, we create boolean\nexpressions, typically involving relational operators, describing conditions\nalong the traced path. Theseexpressions are evaluated to decide which path\nto trace through the program, and recorded in ashape environmentto guard the correctness of the traced path and to evaluate subsequently\ncreated expressions.\nWe briefly introduce these subsystems next.\n\n## Fake Implementations of PyTorch Operators#\n\nRecall that during tracing, we are executing the program withfake Tensors,\nwhich have no data. In general we cannot call the actual implementations of\nPyTorch operators with fake Tensors. Thus each operator needs to have an\nadditional fake (a.k.a. \u201cmeta\u201d) implementation, which inputs and outputs fake\nTensors, that matches the behavior of the actual implementation in terms of\nshapes and other forms of metadata carried by fake Tensors.\nFor example, note how the fake implementation oftorch.index_select()computes the shape of the output using the shape of the input (while ignoring\ninput data and returning empty output data).\ntorch.index_select()\n\n```python\ndef meta_index_select(self, dim, index):\n    result_size = list(self.size())\n    if self.dim() > 0:\n        result_size[dim] = index.numel()\n    return self.new_empty(result_size)\n\n```\n\nShapes are propagated using fake implementations of PyTorch operators.\nA key concept to understand the propagation of dynamic shapes in particular\nis the difference betweenbackedandunbackeddynamic shapes: we know the\nconcrete values of the former but not the latter.\nPropagation of shapes, including tracking backed and unbacked dynamic shapes,\nproceeds as follows:\nThe shapes of Tensors representing inputs can be static or dynamic. When\ndynamic, they are described by symbols; moreover,such symbols are backed\nsince we also know their concrete values given the \u201creal\u201d example inputs\nprovided by the user at export time.\nThe output shape of an operator is computed by its fake implementation, and\nis either static or dynamic. When dynamic, in general it is described by a\nsymbolic expression. Moreover:\nIf the output shape depends only on input shapes, it is either static or\nbacked dynamic whenever the input shapes are all static or backed dynamic.\nOn the other hand,if the output shape depends on input data, it is\nnecessarily dynamic, and moreover,because we cannot know its concrete\nvalue it is unbacked.\n\n## Control Flow: Guards and Assertions#\n\nWhen a condition on shapes is encountered, it either involves only static\nshapes, in which case it is abool, or it involves dynamic shapes, in which\ncase it is a symbolic boolean expression. For the latter:\nbool\nWhen the condition involves only backed dynamic shapes, we can use the\nconcrete values of those dynamic shapes to evaluate the condition toTrueorFalse. We can then add a guard to the shape environment that states\nthat the corresponding symbolic boolean expression isTrueorFalse,\nand continue tracing.\nTrue\nFalse\nTrue\nFalse\nOtherwise the condition involves unbacked dynamic shapes. In general we\ncannot evaluate such a condition without additional information; thus we\ncannot continue tracing, and we must raise an error at export time. The\nuser is expected to use an explicit PyTorch operator for tracing to\ncontinue. This information is added as a guard in the shape environment,\nand can also possibly help evaluate other subsequently encountered\nconditions toTrueorFalse.\nTrue\nFalse\nOnce the model is exported,any guards on backed dynamic shapes can be\nunderstood as conditions on input dynamic shapes. These are verified against\na dynamic shape specification that must have been provided to export,\ndescribing conditions on dynamic shapes that not only example inputs but also\nall future inputs are expected to satisfy for the generated code to be\ncorrect. More precisely, the dynamic shape specification must logically imply\nthe generated guards, otherwise an error is raised at export time (along with\nsuggested fixes to the dynamic shape specification). On the other hand, when\nthere are no generated guards on backed dynamic shapes (in particular, when\nall shapes are static) no dynamic shape specification needs to be provided to\nexport. In general, the dynamic shape specification is converted to runtime\nassertions on the inputs of the generated code.\nFinally,any guards on unbacked dynamic shapes are converted to \u201cinline\u201d\nruntime assertions. These are added in the generated code at the locations\nwhere those unbacked dynamic shapes were created: typically, right after\ndata-dependent operator calls.\n\n## Allowed PyTorch operators#\n\nAll PyTorch operators are permitted.\n\n## Custom operators#\n\nIn addition, you can define and usecustom operators.\nDefining a custom operator includes defining a fake implementation for it,\njust like any other PyTorch operator (see previous section).\nHere\u2019s an example of a customsinoperator that wraps NumPy, and its\nregistered (trivial) fake implementation.\nsin\n\n```python\n@torch.library.custom_op(\"mylib::sin\", mutates_args=())\ndef sin(x: Tensor) -> Tensor:\n    x_np = x.numpy()\n    y_np = np.sin(x_np)\n    return torch.from_numpy(y_np)\n\n@torch.library.register_fake(\"mylib::sin\")\ndef _(x: Tensor) -> Tensor:\n    return torch.empty_like(x)\n\n```\n\nSometimes your custom operator\u2019s fake implementation will involve\ndata-dependent shapes. Here\u2019s how a fake implementation for a customnonzeromight look like.\nnonzero\n\n```python\n...\n\n@torch.library.register_fake(\"mylib::custom_nonzero\")\ndef _(x):\n    nnz = torch.library.get_ctx().new_dynamic_size()\n    shape = [nnz, x.dim()]\n    return x.new_empty(shape, dtype=torch.int64)\n\n```\n\n\n## Module State: Reads vs. Updates#\n\nModule states include parameters, buffers, and regular attributes.\nA regular attribute can be of any type.\nOn the other hand, parameters and buffers are always Tensors.\nModule states can be dynamic or static, based on their types as outlined\nabove. For example,self.trainingis abool, which means it is static; on\nthe other hand, any parameter or buffer is dynamic.\nself.training\nbool\nTheshapesof any Tensors contained in module states cannot be dynamic, i.e.,\nthose shapes are fixed at export time, and cannot change between executions\nof the exported program.\n\n## Access rules#\n\nAll module states must be initialized. Accessing a module state that is\nnot already initialized causes an error to be raised at export time.\nReading module states is always permitted.\nUpdating module states is possible, but must follow the rules below:\nA static regular attribute(e.g., of primitive type)can be updated.\nReads and updates can be freely interleaved, and as expected, any reads\nwill always see the values of the latest updates. Because these attributes\nare static, we will also burn the values in, so the generated code will not\nhave any instructions to actually \u201cget\u201d or \u201cset\u201d such attributes.\nA dynamic regular attribute(e.g., of Tensor type)cannot be updated.\nTo do so, it must be registered as a buffer during module initialization.\nA buffer can be updated, where the updating can be in-place (e.g.,self.buffer[:]=...) or not (e.g.,self.buffer=...).\nself.buffer[:]=...\nself.buffer=...\nA parameter cannot be updated. Typically parameters are updated only\nduring training, not during inference. We recommend exporting withtorch.no_grad()to avoid parameter updates at export time.\ntorch.no_grad()\n\n## Effects of functionalization#\n\nAny dynamic module state that is read and/or updated is \u201clifted\u201d\n(respectively) as an input and/or output of the generated code.\nThe exported program stores, along with the generated code, the initial\nvalues of parameters and buffers and the constant values of other Tensor\nattributes.",
  "url": "https://pytorch.org/docs/stable/export/programming_model.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
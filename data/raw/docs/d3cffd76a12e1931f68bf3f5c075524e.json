{
  "doc_id": "d3cffd76a12e1931f68bf3f5c075524e",
  "source": "pytorch_docs",
  "title": "torch.nn.init \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.nn.init#\n\nCreated On: Jun 11, 2019 | Last Updated On: Jul 07, 2022\nWarning\nAll the functions in this module are intended to be used to initialize neural network\nparameters, so they all run intorch.no_grad()mode and will not be taken into\naccount by autograd.\ntorch.no_grad()\nReturn the recommended gain value for the given nonlinearity function.\nThe values are as follows:\nnonlinearity\ngain\nLinear / Identity\n111\nConv{1,2,3}D\n111\nSigmoid\n111\nTanh\n53\\frac{5}{3}35\u200b\nReLU\n2\\sqrt{2}2\u200b\nLeaky Relu\n21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b\nSELU\n34\\frac{3}{4}43\u200b\nWarning\nIn order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalization\neffect for more stable gradient flow in rectangular layers.\nnonlinearity='linear'\nnonlinearity='selu'\n1/N\nSELU\nnonlinearity(Literal['linear','conv1d','conv2d','conv3d','conv_transpose1d','conv_transpose2d','conv_transpose3d','sigmoid','tanh','relu','leaky_relu','selu']) \u2013 the non-linear function (nn.functionalname)\nparam(Optional[Union[int,float]]) \u2013 optional parameter for the non-linear function\nfloat\nExamples\n\n```python\n>>> gain = nn.init.calculate_gain(\n...     \"leaky_relu\", 0.2\n... )  # leaky_relu with negative_slope=0.2\n\n```\n\nFill the input Tensor with values drawn from the uniform distribution.\nU(a,b)\\mathcal{U}(a, b)U(a,b).\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\na(float) \u2013 the lower bound of the uniform distribution\nb(float) \u2013 the upper bound of the uniform distribution\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.uniform_(w)\n\n```\n\nFill the input Tensor with values drawn from the normal distribution.\nN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2).\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nmean(float) \u2013 the mean of the normal distribution\nstd(float) \u2013 the standard deviation of the normal distribution\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.normal_(w)\n\n```\n\nFill the input Tensor with the valueval\\text{val}val.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nval(float) \u2013 the value to fill the tensor with\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.constant_(w, 0.3)\n\n```\n\nFill the input Tensor with the scalar value1.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.ones_(w)\n\n```\n\nFill the input Tensor with the scalar value0.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)\n\n```\n\nFill the 2-dimensional inputTensorwith the identity matrix.\nPreserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible.\ntensor(Tensor) \u2013 a 2-dimensionaltorch.Tensor\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.eye_(w)\n\n```\n\nFill the {3, 4, 5}-dimensional inputTensorwith the Dirac delta function.\nPreserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity\ntensor(Tensor) \u2013 a {3, 4, 5}-dimensionaltorch.Tensor\ngroups(int,optional) \u2013 number of groups in the conv layer (default: 1)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 16, 5, 5)\n>>> nn.init.dirac_(w)\n>>> w = torch.empty(3, 24, 5, 5)\n>>> nn.init.dirac_(w, 3)\n\n```\n\nFill the inputTensorwith values using a Xavier uniform distribution.\nThe method is described inUnderstanding the difficulty of training\ndeep feedforward neural networks- Glorot, X. & Bengio, Y. (2010).\nThe resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where\nAlso known as Glorot initialization.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\ngain(float) \u2013 an optional scaling factor\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(\"relu\"))\n\n```\n\nFill the inputTensorwith values using a Xavier normal distribution.\nThe method is described inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010). The resulting tensor\nwill have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where\nAlso known as Glorot initialization.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\ngain(float) \u2013 an optional scaling factor\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.xavier_normal_(w)\n\n```\n\nFill the inputTensorwith values using a Kaiming uniform distribution.\nThe method is described inDelving deep into rectifiers: Surpassing\nhuman-level performance on ImageNet classification- He, K. et al. (2015).\nThe resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where\nAlso known as He initialization.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\na(float) \u2013 the negative slope of the rectifier used after this layer (only\nused with'leaky_relu')\n'leaky_relu'\nmode(Literal['fan_in','fan_out']) \u2013 either'fan_in'(default) or'fan_out'. Choosing'fan_in'preserves the magnitude of the variance of the weights in the\nforward pass. Choosing'fan_out'preserves the magnitudes in the\nbackwards pass.\n'fan_in'\n'fan_out'\n'fan_in'\n'fan_out'\nnonlinearity(Literal['linear','conv1d','conv2d','conv3d','conv_transpose1d','conv_transpose2d','conv_transpose3d','sigmoid','tanh','relu','leaky_relu','selu']) \u2013 the non-linear function (nn.functionalname),\nrecommended to use only with'relu'or'leaky_relu'(default).\n'relu'\n'leaky_relu'\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode=\"fan_in\", nonlinearity=\"relu\")\n\n```\n\nNote\nBe aware thatfan_inandfan_outare calculated assuming\nthat the weight matrix is used in a transposed manner,\n(i.e.,x@w.TinLinearlayers, wherew.shape=[fan_out,fan_in]).\nThis is important for correct initialization.\nIf you plan to usex@w, wherew.shape=[fan_in,fan_out],\npass in a transposed weight matrix, i.e.nn.init.kaiming_uniform_(w.T,...).\nfan_in\nfan_out\nx@w.T\nLinear\nw.shape=[fan_out,fan_in]\nx@w\nw.shape=[fan_in,fan_out]\nnn.init.kaiming_uniform_(w.T,...)\nFill the inputTensorwith values using a Kaiming normal distribution.\nThe method is described inDelving deep into rectifiers: Surpassing\nhuman-level performance on ImageNet classification- He, K. et al. (2015).\nThe resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where\nAlso known as He initialization.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\na(float) \u2013 the negative slope of the rectifier used after this layer (only\nused with'leaky_relu')\n'leaky_relu'\nmode(Literal['fan_in','fan_out']) \u2013 either'fan_in'(default) or'fan_out'. Choosing'fan_in'preserves the magnitude of the variance of the weights in the\nforward pass. Choosing'fan_out'preserves the magnitudes in the\nbackwards pass.\n'fan_in'\n'fan_out'\n'fan_in'\n'fan_out'\nnonlinearity(Literal['linear','conv1d','conv2d','conv3d','conv_transpose1d','conv_transpose2d','conv_transpose3d','sigmoid','tanh','relu','leaky_relu','selu']) \u2013 the non-linear function (nn.functionalname),\nrecommended to use only with'relu'or'leaky_relu'(default).\n'relu'\n'leaky_relu'\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode=\"fan_out\", nonlinearity=\"relu\")\n\n```\n\nNote\nBe aware thatfan_inandfan_outare calculated assuming\nthat the weight matrix is used in a transposed manner,\n(i.e.,x@w.TinLinearlayers, wherew.shape=[fan_out,fan_in]).\nThis is important for correct initialization.\nIf you plan to usex@w, wherew.shape=[fan_in,fan_out],\npass in a transposed weight matrix, i.e.nn.init.kaiming_normal_(w.T,...).\nfan_in\nfan_out\nx@w.T\nLinear\nw.shape=[fan_out,fan_in]\nx@w\nw.shape=[fan_in,fan_out]\nnn.init.kaiming_normal_(w.T,...)\nFill the input Tensor with values drawn from a truncated normal distribution.\nThe values are effectively drawn from the\nnormal distributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2)with values outside[a,b][a, b][a,b]redrawn until they are within\nthe bounds. The method used for generating the random values works\nbest whena\u2264mean\u2264ba \\leq \\text{mean} \\leq ba\u2264mean\u2264b.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nmean(float) \u2013 the mean of the normal distribution\nstd(float) \u2013 the standard deviation of the normal distribution\na(float) \u2013 the minimum cutoff value\nb(float) \u2013 the maximum cutoff value\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.trunc_normal_(w)\n\n```\n\nFill the inputTensorwith a (semi) orthogonal matrix.\nDescribed inExact solutions to the nonlinear dynamics of learning in deep\nlinear neural networks- Saxe, A. et al. (2013). The input tensor must have\nat least 2 dimensions, and for tensors with more than 2 dimensions the\ntrailing dimensions are flattened.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor, wheren\u22652n \\geq 2n\u22652\ngain(float) \u2013 optional scaling factor\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.orthogonal_(w)\n\n```\n\nFill the 2D inputTensoras a sparse matrix.\nThe non-zero elements will be drawn from the normal distributionN(0,0.01)\\mathcal{N}(0, 0.01)N(0,0.01), as described inDeep learning via\nHessian-free optimization- Martens, J. (2010).\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nsparsity(float) \u2013 The fraction of elements in each column to be set to zero\nstd(float) \u2013 the standard deviation of the normal distribution used to generate\nthe non-zero values\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.sparse_(w, sparsity=0.1)\n\n```\n",
  "url": "https://pytorch.org/docs/stable/nn.init.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
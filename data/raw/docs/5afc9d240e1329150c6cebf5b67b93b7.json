{
  "doc_id": "5afc9d240e1329150c6cebf5b67b93b7",
  "source": "pytorch_docs",
  "title": "Patching Batch Norm \u2014 PyTorch 2.9 documentation",
  "text": "\n## Patching Batch Norm#\n\nCreated On: Jan 03, 2023 | Last Updated On: Jun 11, 2025\n\n## What\u2019s happening?#\n\nBatch Norm requires in-place updates to running_mean and running_var of the same size as the input.\nFunctorch does not support inplace update to a regular tensor that takes in a batched tensor (i.e.regular.add_(batched)is not allowed). So when vmapping over a batch of inputs to a single module,\nwe end up with this error\nregular.add_(batched)\n\n## How to fix#\n\nOne of the best supported ways is to switch BatchNorm for GroupNorm. Options 1 and 2 support this\nAll of these options assume that you don\u2019t need running stats. If you\u2019re using a module this means\nthat it\u2019s assumed you won\u2019t use batch norm in evaluation mode. If you have a use case that involves\nrunning batch norm with vmap in evaluation mode, please file an issue\n\n## Option 1: Change the BatchNorm#\n\nIf you want to change for GroupNorm, anywhere that you have BatchNorm, replace it with:\n\n```python\nBatchNorm2d(C, G, track_running_stats=False)\n\n```\n\nHereCis the sameCas in the original BatchNorm.Gis the number of groups to\nbreakCinto. As such,C%G==0and as a fallback, you can setC==G, meaning\neach channel will be treated separately.\nC\nC\nG\nC\nC%G==0\nC==G\nIf you must use BatchNorm and you\u2019ve built the module yourself, you can change the module to\nnot use running stats. In other words, anywhere that there\u2019s a BatchNorm module, set thetrack_running_statsflag to be False\ntrack_running_stats\n\n```python\nBatchNorm2d(64, track_running_stats=False)\n\n```\n\n\n## Option 2: torchvision parameter#\n\nSome torchvision models, like resnet and regnet, can take in anorm_layerparameter. These are\noften defaulted to be BatchNorm2d if they\u2019ve been defaulted.\nnorm_layer\nInstead you can set it to be GroupNorm.\n\n```python\nimport torchvision\nfrom functools import partial\ntorchvision.models.resnet18(norm_layer=lambda c: GroupNorm(num_groups=g, c))\n\n```\n\nHere, once again,c%g==0so as a fallback, setg=c.\nc%g==0\ng=c\nIf you are attached to BatchNorm, be sure to use a version that doesn\u2019t use running stats\n\n```python\nimport torchvision\nfrom functools import partial\ntorchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))\n\n```\n\n\n## Option 3: functorch\u2019s patching#\n\nfunctorch has added some functionality to allow for quick, in-place patching of the module to not\nuse running stats. Changing the norm layer is more fragile, so we have not offered that. If you\nhave a net where you want the BatchNorm to not use running stats, you can runreplace_all_batch_norm_modules_to update the module in-place to not use running stats\nreplace_all_batch_norm_modules_\n\n```python\nfrom torch.func import replace_all_batch_norm_modules_\nreplace_all_batch_norm_modules_(net)\n\n```\n\n\n## Option 4: eval mode#\n\nWhen run under eval mode, the running_mean and running_var will not be updated. Therefore, vmap can support this mode\n\n```python\nmodel.eval()\nvmap(model)(x)\nmodel.train()\n\n```\n",
  "url": "https://pytorch.org/docs/stable/func.batch_norm.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
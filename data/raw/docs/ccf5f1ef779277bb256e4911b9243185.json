{
  "doc_id": "ccf5f1ef779277bb256e4911b9243185",
  "source": "pytorch_docs",
  "title": "python.closure \u2014 PyTorch 2.9 documentation",
  "text": "\n## python.closure#\n\n\n## cond_closed_over_variable#\n\nNote\nTags:python.closure,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondClosedOverVariable(torch.nn.Module):\n    \"\"\"\n    torch.cond() supports branches closed over arbitrary variables.\n    \"\"\"\n\n    def forward(self, pred, x):\n        def true_fn(val):\n            return x * 2\n\n        def false_fn(val):\n            return x - 2\n\n        return cond(pred, true_fn, false_fn, [x + 1])\n\nexample_args = (torch.tensor(True), torch.randn(3, 2))\ntags = {\"torch.cond\", \"python.closure\"}\nmodel = CondClosedOverVariable()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pred: \"b8[]\", x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1);  add = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, (x,));  pred = true_graph_0 = false_graph_0 = x = None\n            getitem: \"f32[3, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, 2);  x = None\n                return (mul,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(x, 2);  x = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    pred: USER_INPUT\n    x: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## nested_function#\n\nNote\nTags:python.closure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass NestedFunction(torch.nn.Module):\n    \"\"\"\n    Nested functions are traced through. Side effects on global captures\n    are not supported though.\n    \"\"\"\n\n    def forward(self, a, b):\n        x = a + b\n        z = a - b\n\n        def closure(y):\n            nonlocal x\n            x += 1\n            return x * y + z\n\n        return closure(x)\n\nexample_args = (torch.randn(3, 2), torch.randn(2))\ntags = {\"python.closure\"}\nmodel = NestedFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, a: \"f32[3, 2]\", b: \"f32[2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(a, b)\n\n                 sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(a, b);  a = b = None\n\n                 add_: \"f32[3, 2]\" = torch.ops.aten.add_.Tensor(add, 1);  add = None\n\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(add_, add_);  add_ = None\n            add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(mul, sub);  mul = sub = None\n            return (add_1,)\n\nGraph signature:\n    # inputs\n    a: USER_INPUT\n    b: USER_INPUT\n\n    # outputs\n    add_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
  "url": "https://pytorch.org/docs/stable/generated/exportdb/python.closure.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
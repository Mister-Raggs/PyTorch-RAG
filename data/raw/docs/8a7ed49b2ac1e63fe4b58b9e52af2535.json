{
  "doc_id": "8a7ed49b2ac1e63fe4b58b9e52af2535",
  "source": "pytorch_docs",
  "title": "Writing Graph Transformations on ATen IR \u2014 PyTorch 2.9 documentation",
  "text": "\n## Writing Graph Transformations on ATen IR#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\n\n## Passes#\n\nSince the ATen IR sits at the FX Graph/GraphModule level, any\ntransformations written for FX Graphs can be easily applied onto the\nATen IR. If you\u2019re familiar with writing FX graph transformations, then\nthis will be the same.\nThe most direct way of writing transformations is by looping through the\ngiven graph and directly manipulating the nodes within the graph.\nFor example, let\u2019s say we want to replacetorch.ops.aten.add.Tensor()calls withtorch.ops.aten.mul.Tensor()calls:\ntorch.ops.aten.add.Tensor()\ntorch.ops.aten.mul.Tensor()\n\n```python\nimport torch\n\ndef replace_add_with_mul(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n    for node in gm.graph.nodes:\n        if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor:\n            node.target = torch.ops.aten.mul.Tensor\n\n```\n\nWe can also delete and append new nodes through FX utility functions\nthat can be found in theGraphdocumentation. For example, if we want to insert atorch.ops.aten.relu.default()after theaddcall:\ntorch.ops.aten.relu.default()\nadd\n\n```python\nimport torch\n\ndef insert_relu_after_add(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n    for node in gm.graph.nodes:\n        if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor:\n\n            # Specifies the insertion point. Any nodes added to the graph within\n            # this scope will be inserted after `node`\n            with gm.graph.inserting_after(node):\n                # Insert a new `call_function` node with op `torch.ops.aten.relu.default`\n                new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, args=(node,))\n                # Replace all the places that use `node` to now use the `new_relu_node`\n                node.replace_all_uses_with(new_relu_node)\n\n```\n\nIn general, transformations can be roughly categorized into a couple of\naxis:\nAxis A: 1. Creating one-to-X mapping (eg. decomposition) 2. Creating\nmany-to-one mapping (eg. fusion)\nAxis B: 1. Doing forwards iteration (eg. shape propagation) 2. Doing\nbackwards iteration (eg. dead code elimination)\nAxis C: 1. Dependent on local node information (eg. out-variant\nconversion) 2. Dependent on global graph information (eg. memory\nplanning)\nOur projection on the frequency of these use cases are: 1. A.1, B.1, C.1\n2. A.2 3. B.2, C.2\nAlthough we can make all graph transformations through directly\nmanipulating the graph, we also provide some helper utilities for some\nease of use for the level 1 and 2 use-cases.\n\n## Transformer#\n\nFor level 1 uses cases (creating one-to-X mappings, doing forwards\niterations, and looking at local node information), we can utilize theTransformerclass to execute each node and recreate a graph, except with the\ntransformations specified.\nAn example for one-to-one mappings, if we wanted to replace an op A with\nanother op B, we can run the GraphModule, and very time we see op A,\nreturn op B.\nAn example is:\n\n```python\nclass ReplaceAddWithMul(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        if target != torch.ops.aten.add.Tensor:\n            return super().call_function(target, args, kwargs)\n        return super().call_function(torch.ops.aten.mul.Tensor, args, kwargs)\n\ntransformed_graph_module = ReplaceAddWithMul(graph_module).transform()\n\n```\n\nThesuper().call_function(target,args,kwargs,meta)call creates acall_functionFX node, and returns the result of running the\noperator with the given arguments.\nsuper().call_function(target,args,kwargs,meta)\ncall_function\nIf we wanted to do one-to-X mappings, like replacing op A with 2 other\nops B and C, we would then make 2 calls tosuper().call_functionto\ncreate 2 FX nodes, one with op B and another with op C, and return the\nresult of running op C.\nsuper().call_function\nFor example:\n\n```python\nclass ReplaceAddWithMulSub(torch.fx.Transformer):\n    \"\"\"\n    Original:\n        def f(x, y):\n            return x + y\n\n    After pass:\n        def f(x, y):\n            z = x * y\n            return z - y\n    \"\"\"\n    def call_function(self, target, args, kwargs):\n        if target != torch.ops.aten.add.Tensor:\n            return super().call_function(target, args, kwargs)\n\n        x, y = args\n\n        mul_res = super().call_function(torch.ops.aten.mul.Tensor, args, {})\n        return super().call_function(torch.ops.aten.sub.Tensor, (mul_res, y), {})\n\ntransformed_graph_module = ReplaceAddWithMulSub(graph_module).transform()\n\n```\n\nIf we wanted to remove an op, we can just return the value passed into\nthe function:\n\n```python\nclass RemoveDetachPass(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        if target not in (\n            torch.ops.aten.detach.default,\n            torch.ops.aten.detach_copy.default,\n        ):\n            return super().call_function(target, args, kwargs, meta)\n\n        assert len(args) == 1\n        return args[0]\n\ntransformed_graph_module = RemoveDetachPass(graph_module).transform()\n\n```\n\nAn example of utilizing local node information is, if we wanted to\nconvert all the scalars within the graph to tensors, we can run the\ngivenfx.GraphModule, and for every argument that contains a scalar,\nwe convert it to a tensor. It might look something like:\nfx.GraphModule\n\n```python\ndef args_map(target, fn, args, kwargs):\n    assert isinstance(args, tuple)\n    assert isinstance(kwargs, dict)\n    args = list(args)\n    kwargs = kwargs.copy()\n\n    # Update the argument based on the function passed\n    def update(key, args, schema):\n        args[key] = fn(args[key], schema)\n\n    # Update each argument in the schema\n    for i, schema in enumerate(target._schema.arguments):\n        if schema.name in kwargs:\n            update(schema.name, kwargs, schema)\n        elif not schema.kwarg_only and i < len(args):\n            update(i, args, schema)\n    return tuple(args), kwargs\n\nclass ScalarToTensorPass(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        breakpoint()\n        def try_coerce(value, arg):\n            return (\n                torch.tensor(value)\n                if isinstance(value, (float, int, bool))\n                and type(arg.type) == torch.TensorType\n                else value\n            )\n\n        args, kwargs = args_map(target, try_coerce, args, kwargs)\n        return super().call_function(target, args, kwargs)\n\ntransformed_graph_module = ScalarToTensorPass(graph_module).transform()\n\n```\n\n\n## Subgraph Rewriter#\n\nFor creating many-to-one mappings, we can utilize FX\u2019ssubgraph\nrewriter.\nGiven apattern, it creates a subgraph of operators matching to the\npattern, and then replaces each matched subgraph with thereplacement.\npattern\nreplacement\nNote:\n\n```python\nThis is an inplace operation.\n\n```\n\nThepatternandreplacementinputs must be callable functions or\nGraphModules containing the same operators that are used within the\ngraph (ATen ops) so that the subgraph rewriter can find the correct\npattern in the graph. Inputs to the pattern/replacement callables will\nbe treated as wildcards when matching.\npattern\nreplacement\nAn example:\n\n```python\nfrom torch.fx import subgraph_rewriter\n\ndef replace_patterns(graph_module):\n    def pattern(x, y):\n        x = torch.ops.aten.add.Tensor(x, y)\n        x = torch.ops.aten.mul.Tensor(x, y)\n        return x\n\n    def replacement(x, y):\n        return torch.ops.aten.sub.Tensor(x, y)\n\nreplaced_patterns = subgraph_rewriter.replace_pattern_with_filters(\n    traced_module, pattern, replacement\n)\n\n```\n\nThe subgraph rewriter returns a list ofReplacedPatterns:\nReplacedPatterns\n\n```python\n@dataclass\nclass ReplacedPatterns:\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n    # List of nodes that were added into the graph\n    replacements: List[Node]\n\n```\n\nNote:\n\n```python\nThe nodes created by the subgraph rewriter will not have the metadata that\nis populated in the matched nodes, but you can use\n`ReplacedPatterns.nodes_map` to find the nodes in the original graph that\nwere matched, and `ReplacedPatterns.replacements` to find the nodes that\nwere replaced in the transformed graph.\n\n```\n\n\n## Pass Manager#\n\nThePassManageris a class used to run multiple passes on a given graph module. When\ninitializing aPassManagerinstance, we pass in a list of passes\nthat we want to run and set a couple of flags. To run the collection of\npasses on a graph module, we can pass the graph module directly to thePassManagerinstance.\nPassManager\nPassManager\nAn example:\n\n```python\nfrom torch.fx.passes.infra.pass_manager import PassManager\n\npm = PassManager(\n    passes=[replace_add_with_div, replace_div_with_mul],\n    run_checks_after_each_pass=True,\n    suppress_check_failures=False,\n)\ngraph_module_out = pm(graph_module)\n\n```\n\nTo add a common set of checks that are run after each pass, we can call\nthe functionset_checks(check:Callable)which takes in a callable\nfunction as input. If therun_checks_after_each_passflag is set,\nthecheckwill be called after each pass is run on the graph module.\nset_checks(check:Callable)\nrun_checks_after_each_pass\ncheck\nAn example:\n\n```python\npm = PassManager(passes=[replace_add_with_div, replace_div_with_mul])\n\ndef check_div_target(graph_module):\n    for node in graph_module.graph.nodes:\n        if node.op == \"call_function\" and node.target != torch.div:\n            raise ValueError(\"Target should be div!\")\n\npm.add_checks(check_div_target)\n\npm(graph_module)    # raises ValueError after replace_div_with_mul pass\n\n```\n\n\n## Partitioner#\n\nThere are a couple of common FX graph based partitioners we can use to\npartition the graph.\n\n## Subgraph Matcher#\n\nFor finding subgraphs within a graph that match a specific pattern, we\ncan utilize FX\u2019sSubgraphMatcher.\nSubgraphMatcher\nClass Attributes:\npattern(Graph): The targeted matching pattern. Placeholder nodes\nin the graph will be treated as wildcards when matching.\npattern(Graph)\nmatch_output(bool): If True, output node in the pattern graph\nwill be treated as a part of the targeted pattern. If False, output\nnode is ignored during match.\nmatch_output(bool)\nmatch_placeholder(bool): If True, placeholder node in the\npattern graph will be treated as a part of the targeted pattern. If\nFalse, placeholder nodes will be used a wildcard.\nmatch_placeholder(bool)\nremove_overlapping_matches(bool): If True, in the case of\noverlapping matches, only the first match will be returned.\nremove_overlapping_matches(bool)\nignore_literals(bool): If True, will not check if literals are\nequal and will instead treat them as wildcards.\nignore_literals(bool)\nAn example:\n\n```python\nfrom torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n\nclass LargeModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._weight = torch.nn.Parameter(torch.ones(3, 3))\n        self._bias = torch.nn.Parameter(torch.ones(3, 3))\n\n    def forward(self, x):\n        return torch.ops.aten.addmm.default(self._bias, x, self._weight)\n\nlarge_model_graph = torch.export(LargeModel(), inputs).graph\n\nclass PatternModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._weight_1 = torch.nn.Parameter(torch.ones(5, 5))\n        self._bias_1 = torch.nn.Parameter(torch.ones(5, 5))\n\n    def forward(self, x):\n        return torch.ops.aten.addmm.default(self._bias_1, x, self._weight_1)\n\npattern_graph = torch.export(PatternModel(), inputs).graph\n\nsubgraph_matcher = SubgraphMatcher(pattern_graph)\nmatch_result = subgraph_matcher.match(large_model_graph)\n\n```\n\nThematchfunction returns a list ofInternalMatch:\nmatch\nInternalMatch\n\n```python\n@dataclass\nclass InternalMatch():\n    # Nodes from which the match was found\n    anchors: List[Node]\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node] = field(default_factory=dict)\n    # Nodes in target graph that are matched placeholder in pattern\n    placeholder_nodes: List[Node] = field(default_factory=list)\n    # Nodes in matched subgraph returned by output\n    returning_nodes: List[Node] = field(default_factory=list)\n\n```\n\n\n## Capability Based Partitioner#\n\nTo find the largest subgraphs of nodes that support a specific\ninvariant, we can utilize FX\u2019sCapabilityBasedPartitioner.\nCapabilityBasedPartitioner\nClass Attributes\ngraph_module(torch.fx.GraphModule): The graph module we are\npartitioning on.\ngraph_module(torch.fx.GraphModule)\noperator_support(OperatorSupportBase): The object used to\ndetermine if a node in the graph is supported in the partition.\noperator_support(OperatorSupportBase)\nallows_single_node_partition(bool): If True, allows single node\npartitions to be formed.\nallows_single_node_partition(bool)\nnon_compute_ops(Optional[Sequence[str]]): A set of ops that are\nconsidered to be \u201cnon-compute\u201d (extorch.ops.aten.viewand_operator.getitem, so that the partitioner will not create graphs\nthat only contain these non-compute ops\nnon_compute_ops(Optional[Sequence[str]])\ntorch.ops.aten.view\n_operator.getitem\nallowed_single_node_partition_ops(Optional[Sequence[str]]): A\nset of ops that are allowed to be in a single node partition.\nallowed_single_node_partition_ops(Optional[Sequence[str]])\nTheOperatorSupportBaseclass is used by the partitioner to determine if a specific node in the\ngraph belongs in the partition. This is done by overriding theis_node_supportedfunction. You can chain multipleOperatorSupportBaseby usingchain(which\nreturns False if any of the OperatorSupportBase return False) andany_chain(which returns True if any of the OperatorSupportBase returns True).\nOperatorSupportBase\nis_node_supported\nOperatorSupportBase\nchain\nany_chain\nAn example:\n\n```python\nfrom torch.fx.passes.infra.partitioner import CapabilityBasedPartitioner\nfrom torch.fx.passes.operator_support import any_chain, OperatorSupportBase\n\nclass AddMulOperatorSupport(OperatorSupportBase):\n    def is_node_supported(self, submodules, node: torch.fx.Node) -> bool:\n        return node.op == \"call_function\" and node.target in [\n            torch.ops.aten.add.Tensor, torch.ops.aten.mul.Tensor,\n        ]\n\ncapability_partitioner = CapabilityBasedPartitioner(\n    graph_module,\n    op_support,\n)\n\n# Returns a list of partitions (list of nodes that belong in each partition)\npartition_list = capability_partitioner.propose_partitions()\n# Fuses the partitions into graph modules and inserts `call_module` nodes in the graph\nfused_graph_module = capability_partitioner.fuse_partitions(partition_list)\n\n```\n",
  "url": "https://pytorch.org/docs/stable/torch.compiler_transformations.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
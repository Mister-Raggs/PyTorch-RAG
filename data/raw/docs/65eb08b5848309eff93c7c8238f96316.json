{
  "doc_id": "65eb08b5848309eff93c7c8238f96316",
  "source": "pytorch_docs",
  "title": "AOTInductor Minifier \u2014 PyTorch 2.9 documentation",
  "text": "\n## AOTInductor Minifier#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jun 13, 2025\nIf you encounter an error while using AOT Inductor APIs such astorch._inductor.aoti_compile_and_package,torch._indcutor.aoti_load_package,\nor running the loaded model ofaoti_load_packageon some inputs, you can use the AOTInductor Minifier\nto create a minimal nn.Module that reproduce the error by settingfromtorch._inductorimportconfig;config.aot_inductor.dump_aoti_minifier=True.\ntorch._inductor.aoti_compile_and_package\ntorch._indcutor.aoti_load_package\naoti_load_package\nfromtorch._inductorimportconfig;config.aot_inductor.dump_aoti_minifier=True\nOne a high-level, there are two steps in using the minifier:\nSetfromtorch._inductorimportconfig;config.aot_inductor.dump_aoti_minifier=Trueor set the environment variableDUMP_AOTI_MINIFIER=1. Then running the script that errors would produce aminifier_launcher.pyscript. The output directory is configurable by settingtorch._dynamo.config.debug_dir_rootto a valid directory name.\nfromtorch._inductorimportconfig;config.aot_inductor.dump_aoti_minifier=True\nDUMP_AOTI_MINIFIER=1\nminifier_launcher.py\ntorch._dynamo.config.debug_dir_root\nRun theminifier_launcher.pyscript. If the minifier runs successfully, it generates runnable python code inrepro.pywhich reproduces the exact error.\nminifier_launcher.py\nrepro.py\n\n## Example Code#\n\nHere is sample code which will generate an error because we injected an error on relu withtorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY=\"compile_error\".\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY=\"compile_error\"\n\n```python\nimport torch\nfrom torch._inductor import config as inductor_config\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 16)\n        self.relu = torch.nn.ReLU()\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.sigmoid(x)\n        return x\n\n\ninductor_config.aot_inductor.dump_aoti_minifier = True\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = \"compile_error\"\n\nwith torch.no_grad():\n    model = Model().to(\"cuda\")\n    example_inputs = (torch.randn(8, 10).to(\"cuda\"),)\n    ep = torch.export.export(model, example_inputs)\n    package_path = torch._inductor.aoti_compile_and_package(ep)\n    compiled_model = torch._inductor.aoti_load_package(package_path)\n    result = compiled_model(*example_inputs)\n\n```\n\nThe code above generates the following error:\n\n```python\nRuntimeError: Failed to import /tmp/torchinductor_shangdiy/fr/cfrlf4smkwe4lub4i4cahkrb3qiczhf7hliqqwpewbw3aplj5g3s.py\nSyntaxError: invalid syntax (cfrlf4smkwe4lub4i4cahkrb3qiczhf7hliqqwpewbw3aplj5g3s.py, line 29)\n\n```\n\nThis is because we injected an error on relu, and so the generated triton kernel looks like below. Note that we havecompileerror!instead ifrelu, so we get aSyntaxError.\ncompileerror!\nrelu\nSyntaxError\n\n```python\n@triton.jit\ndef triton_poi_fused_addmm_relu_sigmoid_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 16\n    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = compile error!\n    tmp4 = tl.sigmoid(tmp3)\n    tl.store(in_out_ptr0 + (x2), tmp4, xmask)\n\n```\n\nSince we havetorch._inductor.config.aot_inductor.dump_aoti_minifier=True, we also see an additional line indicating whereminifier_launcher.pyhas\nbeen written to. The output directory is configurable by settingtorch._dynamo.config.debug_dir_rootto a valid directory name.\ntorch._inductor.config.aot_inductor.dump_aoti_minifier=True\nminifier_launcher.py\ntorch._dynamo.config.debug_dir_root\n\n```python\nW1031 16:21:08.612000 2861654 pytorch/torch/_dynamo/debug_utils.py:279] Writing minified repro to:\nW1031 16:21:08.612000 2861654 pytorch/torch/_dynamo/debug_utils.py:279] /data/users/shangdiy/pytorch/torch_compile_debug/run_2024_10_31_16_21_08_602433-pid_2861654/minifier/minifier_launcher.py\n\n```\n\n\n## Minifier Launcher#\n\nTheminifier_launcher.pyfile has the following code. Theexported_programcontains the inputs totorch._inductor.aoti_compile_and_package.\nThecommand='minify'parameter means the script will run the minifier to create a minimal graph module that reproduce the error. Alternatively, you set\nusecommand='run'to just compile, load, and run the loaded model (without running the minifier).\nminifier_launcher.py\nexported_program\ntorch._inductor.aoti_compile_and_package\ncommand='minify'\ncommand='run'\n\n```python\nimport torch\nimport torch._inductor.inductor_prims\n\nimport torch._dynamo.config\nimport torch._inductor.config\nimport torch._functorch.config\nimport torch.fx.experimental._config\n\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error'\ntorch._inductor.config.aot_inductor.dump_aoti_minifier = True\n\n\n\n\nisolate_fails_code_str = None\n\n\n\n# torch version: 2.6.0a0+gitcd9c6e9\n# torch cuda version: 12.0\n# torch git version: cd9c6e9408dd79175712223895eed36dbdc84f84\n\n\n# CUDA Info:\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2023 NVIDIA Corporation\n# Built on Fri_Jan__6_16:45:21_PST_2023\n# Cuda compilation tools, release 12.0, V12.0.140\n# Build cuda_12.0.r12.0/compiler.32267302_0\n\n# GPU Hardware Info:\n# NVIDIA PG509-210 : 8\n\nexported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints/exported_program.pt2')\n# print(exported_program.graph)\nconfig_patches={}\nif __name__ == '__main__':\n    from torch._dynamo.repro.aoti import run_repro\n    with torch.no_grad():\n        run_repro(exported_program, config_patches=config_patches, accuracy=False, command='minify', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints', check_str=None)\n\n```\n\nSuppose we kept thecommand='minify'option, and run the script, we would get the following output:\ncommand='minify'\n\n```python\n...\nW1031 16:48:08.938000 3598491 torch/_dynamo/repro/aoti.py:89] Writing checkpoint with 3 nodes to /data/users/shangdiy/pytorch/torch_compile_debug/run_2024_10_31_16_48_02_720863-pid_3598491/minifier/checkpoints/3.py\nW1031 16:48:08.975000 3598491 torch/_dynamo/repro/aoti.py:101] Copying repro file for convenience to /data/users/shangdiy/pytorch/repro.py\nWrote minimal repro out to repro.py\n\n```\n\nIf you get anAOTIMinifierErrorwhen runningminifier_launcher.py, please report a bughere.\nAOTIMinifierError\nminifier_launcher.py\n\n## Minified Result#\n\nTherepro.pylooks like this. Notice that the exported program is printed at the top of the file, and it contains only the relu node. The minifier successfully reduced the graph to the op that raises the error.\nrepro.py\n\n```python\n# from torch.nn import *\n# class Repro(torch.nn.Module):\n#     def __init__(self) -> None:\n#         super().__init__()\n\n\n\n#     def forward(self, linear):\n#         relu = torch.ops.aten.relu.default(linear);  linear = None\n#         return (relu,)\n\nimport torch\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nimport torch._inductor.inductor_prims\n\nimport torch._dynamo.config\nimport torch._inductor.config\nimport torch._functorch.config\nimport torch.fx.experimental._config\n\ntorch._inductor.config.generate_intermediate_hooks = True\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error'\ntorch._inductor.config.aot_inductor.dump_aoti_minifier = True\n\n\n\n\nisolate_fails_code_str = None\n\n\n\n# torch version: 2.6.0a0+gitcd9c6e9\n# torch cuda version: 12.0\n# torch git version: cd9c6e9408dd79175712223895eed36dbdc84f84\n\n\n# CUDA Info:\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2023 NVIDIA Corporation\n# Built on Fri_Jan__6_16:45:21_PST_2023\n# Cuda compilation tools, release 12.0, V12.0.140\n# Build cuda_12.0.r12.0/compiler.32267302_0\n\n# GPU Hardware Info:\n# NVIDIA PG509-210 : 8\n\n\nexported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints/exported_program.pt2')\n# print(exported_program.graph)\nconfig_patches={'aot_inductor.package': True}\nif __name__ == '__main__':\n    from torch._dynamo.repro.aoti import run_repro\n    with torch.no_grad():\n        run_repro(exported_program, config_patches=config_patches, accuracy=False, command='run', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints', check_str=None)\n\n```\n",
  "url": "https://pytorch.org/docs/stable/torch.compiler_aot_inductor_minifier.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
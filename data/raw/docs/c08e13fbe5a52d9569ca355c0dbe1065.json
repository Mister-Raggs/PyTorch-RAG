{
  "doc_id": "c08e13fbe5a52d9569ca355c0dbe1065",
  "source": "pytorch_docs",
  "title": "Draft Export \u2014 PyTorch 2.9 documentation",
  "text": "\n## Draft Export#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jul 16, 2025\nWarning\nThis feature is not meant to be used in production and is designed to be\nused as a tool for debugging torch.export tracing errors.\nDraft-export is a new version of export, which is designed to consistently\nproduce a graph, even if there are potential soundness issues, and to generate a\nreport listing out all of the issues export encountered during\ntracing and providing additional debugging information. For custom operators that\ndon\u2019t have fake kernels, it will also generate a profile which you can register\nto automatically generate a fake kernel.\nHave you ever tried to export a model usingtorch.export.export(), only to\nencounter a data-dependent issue? You fix it, but then run into a missing fake\nkernel problem. And after resolving that, you get hit with another\ndata-dependent issue. You wonder to yourself, I wish there was a way I could\njust get a graph to play around with, and be able to view all the issues in one\nplace so that I can fix them later\u2026\ntorch.export.export()\ndraft_exportto the rescue!\ndraft_export\ndraft_exportis a version of export which will always successfully export a\ngraph, even if there are potential soundness issues. These issues will then be\ncompiled into a report for clearer visualization, which can be fixed later on.\ndraft_export\n\n## What sort of errors does it catch?#\n\nDraft-export helps to catch and debug the following errors:\nGuard on data-dependent errors\nConstraint violation errors\nMissing fake kernels\nIncorrectly written fake kernels\n\n## How does it work?#\n\nIn normal export, we will convert the sample inputs into FakeTensors and use\nthem to record operations and trace the program into a graph. Input tensor\nshapes that can change (which are marked throughdynamic_shapes), or values\nwithin tensors (typically from an.item()call) will be represented as a symbolic\nshape (SymInt) instead of a concrete integer. However some issues may occur\nwhile tracing - we may run into guards that we cannot evaluate, like if we want\nto check if some item in a tensor is greater than 0 (u0>=0). Since the tracer\ndoesn\u2019t know anything about the value ofu0, it will throw a data-dependent\nerror. If the model uses a custom operator but a fake kernel hasn\u2019t been\ndefined for it, then we will error withfake_tensor.UnsupportedOperatorExceptionbecause export doesn\u2019t know how to apply this onFakeTensors. If a custom\noperator has a fake kernel implemented incorrectly, export will silently produce\nan incorrect graph that doesn\u2019t match the eager behavior.\ndynamic_shapes\n.item()\nSymInt\nu0>=0\nu0\nfake_tensor.UnsupportedOperatorException\nFakeTensors\nTo fix the above errors, draft-export usesreal tensor tracingto guide us on\nhow to proceed when tracing. As we trace the model with fake tensors, for every\noperation that happens on a fake tensor, draft-export will also run the operator\non stored real tensors which come from the example inputs passed to export. This\nallows us to address the above errors: When we reach a guard that we cannot\nevaluate, likeu0>=0, we will use the stored real tensor values to\nevaluate this guard. Runtime asserts will be added into the graph to ensure that\nthe graph asserts the same guard that we assumed while tracing. If we run into\na custom operator without a fake kernel, we will run the operator\u2019s normal\nkernel with the stored real tensors, and return a fake tensor with the same rank\nbut unbacked shapes. Since we have the real tensor output for every operation,\nwe will compare this with the fake tensor output from the fake kernel. If the\nfake kernel is implemented incorrectly, we will then catch this behavior and\ngenerate a more correct fake kernel.\nu0>=0\n\n## How can I use draft export?#\n\nLet\u2019s say you\u2019re trying to export this piece of code:\n\n```python\nclass M(torch.nn.Module):\n    def forward(self, x, y, z):\n        res = torch.ops.mylib.foo2(x, y)\n\n        a = res.item()\n        a = -a\n        a = a // 3\n        a = a + 5\n\n        z = torch.cat([z, z])\n\n        torch._check_is_size(a)\n        torch._check(a < z.shape[0])\n\n        return z[:a]\n\ninp = (torch.tensor(3), torch.tensor(4), torch.ones(3, 3))\n\nep = torch.export.export(M(), inp)\n\n```\n\nThis runs into a \u201cmissing fake kernel\u201d error formylib.foo2and then aGuardOnDataDependentExpressionbecause of the slicing ofzwitha,\nan unbacked symint.\nmylib.foo2\nGuardOnDataDependentExpression\nz\na\nTo calldraft-export, we can replace thetorch.exportline with the following:\ndraft-export\ntorch.export\n\n```python\nep = torch.export.draft_export(M(), inp)\n\n```\n\nepis a valid ExportedProgram which can now be passed through further environments!\nep\n\n## Debugging with draft-export#\n\nIn the terminal output from draft-export, you should see the following message:\n\n```python\n#########################################################################################\nWARNING: 2 issue(s) found during export, and it was not able to soundly produce a graph.\nTo view the report of failures in an html page, please run the command:\n    `tlparse /tmp/export_angelayi/dedicated_log_torch_trace_axpofwe2.log --export`\nOr, you can view the errors in python by inspecting `print(ep._report)`.\n########################################################################################\n\n```\n\nDraft-export automatically dumps logs fortlparse. You can view the tracing\nerrors by usingprint(ep._report), or you can pass the logs intotlparseto generate an html report.\ntlparse\nprint(ep._report)\ntlparse\nRunning thetlparsecommand in the terminal will generate atlparseHTML report. Here is an example of thetlparsereport:\ntlparse\ntlparse\nClicking into the Data Dependent Error, we will see the following page which\ncontains information to help debug this error. Specifically, it contains:\nThe stacktrace at which this error occurs\nA list of local variables and their shapes\nInformation for how this guard was created\n\n## The returned Exported Program#\n\nBecause draft-export specializes on code paths based on the example inputs, the\nexported program resulting from draft-export is guaranteed to be runnable and\nreturn correct results forat leastthe given example inputs. Other inputs can\nwork, as long as they match the same guards that were taken when we were\ndraft-exporting.\nFor example, if we have a graph branching on if a value is greater than 5, if in\ndraft-export our example inputs were greater than 5, then the returnedExportedProgramwill specialize on that branch, and will assert that the value\nis greater than 5. This means that the program will succeed if you pass in\nanother value greater than 5, but will fail if you pass in a value less than 5.\nThis is more sound thantorch.jit.trace, which will silently specialize on the\nbranch. The proper way fortorch.exportto support both branches would be to\nrewrite the code usingtorch.cond, which will then capture both branches.\nExportedProgram\ntorch.jit.trace\ntorch.export\ntorch.cond\nBecause of the runtime assertions in the graph, the returned exported-program is\nalso retraceable withtorch.exportortorch.compile, with a minor addition in\nthe case where a custom operator is missing a fake kernel.\ntorch.export\ntorch.compile\n\n## Generating Fake Kernels#\n\nIf a custom operator does not contain a fake implementation, currently\ndraft-export will use the real-tensor propagation to get an output for the\noperator and continue tracing. However, if we run the exported program with fake\ntensors or retrace the exported model, we will still fail because there is still\nno fake kernel implementation.\nTo address this, after draft-export, we will generate an operator profile for\neach custom operator call that we encounter, and store this on the report\nattached to the exported program:ep._report.op_profiles. Users can then use the\ncontext managertorch._library.fake_profile.unsafe_generate_fake_kernelsto\ngenerate and register a fake implementation based on these operator profiles.\nThis way future fake tensor retracing will work.\nep._report.op_profiles\ntorch._library.fake_profile.unsafe_generate_fake_kernels\nThe workflow would look something like:\n\n```python\nclass M(torch.nn.Module):\n    def forward(self, a, b):\n        res = torch.ops.mylib.foo(a, b)  # no fake impl\n        return res\n\nep = draft_export(M(), (torch.ones(3, 4), torch.ones(3, 4)))\n\nwith torch._library.fake_profile.unsafe_generate_fake_kernels(ep._report.op_profiles):\n    decomp = ep.run_decompositions()\n\nnew_inp = (\n    torch.ones(2, 3, 4),\n    torch.ones(2, 3, 4),\n)\n\n# Save the profile to a yaml and check it into a codebase\nsave_op_profiles(ep._report.op_profiles, \"op_profile.yaml\")\n# Load the yaml\nloaded_op_profile = load_op_profiles(\"op_profile.yaml\")\n\n```\n\nThe operator profile is a dictionary mapping operator name to a set of profiles\nwhich describe the input and outputs of the operator, and could be manually\nwritten, saved into a yaml file, and checked into a codebase. Here\u2019s an example\nof a profile formylib.foo.default:\nmylib.foo.default\n\n```python\n\"mylib.foo.default\": {\n    OpProfile(\n        args_profile=(\n            TensorMetadata(\n                rank=2,\n                dtype=torch.float32,\n                device=torch.device(\"cpu\"),\n                layout=torch.strided,\n            ),\n            TensorMetadata(\n                rank=2,\n                dtype=torch.float32,\n                device=torch.device(\"cpu\"),\n                layout=torch.strided,\n            ),\n        ),\n        out_profile=TensorMetadata(\n            rank=2,\n            dtype=torch.float32,\n            device=torch.device(\"cpu\"),\n            layout=torch.strided,\n        ),\n    )\n}\n\n```\n\nmylib.foo.default\u2019s profile contains only one profile, which says that for 2\ninput tensors of rank 2, dtypetorch.float32, devicecpu, we will return\none tensor of rank 2, dtypetorch.float32, and devicecpu. Using the\ncontext manager, will then generate a fake kernel where given 2 input tensors of\nrank 2 (and the other tensor metadata), we will output one tensor of rank 2 (and\nthe other tensor metadata).\nmylib.foo.default\ntorch.float32\ncpu\ntorch.float32\ncpu\nIf the operator also supports other input ranks, then we can add the profile to\nthis list of profiles, either by manually adding it into the existing profile or\nrerunning draft-export with new inputs to get new profiles, so that the\ngenerated fake kernel will support more input types. Otherwise it will error.\n\n## Where to go from here?#\n\nNow that we have successfully created anExportedProgramusing draft-export,\nwe can use further compilers such asAOTInductorto optimize its performance\nand produce a runnable artifact. This optimized version can then be used for\ndeployment. In parallel, we can utilize the report generated by draft-export to\nidentify and fixtorch.exporterrors that were encountered so that the\noriginal model can be directly traceable withtorch.export.\nExportedProgram\nAOTInductor\ntorch.export\ntorch.export",
  "url": "https://pytorch.org/docs/stable/export/draft_export.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
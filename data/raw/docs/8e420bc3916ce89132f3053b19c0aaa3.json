{
  "doc_id": "8e420bc3916ce89132f3053b19c0aaa3",
  "source": "pytorch_docs",
  "title": "torch.distributed.fsdp.fully_shard \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.distributed.fsdp.fully_shard#\n\nCreated On: Dec 04, 2024 | Last Updated On: Jun 16, 2025\n\n## PyTorch FSDP2 (fully_shard)#\n\nfully_shard\nPyTorch FSDP2 (RFC) provides\na fully sharded data parallelism (FSDP) implementation targeting performant\neager-mode while using per-parameter sharding for improved usability\nSee theGetting Started with FSDP2tutorial for more information.\nIf you are currently using FSDP1, consider migrating to FSDP2 using ourmigration guide.\nThe user contract forfully_shard(model)is as follows\nfully_shard(model)\nFor model initialization, fully_shard converts model.parameters() from\nplain torch.Tensor to DTensor in-place. The parameters are moved to the\nappropriate device according to the device mesh.\nBefore forward and backward passes, pre-forward/backward hooks are\nresponsible for all-gathering the parameters and converting model.parameters()\nfrom DTensor to plain torch.Tensor.\nAfter forward and backward passes, post-forward/backward hooks free\nthe unsharded parameters (no communication needed) and convert\nmodel.parameters() from plain torch.Tensor back to DTensor.\nFor the optimizer, it must be initialized with the DTensor model.parameters(),\nand the optimizer step should be performed on DTensor parameters.\nCallmodel(input)instead ofmodel.forward(input)to trigger pre-forward\nhooks to all-gather parameters. To make model.forward(input) work, users must\neither callmodel.unshard()explicitly or useregister_fsdp_forward_method(model,\"forward\")to register the forward method for hooking.\nmodel(input)\nmodel.forward(input)\nmodel.unshard()\nregister_fsdp_forward_method(model,\"forward\")\nfully_shard groups parameters together for a single all-gather. User should apply\nfully_shard in a bottom-up manner. For example, in a Transformer model, fully_shard\nshould be applied to each layer before applying it to the root model. When applied\nto the root model, fully_shard excludes model.parameters() from each layer and groups\nthe remaining parameters (e.g., embeddings, output projection) into a single\nall-gather group.\ntype(model)is \u201cunioned\u201d withFSDPModulein-place. For example, if model\nis originally of type nn.Linear, then fully_shard changestype(model)from\nnn.Linear toFSDPLinearin-place.FSDPLinearis an instance of both\nnn.Linear andFSDPModule. It retains all methods of nn.Linear while also\nexposing FSDP2-specific APIs under FSDPModule, such asreshard()andunshard().\ntype(model)\nFSDPModule\ntype(model)\nFSDPLinear\nFSDPLinear\nFSDPModule\nreshard()\nunshard()\nFully Qualified Names (FQNs) for parameters remain unchanged. If we callmodel.state_dict(), the FQNs are the same before and after applying\nfully_shard. This is because fully_shard does not wrap the module but only\nregisters hooks to the original module.\nmodel.state_dict()\nCompared to PyTorch FSDP1 (FullyShardedDataParallel):\nFullyShardedDataParallel\nFSDP2 usesDTensor-based dim-0 per-parameter sharding for a simpler\nsharding representation compared to FSDP1\u2019s flat-parameter sharding, while\npreserving similar throughput performance. More specifically, FSDP2 chunks\neach parameter on dim-0 across the data parallel workers (usingtorch.chunk(dim=0)), whereas FSDP1 flattens, concatenates, and chunks a\ngroup of tensors together, making reasoning about what data is present on\neach worker and resharding to different parallelisms complex. Per-parameter\nsharding provides a more intuitive user experience, relaxes constraints\naround frozen parameters, and allows for communication-free (sharded) state\ndicts, which otherwise require all-gathers in FSDP1.\nDTensor\ntorch.chunk(dim=0)\nFSDP2 implements a different memory management approach to handle the\nmulti-stream usages that avoidstorch.Tensor.record_stream. This ensures\ndeterministic and expected memory usage and does not require blocking the CPU\nlike in FSDP1\u2019slimit_all_gathers=True.\ntorch.Tensor.record_stream\nlimit_all_gathers=True\nFSDP2 exposes APIs for manual control over prefetching and collective\nscheduling, allowing power users more customization. See the methods onFSDPModulebelow for details.\nFSDPModule\nFSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly\nsupport full state dicts. Instead, users can reshard the sharded state dicts\ncontainingDTensors to full state dicts themselves usingDTensorAPIs likeDTensor.full_tensor()or by using higher-level APIs likePyTorch Distributed Checkpoint\u2018s\ndistributed state dict APIs. Also, some other args have been removed; seeherefor\ndetails.\nDTensor\nDTensor\nDTensor.full_tensor()\nThe frontend API isfully_shardthat can be called on amodule:\nfully_shard\nmodule\nApply fully sharded data parallelism (FSDP) tomodule, where FSDP\nshards module parameters, gradients, and optimizer states across data\nparallel workers to save memory at the cost of communication.\nmodule\nAt initialization, FSDP shards the module\u2019s parameters across the data\nparallel workers given bymesh. Before forward, FSDP all-gathers the\nsharded parameters across the data-parallel workers to get the unsharded\nparameters for forward computation. Ifreshard_after_forwardisTrue, then FSDP frees the unsharded parameters after forward and\nre-all-gathers them in backward before gradient computation. After gradient\ncomputation, FSDP frees the unsharded parameters and reduce-scatters the\nunsharded gradients across data-parallel workers.\nmesh\nreshard_after_forward\nTrue\nThis implementation represents the sharded parameters asDTensors\nsharded on dim-0, while the unsharded parameters will be like the original\nparameters onmodule(e.g.torch.Tensorif originallytorch.Tensor). A moduleforward pre-hookonmoduleall-gathers the parameters, and a moduleforward hookonmodulefrees them (if needed). Similar backward hooks all-gather\nparameters and later free parameters and reduce-scatter gradients.\nDTensor\nmodule\ntorch.Tensor\ntorch.Tensor\nmodule\nmodule\nSince grouping multiple tensors together for one collective is critical for\ncommunication efficiency, this implementation makes this grouping first\nclass. Callingfully_shard()onmoduleconstructs one group that\nincludes the parameters inmodule.parameters()except those already\nassigned to a group from an earlier call on a submodule. This means thatfully_shard()should be called bottom-up on your model. Each group\u2019s\nparameters are all-gathered in one collective, and its gradients are\nreduce-scattered in one collective. Partitioning the model into multiple\ngroups (\u201clayer by layer\u201d) allows for peak memory savings and communication/computation\noverlap. Users generally shouldnotcallfully_shard()only on the\ntopmost root module.\nfully_shard()\nmodule\nmodule.parameters()\nfully_shard()\nfully_shard()\nmodule(Union[nn.Module,List[nn.Module]) \u2013 The module or modules to\nshard with FSDP and group together for communication.\nmesh(Optional[DeviceMesh]) \u2013 This data parallel mesh defines the\nsharding and device. If 1D, then parameters are fully sharded\nacross the 1D mesh (FSDP) with(Shard(0),)placement. If 2D,\nthen parameters are sharded across the 1st dim and replicated\nacross the 0th dim (HSDP) with(Replicate(),Shard(0))placement. The mesh\u2019s device type gives the device type used for\ncommunication; if a CUDA or CUDA-like device type, then we use the\ncurrent device.\n(Shard(0),)\n(Replicate(),Shard(0))\nreshard_after_forward(Optional[Union[bool,int]]) \u2013This controls the parameter\nbehavior after forward and can trade off memory and communication:IfTrue, then this reshards parameters after forward and\nre-all-gathers in backward.IfFalse, then this keeps the unsharded parameters in memory\nafter forward and avoids the all-gather in backward. For best performance,\nwe usually setFalsefor the root module, because the root module\nis typically required immediately when the backward pass begins.IfNone, it is set toTruefor non-root modules andFalsefor root modules.If anint, then this represents the world size to reshard to\nafter forward. It should be a non-trivial divisor of themeshshard dim size (i.e. excluding 1 and the dim size itself). A\nchoice may be the intra-node size (e.g.torch.cuda.device_count()).\nThis allows the all-gather in backward to be over a smaller world\nsize at the cost of higher memory usage than setting toTrue.After forward, the parameters registered to the module depend on\nto this: The registered parameters are the sharded parameters ifTrue; unsharded parameters ifFalse; and the parameters\nresharded to the smaller mesh otherwise. To modify the parameters\nbetween forward and backward, the registered parameters must be\nthe sharded parameters. ForFalseor anint, this can be\ndone by manually resharding viareshard().\nThis controls the parameter\nbehavior after forward and can trade off memory and communication:\nIfTrue, then this reshards parameters after forward and\nre-all-gathers in backward.\nTrue\nIfFalse, then this keeps the unsharded parameters in memory\nafter forward and avoids the all-gather in backward. For best performance,\nwe usually setFalsefor the root module, because the root module\nis typically required immediately when the backward pass begins.\nFalse\nFalse\nIfNone, it is set toTruefor non-root modules andFalsefor root modules.\nNone\nTrue\nFalse\nIf anint, then this represents the world size to reshard to\nafter forward. It should be a non-trivial divisor of themeshshard dim size (i.e. excluding 1 and the dim size itself). A\nchoice may be the intra-node size (e.g.torch.cuda.device_count()).\nThis allows the all-gather in backward to be over a smaller world\nsize at the cost of higher memory usage than setting toTrue.\nint\nmesh\ntorch.cuda.device_count()\nTrue\nAfter forward, the parameters registered to the module depend on\nto this: The registered parameters are the sharded parameters ifTrue; unsharded parameters ifFalse; and the parameters\nresharded to the smaller mesh otherwise. To modify the parameters\nbetween forward and backward, the registered parameters must be\nthe sharded parameters. ForFalseor anint, this can be\ndone by manually resharding viareshard().\nTrue\nFalse\nFalse\nint\nreshard()\nshard_placement_fn(Optional[Callable[[nn.Parameter],Optional[Shard]]]) \u2013 This callable can be used to override the sharding placement for a\nparameter to shard a parameter on a dimension other than dim-0. If\nthis callable returns aShardplacement (notNone),\nthen FSDP will shard according to that placement (e.g.Shard(1)).\nIf sharding on a nonzero dim, we currently require even sharding,\ni.e. the tensor dim size on that dim must be divisible by the FSDP\nshard mesh size.\nShard\nNone\nShard(1)\nmp_policy(MixedPrecisionPolicy) \u2013 This controls the mixed precision\npolicy, which offers parameter/reduction mixed precision for this\nmodule. SeeMixedPrecisionPolicyfor details.\nMixedPrecisionPolicy\noffload_policy(OffloadPolicy) \u2013 This controls the offloading policy,\nwhich offers parameter/gradient/optimizer state offloading. SeeOffloadPolicyand its subclasses for details.\nOffloadPolicy\nignored_params(Optional[set[nn.Parameter]]) \u2013 Optional(Set[nn.Parameter]): The set of parameters to be\nignored by FSDP. They will not be sharded, nor moved to the device\nduring init, nor have their gradients reduced in backward.\nThe module with FSDP applied (in-place).\nFSDPModule\nReshards the module\u2019s parameters, freeing the unsharded parameters if\nthey are allocated and registering the sharded parameters to the\nmodule. This method isnotrecursive.\nhook(Callable[[torch.Tensor],None]) \u2013 User-defined all-reduce hook\nwith expected signaturehook(reduce_output:torch.Tensor)->Nonewherereduce_outputis the reduce-scatter output if only\nusing FSDP or the all-reduce output if using native HSDP.\nhook(reduce_output:torch.Tensor)->None\nreduce_output\nstream(Optional[torch.cuda.Stream]) \u2013 Stream to run the all-reduce\nhook in. This should only be set if not using native HSDP. If\nusing native HSDP, the hook will run in the internally defined\nall-reduce stream used by the native HSDP all-reduce.\nSets whether the temporary staging buffers used to send and receive data\nover collective communications should be allocated using the custom\noptimized allocator provided by the ProcessGroup itself (if any). This\nmight allow the ProcessGroup to be more efficient. For example, when\nusing NCCL, this enables it to leverage zero-copy transfers over SHARP\n(for NVLink and/or InfiniBand).\nThis cannot be used together withset_custom_all_gather()orset_custom_reduce_scatter()as those APIs allow for\nfiner-grained control over each communication, and this method cannot\ndetermine their staging buffer allocation strategy.\nset_custom_all_gather()\nset_custom_reduce_scatter()\nenable(bool) \u2013 Whether to turn on ProcessGroup allocation.\nOverrides the defaultall_gathercommunication behavior,\nto have better control over the communication and memory usage.\nSeeCommandReduceScatterfor details.\nall_gather\ncomm(AllGather) \u2013 Custom all-gather communication.\nOverrides the defaultreduce_scattercommunication behavior,\nto have better control over the communication and memory usage.\nSeeCommandReduceScatterfor details.\nreduce_scatter\ncomm(ReduceScatter) \u2013 Custom reduce_scatter communication.\nSets whether to require the low-level collective communication\nprimitives to exclusively use \u201csum\u201d-type reductions, even if it comes\nat the cost of separate additional pre- or post-scaling operations.\nThis is needed for example because NCCL currently supports zero-copy\ntransfers only for this kind of collectives.\nNB: for MTIA devices, this is always implicitly enabled.\nNB: ifset_all_reduce_hookis used under FSDP setup, the caller needs\nto ensure the custom all-reduce across FSDP units follow this strategy\nas well, as FSDP can no longer automatically handle that.\nenable(bool) \u2013 Whether to only ever use ReduceOp.SUM for comms.\nSets a custom divide factor for the gradient reduction. This might use\na custom reduce op using NCCL\u2019s PreMulSum, which allows multiplying by\nthe factor before reduction.\nfactor(float) \u2013 Custom divide factor.\nSets whether the next backward is the last one. On the last backward,\nFSDP waits on pending gradient reduction and clears internal data\ndata structures for backward prefetching. This can be useful for\nmicrobatching.\nSets the FSDP modules for which this FSDP module should explicitly\nprefetch all-gathers in backward. This overrides the default backward\npretching implementation that prefetches the next FSDP module based on\nthe reverse post-forward order.\nPassing a singleton list containing the previous FSDP module gives the\nsame all-gather overlap behavior as the default overlap behavior.\nPassing a list with at least length two is required for more aggressive\noverlap and will use more reserved memory.\nmodules(List[FSDPModule]) \u2013 FSDP modules to prefetch.\nSets the FSDP modules for which this FSDP module should explicitly\nprefetch all-gathers in forward. The prefetching runs after this\nmodule\u2019s all-gather copy-out.\nPassing a singleton list containing the next FSDP module gives the same\nall-gather overlap behavior as the default overlap behavior, except the\nprefetched all-gather is issued earlier from the CPU. Passing a list\nwith at least length two is required for more aggressive overlap and\nwill use more reserved memory.\nmodules(List[FSDPModule]) \u2013 FSDP modules to prefetch.\nSets a post-optimizer-step event for the root FSDP module to wait the\nall-gather streams on.\nBy default, the root FSDP module waits the all-gather streams on the\ncurrent stream to ensure that the optimizer step has finished before\nall-gathering. However, this may introduce false dependencies if\nthere is unrelated computation after the optimizer step. This API\nallows the user to provide their own event to wait on. After the root\nwaits on the event, the event is discarded, so this API should be\ncalled with a new event each iteration.\nevent(torch.Event) \u2013 Event recorded after the optimizer step\nto wait all-gather streams on.\nUseset_gradient_divide_factor()instead\nset_gradient_divide_factor()\nSets if the module should all-reduce gradients. This can be used to\nimplement gradient accumulation with only reduce-scatter but not\nall-reduce for HSDP.\nSets if the module should sync gradients. This can be used to implement\ngradient accumulationwithout communication. For HSDP, this controls\nboth reduce-scatter and all-reduce together. This is the equivalence ofno_syncin FSDP1.\nrequires_gradient_sync(bool) \u2013 Whether to reduce gradients for the\nmodule\u2019s parameters.\nrecurse(bool) \u2013 Whether to set for all FSDP submodules or just the\npassed-in module.\nSets if the module should reshard parameters after backward. This can\nbe used during gradient accumulation to trade off higher memory for\nreduced communication since the unsharded parameters do not need to be\nre-all-gathered before the next forward.\nreshard_after_backward(bool) \u2013 Whether to reshard parameters after\nbackward.\nrecurse(bool) \u2013 Whether to set for all FSDP submodules or just the\npassed-in module.\nSets if the module should reshard parameters after forward. This can be\nused to change thereshard_after_forwardFSDP arg at runtime. For\nexample, this can be used to set the FSDP root module\u2019s value toTrue(since it is otherwise specially set toFalse), or it can\nset an FSDP module\u2019s value toFalsefor running evals and set back\ntoTruefor training.\nreshard_after_forward\nTrue\nFalse\nFalse\nTrue\nreshard_after_forward(bool) \u2013 Whether to reshard parameters after\nforward.\nrecurse(bool) \u2013 Whether to set for all FSDP submodules or just the\npassed-in module.\nSets whether the FSDP module\u2019s parameters need to be unsharded in\nbackward. This can be used in expert cases when the user knows that all\nparameters in this FSDP module\u2019s parameter group are not needed for\nbackward computation (e.g. embedding).\nUnshards the module\u2019s parameters by allocating memory and all-gathering\nthe parameters. This method isnotrecursive. The unshard follows theMixedPrecisionPolicy, so it will all-gather followingparam_dtypeif set.\nMixedPrecisionPolicy\nparam_dtype\nasync_op(bool) \u2013 IfTrue, then returns aUnshardHandlethat has await()method to wait on the unshard op. IfFalse, then returnsNoneand waits on the handle inside\nthis function.\nTrue\nUnshardHandle\nwait()\nFalse\nNone\nOptional[UnshardHandle]\nNote\nIfasync_op=True, then FSDP will wait on the pending\nunshard in the module\u2019s pre-forward for the user. The user only\nneeds to callwait()explicitly if the wait should happen\nbefore pre-forward.\nasync_op=True\nwait()\nA handle to wait on aFSDPModule.unshard()op.\nFSDPModule.unshard()\nWaits on the unshard op. This ensures that the current stream can use\nthe unsharded parameters, which are now registered to the module.\nRegisters a method onmoduleto be considered a forward method for\nFSDP.\nmodule\nFSDP all-gathers parameters pre-forward and optionally frees parameters\npost-forward (depending onreshard_after_forward). FSDP only knows to\ndo this fornn.Module.forward()by default. This function patches a\nuser-specified method to run the pre/post-forward hooks before/after the\nmethod, respectively. Ifmoduleis not anFSDPModule, then\nthis is a no-op.\nreshard_after_forward\nnn.Module.forward()\nmodule\nFSDPModule\nmodule(nn.Module) \u2013 Module to register the forward method on.\nmethod_name(str) \u2013 Name of the forward method.\nThis configures FSDP\u2019s mixed precision. Unlike autocast, this applies mixed\nprecision at the module level, not op level, which means low-precision\nactivations are saved for backward and high-to-low-precision casts are\nincurred only at module boundaries.\nFSDP works well with module-level mixed precision since it keeps the\nhigh-precision sharded parameters in memory anyway. In other words, FSDP\ndoes not require any extra memory to keep a high-precision copy of the\nparameters for the optimizer step.\nparam_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\nthe unsharded parameter and hence the dtype for forward/backward\ncomputation and the parameter all-gather. If this isNone, then\nthe unsharded parameter uses the original dtype. The optimizer step\nuses the sharded parameter in the original dtype. (Default:None)\nNone\nNone\nreduce_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\ngradient reduction (i.e. reduce-scatter or all-reduce). If this isNonebutparam_dtypeis notNone, then the reduction\nuses the compute dtype. This can be used to run gradient reduction\nin full precision while using low precision for compute. If also\ngradient reduction is disabled viaset_requires_gradient_sync(),\nthen FSDP will accumulate gradients usingreduce_dtype.\n(Default:None)\nNone\nparam_dtype\nNone\nset_requires_gradient_sync()\nreduce_dtype\nNone\noutput_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\ncasting floating-point forward outputs. This can be used to\nhelp implement cases where different modules have different mixed\nprecision policies. (Default:None)\nNone\ncast_forward_inputs(bool) \u2013 This specifies whether FSDP should cast the\nforward\u2019s floating-point input tensors toparam_dtypeor not.\nparam_dtype\nThis base class represents the policy of no offloading and is only used as\nthe default value for theoffload_policyarg.\noffload_policy\nThis offload policy offloads parameters, gradients, and optimizer states to\nCPU. Sharded parameters are copied host-to-device before all-gather. The\nall-gathered parameters are freed according toreshard_after_forward.\nSharded gradients are copied device-to-host in backward, and the optimizer\nstep runs on CPU with CPU optimizer states.\nreshard_after_forward\npin_memory(bool) \u2013 Whether to pin sharded parameter and gradient\nmemory. Pinning memory allows both more efficient H2D/D2H copies\nand for the copies to overlap with compute. However, the pinned\nmemory cannot be used by other processes. Set this toFalseif\nyou have insufficient CPU memory. (Default:True)\nFalse\nTrue",
  "url": "https://pytorch.org/docs/stable/distributed.fsdp.fully_shard.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
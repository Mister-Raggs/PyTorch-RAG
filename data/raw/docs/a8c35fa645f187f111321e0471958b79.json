{
  "doc_id": "a8c35fa645f187f111321e0471958b79",
  "source": "pytorch_docs",
  "title": "Distributed Checkpoint - torch.distributed.checkpoint \u2014 PyTorch 2.9 documentation",
  "text": "\n## Distributed Checkpoint - torch.distributed.checkpoint#\n\nCreated On: Nov 16, 2022 | Last Updated On: Sep 04, 2025\nDistributed Checkpoint (DCP) support loading and saving models from multiple ranks in parallel.\nIt handles load-time resharding which enables saving in one cluster topology and loading into another.\nDCP is different thantorch.saveandtorch.loadin a few significant ways:\ntorch.save\ntorch.load\nIt produces multiple files per checkpoint, with at least one per rank.\nIt operates in place, meaning that the model should allocate its data first and DCP uses that storage instead.\nThe entrypoints to load and save a checkpoint are the following:\n\n## Additional resources:#\n\nGetting Started with Distributed Checkpoint (DCP)\nAsynchronous Saving with Distributed Checkpoint (DCP)\nTorchTitan Checkpointing Docs\nTorchTitan DCP Implementation\nEnum for async checkpointer type.\nThis class contains futures for staging and upload completion.\nIt is returned by async_save().\nstaging_completion is a future that indicates when local copy\nof state_dict is complete.\nupload_completion is a future that indicates when a checkpoint\ncompleted saving.\nSave a distributed model in SPMD style.\nThis function is different fromtorch.save()as it handlesShardedTensor, andDTensorby having each rank only save their local shards.\ntorch.save()\nShardedTensor\nDTensor\nFor eachStatefulobject (having both astate_dictand aload_state_dict),\nsave will callstate_dictbefore serialization.\nStateful\nstate_dict\nload_state_dict\nstate_dict\nWarning\nThere is no guarantees of Backwards Compatibility across PyTorch versions\nfor saved state_dicts.\nWarning\nIf using theprocess_groupargument, make sure that only its ranks\ncallsave_state_dictand that all data in state_dict belong to it.\nNote\nWhen saving checkpoint for FSDP\u2019sShardingStrategy.HYBRID_SHARD, only one of\nthe shard_group should be callingsave_state_dictand the corresponding process\ngroup needs to be passed in.\nNote\nstate_dict in the local process.\nstate_dict(Dict[str,Any]) \u2013 The state_dict to save.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default:None)\nNone\nstorage_writer(Optional[StorageWriter]) \u2013 Instance of StorageWriter used to perform writes. If this is not\nspecified, DCP will automatically infer the writer based on the\ncheckpoint_id. If checkpoint_id is also None, an exception will\nbe raised. (Default:None)\nNone\nplanner(Optional[SavePlanner]) \u2013 Instance of SavePlanner. If this is not specified, the default\nplanner will be used. (Default:None)\nNone\nprocess_group(Optional[ProcessGroup]) \u2013 ProcessGroup to be used for cross-rank synchronization.\n(Default:None)\nNone\nno_dist(bool) \u2013 IfTrue, this function will assume the intent is to load\na checkpoint on a single rank/process.\n(Default:False)\nTrue\nFalse\nuse_collectives(bool) \u2013 IfFalse, this function will assume the intent is to save\na checkpoint without using cross-rank synchronization.\n(Default:True)\nThis configuration is experimental and should be used with caution.\nIt will change the format of the saved checkpoint and may not be backward compatible.\nFalse\nTrue\nMetadata object for the saved checkpoint.\nMetadata\nExample\n\n```python\n>>> my_model = MyModule()\n\n```\n\n\n```python\n>>> state_dict = {\"model\": my_model}\n\n```\n\n\n```python\n>>> fs_storage_writer = torch.distributed.checkpoint.FileSystemWriter(\n...     \"/checkpoint/1\"\n... )\n>>> torch.distributed.checkpoint.save(\n>>>     state_dict=state_dict,\n>>>     storage_writer=fs_storage_writer,\n>>> )\n\n```\n\nNote\nsave_state_dict uses collectives to coordinate writes across ranks.\nFor NCCL-based process groups, internal tensor representations of\nobjects must be moved to the GPU device before communication takes place.\nIn this case, the device used is given bytorch.cuda.current_device()and it is the user\u2019s responsibility to ensure that this is set so that\neach rank has an individual GPU, viatorch.cuda.set_device().\ntorch.cuda.current_device()\ntorch.cuda.set_device()\nAsynchronous version ofsave. This code first de-stages the state_dict on to the\nstaging storage (defaults to CPU memory), and then calls thesavein a separate thread.\nsave\nWarning\nThis feature is experimental and subject to change.\nMUST CALL CLOSE AFTER LAST CHECKPOINT IS SAVED\nstate_dict(Dict[str,Any]) \u2013 The state_dict to save.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default:None)\nNone\nstorage_writer(Optional[StorageWriter]) \u2013 Instance of StorageWriter used to perform \u2018stage\u2019 and  \u2018save\u2019. If\nthis is not specified, DCP will automatically infer the writer based on the\ncheckpoint_id. If checkpoint_id is also None, an exception will\nbe raised. (Default:None)\nNone\nplanner(Optional[SavePlanner]) \u2013 Instance of SavePlanner. If this is not specified, the default\nplanner will be used. (Default:None)\nNone\nprocess_group(Optional[ProcessGroup]) \u2013 ProcessGroup to be used for cross-rank synchronization.\n(Default:None)\nNone\nasync_checkpointer_type(AsyncCheckpointerType) \u2013 whether to do checkpoint in separate thread or process\n(Default:AsyncCheckpointerType.THREAD)\nAsyncCheckpointerType.THREAD\nasync_stager(AsyncStager) \u2013 provides staging implementation. If storage_writer implements AsyncStager\nand async_stager is provided, async_stager will be used for staging\nno_dist(bool) \u2013 IfTrue, this function will assume the intent is to save\na checkpoint on a single rank/process.\n(Default:False)\nTrue\nFalse\nuse_collectives(bool) \u2013 If False, Save the checkpoint without rank coordination. (Default:True)\nThis configuration is experimental and should be used with caution.\nIt will change the format of the saved checkpoint and may not be backward compatible.\nTrue\nA future holding the resultant Metadata object fromsave.\nFuture\nExample\n\n```python\n>>> my_model = MyModule()\n\n```\n\n\n```python\n>>> state_dict = {\"model\": my_model}\n\n```\n\n\n```python\n>>> fs_storage_writer = torch.distributed.checkpoint.FileSystemWriter(\n...     \"/checkpoint/1\"\n... )\n>>> checkpoint_future = torch.distributed.checkpoint.async_save(\n>>>     state_dict=state_dict,\n>>>     storage_writer=fs_storage_writer,\n>>> )\n>>>\n>>> # ... do some work ...\n>>>\n>>> checkpoint_future.result()\n\n```\n\nThis method is deprecated. Please switch to \u2018save\u2019.\nMetadata\nLoad a checkpoint into a distributed state dict in SPMD style.\nEach rank must have the same keys in theirstate_dictprovided to this\nAPI. Mismatched keys may result in hangs or errors. If unsure, you can use\ntheutils._assert_same_keysAPI to check (but may incur communication\ncosts).\nstate_dict\nutils._assert_same_keys\nEach rank will try to read the least amount of data necessary\nto fulfill the requestedstate_dict. When loadingShardedTensororDTensorinstances, each rank only reads data for their local shards.\nShardedTensor\nDTensor\nFor eachStatefulobject (having both astate_dictand aload_state_dict),\nload will first callstate_dictbefore attempting deserialization, followed byload_state_dictonce the deserialization is complete.\nFor each non-Statefulobject, load will deserialize the object, and then replace\nit in thestate_dictwith the deserialized object.\nStateful\nstate_dict\nload_state_dict\nstate_dict\nload_state_dict\nStateful\nstate_dict\nWarning\nAll tensors instate_dictmust be allocated on their\ndestination deviceprior tocalling this function.\nstate_dict\nAll non-tensor data is loaded usingtorch.load()and modified in place\non state_dict.\nWarning\nUsers must callload_state_dicton the root module to ensure load\npos-processing and non-tensor data properly propagates.\nstate_dict(Dict[str,Any]) \u2013 The state_dict to load the checkpoint into.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default:None)\nNone\nstorage_reader(Optional[StorageReader]) \u2013 Instance of StorageWriter used to perform reads. If this is not\nspecified, DCP will automatically infer the reader based on the\ncheckpoint_id. If checkpoint_id is also None, an exception will\nbe raised. (Default:None)\nNone\nplanner(Optional[LoadPlanner]) \u2013 Instance of LoadPlanner. If this is not specified, the default\nplanner will be used. (Default:None)\nNone\nprocess_group(Optional[ProcessGroup]) \u2013 ProcessGroup to be used for cross-rank synchronization.\n(Default:None)\nNone\nno_dist(bool) \u2013 IfTrue, this function will assume the intent is to load\na checkpoint without using cross-rank synchronization. (Default:False)\nTrue\nFalse\nNone.\nNone\n\n```python\n>>> my_model = MyModule()\n>>> optimizer = Adagrad(my_model.parameters())\n>>> model_state_dict = my_model.state_dict()\n>>> fs_storage_reader = torch.distributed.checkpoint.FileSystemReader(\n...     \"/checkpoint/1\"\n... )\n\n```\n\n\n```python\n>>> torch.distributed.checkpoint.load_state_dict(\n>>>     state_dict=model_state_dict,\n>>>     storage_reader=fs_storage_reader,\n>>> )\n\n```\n\n\n```python\n>>> # module.load_state_dict() function might have customized steps\n>>> # to flush the state_dict, must call it to\n>>> # ensure correct behavior.\n>>> my_model.load_state_dict(model_state_dict)\n\n```\n\nNote\nload_state_dict uses collectives to coordinate reads across ranks.\nFor NCCL-based process groups, internal tensor representations of\nobjects must be moved to the GPU device before communication takes place.\nIn this case, the device used is given bytorch.cuda.current_device()and it is the user\u2019s responsibility to ensure that this is set so that each\nrank has an individual GPU, viatorch.cuda.set_device().\ntorch.cuda.current_device()\ntorch.cuda.set_device()\nThis method is deprecated. Please switch to \u2018load\u2019.\nThe following module is also useful for additional customization of the staging mechanisms used for asynchronous checkpointing (torch.distributed.checkpoint.async_save):\ntorch.distributed.checkpoint.async_save\nThis protocol is meant to provide customization and extensibility for dcp.async_save, allowing users\nto customize how data is staged previous to executing the usual dcp.save path in parallel.\nThe expected order of operations (concretely defined intorch.distributed.state_dict_saver.async_save)\nis the following:\nThis call gives the AsyncStager the opportunity to \u2018stage\u2019\nthe state_dict. The expectation and purpose of staging in this context is to create a \u201ctraining-safe\u201d\nrepresentation of the state dict, meaning that any updates to module data after staging is complete\nshould not be reflected in the state dict returned from this method. For example, in the default\ncase a copy of the entire state dict is created on CPU RAM and returned here, allowing users\nto continue training without risking changes to data which is being serialized.\nfor serializing the state_dict and writing it to storage.\nthe serialization thread starts and before returning from dcp.async_save. If this is set to False,\nthe assumption is the user has defined a custom synchronization point for the the purpose of further\noptimizing save latency in the training loop (for example, by overlapping staging with the\nforward/backward pass), and it is the respondsibility of the user to callAsyncStager.synchronize_stagingat the appropriate time.\nClean up all resources used by the stager.\nWhether to synchronize after executing the stage.\nReturns a \u201cstaged\u201d copy ofstate_dict. The expectation of the staged copy is that it is\ninoculated from any updates incurred after the stage call is complete.\nUnion[Future[dict[str,Union[~StatefulT,Any]]],dict[str,Union[~StatefulT,Any]]]\nIn the casestageis async in some way, this method should be called to ensure staging\nis complete and it is safe to begin modifying the originalstate_dict\nDefaultStager provides a full-featured staging implementation that combines\nmultiple optimization techniques for efficient checkpoint preparation.\nThe staging process works as follows:\n1. State dictionary is submitted for staging (sync or async)\n2. Tensors are copied from GPU to optimized CPU storage\n3. CUDA operations are synchronized if non-blocking copies are used\n4. Staged state dictionary is returned or made available via Future\n# Synchronous staging\nstager = DefaultStager(StagingOptions(use_async_staging=False))\nstaged_dict = stager.stage(state_dict)\nstager.close()\n# Asynchronous staging\nstager = DefaultStager(StagingOptions(use_async_staging=True))\nfuture = stager.stage(state_dict)\n# \u2026 do other work \u2026\nstaged_dict = future.result()\nstager.close()\n# Context manager pattern (recommended)\nstager = DefaultStager(config)\nwith stager:\nresult = stager.stage(state_dict)\nAsync staging provides best performance when model computation\ncan overlap with staging operations\nPinned memory improves CPU-GPU transfer speeds but uses more memory\nShared memory allows efficient IPC to checkpoint process\nNon-blocking copies reduce GPU idle time during memory transfers\nDefaultStager is not thread-safe. Each thread should use its own\ninstance, or external synchronization should be provided.\nClean up all resources used by the DefaultStager. Shuts down the ThreadPoolExecutor\nused for async staging operations and cleans up the underlying StateDictStager\u2019s\ncached storages. Should be called when the stager is no longer needed to prevent\nresource leaks, especially in long-running applications. After calling close(),\nthe stager should not be used for further staging operations.\nstager = DefaultStager(StagingOptions(use_async_staging=True))\nfuture = stager.stage(state_dict)\nresult = future.result()\nstager.close()  # Clean up all resources\nThis function is responsible for staging staging the state_dict.\nSee class docstring for more details on staging.\nIf use_async_staging is True, it will return a Future object that will be\nfulfilled when staging is complete.\nIf use_async_staging is False, it will return the fully staged state_dict.\nstate_dict(STATE_DICT_TYPE) \u2013 The state_dict to be staged.\nUnion[dict[str,Union[~StatefulT,Any]],Future[dict[str,Union[~StatefulT,Any]]]]\nWhen use_async_staging is True, this method will wait until staging is complete.\nIf use_async_staging is False, this method is a no-op.\nConfiguration options for checkpoint staging behavior.\nuse_pinned_memory(bool) \u2013 Enable pinned memory allocation for faster\nCPU-GPU transfers. Requires CUDA to be available. Default: True\nuse_shared_memory(bool) \u2013 Enable shared memory for multi-process\nscenarios. Useful when multiple processes need access to the\nsame staged data. Default: True\nuse_async_staging(bool) \u2013 Enable asynchronous staging using a\nbackground thread pool. Allows overlapping computation with\nstaging operations. Requires CUDA. Default: True\nuse_non_blocking_copy(bool) \u2013 Use non-blocking device memory\ncopies with stream synchronization. Improves performance by\nallowing CPU work to continue during GPU transfers. Default: True\nNote\nCUDA-dependent features will raise exception if CUDA is not available.\nAn implementation of AsyncStager which stages the state_dict on CPU RAM and blocks until the copy is complete.\nThis implementation also provides an option to optimize stage latency using pinned memory.\nN.B. synchronize_staging is a no-op in this case.\nReturns a copy ofstate_dicton the CPU.\ndict[str,Union[~StatefulT,Any]]\nNo-op function, since staging is blocking.\nIn addition to the above entrypoints,Statefulobjects, as described below, provide additional customization during saving/loading\nStateful\nStateful protocol for objects that can be checkpointed and restored.\nRestore the object\u2019s state from the provided state_dict.\nstate_dict(dict[str,Any]) \u2013 The state dict to restore from\nObjects should return their state_dict representation as a dictionary.\nThe output of this function will be checkpointed, and later restored inload_state_dict().\nWarning\nBecause of the inplace nature of restoring a checkpoint, this function\nis also called duringtorch.distributed.checkpoint.load.\nThe objects state dict\nDict\nThisexampleshows how to use Pytorch Distributed Checkpoint to save a FSDP model.\nThe following types define the IO interface used during checkpoint:\nInterface used byload_state_dictto read from storage.\nload_state_dict\nOne StorageReader instance acts as both the coordinator and the follower\nin a distributed checkpoint. As part of initialization, each instance\nis told its role.\nA subclass should expected the following sequence of calls byload_state_dict:\nload_state_dict\n(all ranks) set checkpoint_id if users pass a valid checkpoint_id.\n(all ranks) read_metadata()\n(all ranks) set_up_storage_reader()\n(all ranks) prepare_local_plan()\n(coordinator) prepare_global_plan()\n(all ranks) read_data()\nPerform centralized planning of storage loading.\nThis method is only called on the coordinator instance.\nWhile this method can produce a completely different plan, the preferred\nway is to store storage specific data in LoadPlan::storage_data.\nplans(list[torch.distributed.checkpoint.planner.LoadPlan]) \u2013 A list ofLoadPlaninstances, one for each rank.\nLoadPlan\nA list of transformedLoadPlanafter storage global planning\nLoadPlan\nlist[torch.distributed.checkpoint.planner.LoadPlan]\nPerform storage-specific local planning.\nWhile this method can produce a completely different plan, the recommended\nway is to store storage specific data in LoadPlan::storage_data.\nplan(LoadPlan) \u2013 The local plan from theLoadPlanin use.\nLoadPlan\nA transformedLoadPlanafter storage local planning\nLoadPlan\nLoadPlan\nRead all items fromplanusingplannerto resolve the data.\nplan\nplanner\nA subclass should callLoadPlanner::load_bytesto deserialize a BytesIO\nobject into the right place.\nLoadPlanner::load_bytes\nA subclass should callLoadPlanner::resolve_tensorto get access to the\ntensors that in should load data into.\nLoadPlanner::resolve_tensor\nIt\u2019s the StorageLayer responsibility to properly schedule any cross device copies\nrequired.\nplan(LoadPlan) \u2013 The local plan to execute on\nplanner(LoadPlanner) \u2013 The planner object to use to resolve items.\nA future that completes once all reads are finished.\nFuture[None]\nRead the checkpoint metadata.\nThe metadata object associated with the checkpoint being loaded.\nMetadata\nCalls to indicates a brand new checkpoint read is going to happen.\nA checkpoint_id may be present if users set the checkpoint_id for\nthis checkpoint read. The meaning of the checkpiont_id is\nstorage-dependent. It can be a path to a folder/file or a key for\na key-value storage.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is more like a key-value store.\n(Default:None)\nNone\nInitialize this instance.\nmetadata(Metadata) \u2013 The metadata schema to use.\nis_coordinator(bool) \u2013 Whether this instance is responsible for coordinating\nthe checkpoint.\nCheck if the given checkpoint_id is supported by the storage. This allow\nus to enable automatic storage selection.\nbool\nInterface used bysave_state_dictto write to storage.\nsave_state_dict\nOne StorageWriter instance acts as both the coordinator and the follower\nin a distributed checkpoint. As part of initialization, each instance\nis told its role.\nA subclass should expect the following sequence of calls.\n(all ranks) set checkpoint_id if users pass a valid checkpoint_id.\n(all ranks) set_up_storage_writer()\n(all ranks) prepare_local_plan()\n(coordinator) prepare_global_plan()\n(all ranks) write_data()\n(coordinator) finish()\nWrite the metadata and marks the current checkpoint as successful.\nThe actual format/schema used for serializingmetadatais an\nimplementation detail. The only requirement is that it\u2019s recoverable\nin to the same object graph.\nmetadata(Metadata) \u2013 metadata for the new checkpoint\nresults(list[list[torch.distributed.checkpoint.storage.WriteResult]]) \u2013 A list of WriteResults from all ranks.\nNone\nNone\nPerform centralized planning of storage.\nThis method is only called on the coordinator instance.\nWhile this method can produce a completely different plan, the preferred\nway is to store storage specific data in SavePlan::storage_data.\nplans(list[torch.distributed.checkpoint.planner.SavePlan]) \u2013 A list ofSavePlaninstances, one for each rank.\nSavePlan\nA list of transformedSavePlanafter storage global planning\nSavePlan\nlist[torch.distributed.checkpoint.planner.SavePlan]\nPerform storage-specific local planning.\nWhile this method can produce a completely different plan, the recommended\nway is to store storage specific data in SavePlan::storage_data.\nplan(SavePlan) \u2013 The local plan from theSavePlannerin use.\nSavePlanner\nA transformedSavePlanafter storage local planning\nSavePlan\nSavePlan\nCalls to indicates a brand new checkpoint write is going to happen.\nA checkpoint_id may be present if users set the checkpoint_id for\nthis checkpoint write. The meaning of the checkpiont_id is\nstorage-dependent. It can be a path to a folder/file or a key for\na key-value storage.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default:None)\nNone\nInitialize this instance.\nis_coordinator(bool) \u2013 Whether this instance is responsible for coordinating\nthe checkpoint.\nReturn the storage-specific metadata. This is used to store additional information\nin a checkpoint that can be useful for providing request-level observability. StorageMeta\nis passed to theSavePlannerduring save calls. Returns None by default.\nSavePlanner\nTODO: provide an example\nOptional[StorageMeta]\nCheck if the given checkpoint_id is supported by the storage. This allow\nus to enable automatic storage selection.\nbool\nWrite all items fromplanusingplannerto resolve the data.\nplan\nplanner\nA subclass should callSavePlanner::resolve_dataon each item\nfrom the plan to get access to the underlying object to write.\nSavePlanner::resolve_data\nSubclasses should lazily callresolve_dataas it can allocate memory.\nIn case of tensors, make following assumptions:\nThey might be on any device, including not matching the one onWriteItem::tensor_data\nWriteItem::tensor_data\nThey might be views or not contiguous. Only the projection needs to be saved.\nplan(SavePlan) \u2013 The save plan to execute.\nplanner(SavePlanner) \u2013 Planner object to be used to resolve items to data.\nA future that completes to a list of WriteResult\nFuture[list[torch.distributed.checkpoint.storage.WriteResult]]\nThe following types define the planner interface used during checkpoint:\nAbstract class defining the protocol used by load_state_dict to plan the load process.\nLoadPlanner are stateful objects that can be used to customize the whole load process.\nLoadPlanner acts as an access proxy to the state_dict, so any transformation done to it\nwill be visible to the whole process.\nA planner subclass can expect the following sequence of calls during load_state_dict:\nSignals the start of loading a checkpoint.\nProcess the state_dict and produces aLoadPlanthat will be sent for global planning.\nTakes the LoadPlan from all ranks and make any global decision.\nThis is called once per non-tensor value in state_dict.\nThey are called in pair for each Tensor value in state_dict.\nUsers are recommended to extend DefaultLoadPlanner instead of this interface directly as\nmost changes can be expressed by changes in a single method.\nThere are two usual patterns of extension:\nRewriting state_dict. This is the simplest way to extend the load process as it\ndoesn\u2019t requite understanding the intrincacies of how LoadPlan works. We need\nto keep a reference to the original state_dict as load happens in place so\nwe need to be able to perform it in place\n\n```python\n>>> class RenamePlanner(DefaultLoadPlanner):\n>>>     def set_up_planner(\n>>>         self,\n>>>         state_dict: STATE_DICT_TYPE,\n>>>         metadata: Metadata,\n>>>         is_coordinator: bool,\n>>>     ) -> None:\n>>>         self.original_state_dict = state_dict\n>>>         state_dict = {\"foo_\" + k: v for k, v in state_dict.items()}\n>>>\n>>>         if self.flatten_sharded_tensors:\n>>>             state_dict = _flatten_sharded_tensors(state_dict)\n>>>\n>>>         if self.flatten_state_dict:\n>>>             state_dict, self.mappings = flatten_state_dict(state_dict)\n>>>\n>>>         self.state_dict = state_dict\n>>>         self.metadata = metadata\n>>>         self.is_coordinator = is_coordinator\n>>>\n>>>     def load_bytes(self, read_item, value):\n>>> # Remove the \"foo_\" prefix\n>>>         self.original_state_dict[read_item.dest_index.fqn[4:]] = torch.load(value, weights_only=False)\n\n```\n\nModifying resolve_tensor and commit_tensor to handle load time transformation.\n\n```python\n>>> class MetaModelMaterialize(DefaultSavePlanner):\n>>>     def resolve_tensor(self, read_item):\n>>>         tensor = super().resolve_tensor(read_item)\n>>>         return torch.empty_like(tensor, device=\"cpu\")\n>>>\n>>>     def commit_tensor(self, read_item, tensor):\n>>>         self.state_dict[read_item.dest_index.fqn] = tensor\n\n```\n\nCall once the StorageReader finished loading data intotensor.\ntensor\nThe provided tensor is the same one returned by the call toresolve_tensor.\nThis method is only needed if this LoadPlanner needs to post processtensorprior to\ncopying it back to the one in the state_dict.\nresolve_tensor\ntensor\nThe contents of tensor will follow its device synchronization model.\nCompute the global load plan and return plans for each rank.\n. N.B. This is called on the coordinator rank only\nlist[torch.distributed.checkpoint.planner.LoadPlan]\nCreate a LoadPlan based on state_dict and metadata provided by set_up_planner.\n. N.B. This is called on every rank.\nLoadPlan\nAccept the plan from coordinator and return final LoadPlan.\nLoadPlan\nLoad the item described byread_item``and``value.\nread_item``and``value\nThis method is expected to modify in-place the underlying state_dict.\nThe contents ofvalueare defined by the SavePlanner used to produce\nthe checkpoint being loaded.\nvalue\nReturn the BytesIO to be used by the StorageReader to loadread_item.\nThe BytesIO should alias with one on the underlying state_dict as StorageReader will replace its contents.\nBytesIO\nReturn the tensor described byread_itemto be used by the StorageReader to loadread_item.\nread_item\nThe tensor should alias with one on the underlying state_dict as StorageReader will replace its contents.\nIf, for any reason, that\u2019s not possible, the planner can use thecommit_tensormethod to copy the data\nback to the one in state_dict.\ncommit_tensor\nTensor\nInitialize this instance to load data intostate_dict.\nstate_dict\n. N.B. This is called on every rank.\nAbstract class defining the protocol used by save_state_dict to plan the save process.\nSavePlanners are stateful objects that can be used to customize the whole save process.\nSavePlanner acts as an access proxy to the state_dict, so any transformation done to it\nwill be visible to the whole process.\nA planner subclass can expect the following sequence of calls during save_state_dict:\nSignals the start of a checkpoint save.\nProcess the state_dict and produces aSavePlanthat will be sent for global planning.\nTakes the SavePlan from all ranks and make any global decision.\nThis gives each rank a chance to adjust to global planning decisions.\nLookups a value on thestate_dictfor the storage layer to write.\nUsers are recommended to extend DefaultSavePlanner instead of this interface directly as\nmost changes can be expressed by changes in a single method.\nThere are 3 usual patterns of extension:\nRewriting state_dict. This is the simplest way to extend the save process as it\ndoesn\u2019t requite understanding the intrincacies of how SavePlan works:\n\n```python\n>>> class RenamePlanner(DefaultSavePlanner):\n>>>     def set_up_planner(\n>>>         self,\n>>>         state_dict: STATE_DICT_TYPE,\n>>>         storage_meta: Optional[StorageMeta],\n>>>         is_coordinator: bool,\n>>>     ) -> None:\n>>> # prefix all keys with `foo_``\n>>>         super().set_up_planner({\"foo_\" + k: v for k, v in state_dict.items()}, storage_meta, is_coordinator)\n\n```\n\nModifying local plan and lookup in tandem. This is useful when fine control of how data is persisted\n\n```python\n>>> class FP16Planner(DefaultSavePlanner):\n>>>     def create_local_plan(self):\n>>>         plan = super().create_local_plan()\n>>>         for p in plan:\n>>>             if p.tensor_data is not None:\n>>>                 p.tensor_data.properties.dtype = torch.float16\n>>>         return plan\n>>>\n>>>     def resolve_data(self, write_item):\n>>>         item = super().resolve_data(write_item)\n>>>         return item if write_item.type == WriteItemType.BYTE_IO else item.to(torch.float16)\n\n```\n\nUsing the global planning step to make central decisions that can\u2019t be made individually by each rank\n\n```python\n>>> from itertools import zip_longest\n>>> from dataclasses import replace\n>>> class DDPLoadBalancingPlanner(DefaultSavePlanner):\n>>> # This uses the default local plan behavior of having all non-sharded writes in rank 0\n>>> # This sample doesn't handle ShardedTensors\n>>>     def create_global_plan(self, all_plans):\n>>>         iters = [iter(all_plans[0].items)] * len(all_plans)\n>>>         items_per_rank = [\n>>>             [item for item in items if item is not None]\n>>>             for items in zip(*zip_longest(*iters), strict=True)\n>>>         ]\n>>>         all_plans = [\n>>>             replace(plan, items=items)\n>>>             for plan, items in zip(all_plans, items_per_rank, strict=True)\n>>>         ]\n>>>         return super().create_global_plan(all_plans)\n\n```\n\nFinally, some planners need to save additional metadata in the checkpoint, this is\naccomplished by having each rank contribute their data items in the local plan and\nthe global planner aggregate them:\n\n```python\n>>> class SaveExtraDataPlanner(DefaultSavePlanner):\n>>>     def create_local_plan(self) -> SavePlan:\n>>>         plan = super().create_local_plan()\n>>>         return replace(plan, planner_data=\"per-rank-data\")\n>>>\n>>>     def create_global_plan(self, all_plans: List[SavePlan]) -> Tuple[List[SavePlan], Metadata]:\n>>>         global_plan, metadata = super().create_global_plan(all_plans)\n>>>         merged_data = [p.planner_data for p in global_plan]\n>>>         metadata = replace(metadata, planner_data=merged_data)\n>>>         return global_plan, metadata\n\n```\n\nCompute the global checkpoint plan and return the local plan of each rank.\nThis is called on the coordinator rank only.\ntuple[list[torch.distributed.checkpoint.planner.SavePlan], torch.distributed.checkpoint.metadata.Metadata]\nCompute the save plan for the current rank.\nThis will be aggregated and passed to create_global_plan.\nPlanner specific data can be passed through SavePlan::planner_data.\nThis is called on all ranks.\nSavePlan\nMerge the plan created bycreate_local_planand the result ofcreate_global_plan.\nThis is called on all ranks.\nSavePlan\nTransform and preparewrite_itemfromstate_dictfor storage, ensuring idempotency and thread-safety.\nwrite_item\nstate_dict\nLookup the object associated withwrite_iteminstate_dictand apply any\ntransformation (such as serialization) prior to the storage layer consuming it.\nwrite_item\nstate_dict\nCalled on each rank multiple times, at least once per WriteItem in the final SavePlan.\nThis method should be idempotent and thread-save. StorageWriter implementations\nare free to call it as frequently as they need.\nAny transformation that allocates memory should be lazily done when his method\nis called in order to reduce peak memory required by checkpointing.\nWhen returning tensors, they can be on any device or format, they can be views too.\nIt\u2019s the storage layer responsibility to figure out how to save them.\nUnion[Tensor,BytesIO]\nInitialize this planner to savestate_dict.\nstate_dict\nImplementations should save those values as they won\u2019t be provided lated in the save process.\nThis is called on all ranks.\nDataclass which holds information about what needs to be written to storage.\nCalculates the storage size of the underlying tensor, or None if this is not a tensor write.\nOptional[int] storage size, in bytes of underlying tensor if any.\nOptional[int]\nWe provide a filesystem based storage layer:\nreturn the checkpoint_id that will be used to load the checkpoint.\nBasic implementation of StorageWriter using file IO.\nThis implementation makes the following assumptions and simplifications:\nThe checkpoint path is an empty or non-existing directory.\nFile creation is atomic\nThe checkpoint consist of one file per write request plus\na global.metadatafile with the serialized metadata if rank coordination is enabled.\na rank local__{rank}.metadatafile with the serialized metadata if rank coordination is NOT enabled.\nOverride of AsyncStager.stage\ndict[str,Union[~StatefulT,Any]]\nWe also provide other storage layers, including ones to interact with HuggingFace safetensors:\n.. autoclass:: torch.distributed.checkpoint.HuggingFaceStorageReader\n:members:\n.. autoclass:: torch.distributed.checkpoint.HuggingFaceStorageWriter\n:members:\n.. autoclass:: torch.distributed.checkpoint.QuantizedHuggingFaceStorageReader\n:members:\nWe provide default implementations ofLoadPlannerandSavePlannerthat\ncan handle all of torch.distributed constructs such as FSDP, DDP, ShardedTensor and DistributedTensor.\nLoadPlanner\nSavePlanner\nExtension from the planner interface to make it easy to extend the default planner.\nAny\nExtension from the planner interface to make it easy to extend the default planner.\nDefaultLoadPlanner that adds multiple features on top of LoadPlanner.\nIn particular it adds the following:\nflatten_state_dict: Handle state_dict with nested dicts\nflatten_sharded_tensors: For FSDP in 2D parallel mode\nallow_partial_load: If False, will raise a runtime error if a key is present in state_dict, but not in the checkpoint.\nExtension from the planner interface to make it easy to extend the default planner.\nTensor\nExtension from the planner interface to make it easy to extend the default planner.\nDue to legacy design decisions, the state dictionaries ofFSDPandDDPmay have different keys or fully qualified names (e.g., layer1.weight) even when the original unparallelized model is identical. Moreover,FSDPoffers various types of model state dictionaries, such as full and sharded state dictionaries. Additionally, optimizer state dictionaries employ parameter IDs instead of fully qualified names to identify parameters, potentially causing issues when parallelisms are used (e.g., pipeline parallelism).\nFSDP\nDDP\nFSDP\nTo tackle these challenges, we offer a collection of APIs for users to easily manage state_dicts.get_model_state_dict()returns a model state dictionary with keys consistent with those returned by the unparallelized model state dictionary. Similarly,get_optimizer_state_dict()provides the optimizer state dictionary with keys uniform across all parallelisms applied. To achieve this consistency,get_optimizer_state_dict()converts parameter IDs to fully qualified names identical to those found in the unparallelized model state dictionary.\nget_model_state_dict()\nget_optimizer_state_dict()\nget_optimizer_state_dict()\nNote that results returned by these APIs can be used directly with thetorch.distributed.checkpoint.save()andtorch.distributed.checkpoint.load()methods without requiring any additional conversions.\ntorch.distributed.checkpoint.save()\ntorch.distributed.checkpoint.load()\nset_model_state_dict()andset_optimizer_state_dict()are provided to load the model and optimizer state_dict generated by by their respective getter APIs.\nset_model_state_dict()\nset_optimizer_state_dict()\nNote thatset_optimizer_state_dict()can only be called beforebackward()or afterstep()is called on optimizers.\nset_optimizer_state_dict()\nbackward()\nstep()\nNote that this feature is experimental, and API signatures might change in the future.\nReturn the model state_dict and optimizers state_dict.\nget_state_dictcan process any module that is parallelized by PyTorch\nFSDP/fully_shard, DDP/replicate, tensor_parallel/parallelize_module, and any\ncombination of these parallelisms. The main functions ofget_state_dictare: 1.) returning a model and optimizer state_dict that can be resharded\nwith a different number of trainers and/or different parallelisms.\n2.) hiding the parallelism-specific state_dict APIs. Users don\u2019t have to call\nthese APIs.\n3.) sanity checking the result state_dict.\nget_state_dict\nget_state_dict\nThe keys of the result state dictionary are the canonical FQNs (Fully\nQualified Names).  A canonical FQN refers to the FQN based on a parameter\u2019s\nposition in an nn.Module hierarchy. More specifically, a canonical FQN to a\nparameter is the FQN returned bymodule.named_parameters()ormodule.named_buffers()when the module is not distributed by any\nparallelisms. Since the optimizer internally uses parameter IDs to represent\na parameter, there will be a conversion from the parameter IDs to the\ncanonical FQNs when calling this API.\nmodule.named_parameters()\nmodule.named_buffers()\nget_state_dictcan also process a module that is not parallelized. In\nsuch a case,get_state_dictonly performs one function \u2013 converting the\noptimizer parameter IDs to the canonical FQNs.\nget_state_dict\nget_state_dict\nExample\n\n```python\n>>> import torch\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n>>> from torch.distributed.checkpoint.state_dict import get_state_dict\n\n```\n\n\n```python\n>>> fsdp_model = FSDP(copy.deepcopy(model))\n>>> fsdp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n>>> ddp_model = DDP(copy.deepcopy(model))\n>>> ddp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n```\n\n\n```python\n>>> ddp_state_dict, ddp_optim_state_dict = get_state_dict(ddp_model, ddp_optim)\n>>> fsdp_state_dict, fsdp_optim_state_dict = get_state_dict(\n...     fsdp_model, fsdp_optim\n... )\n\n```\n\n\n```python\n>>> # if we simply call ddp_model.state_dict() and fsdp_model.state_dict(),\n>>> # the asserts will fail.\n>>> assert ddp_state_dict == fsdp_state_dict\n>>> assert ddp_optim_state == fsdp_optim_state_dict\n\n```\n\nmodel(nn.Module) \u2013 the nn.Module to the model.\noptimizers(Union[None,Optimizer,Iterable[Optimizer]]) \u2013 The optimizers that are used to optimizemodel.\nmodel\nsubmodules(deprecated) \u2013 Optional[set[nn.Module]]: only return the model parameters\nthat belong to the submodules.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be returned. SeeStateDictOptionsfor the details.\nTuplethat contain model state_dict and optimizer state_dict.\nTuple\nTuple[Dict[str, ValueType], OptimizerStateType]\nReturn the model state_dict ofmodel.\nmodel\nSeeget_state_dictfor the detail usage.\nget_state_dict\nmodel(nn.Module) \u2013 the nn.Module to the model.\nsubmodules(deprecated) \u2013 Optional[set[nn.Module]]: only return the model parameters\nthat belong to the submodules.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be returned. SeeStateDictOptionsfor the details.\nThe state_dict formodel.\nmodel\nDict[str, ValueType]\nReturn the combined state_dict for optimizers.\nSeeget_state_dictfor the detail usage.\nget_state_dict\nmodel(nn.Module) \u2013 the nn.Module to the model.\noptimizers(Union[None,Optimizer,Iterable[Optimizer]]) \u2013 The optimizers that are used to optimizemodel.\nmodel\nsubmodules(deprecated) \u2013 Optional[set[nn.Module]]: only return the model parameters\nthat belong to the submodules.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be returned. SeeStateDictOptionsfor the details.\nThe state_dict foroptimizers.\noptimizers\nOptimizerStateType\nLoad the model state_dict and optimizers state_dict.\nThe counterpart ofget_state_dictto set the state_dict to the model and\noptimizers.  The givenmodel_state_dictandoptim_state_dictdo not\nhave to be returned byget_state_dictbut must meet the following\nrequirements: 1) all FQNs are canonical FQNs as defined inget_state_dict,\n2) if a tensor is sharded, it must be either a ShardedTensor or DTensor,\n3) optimizer state_dict cannot contain the parameter IDs; the keys should be\nthe canonical FQNs.\nget_state_dict\nmodel_state_dict\noptim_state_dict\nget_state_dict\nget_state_dict\nset_state_dict\nbackward()\nstep()\nis called on the optimizers. Otherwise, the optimizer states won\u2019t be initialized\ncorrectly.\nmodel(nn.Module) \u2013 the nn.Module to the model.\noptimizers(Union[Optimizer,Iterable[Optimizer]]) \u2013 The optimizers that are used to optimizemodel.\nmodel\nmodel_state_dict(Dict[str,ValueType]) \u2013 (Union[Dict[nn.Module, Dict[str, ValueType]], Dict[str, ValueType]]):\nthe model state_dict to load. If the key of themodel_state_dictis nn.Module, the key is a submodule ofmodeland the value should\nbe the state_dict of the submodule. When loading the state_dict,\nthe prefix of the submodule will be append to the state_dict.\nmodel_state_dict\nmodel\noptim_state_dict(OptimizerStateType) \u2013 OptimizerStateType:\nthe optimizer state_dict to load.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be loaded. SeeStateDictOptionsfor the details.\nmissing_keysis a list of str containing the missing keys of the model state_dict.unexpected_keysis a list of str containing the unexpected keys of the model state_dict.\nmissing_keysis a list of str containing the missing keys of the model state_dict.\nunexpected_keysis a list of str containing the unexpected keys of the model state_dict.\nNamedTuplewithmissing_keysandunexpected_keysfields\nNamedTuple\nmissing_keys\nunexpected_keys\nLoad the model state_dict.\nThe counterpart ofget_model_state_dictto set the state_dict to the\nmodel. Seeset_state_dictfor the detail usage.\nget_model_state_dict\nset_state_dict\nmodel(nn.Module) \u2013 the nn.Module to the model.\nmodel_state_dict(Dict[str,ValueType]) \u2013 (Dict[str, ValueType]):\nthe model state_dict to load. If the key of themodel_state_dictis nn.Module, the key is a submodule ofmodeland the value should\nbe the state_dict of the submodule. When loading the state_dict,\nthe prefix of the submodule will be append to the state_dict.\nmodel_state_dict\nmodel\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be loaded. SeeStateDictOptionsfor the details.\nmissing_keysis a list of str containing the missing keysunexpected_keysis a list of str containing the unexpected keys\nmissing_keysis a list of str containing the missing keys\nunexpected_keysis a list of str containing the unexpected keys\nNamedTuplewithmissing_keysandunexpected_keysfields\nNamedTuple\nmissing_keys\nunexpected_keys\nLoad the optimizers state_dict.\nThe counterpart ofget_optimizer_state_dictto set the state_dict to the\noptimizers. Seeset_state_dictfor the detail usage.\nget_optimizer_state_dict\nset_state_dict\nset_optimizer_state_dict\nbackward()\nstep()is called on the optimizers. Otherwise, the optimizer states won\u2019t be\ninitialized correctly.\nstep()\nmodel(nn.Module) \u2013 the nn.Module to the model.\noptimizers(Union[Optimizer,Iterable[Optimizer]]) \u2013 The optimizers that are used to optimizemodel.\nmodel\noptim_state_dict(OptimizerStateType) \u2013 OptimizerStateType:\nthe optimizer state_dict to load.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be loaded. SeeStateDictOptionsfor the details.\nNone\nNone\nThis dataclass specifies how get_state_dict/set_state_dict will work.\nfull_state_dict: if this is set to True, all the tensors in the\nreturned state_dict will be gathered. No ShardedTensor and DTensor\nwill be in the returned state_dict.\nfull_state_dict\ncpu_offload: offload all the tensors to cpu. To prevent CPU OOM, iffull_state_dictis also true, then only the rank0 will get the\nstate_dict and all other ranks will get empty state_dict.\ncpu_offload\nfull_state_dict\nignore_frozen_params: if the value is True, the returned state_dict\nwon\u2019t contain any frozen parameters \u2013 therequires_gradis False.\nThe default value is False.\nignore_frozen_params\nrequires_grad\nkeep_submodule_prefixes(deprecated): whensubmodulesis not None, this option\nindicates whether to keep the submodule prefixes from the state_dict keys.\nor example, if the submodule ismodule.pretrainand the full FQN of\nthe parameter ispretrain.layer1.weightof the param. When this option\nis True, the parameter\u2019s key in the returned state_dict will bepretrain.layer1.weight. If the options is False, the key will belayer1.weight.\nNote that ifkeep_submodule_prefixesis False, there may be conflicted\nFQNs, hence there should be only one submodule insubmodules.\nkeep_submodule_prefixes\nsubmodules\nmodule.pretrain\npretrain.layer1.weight\npretrain.layer1.weight\nlayer1.weight\nkeep_submodule_prefixes\nsubmodules\nstrict: thestrictoption whenset_state_dictcalls\nmodel.load_state_dict().\nstrict\nstrict\nset_state_dict\nbroadcast_from_rank0\nfull state_dict and will broadcast the tensors in the state_dict/\noptim_state_dict one by one to other ranks. Other ranks will receive\nthe tensors and shard according to the local shards in the model and\noptimizer.full_state_dictmust be set to True when using this option.\nThis option currently only supports DTensor, not the legacy ShardedTensor.\nfull_state_dict\nFor users which are used to using and sharing models in thetorch.saveformat, the following methods are provided which provide offline utilities for converting betweeing formats.\ntorch.save\nGiven a directory containing a DCP checkpoint, this function will convert it into a\nTorch save file.\ndcp_checkpoint_dir(Union[str,PathLike]) \u2013 Directory containing the DCP checkpoint.\ntorch_save_path(Union[str,PathLike]) \u2013 Filename to store the converted Torch save file.\nWarning\nTo avoid OOM, it\u2019s recommended to only run this function on a single rank.\nGiven the location of a torch save file, converts it into a DCP checkpoint.\ntorch_save_path(Union[str,PathLike]) \u2013 Filename of the Torch save file.\ndcp_checkpoint_dir(Union[str,PathLike]) \u2013 Directory to store the DCP checkpoint.\nWarning\nTo avoid OOM, it\u2019s recommended to only run this function on a single rank.\nThe following classes can also be utilized for online loading and resharding of models from the torch.save format.\nStorageReader for reading a Torch Save file. This reader will read the entire checkpoint\non the coordinator rank, and then broadcast and shard each tensor to all ranks.\n. N.B. Intended to be used with DynamicMetaLoadPlanner\nWarning\nCurrent implementation only supports loading Tensors.\n\n```python\n>>> sd = {\"mode\": model}\n>>> dcp.load(\n>>>    sd,\n>>>    storage_reader=BroadcastingTorchSaveReader(),\n>>>    planner=DynamicMetaLoadPlanner(),\n>>>    checkpoint_id=\"path_to_model.pt\"\n>>> )\n\n```\n\nImplementation of the StorageReader method\nlist[torch.distributed.checkpoint.planner.LoadPlan]\nImplementation of the StorageReader method\nLoadPlan\nReads torch save data on the coordinator rank, and broadcast afterwards\nthis incurrs a communication cost, but avoids having to load\nthe entire checkpoint on each rank, hopefully preventing OOM issues\nFuture[None]\nExtends the default StorageReader to support building the metadata file\nMetadata\nImplementation of the StorageReader method\nImplementation of the StorageReader method\nImplementation of the StorageReader method\nbool\nExtension of DefaultLoadPlanner, which creates a new Metadata object based on the passed in state dict,\navoiding the need to read metadata from disk. This is useful when reading formats which don\u2019t have a\nmetadata file, like Torch Save files.\n. N.B. Intended to be used with BroadcastingTorchSaveReader\nWarning\nCurrent implementation only supports loading Tensors.\n\n```python\n>>> sd = {\"mode\": model}\n>>> dcp.load(\n>>>    sd,\n>>>    storage_reader=BroadcastingTorchSaveReader(),\n>>>    planner=DynamicMetaLoadPlanner(),\n>>>    checkpoint_id=\"path_to_model.pt\"\n>>> )\n\n```\n\nSetups of the planner, extnding default behavior by creating the Metadata object from the state dict\nThe following experimental interfaces are provided for improved observability in production environments:",
  "url": "https://pytorch.org/docs/stable/distributed.checkpoint.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "0fa1e5dc10e0108f482c834d1b2a3970",
  "source": "pytorch_docs",
  "title": "Profiling to understand torch.compile performance \u2014 PyTorch 2.9 documentation",
  "text": "\n## Profiling to understand torch.compile performance#\n\nCreated On: Jun 06, 2023 | Last Updated On: Jul 11, 2025\n\n## What to use torch.profiler for:#\n\ntorch.profiler is helpful for understanding the performance of your program at a kernel-level granularity - for example, it can show graph breaks and resources utilization at the level of the program. The data provided by the profiler can often help users understand where to investigate further to understand model performance.\nTo understand kernel-level performance, other tools exist, such asNvidia Nsight compute tool,AMD Omnitrace,  Intel\u00ae VTune\u2122 Profiler orinductor\u2019s profiling toolscan be used.\nSee also thegeneral pytorch profiler guide.\n\n## Basics of using torch.profiler and viewing traces#\n\nExample program: We\u2019ll use this example of profiling resnet18. Notice the following parts of this example program:\nInclude a warm-up run to wait for compilation to complete (this will warm up systems like the CUDA caching allocator)\nUsetorch.profiler.profile()context for profiling the section we are interested in\ntorch.profiler.profile()\nUseprof.export_chrome_trace(\"trace.json\")to export the profiling artifact.\nprof.export_chrome_trace(\"trace.json\")\n\n```python\n\n    import torch\n    from torchvision.models import resnet18\n\n    device = 'cuda'      # or 'cpu', 'xpu', etc.\n    model = resnet18().to(device)\n\n    inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]\n\n    model_c = torch.compile(model)\n\n    def fwd_bwd(inp):\n        out = model_c(inp)\n        out.sum().backward()\n\n    # warm up\n    fwd_bwd(inputs[0])\n\n    with torch.profiler.profile() as prof:\n        for i in range(1, 4):\n            fwd_bwd(inputs[i])\n            prof.step()\n\n    prof.export_chrome_trace(\"trace.json\")\n\n```\n\nViewing chrome traces: In the Chrome browser, open chrome://tracing and load the json file. Use the \u201cw\u201d and \u201cs\u201d keys to zoom in and out, and use \u201ca\u201d and \u201cd\u201d to scroll left and right. \u201c?\u201d will show a \u201chelp\u201d screen with a list of shortcuts.\nHere, we observe:\nCompiledFunction and CompiledFunctionBackward events, which correspond to the dynamo-compiled regions.\nCPU events at the top, and GPU events at the bottom.\nFlows between CPU and accelerator events\nEvery kernel on the accelerator occurs after being launched by code running on the CPU. The profiler can draw connections (i.e. \u201cflows\u201d) between the accelerator and CPU events to show which CPU event launched a accelerator kernel. This is particularly helpful because, with a few exceptions, accelerator kernels are launched asynchronously.\nTo view a flow connection, click on a GPU kernel and click \u201cac2g\u201d:\nAlternatively, turn onallflows with the \u201cFlow events\u201d dropdown at the top.\n\n## Working around CUDA Graph profiling issues#\n\nWhen CUDA graphs are enabled, some CUDA configurations (driver version under 525.85.12 or CUDA < 12)  can encounter issues between the profiling tools and CUDA graphs. To fix these issues, add an empty profiling context at the top of your program:\n\n```python\n\n    import torch\n\n    torch.profiler._utils._init_for_cuda_graphs()\n\n    # ... rest of program\n\n```\n\n\n## Understanding compilation time#\n\nTo understand why compilation is taking a long time, you can profile the first invocation of a torch.compile-ed program. Keep in mind that profile traces of compilations can be distorted more than typical profiling, because compilation workloads can be quite different from typical PyTorch workloads. In some cases, trace files may also be quite large. Traces > 1GB can be difficult to open with the chrome tracing tool.\nNote: roughly the same information can also be obtained in non-graphical format with :code:torch._dynamo.utils.compile_times(). This utility won\u2019t show when the compilation steps occur, but it will show the amount of time spent on each step - and times will not be affected by any profiling overhead.\ntorch._dynamo.utils.compile_times()\nSee an example below:\n\n```python\n\n    import torch\n    from torchvision.models import resnet18\n\n    # user can switch between cuda and xpu\n    device = 'cuda'\n    model = resnet18().to(device)\n    inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]\n\n    model_c = torch.compile(model)\n\n    def fwd_bwd(inp):\n        out = model_c(inp)\n        out.sum().backward()\n\n    def warmup_compile():\n        def fn(x):\n            return x.sin().relu()\n\n        x = torch.rand((2, 2), device=device, requires_grad=True)\n        fn_c = torch.compile(fn)\n        out = fn_c(x)\n        out.sum().backward()\n\n    with torch.profiler.profile() as prof:\n        with torch.profiler.record_function(\"warmup compile\"):\n            warmup_compile()\n\n        with torch.profiler.record_function(\"resnet18 compile\"):\n            fwd_bwd(inputs[0])\n\n    prof.export_chrome_trace(\"trace_compile.json\")\n\n```\n\nNote a few things:\nThe first invocation should occurduringprofiling in order to capture compilation\nAdd a warm-up compilation in order to initialize any systems that need to be lazily initialized.\n\n## Finding graph breaks: \u201cTorch-Compiled Region\u201d and \u201cCompiledFunction\u201d#\n\nAlthough there are logging tools for identifying graph breaks, the profiler provides a quick visual method of identifying :ref:graphbreaks<torch.compiler_graph_breaks>. There are two profiler events to look for:Torch-Compiled RegionandCompiledFunction.\ngraphbreaks<torch.compiler_graph_breaks>\nTorch-Compiled Region- which was introduced in PyTorch 2.2 - is a profiler event that covers the entire compiled region. Graph breaks almost always look the same: nested \u201cTorch-Compiled Region\u201d events. Starting in PyTorch 2.5, the profiler event will also contain the frame ID and the frame compile ID. The frame ID is a unique identifier for the frame, and the frame compile ID denotes how many times the frame has been compiled.\nIf you run two separate functions with torch.compile() applied independently on each of them, you should generally expect to see two adjacent (i.e NOT stacked/nested) Torch-Compiled regions. Meanwhile, if you encounter graph breaks (or disable()\u2019ed/skipped regions), expect nested \u201cTorch-Compiled Region\u201d events.\nCompiledFunction- introduced in PyTorch 2.0 - is a profiler event that appears when gradients are required for any inputs.  Each graph break will interrupt a CompiledFunction block, splitting it in two. CompiledFunction events only appear when Autograd is involved, i.e. some of the input tensors to the graph have requires_grad=True.\nWhen a CompiledFunction appears in a trace, it is typically paired with a CompiledFunctionBackward event in the backward pass. A \u201cfwd-bwd link\u201d should appear in the trace connecting the two, if the backward function is called.\nIf your use case includes a graph that doesn\u2019t require grad and doesn\u2019t include \u201cTorch-Compiled Region\u201d events, it can be more difficult to identify whether torch.compile is being applied correctly. One clue can be the existence of Inductor-generated Triton kernels.\nSee the synthetic example below for a demonstration:\n\n```python\n\n    import torch\n    import torch._dynamo\n    # user can switch between cuda and xpu\n    device = 'cuda'\n\n    class ModelWithBreaks(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            def create_sequential():\n                return torch.nn.Sequential(\n                    torch.nn.Linear(128, 128),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(128, 128),\n                    torch.nn.ReLU(),\n                )\n            self.mod1 = create_sequential()\n            self.mod2 = create_sequential()\n            self.mod3 = create_sequential()\n            self.mod4 = create_sequential()\n\n        def forward(self, inp):\n            mod1 = self.mod1(inp)\n            torch._dynamo.graph_break()\n            mod2 = self.mod2(mod1)\n            torch._dynamo.graph_break()\n            mod3 = self.mod3(mod2)\n            torch._dynamo.graph_break()\n            mod4 = self.mod4(mod3)\n            return mod4\n\n    model = ModelWithBreaks().to(device)\n    inputs = [torch.randn((128, 128), device=device) for _ in range(10)]\n\n    model_c = torch.compile(model)\n\n    def fwd_bwd(inp):\n        out = model_c(inp)\n        out.sum().backward()\n\n    # warm up\n    fwd_bwd(inputs[0])\n\n    with torch.profiler.profile() as prof:\n        for i in range(1, 4):\n            fwd_bwd(inputs[i])\n            prof.step()\n\n    prof.export_chrome_trace(\"trace_break.json\")\n\n```\n\n\n## Operator Kernels#\n\nWhen an operator is launched, we expect to see a few events:\nCPU-side event\nKernel launch (if dealing with a GPU kernel)\nGPU-side event\nInductor-generated Triton kernels:\nTheCPU-side eventshould appear as an event prefixed with \u201ctriton_\u201d. The events currently have minimal information - the kernel name and a launch, but less information than typical aten kernel launches (which contain input shapes, types, etc.).\nThekernel launchshould appear as cuLaunchKernel instead of cudaLaunchKernel (cudaLaunchKernel is typical for aten ops)\nTheGPU-side eventshould appear, and how descriptive the name will be depends on the inductor config for unique_kernel_names\nNon-Inductor generated Triton kernels:\nTheCPU-sideevent may not appear in traces; the machinery for automatically inserting a profiler event is currently implemented at the Inductor level, so Triton kernels that bypass Inductor may not appear in traces, unless users have annotated them manually\nThekernel launchshould appear s cuLaunchKernel instead of cudaLaunchKernel (cudaLaunchKernel is typical for aten ops)\nTheGPU-sideevent should appear, named similarly to the triton kernel that was authored.\nInductor-generated CPU kernels:\nTheCPU-side eventwill not appear in traces; we haven\u2019t added profiling for this yet.\nThekernel launchandGPU-side eventsdon\u2019t exist\nNon-Triton kernels(i.e. aten kernels or custom ops) should also be expected to sometimes appear in traces. Sometimes, Inductor will fall back to the original op implementation, in which case you will see a call to the aten op.\n\n## Launch overhead#\n\nOne common issue is bad GPU utilization. A quick way to identify this is if there are large gaps between kernels on the GPU:\nThis is often the result of CPU overhead, e.g. if the amount of time spent on the CPU between kernel launches is larger than the amount of time spent by the GPU to process the kernels. The issue is more common for small batch sizes.\nWhen using inductor, enabling CUDA graphs can often help improve performance when launch overhead is a concern.",
  "url": "https://pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
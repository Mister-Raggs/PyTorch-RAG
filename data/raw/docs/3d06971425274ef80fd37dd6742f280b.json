{
  "doc_id": "3d06971425274ef80fd37dd6742f280b",
  "source": "pytorch_docs",
  "title": "Non-strict Tracing Programming Model \u2014 PyTorch 2.9 documentation",
  "text": "\n## Non-strict Tracing Programming Model#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nSummary:\nNon-strict tracingis a way to trace Python code that is less strict than Dynamo, but may result in silent incorrectness.\nNon-strict tracing runs a Python function and uses Python and PyTorch\u2019s operator overloading capabilities to record what Tensor operations occurred during execution into a trace.\nA function isnon-strict traceableif it complies with some constraints, namely, that the function ispureand does not directly manipulate Tensor.data_ptr().\nNon-strict tracing mayspecializeon certain variables and treat them asconstants, baking the values of the variables into the trace.\ntorch.compileinternals (make_fx, AOTDispatcher) usenon-strict tracing.torch._dynamo.nonstrict_tracecan also be used intorch.compiled code to mark sections of code to be traced with non-strict tracing.\nNon-strict tracing runs a Python function and uses Python and PyTorch\u2019s operator overloading capabilities to record what Tensor operations occurred during execution into a trace.\ntorch.compile\nmake_fx\ntorch._dynamo.nonstrict_trace\ntorch.compile\nmake_fxis the main entrypoint for non-strict tracing. For the following function, only the top branch is taken during execution of the inputs, so it captures a graph with only that branch.\nmake_fx\n\n```python\nfrom torch.fx.experimental.proxy_tensor import make_fx\ndef f(x):\n    if x.shape[0] > 2:\n        return x ** 2 / 6\n    else:\n        return x * 3\nx = torch.randn(3)\ngm = make_fx(f, tracing_mode=\"fake\")(x)\ngm.print_readable()\n\n```\n\n\n```python\nclass f(torch.nn.Module):\n    def forward(self, x_1: \"f32[3]\"):\n        # No stacktrace found for following nodes\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\n        return div\n        \n\n```\n\n\n```python\n'class f(torch.nn.Module):\\n    def forward(self, x_1: \"f32[3]\"):\\n        # No stacktrace found for following nodes\\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\\n        return div\\n        '\n\n```\n\nNon-strict tracing differs from Dynamo (strict) tracing in thatit is unsafe, that is, given a function, it captures a graph of Tensor operations that may have different semantics than the original function.\nGiven a Python function, Dynamo Tracing captures a graph of Tensor operations and residual bytecode that when combined give the same semantics as the Python function.\n\n## Pure Functions#\n\nNon-strict tracing is sound only onpure functions, and thus only pure functions should be non-strict traced.\nA pure function is a function with the following properties:\nDeterminism.Given the same inputs, the pure function will always return the same output.\nNo side effects.A pure function does not have any side effects such as modifying external state or performing I/O operations.\nExplicit input/output.All the input data must be passed through the function parameters and all of the outputs are returned from the function.\nHere are some examples of impure functions for which the captured graph behaves differently from the original function.\n\n## Example 1: No explicit input (e.g. accesses global tensor)#\n\n\n```python\nvar = torch.tensor(1)\ndef function_with_global_access(y):\n    return y + var\nx = torch.tensor([0, 1, 2])\n# _allow_non_fake_inputs=True is needed to capture the global variable\n# for demonstration purposes.\ngm = make_fx(\n    function_with_global_access, tracing_mode=\"fake\", _allow_non_fake_inputs=True\n)(x)\n# Non-strict Tracing captures the value of the global (1.)\nprint(\"1. call function\", function_with_global_access(x))\nprint(\"1. call graph\", gm(x))\n# However, after changing the global, the captured graph\n# produces a different result from the original function\nvar = torch.tensor(2)\nprint(\"2. call function\", function_with_global_access(x))\nprint(\"2. call graph\", gm(x))\n# To capture a graph that can have a varying `var` tensor,\n# it must be an explicit input:\ndef function_fixed(y, var):\n    return y + var\nvar = torch.tensor(3)\ngm = make_fx(function_fixed, tracing_mode=\"fake\")(x, var)\nprint(\"3. call function\", function_fixed(x, var))\nprint(\"3. call graph\", gm(x, var))\nvar = torch.tensor(4)\nprint(\"4. call function\", function_fixed(x, var))\nprint(\"4. call graph\", gm(x, var))\n\n```\n\n\n```python\n1. call function tensor([1, 2, 3])\n1. call graph tensor([1, 2, 3])\n2. call function tensor([2, 3, 4])\n2. call graph tensor([1, 2, 3])\n3. call function tensor([3, 4, 5])\n3. call graph tensor([3, 4, 5])\n4. call function tensor([4, 5, 6])\n4. call graph tensor([4, 5, 6])\n\n```\n\nSeeSpecialization and Constantsfor an explanation of why.\n\n## Example 2: Side effect (printing)#\n\n\n```python\ndef function_with_side_effect(y):\n    print(y)\nx = torch.tensor([0, 1, 2])\n_ = function_with_side_effect(x)\n\n```\n\n\n```python\ntensor([0, 1, 2])\n\n```\n\nRunningfin Python prints a Tensor as a side effect.\nf\n\n```python\ngm = make_fx(function_with_side_effect, tracing_mode=\"fake\")(x)\n\n```\n\n\n```python\nFakeTensor(..., size=(3,), dtype=torch.int64)\n\n```\n\nDuring non-strict tracing, this print occurs during the graph capture.\n\n```python\n_ = gm(x)\n\n```\n\nThe graph does not store a call to theprintstatement, so executing the graph doesn\u2019t print anything.\nprint\n\n## Example 3: Side effect (input list mutation)#\n\n\n```python\nlst = []\ndef function_with_input_list_mutation(lst):\n    val = lst.pop()\n    return val\nx = torch.tensor([0, 1, 2])\ny = torch.tensor([0, 1, 2])\n# Each time the function is executed, the list shrinks in size\nlst = [x, y]\nfunction_with_input_list_mutation(lst)\nprint(\"len(lst) after one call\", len(lst))\nfunction_with_input_list_mutation(lst)\nprint(\"len(lst) after two calls\", len(lst))\n# With Non-strict Tracing, the length of the list shrinks during\n# the graph capture but not in invocations of the graph.\nlst = [x, y]\ngm = make_fx(function_with_input_list_mutation, tracing_mode=\"fake\")(lst)\nprint(\"len(lst) after graph capture\", len(lst))\ngm(lst)\nprint(\"len(lst) after one call to graph\", len(lst))\ngm(lst)\nprint(\"len(lst) after two calls to graph\", len(lst))\n\n```\n\n\n```python\nlen(lst) after one call 1\nlen(lst) after two calls 0\nlen(lst) after graph capture 2\nlen(lst) after one call to graph 2\nlen(lst) after two calls to graph 2\n\n```\n\n\n## No direct data_ptr manipulation#\n\nDirectly manipulatingTensor.data_ptris not non-strict traceable. The intuition behind this is that PyTorch is unable to tellhowyou manipulated thedata_ptr.\nTensor.data_ptr\ndata_ptr\n\n```python\nimport ctypes\n# Create a tensor with a single element\ntensor = torch.tensor([42], dtype=torch.int32)  # Using int32 for simplicity\ndef function_with_data_ptr(tensor):\n    # Get the data pointer\n    ptr = tensor.data_ptr()\n    # Cast the pointer to a ctypes pointer\n    ctypes_ptr = ctypes.cast(ptr, ctypes.POINTER(ctypes.c_int32))\n    # Increment the value at the pointer\n    ctypes_ptr.contents.value += 1\n    return tensor\ntry:\n    make_fx(function_with_data_ptr, tracing_mode=\"fake\")(tensor)\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nCannot access data pointer of Tensor (e.g. FakeTensor, FunctionalTensor). If you're using torch.compile/export/fx, it is likely that we are erroneously tracing into a custom kernel. To fix this, please wrap the custom kernel into an opaque custom op. Please see the following for details: https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html\n\n```\n\n\n## Specialization and Constants#\n\nNon-strict tracing captures a graph that may be specialized on some values. What this means is the captured graph is only valid for these values. We say the graph treats those values asconstant.\nAll non-Tensor variables are treated as constant during Non-strict Tracing:\n\n```python\ndef f(x, y):\n    return x + y\nx = torch.tensor([0, 1, 2])\ny = 3.14\ngm = make_fx(f, tracing_mode=\"fake\")(x, y)\ngm.print_readable()\n\n```\n\n\n```python\nclass f(torch.nn.Module):\n    def forward(self, x_1: \"i64[3]\", y_1):\n        # No stacktrace found for following nodes\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\n        return add\n        \n\n```\n\n\n```python\n'class f(torch.nn.Module):\\n    def forward(self, x_1: \"i64[3]\", y_1):\\n        # No stacktrace found for following nodes\\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\\n        return add\\n        '\n\n```\n\n3.14 is a constant in the graph.\nNon-strict tracing will also specialize on properties of the input Tensors.\n\n```python\ndef f(x):\n    if x.shape[0] > 2:\n        return x ** 2 / 6\n    else:\n        return x * 3\nx = torch.randn(3)\ngm = make_fx(f, tracing_mode=\"fake\")(x)\ngm.print_readable()\n\n```\n\n\n```python\nclass f(torch.nn.Module):\n    def forward(self, x_1: \"f32[3]\"):\n        # No stacktrace found for following nodes\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\n        return div\n        \n\n```\n\n\n```python\n'class f(torch.nn.Module):\\n    def forward(self, x_1: \"f32[3]\"):\\n        # No stacktrace found for following nodes\\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\\n        return div\\n        '\n\n```\n\nAnd it will also specialize on any variables not directly passed into the function:\n\n```python\nvar = torch.tensor(1)\ndef f(x):\n    return x + y\nx = torch.randn(3)\ngm = make_fx(f, tracing_mode=\"fake\")(x)\ngm.print_readable()\n\n```\n\n\n```python\nclass f(torch.nn.Module):\n    def forward(self, x_1: \"f32[3]\"):\n        # No stacktrace found for following nodes\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\n        return add\n        \n\n```\n\n\n```python\n'class f(torch.nn.Module):\\n    def forward(self, x_1: \"f32[3]\"):\\n        # No stacktrace found for following nodes\\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\\n        return add\\n        '\n\n```\n",
  "url": "https://pytorch.org/docs/stable/compile/programming_model.non_strict_tracing_model.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
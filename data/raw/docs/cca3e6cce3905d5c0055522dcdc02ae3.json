{
  "doc_id": "cca3e6cce3905d5c0055522dcdc02ae3",
  "source": "pytorch_docs",
  "title": "Modules \u2014 PyTorch 2.9 documentation",
  "text": "\n## Modules#\n\nCreated On: Feb 04, 2021 | Last Updated On: Nov 08, 2024\nPyTorch uses modules to represent neural networks. Modules are:\nBuilding blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks.\nTightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update.\nEasy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more.\nThis note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well.\nA Simple Custom Module\nModules as Building Blocks\nNeural Network Training with Modules\nModule State\nModule Initialization\nModule Hooks\nAdvanced Features\nDistributed Training\nProfiling Performance\nImproving Performance with Quantization\nImproving Memory Usage with Pruning\nParametrizations\nTransforming Modules with FX\n\n## A Simple Custom Module#\n\nTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input.\nLinear\n\n```python\nimport torch\nfrom torch import nn\n\nclass MyLinear(nn.Module):\n  def __init__(self, in_features, out_features):\n    super().__init__()\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n    self.bias = nn.Parameter(torch.randn(out_features))\n\n  def forward(self, input):\n    return (input @ self.weight) + self.bias\n\n```\n\nThis simple module has the following fundamental characteristics of modules:\nIt inherits from the base Module class.All modules should subclassModulefor composability with other modules.\nModule\nIt defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless.\nweight\nbias\nParameter\nparameters()\nIt defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs.\nweight\n@\nbias\nforward()\nThis simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:\n\n```python\nm = MyLinear(4, 3)\nsample_input = torch.randn(4)\nm(sample_input)\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)\n\n```\n\nNote that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules.\nforward()\nbackward()\nThe full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name:\nparameters()\nnamed_parameters()\n\n```python\nfor parameter in m.named_parameters():\n  print(parameter)\n: ('weight', Parameter containing:\ntensor([[ 1.0597,  1.1796,  0.8247],\n        [-0.5080, -1.2635, -1.1045],\n        [ 0.0593,  0.2469, -1.4299],\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n('bias', Parameter containing:\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))\n\n```\n\nIn general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another.\n\n## Modules as Building Blocks#\n\nModules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules:\nSequential\n\n```python\nnet = nn.Sequential(\n  MyLinear(4, 3),\n  nn.ReLU(),\n  MyLinear(3, 1)\n)\n\nsample_input = torch.randn(4)\nnet(sample_input)\n: tensor([-0.6749], grad_fn=<AddBackward0>)\n\n```\n\nNote thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules with a single input and output.\nSequential\nMyLinear\nReLU\nMyLinear\nIn general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation.\nFor example, here\u2019s a simple neural network implemented as a custom module:\n\n```python\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l0 = MyLinear(4, 3)\n    self.l1 = MyLinear(3, 1)\n  def forward(self, x):\n    x = self.l0(x)\n    x = F.relu(x)\n    x = self.l1(x)\n    return x\n\n```\n\nThis module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children():\nl0\nl1\nforward()\nchildren()\nnamed_children()\n\n```python\nnet = Net()\nfor child in net.named_children():\n  print(child)\n: ('l0', MyLinear())\n('l1', MyLinear())\n\n```\n\nTo go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:\nmodules()\nnamed_modules()\n\n```python\nclass BigNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l1 = MyLinear(5, 4)\n    self.net = Net()\n  def forward(self, x):\n    return self.net(self.l1(x))\n\nbig_net = BigNet()\nfor module in big_net.named_modules():\n  print(module)\n: ('', BigNet(\n  (l1): MyLinear()\n  (net): Net(\n    (l0): MyLinear()\n    (l1): MyLinear()\n  )\n))\n('l1', MyLinear())\n('net', Net(\n  (l0): MyLinear()\n  (l1): MyLinear()\n))\n('net.l0', MyLinear())\n('net.l1', MyLinear())\n\n```\n\nSometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict:\nModuleList\nModuleDict\n\n```python\nclass DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n      x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu')\n\n```\n\nFor any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:\nparameters()\nnamed_parameters()\n\n```python\nfor parameter in dynamic_net.named_parameters():\n  print(parameter)\n: ('linears.0.weight', Parameter containing:\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\n        [-0.4446,  0.4628,  0.8774,  1.6848],\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n        [-0.0550,  1.5317,  1.1064, -0.5562],\n        [-0.4028, -0.6942,  1.5793, -1.0140],\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n        [-0.3526,  0.8756, -1.5847, -0.6016],\n        [-0.3269, -0.1608,  0.2897, -2.0829],\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n('final.weight', Parameter containing:\ntensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\n('final.bias', Parameter containing:\ntensor([0.3381], requires_grad=True))\n\n```\n\nIt\u2019s also easy to move all parameters to a different device or change their precision usingto():\nto()\n\n```python\n# Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n\n```\n\nMore generally, an arbitrary function can be applied to a module and its submodules recursively by\nusing theapply()function. For example, to apply custom initialization to parameters\nof a module and its submodules:\napply()\n\n```python\n# Define a function to initialize Linear weights.\n# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n@torch.no_grad()\ndef init_weights(m):\n  if isinstance(m, nn.Linear):\n    nn.init.xavier_normal_(m.weight)\n    m.bias.fill_(0.0)\n\n# Apply the function recursively on the module and its submodules.\ndynamic_net.apply(init_weights)\n\n```\n\nThese examples show how elaborate neural networks can be formed through module composition and conveniently\nmanipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch\nprovides a large library of performant modules within thetorch.nnnamespace that perform common neural\nnetwork operations like pooling, convolutions, loss functions, etc.\ntorch.nn\nIn the next section, we give a full example of training a neural network.\nFor more information, check out:\nLibrary of PyTorch-provided modules:torch.nn\nDefining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html\n\n## Neural Network Training with Modules#\n\nOnce a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:\ntorch.optim\n\n```python\n# Create the network (from previous section) and optimizer\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n# Run a sample training loop that \"teaches\" the network\n# to output the constant zero function\nfor _ in range(10000):\n  input = torch.randn(4)\n  output = net(input)\n  loss = torch.abs(output)\n  net.zero_grad()\n  loss.backward()\n  optimizer.step()\n\n# After training, switch the module to eval mode to do inference, compute performance metrics, etc.\n# (see discussion below for a description of training and evaluation modes)\n...\nnet.eval()\n...\n\n```\n\nIn this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present:\ntorch.abs()\nA network is created.\nAn optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it.\nacquires an input,\nruns the network,\ncomputes a loss,\nzeros the network\u2019s parameters\u2019 gradients,\ncalls loss.backward() to update the parameters\u2019 gradients,\ncalls optimizer.step() to apply the gradients to the parameters.\nAfter the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2018sweightparameter shows that its values are now much closer to 0 (as may be expected):\nl1\nweight\n\n```python\nprint(net.l1.weight)\n: Parameter containing:\ntensor([[-0.0013],\n        [ 0.0030],\n        [-0.0008]], requires_grad=True)\n\n```\n\nNote that the above process is done entirely while the network module is in \u201ctraining mode\u201d. Modules default to\ntraining mode and can be switched between training and evaluation modes usingtrain()andeval(). They can behave differently depending on which mode they are in. For example, theBatchNormmodule maintains a running mean and variance during training that are not updated\nwhen the module is in evaluation mode. In general, modules should be in training mode during training\nand only switched to evaluation mode for inference or evaluation. Below is an example of a custom module\nthat behaves differently between the two modes:\ntrain()\neval()\nBatchNorm\n\n```python\nclass ModalModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n  def forward(self, x):\n    if self.training:\n      # Add a constant only in training mode.\n      return x + 1.\n    else:\n      return x\n\n\nm = ModalModule()\nx = torch.randn(4)\n\nprint('training mode output: {}'.format(m(x)))\n: tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481])\n\nm.eval()\nprint('evaluation mode output: {}'.format(m(x)))\n: tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])\n\n```\n\nTraining neural networks can often be tricky. For more information, check out:\nUsing Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html.\nNeural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\nIntroduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n\n## Module State#\n\nIn the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d):\nstate_dict\n\n```python\n# Save the module\ntorch.save(net.state_dict(), 'net.pt')\n\n...\n\n# Load the module later on\nnew_net = Net()\nnew_net.load_state_dict(torch.load('net.pt'))\n: <All keys matched successfully>\n\n```\n\nA module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have:\nstate_dict\nParameters: learnable aspects of computation; contained within thestate_dict\nstate_dict\nBuffers: non-learnable aspects of computation\nPersistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading)\nstate_dict\nNon-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization)\nstate_dict\nAs a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this:\nstate_dict\nregister_buffer()\n\n```python\nclass RunningMean(nn.Module):\n  def __init__(self, num_features, momentum=0.9):\n    super().__init__()\n    self.momentum = momentum\n    self.register_buffer('mean', torch.zeros(num_features))\n  def forward(self, x):\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n    return self.mean\n\n```\n\nNow, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk:\nstate_dict\n\n```python\nm = RunningMean(4)\nfor _ in range(10):\n  input = torch.randn(4)\n  m(input)\n\nprint(m.state_dict())\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\n\n# Serialized form will contain the 'mean' tensor\ntorch.save(m.state_dict(), 'mean.pt')\n\nm_loaded = RunningMean(4)\nm_loaded.load_state_dict(torch.load('mean.pt'))\nassert(torch.all(m.mean == m_loaded.mean))\n\n```\n\nAs mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent:\nstate_dict\n\n```python\nself.register_buffer('unserialized_thing', torch.randn(5), persistent=False)\n\n```\n\nBoth persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto():\nto()\n\n```python\n# Moves all module parameters and buffers to the specified device / dtype\nm.to(device='cuda', dtype=torch.float64)\n\n```\n\nBuffers of a module can be iterated over usingbuffers()ornamed_buffers().\nbuffers()\nnamed_buffers()\n\n```python\nfor buffer in m.named_buffers():\n  print(buffer)\n\n```\n\nThe following class demonstrates the various ways of registering parameters and buffers within a module:\n\n```python\nclass StatefulModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n    # Setting a nn.Parameter as an attribute of the module automatically registers the tensor\n    # as a parameter of the module.\n    self.param1 = nn.Parameter(torch.randn(2))\n\n    # Alternative string-based way to register a parameter.\n    self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n\n    # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything\n    # except a parameter. \"None\" entries like this will not be present in the module's state_dict.\n    self.register_parameter('param3', None)\n\n    # Registers a list of parameters.\n    self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n\n    # Registers a dictionary of parameters.\n    self.param_dict = nn.ParameterDict({\n      'foo': nn.Parameter(torch.randn(3)),\n      'bar': nn.Parameter(torch.randn(4))\n    })\n\n    # Registers a persistent buffer (one that appears in the module's state_dict).\n    self.register_buffer('buffer1', torch.randn(4), persistent=True)\n\n    # Registers a non-persistent buffer (one that does not appear in the module's state_dict).\n    self.register_buffer('buffer2', torch.randn(5), persistent=False)\n\n    # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything\n    # except a buffer. \"None\" entries like this will not be present in the module's state_dict.\n    self.register_buffer('buffer3', None)\n\n    # Adding a submodule registers its parameters as parameters of the module.\n    self.linear = nn.Linear(2, 3)\n\nm = StatefulModule()\n\n# Save and load state_dict.\ntorch.save(m.state_dict(), 'state.pt')\nm_loaded = StatefulModule()\nm_loaded.load_state_dict(torch.load('state.pt'))\n\n# Note that non-persistent buffer \"buffer2\" and reserved attributes \"param3\" and \"buffer3\" do\n# not appear in the state_dict.\nprint(m_loaded.state_dict())\n: OrderedDict([('param1', tensor([-0.0322,  0.9066])),\n               ('param2', tensor([-0.4472,  0.1409,  0.4852])),\n               ('buffer1', tensor([ 0.6949, -0.1944,  1.2911, -2.1044])),\n               ('param_list.0', tensor([ 0.4202, -0.1953])),\n               ('param_list.1', tensor([ 1.5299, -0.8747])),\n               ('param_list.2', tensor([-1.6289,  1.4898])),\n               ('param_dict.bar', tensor([-0.6434,  1.5187,  0.0346, -0.4077])),\n               ('param_dict.foo', tensor([-0.0845, -1.4324,  0.7022])),\n               ('linear.weight', tensor([[-0.3915, -0.6176],\n                                         [ 0.6062, -0.5992],\n                                         [ 0.4452, -0.2843]])),\n               ('linear.bias', tensor([-0.3710, -0.0795, -0.3947]))])\n\n```\n\nFor more information, check out:\nSaving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html\nSerialization semantics:https://pytorch.org/docs/main/notes/serialization.html\nWhat is a state dict?https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n\n## Module Initialization#\n\nBy default, parameters and floating-point buffers for modules provided bytorch.nnare initialized during\nmodule instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to\nperform well historically for the module type. For certain use cases, it may be desired to initialize with a different\ndtype, device (e.g. GPU), or initialization technique.\ntorch.nn\nExamples:\n\n```python\n# Initialize module directly onto GPU.\nm = nn.Linear(5, 3, device='cuda')\n\n# Initialize module with 16-bit floating point parameters.\nm = nn.Linear(5, 3, dtype=torch.half)\n\n# Skip default parameter initialization and perform custom (e.g. orthogonal) initialization.\nm = torch.nn.utils.skip_init(nn.Linear, 5, 3)\nnn.init.orthogonal_(m.weight)\n\n```\n\nNote that the device and dtype options demonstrated above also apply to any floating-point buffers registered\nfor the module:\n\n```python\nm = nn.BatchNorm2d(3, dtype=torch.half)\nprint(m.running_mean)\n: tensor([0., 0., 0.], dtype=torch.float16)\n\n```\n\nWhile module writers can use any device or dtype to initialize parameters in their custom modules, good practice is\nto usedtype=torch.floatanddevice='cpu'by default as well. Optionally, you can provide full flexibility\nin these areas for your custom module by conforming to the convention demonstrated above that alltorch.nnmodules follow:\ndtype=torch.float\ndevice='cpu'\ntorch.nn\nProvide adeviceconstructor kwarg that applies to any parameters / buffers registered by the module.\ndevice\nProvide adtypeconstructor kwarg that applies to any parameters / floating-point buffers registered by\nthe module.\ndtype\nOnly use initialization functions (i.e. functions fromtorch.nn.init) on parameters and buffers within the\nmodule\u2019s constructor. Note that this is only required to useskip_init(); seethis pagefor an explanation.\ntorch.nn.init\nskip_init()\nFor more information, check out:\nSkipping module parameter initialization:https://pytorch.org/tutorials/prototype/skip_param_init.html\n\n## Module Hooks#\n\nInNeural Network Training with Modules, we demonstrated the training process for a module, which iteratively\nperforms forward and backward passes, updating module parameters each iteration. For more control\nover this process, PyTorch provides \u201chooks\u201d that can perform arbitrary computation during a forward or backward\npass, even modifying how the pass is done if desired. Some useful examples for this functionality include\ndebugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules\nyou haven\u2019t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules.\nPyTorch provides two types of hooks for modules:\nForward hooksare called during the forward pass. They can be installed for a given module withregister_forward_pre_hook()andregister_forward_hook().\nThese hooks will be called respectively just before the forward function is called and just after it is called.\nAlternatively, these hooks can be installed globally for all modules with the analogousregister_module_forward_pre_hook()andregister_module_forward_hook()functions.\nregister_forward_pre_hook()\nregister_forward_hook()\nregister_module_forward_pre_hook()\nregister_module_forward_hook()\nBackward hooksare called during the backward pass. They can be installed withregister_full_backward_pre_hook()andregister_full_backward_hook().\nThese hooks will be called when the backward for this Module has been computed.register_full_backward_pre_hook()will allow the user to access the gradients for outputs\nwhileregister_full_backward_hook()will allow the user to access the gradients\nboth the inputs and outputs. Alternatively, they can be installed globally for all modules withregister_module_full_backward_hook()andregister_module_full_backward_pre_hook().\nregister_full_backward_pre_hook()\nregister_full_backward_hook()\nregister_full_backward_pre_hook()\nregister_full_backward_hook()\nregister_module_full_backward_hook()\nregister_module_full_backward_pre_hook()\nAll hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function.\nforward()\nBelow is an example demonstrating usage of forward and backward hooks:\n\n```python\ntorch.manual_seed(1)\n\ndef forward_pre_hook(m, inputs):\n  # Allows for examination and modification of the input before the forward pass.\n  # Note that inputs are always wrapped in a tuple.\n  input = inputs[0]\n  return input + 1.\n\ndef forward_hook(m, inputs, output):\n  # Allows for examination of inputs / outputs and modification of the outputs\n  # after the forward pass. Note that inputs are always wrapped in a tuple while outputs\n  # are passed as-is.\n\n  # Residual computation a la ResNet.\n  return output + inputs[0]\n\ndef backward_hook(m, grad_inputs, grad_outputs):\n  # Allows for examination of grad_inputs / grad_outputs and modification of\n  # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and\n  # grad_outputs are always wrapped in tuples.\n  new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n  return new_grad_inputs\n\n# Create sample module & input.\nm = nn.Linear(3, 3)\nx = torch.randn(2, 3, requires_grad=True)\n\n# ==== Demonstrate forward hooks. ====\n# Run input through module before and after adding hooks.\nprint('output with no forward hooks: {}'.format(m(x)))\n: output with no forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                        [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)\n\n# Note that the modified input results in a different output.\nforward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\nprint('output with forward pre hook: {}'.format(m(x)))\n: output with forward pre hook: tensor([[-0.5752, -0.7421,  0.4942],\n                                        [-0.0736,  0.5461,  0.0838]], grad_fn=<AddmmBackward>)\n\n# Note the modified output.\nforward_hook_handle = m.register_forward_hook(forward_hook)\nprint('output with both forward hooks: {}'.format(m(x)))\n: output with both forward hooks: tensor([[-1.0980,  0.6396,  0.4666],\n                                          [ 0.3634,  0.6538,  1.0256]], grad_fn=<AddBackward0>)\n\n# Remove hooks; note that the output here matches the output before adding hooks.\nforward_pre_hook_handle.remove()\nforward_hook_handle.remove()\nprint('output after removing forward hooks: {}'.format(m(x)))\n: output after removing forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                               [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)\n\n# ==== Demonstrate backward hooks. ====\nm(x).sum().backward()\nprint('x.grad with no backwards hook: {}'.format(x.grad))\n: x.grad with no backwards hook: tensor([[ 0.4497, -0.5046,  0.3146],\n                                         [ 0.4497, -0.5046,  0.3146]])\n\n# Clear gradients before running backward pass again.\nm.zero_grad()\nx.grad.zero_()\n\nm.register_full_backward_hook(backward_hook)\nm(x).sum().backward()\nprint('x.grad with backwards hook: {}'.format(x.grad))\n: x.grad with backwards hook: tensor([[42., 42., 42.],\n                                      [42., 42., 42.]])\n\n```\n\n\n## Advanced Features#\n\nPyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare available for custom-written modules, with the small caveat that certain features may require modules to conform\nto particular constraints in order to be supported. In-depth discussion of these features and the corresponding\nrequirements can be found in the links below.\n\n## Distributed Training#\n\nVarious methods for distributed training exist within PyTorch, both for scaling up training using multiple GPUs\nas well as training across multiple machines. Check out thedistributed training overview pagefor\ndetailed information on how to utilize these.\n\n## Profiling Performance#\n\nThePyTorch Profilercan be useful for identifying\nperformance bottlenecks within your models. It measures and outputs performance characteristics for\nboth memory usage and time spent.\n\n## Improving Performance with Quantization#\n\nApplying quantization techniques to modules can improve performance and memory usage by utilizing lower\nbitwidths than floating-point precision. Check out the various PyTorch-provided mechanisms for quantizationhere.\n\n## Improving Memory Usage with Pruning#\n\nLarge deep learning models are often over-parametrized, resulting in high memory usage. To combat this, PyTorch\nprovides mechanisms for model pruning, which can help reduce memory usage while maintaining task accuracy. ThePruning tutorialdescribes how to utilize\nthe pruning techniques PyTorch provides or define custom pruning techniques as necessary.\n\n## Parametrizations#\n\nFor certain applications, it can be beneficial to constrain the parameter space during model training. For example,\nenforcing orthogonality of the learned parameters can improve convergence for RNNs. PyTorch provides a mechanism for\napplyingparametrizationssuch as this, and\nfurther allows for custom constraints to be defined.\n\n## Transforming Modules with FX#\n\nTheFXcomponent of PyTorch provides a flexible way to transform\nmodules by operating directly on module computation graphs. This can be used to programmatically generate or\nmanipulate modules for a broad array of use cases. To explore FX, check out these examples of using FX forconvolution + batch norm fusionandCPU performance analysis.",
  "url": "https://pytorch.org/docs/stable/notes/modules.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "f49b578ba813a87b3a656c274dd795b6",
  "source": "pytorch_docs",
  "title": "torch.cond \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.cond#\n\n\n## cond_branch_class_method#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass MySubModule(torch.nn.Module):\n    def foo(self, x):\n        return x.cos()\n\n    def forward(self, x):\n        return self.foo(x)\n\nclass CondBranchClassMethod(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n\n    This example demonstrates using class method in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.subm = MySubModule()\n\n    def bar(self, x):\n        return x.sin()\n\n    def forward(self, x):\n        return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchClassMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 sin: \"f32[3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nested_function#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNestedFunction(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n    This example demonstrates using nested function in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        def true_fn(x):\n            def inner_true_fn(y):\n                return x + y\n\n            return inner_true_fn(x)\n\n        def false_fn(x):\n            def inner_false_fn(y):\n                return x - y\n\n            return inner_false_fn(x)\n\n        return cond(x.shape[0] < 10, true_fn, false_fn, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNestedFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 add: \"f32[3]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nonlocal_variables#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNonlocalVariables(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n    - both branches must take the same args, which must also match the branch args passed to cond.\n    - both branches must return a single tensor\n    - returned tensor must have the same tensor metadata, e.g. shape and dtype\n    - branch function can be free function, nested function, lambda, class methods\n    - branch function can not have closure variables\n    - no inplace mutations on inputs or global variables\n\n    This example demonstrates how to rewrite code to avoid capturing closure variables in branch functions.\n\n    The code below will not work because capturing closure variables is not supported.\n    ```\n    my_tensor_var = x + 100\n    my_primitive_var = 3.14\n\n    def true_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y + my_tensor_var + my_primitive_var\n\n    def false_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y - my_tensor_var - my_primitive_var\n\n    return cond(x.shape[0] > 5, true_fn, false_fn, [x])\n    ```\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        my_tensor_var = x + 100\n        my_primitive_var = 3.14\n\n        def true_fn(x, y, z):\n            return x + y + z\n\n        def false_fn(x, y, z):\n            return x - y - z\n\n        return cond(\n            x.shape[0] > 5,\n            true_fn,\n            false_fn,\n            [x, my_tensor_var, torch.tensor(my_primitive_var)],\n        )\n\nexample_args = (torch.randn(6),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNonlocalVariables()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, c_lifted_tensor_0: \"f32[]\", x: \"f32[6]\"):\n                 add: \"f32[6]\" = torch.ops.aten.add.Tensor(x, 100)\n\n                 lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(c_lifted_tensor_0);  c_lifted_tensor_0 = None\n            detach_: \"f32[]\" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None\n\n                 add_1: \"f32[6]\" = torch.ops.aten.add.Tensor(x, add);  x = add = None\n            add_2: \"f32[6]\" = torch.ops.aten.add.Tensor(add_1, detach_);  add_1 = detach_ = None\n            return (add_2,)\n\nGraph signature:\n    # inputs\n    c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0'\n    x: USER_INPUT\n\n    # outputs\n    add_2: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_closed_over_variable#\n\nNote\nTags:python.closure,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondClosedOverVariable(torch.nn.Module):\n    \"\"\"\n    torch.cond() supports branches closed over arbitrary variables.\n    \"\"\"\n\n    def forward(self, pred, x):\n        def true_fn(val):\n            return x * 2\n\n        def false_fn(val):\n            return x - 2\n\n        return cond(pred, true_fn, false_fn, [x + 1])\n\nexample_args = (torch.tensor(True), torch.randn(3, 2))\ntags = {\"torch.cond\", \"python.closure\"}\nmodel = CondClosedOverVariable()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pred: \"b8[]\", x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1);  add = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, (x,));  pred = true_graph_0 = false_graph_0 = x = None\n            getitem: \"f32[3, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, 2);  x = None\n                return (mul,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(x, 2);  x = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    pred: USER_INPUT\n    x: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_operands#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.export import Dim\n\nx = torch.randn(3, 2)\ny = torch.randn(2)\ndim0_x = Dim(\"dim0_x\")\n\nclass CondOperands(torch.nn.Module):\n    \"\"\"\n    The operands passed to cond() must be:\n    - a list of tensors\n    - match arguments of `true_fn` and `false_fn`\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x, y):\n        def true_fn(x, y):\n            return x + y\n\n        def false_fn(x, y):\n            return x - y\n\n        return torch.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n\nexample_args = (x, y)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nextra_inputs = (torch.randn(2, 2), torch.randn(2))\ndynamic_shapes = {\"x\": {0: dim0_x}, \"y\": None}\nmodel = CondOperands()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n             #\n            sym_size_int_1: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0)\n\n                 gt: \"Sym(s77 > 2)\" = sym_size_int_1 > 2;  sym_size_int_1 = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x, y));  gt = true_graph_0 = false_graph_0 = x = y = None\n            getitem: \"f32[s77, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         add: \"f32[s77, 2]\" = torch.ops.aten.add.Tensor(x, y);  x = y = None\n                return (add,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         sub: \"f32[s77, 2]\" = torch.ops.aten.sub.Tensor(x, y);  x = y = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {s77: VR[0, int_oo]}\n\n```\n\n\n## cond_predicate#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondPredicate(torch.nn.Module):\n    \"\"\"\n    The conditional statement (aka predicate) passed to cond() must be one of the following:\n      - torch.Tensor with a single element\n      - boolean expression\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        pred = x.dim() > 2 and x.shape[2] > 10\n\n        return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x])\n\nexample_args = (torch.randn(6, 4, 3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondPredicate()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[6, 4, 3]\"):\n                 sin: \"f32[6, 4, 3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
  "url": "https://pytorch.org/docs/stable/generated/exportdb/torch.cond.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
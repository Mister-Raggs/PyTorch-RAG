{
  "doc_id": "8e0064d2ed9eddc09824827f0c5424d8",
  "source": "pytorch_docs",
  "title": "torch.export \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.export#\n\nCreated On: Jun 12, 2025 | Last Updated On: Aug 11, 2025\n\n## Overview#\n\ntorch.export.export()takes atorch.nn.Moduleand produces a traced graph\nrepresenting only the Tensor computation of the function in an Ahead-of-Time\n(AOT) fashion, which can subsequently be executed with different outputs or\nserialized.\ntorch.export.export()\ntorch.nn.Module\n\n```python\nimport torch\nfrom torch.export import export, ExportedProgram\n\nclass Mod(torch.nn.Module):\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        a = torch.sin(x)\n        b = torch.cos(y)\n        return a + b\n\nexample_args = (torch.randn(10, 10), torch.randn(10, 10))\n\nexported_program: ExportedProgram = export(Mod(), args=example_args)\nprint(exported_program)\n\n```\n\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[10, 10]\", y: \"f32[10, 10]\"):\n             # File: /tmp/ipykernel_210/2550508656.py:6 in forward, code: a = torch.sin(x)\n            sin: \"f32[10, 10]\" = torch.ops.aten.sin.default(x);  x = None\n            \n             # File: /tmp/ipykernel_210/2550508656.py:7 in forward, code: b = torch.cos(y)\n            cos: \"f32[10, 10]\" = torch.ops.aten.cos.default(y);  y = None\n            \n             # File: /tmp/ipykernel_210/2550508656.py:8 in forward, code: return a + b\n            add: \"f32[10, 10]\" = torch.ops.aten.add.Tensor(sin, cos);  sin = cos = None\n            return (add,)\n            \nGraph signature: \n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n    \n    # outputs\n    add: USER_OUTPUT\n    \nRange constraints: {}\n\n```\n\ntorch.exportproduces a clean intermediate representation (IR) with the\nfollowing invariants. More specifications about the IR can be foundhere.\ntorch.export\nSoundness: It is guaranteed to be a sound representation of the original\nprogram, and maintains the same calling conventions of the original program.\nNormalized: There are no Python semantics within the graph. Submodules\nfrom the original programs are inlined to form one fully flattened\ncomputational graph.\nGraph properties: The graph is purely functional, meaning it does not\ncontain operations with side effects such as mutations or aliasing. It does\nnot mutate any intermediate values, parameters, or buffers.\nMetadata: The graph contains metadata captured during tracing, such as a\nstacktrace from user\u2019s code.\nUnder the hood,torch.exportleverages the following latest technologies:\ntorch.export\nTorchDynamo (torch._dynamo)is an internal API that uses a CPython feature\ncalled the Frame Evaluation API to safely trace PyTorch graphs. This\nprovides a massively improved graph capturing experience, with much fewer\nrewrites needed in order to fully trace the PyTorch code.\nAOT Autogradprovides a functionalized PyTorch graph and ensures the graph\nis decomposed/lowered to the ATen operator set.\nTorch FX (torch.fx)is the underlying representation of the graph,\nallowing flexible Python-based transformations.\n\n## Existing frameworks#\n\ntorch.compile()also utilizes the same PT2 stack astorch.export, but\nis slightly different:\ntorch.compile()\ntorch.export\nJIT vs. AOT:torch.compile()is a JIT compiler whereas\nwhich is not intended to be used to produce compiled artifacts outside of\ndeployment.\ntorch.compile()\nPartial vs. Full Graph Capture: Whentorch.compile()runs into an\nuntraceable part of a model, it will \u201cgraph break\u201d and fall back to running\nthe program in the eager Python runtime. In comparison,torch.exportaims\nto get a full graph representation of a PyTorch model, so it will error out\nwhen something untraceable is reached. Sincetorch.exportproduces a full\ngraph disjoint from any Python features or runtime, this graph can then be\nsaved, loaded, and run in different environments and languages.\ntorch.compile()\ntorch.export\ntorch.export\nUsability tradeoff: Sincetorch.compile()is able to fallback to the\nPython runtime whenever it reaches something untraceable, it is a lot more\nflexible.torch.exportwill instead require users to provide more\ninformation or rewrite their code to make it traceable.\ntorch.compile()\ntorch.export\nCompared totorch.fx.symbolic_trace(),torch.exporttraces using\nTorchDynamo which operates at the Python bytecode level, giving it the ability\nto trace arbitrary Python constructs not limited by what Python operator\noverloading supports. Additionally,torch.exportkeeps fine-grained track of\ntensor metadata, so that conditionals on things like tensor shapes do not\nfail tracing. In general,torch.exportis expected to work on more user\nprograms, and produce lower-level graphs (at thetorch.ops.atenoperator\nlevel). Note that users can still usetorch.fx.symbolic_trace()as a\npreprocessing step beforetorch.export.\ntorch.fx.symbolic_trace()\ntorch.export\ntorch.export\ntorch.export\ntorch.ops.aten\ntorch.fx.symbolic_trace()\ntorch.export\nCompared totorch.jit.script(),torch.exportdoes not capture Python\ncontrol flow or data structures, unless using explicitcontrol flow operators,\nbut it supports more Python language features due to its comprehensive coverage\nover Python bytecodes. The resulting graphs are simpler and only have straight\nline control flow, except for explicit control flow operators.\ntorch.jit.script()\ntorch.export\nCompared totorch.jit.trace(),torch.exportis sound:\nit can trace code that performs integer computation on sizes and records\nall of the side-conditions necessary to ensure that a particular\ntrace is valid for other inputs.\ntorch.jit.trace()\ntorch.export\n\n## Exporting a PyTorch Model#\n\nThe main entrypoint is throughtorch.export.export(), which takes atorch.nn.Moduleand sample inputs, and\ncaptures the computation graph into antorch.export.ExportedProgram. An\nexample:\ntorch.export.export()\ntorch.nn.Module\ntorch.export.ExportedProgram\n\n```python\nimport torch\nfrom torch.export import export, ExportedProgram\n\n# Simple module for demonstration\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels=3, out_channels=16, kernel_size=3, padding=1\n        )\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3)\n\n    def forward(self, x: torch.Tensor, *, constant=None) -> torch.Tensor:\n        a = self.conv(x)\n        a.add_(constant)\n        return self.maxpool(self.relu(a))\n\nexample_args = (torch.randn(1, 3, 256, 256),)\nexample_kwargs = {\"constant\": torch.ones(1, 16, 256, 256)}\n\nexported_program: ExportedProgram = export(\n    M(), args=example_args, kwargs=example_kwargs\n)\nprint(exported_program)\n\n# To run the exported program, we can use the `module()` method\nprint(exported_program.module()(torch.randn(1, 3, 256, 256), constant=torch.ones(1, 16, 256, 256)))\n\n```\n\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, p_conv_weight: \"f32[16, 3, 3, 3]\", p_conv_bias: \"f32[16]\", x: \"f32[1, 3, 256, 256]\", constant: \"f32[1, 16, 256, 256]\"):\n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n            conv2d: \"f32[1, 16, 256, 256]\" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias, [1, 1], [1, 1]);  x = p_conv_weight = p_conv_bias = None\n            \n             # File: /tmp/ipykernel_210/2848084713.py:16 in forward, code: a.add_(constant)\n            add_: \"f32[1, 16, 256, 256]\" = torch.ops.aten.add_.Tensor(conv2d, constant);  conv2d = constant = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu: \"f32[1, 16, 256, 256]\" = torch.ops.aten.relu.default(add_);  add_ = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(\n            max_pool2d: \"f32[1, 16, 85, 85]\" = torch.ops.aten.max_pool2d.default(relu, [3, 3], [3, 3]);  relu = None\n            return (max_pool2d,)\n            \nGraph signature: \n    # inputs\n    p_conv_weight: PARAMETER target='conv.weight'\n    p_conv_bias: PARAMETER target='conv.bias'\n    x: USER_INPUT\n    constant: USER_INPUT\n    \n    # outputs\n    max_pool2d: USER_OUTPUT\n    \nRange constraints: {}\n\ntensor([[[[1.9458, 1.6273, 1.7019,  ..., 2.1504, 2.5507, 1.8787],\n          [2.2365, 0.7755, 1.9928,  ..., 2.3413, 1.2623, 2.3057],\n          [1.9247, 1.5907, 1.6548,  ..., 2.1173, 1.7600, 1.5872],\n          ...,\n          [1.9187, 1.5949, 1.6456,  ..., 1.9207, 1.7531, 2.1677],\n          [2.3853, 2.1632, 1.7806,  ..., 2.0037, 1.6852, 1.4049],\n          [1.7645, 1.9489, 1.3452,  ..., 2.0768, 2.1393, 1.7513]],\n\n         [[1.5012, 1.6937, 1.3029,  ..., 1.4494, 1.9466, 1.9828],\n          [1.8593, 1.8719, 2.7506,  ..., 1.5848, 1.0953, 1.7283],\n          [2.4618, 1.8617, 1.6075,  ..., 2.2223, 1.7467, 1.6623],\n          ...,\n          [2.0089, 1.4795, 1.7225,  ..., 1.8013, 1.7134, 1.3521],\n          [2.2312, 1.6322, 1.8930,  ..., 2.0357, 1.3973, 1.7693],\n          [1.9949, 1.6649, 1.3789,  ..., 1.7138, 2.0039, 1.9342]],\n\n         [[1.7960, 1.8803, 2.0474,  ..., 1.8290, 1.7625, 1.9906],\n          [1.6626, 1.8577, 2.1852,  ..., 1.8916, 1.9693, 1.2295],\n          [1.6173, 1.6056, 2.2389,  ..., 1.7685, 1.7084, 2.2667],\n          ...,\n          [1.4691, 1.9432, 2.1351,  ..., 1.9473, 2.8299, 1.8726],\n          [2.4949, 2.1844, 1.6385,  ..., 2.6726, 1.8075, 2.0171],\n          [2.3629, 2.2791, 1.7623,  ..., 2.2238, 1.7401, 1.8280]],\n\n         ...,\n\n         [[1.4885, 1.7708, 1.8946,  ..., 2.2083, 1.3406, 1.5833],\n          [2.2450, 2.3450, 2.0719,  ..., 2.3678, 1.9186, 1.9807],\n          [2.1043, 1.7939, 2.1791,  ..., 2.0423, 1.5829, 1.3758],\n          ...,\n          [2.1712, 1.4382, 1.6501,  ..., 1.4854, 1.6417, 1.7364],\n          [1.6107, 2.1640, 2.0138,  ..., 1.4675, 1.5564, 1.8517],\n          [1.5922, 1.9649, 1.9984,  ..., 1.5279, 1.3116, 1.6866]],\n\n         [[1.3002, 2.0303, 1.3138,  ..., 1.2582, 1.9901, 1.7969],\n          [1.8442, 2.0318, 1.7786,  ..., 1.6682, 0.8281, 1.2347],\n          [2.2114, 1.9100, 1.5499,  ..., 2.5738, 1.4758, 1.6086],\n          ...,\n          [1.5610, 1.9579, 2.1318,  ..., 2.2424, 1.9802, 1.9594],\n          [1.5059, 1.6121, 1.0169,  ..., 1.7745, 1.3896, 1.0430],\n          [1.9281, 1.7772, 1.6407,  ..., 2.3685, 2.1403, 1.5720]],\n\n         [[1.8390, 2.7730, 2.1496,  ..., 2.0973, 1.8250, 2.3001],\n          [2.5140, 2.3918, 2.0092,  ..., 1.7859, 2.6470, 2.0772],\n          [1.7845, 2.2558, 2.2929,  ..., 2.2216, 2.1517, 1.8377],\n          ...,\n          [1.8539, 2.0280, 2.3912,  ..., 2.1246, 2.1039, 2.5728],\n          [2.0437, 1.5667, 2.3570,  ..., 2.0728, 2.2498, 2.1027],\n          [1.4684, 2.0560, 1.7849,  ..., 1.7504, 2.0461, 2.0553]]]],\n       grad_fn=<MaxPool2DWithIndicesBackward0>)\n\n```\n\nInspecting theExportedProgram, we can note the following:\nExportedProgram\nThetorch.fx.Graphcontains the computation graph of the original\nprogram, along with records of the original code for easy debugging.\ntorch.fx.Graph\nThe graph contains onlytorch.ops.atenoperators foundhereand custom operators.\ntorch.ops.aten\nThe parameters (weight and bias to conv) are lifted as inputs to the graph,\nresulting in noget_attrnodes in the graph, which previously existed in\nthe result oftorch.fx.symbolic_trace().\nget_attr\ntorch.fx.symbolic_trace()\nThetorch.export.ExportGraphSignaturemodels the input and output\nsignature, along with specifying which inputs are parameters.\ntorch.export.ExportGraphSignature\nThe resulting shape and dtype of tensors produced by each node in the graph is\nnoted. For example, theconv2dnode will result in a tensor of dtypetorch.float32and shape (1, 16, 256, 256).\nconv2d\ntorch.float32\n\n## Expressing Dynamism#\n\nBy defaulttorch.exportwill trace the program assuming all input shapes arestatic, and specializing the exported program to those dimensions. One\nconsequence of this is that at runtime, the program won\u2019t work on inputs with\ndifferent shapes, even if they\u2019re valid in eager mode.\ntorch.export\nAn example:\n\n```python\nimport torch\nimport traceback as tb\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\n\nep = torch.export.export(M(), example_args)\nprint(ep)\n\nexample_args2 = (torch.randn(64, 64), torch.randn(64, 128))\ntry:\n    ep.module()(*example_args2)  # fails\nexcept Exception:\n    tb.print_exc()\n\n```\n\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, p_branch1_0_weight: \"f32[32, 64]\", p_branch1_0_bias: \"f32[32]\", p_branch2_0_weight: \"f32[64, 128]\", p_branch2_0_bias: \"f32[64]\", c_buffer: \"f32[32]\", x1: \"f32[32, 64]\", x2: \"f32[32, 128]\"):\n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n            linear: \"f32[32, 32]\" = torch.ops.aten.linear.default(x1, p_branch1_0_weight, p_branch1_0_bias);  x1 = p_branch1_0_weight = p_branch1_0_bias = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu: \"f32[32, 32]\" = torch.ops.aten.relu.default(linear);  linear = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n            linear_1: \"f32[32, 64]\" = torch.ops.aten.linear.default(x2, p_branch2_0_weight, p_branch2_0_bias);  x2 = p_branch2_0_weight = p_branch2_0_bias = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu_1: \"f32[32, 64]\" = torch.ops.aten.relu.default(linear_1);  linear_1 = None\n            \n             # File: /tmp/ipykernel_210/1522925308.py:19 in forward, code: return (out1 + self.buffer, out2)\n            add: \"f32[32, 32]\" = torch.ops.aten.add.Tensor(relu, c_buffer);  relu = c_buffer = None\n            return (add, relu_1)\n            \nGraph signature: \n    # inputs\n    p_branch1_0_weight: PARAMETER target='branch1.0.weight'\n    p_branch1_0_bias: PARAMETER target='branch1.0.bias'\n    p_branch2_0_weight: PARAMETER target='branch2.0.weight'\n    p_branch2_0_bias: PARAMETER target='branch2.0.bias'\n    c_buffer: CONSTANT_TENSOR target='buffer'\n    x1: USER_INPUT\n    x2: USER_INPUT\n    \n    # outputs\n    add: USER_OUTPUT\n    relu_1: USER_OUTPUT\n    \nRange constraints: {}\n\n```\n\n\n```python\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_210/1522925308.py\", line 28, in <module>\n    ep.module()(*example_args2)  # fails\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n    raise e\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1881, in _call_impl\n    return inner()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1829, in inner\n    result = forward_call(*args, **kwargs)\n  File \"<eval_with_key>.25\", line 11, in forward\n    _guards_fn = self._guards_fn(x1, x2);  _guards_fn = None\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"<string>\", line 3, in _\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/__init__.py\", line 2185, in _assert\n    assert condition, message\nAssertionError: Guard failed: x1.size()[0] == 32\n\n```\n\nHowever, some dimensions, such as a batch dimension, can be dynamic and vary\nfrom run to run. Such dimensions must be specified by using thetorch.export.Dim()API to create them and by passing them intotorch.export.export()through thedynamic_shapesargument.\ntorch.export.Dim()\ntorch.export.export()\ndynamic_shapes\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\n\n# Create a dynamic batch size\nbatch = torch.export.Dim(\"batch\")\n# Specify that the first dimension of each input is that batch size\ndynamic_shapes = {\"x1\": {0: batch}, \"x2\": {0: batch}}\n\nep = torch.export.export(\n    M(), args=example_args, dynamic_shapes=dynamic_shapes\n)\nprint(ep)\n\nexample_args2 = (torch.randn(64, 64), torch.randn(64, 128))\nep.module()(*example_args2)  # success\n\n```\n\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, p_branch1_0_weight: \"f32[32, 64]\", p_branch1_0_bias: \"f32[32]\", p_branch2_0_weight: \"f32[64, 128]\", p_branch2_0_bias: \"f32[64]\", c_buffer: \"f32[32]\", x1: \"f32[s24, 64]\", x2: \"f32[s24, 128]\"):\n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n            linear: \"f32[s24, 32]\" = torch.ops.aten.linear.default(x1, p_branch1_0_weight, p_branch1_0_bias);  x1 = p_branch1_0_weight = p_branch1_0_bias = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu: \"f32[s24, 32]\" = torch.ops.aten.relu.default(linear);  linear = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n            linear_1: \"f32[s24, 64]\" = torch.ops.aten.linear.default(x2, p_branch2_0_weight, p_branch2_0_bias);  x2 = p_branch2_0_weight = p_branch2_0_bias = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu_1: \"f32[s24, 64]\" = torch.ops.aten.relu.default(linear_1);  linear_1 = None\n            \n             # File: /tmp/ipykernel_210/3456136871.py:18 in forward, code: return (out1 + self.buffer, out2)\n            add: \"f32[s24, 32]\" = torch.ops.aten.add.Tensor(relu, c_buffer);  relu = c_buffer = None\n            return (add, relu_1)\n            \nGraph signature: \n    # inputs\n    p_branch1_0_weight: PARAMETER target='branch1.0.weight'\n    p_branch1_0_bias: PARAMETER target='branch1.0.bias'\n    p_branch2_0_weight: PARAMETER target='branch2.0.weight'\n    p_branch2_0_bias: PARAMETER target='branch2.0.bias'\n    c_buffer: CONSTANT_TENSOR target='buffer'\n    x1: USER_INPUT\n    x2: USER_INPUT\n    \n    # outputs\n    add: USER_OUTPUT\n    relu_1: USER_OUTPUT\n    \nRange constraints: {s24: VR[0, int_oo]}\n\n```\n\n\n```python\n(tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 2.0279],\n         [1.0000, 1.0000, 1.0000,  ..., 1.1377, 1.0730, 1.0000],\n         [1.0000, 1.2866, 1.0000,  ..., 1.0000, 1.2756, 1.0392],\n         ...,\n         [1.0000, 1.0000, 1.4028,  ..., 2.4600, 1.1347, 1.0000],\n         [1.0000, 1.5510, 1.0000,  ..., 1.0000, 1.1089, 1.1483],\n         [1.3684, 1.3657, 1.0000,  ..., 1.0855, 1.0000, 1.0822]],\n        grad_fn=<AddBackward0>),\n tensor([[0.0067, 0.0000, 0.1880,  ..., 0.0000, 0.2531, 0.0000],\n         [0.0000, 0.0000, 0.0000,  ..., 0.4300, 0.0000, 0.0000],\n         [0.8456, 0.3823, 0.0000,  ..., 0.1809, 0.0000, 0.8871],\n         ...,\n         [0.3800, 0.0000, 0.3322,  ..., 0.0000, 0.0000, 0.0000],\n         [0.1636, 0.4819, 0.0000,  ..., 0.0525, 0.0000, 0.0000],\n         [0.0000, 0.3364, 0.0000,  ..., 0.0000, 0.0000, 0.7609]],\n        grad_fn=<ReluBackward0>))\n\n```\n\nSome additional things to note:\nThrough thetorch.export.Dim()API and thedynamic_shapesargument, we specified the first\ndimension of each input to be dynamic. Looking at the inputsx1andx2, they have a symbolic shape of(s0,64)and(s0,128), instead of\nthe(32,64)and(32,128)shaped tensors that we passed in as example inputs.s0is a symbol representing that this dimension can be a range\nof values.\ntorch.export.Dim()\ndynamic_shapes\nx1\nx2\n(s0,64)\n(s0,128)\n(32,64)\n(32,128)\ns0\nexported_program.range_constraintsdescribes the ranges of each symbol\nappearing in the graph. In this case, we see thats0has the range\n[0, int_oo]. For technical reasons that are difficult to explain here, they are\nassumed to be not 0 or 1. This is not a bug, and does not necessarily mean\nthat the exported program will not work for dimensions 0 or 1. SeeThe 0/1 Specialization Problemfor an in-depth discussion of this topic.\nexported_program.range_constraints\ns0\nIn the example, we usedDim(\"batch\")to create a dynamic dimension. This is\nthe most explicit way to specify dynamism. We can also useDim.DYNAMICandDim.AUTOto specify dynamism. We will go over both methods in the next section.\nDim(\"batch\")\nDim.DYNAMIC\nDim.AUTO\n\n## Named Dims#\n\nFor every dimension specified withDim(\"name\"), we will allocate a symbolic\nshape. Specifying aDimwith the same name will result in the same symbol\nto be generated. This allows users to specify what symbols are allocated for\neach input dimension.\nDim(\"name\")\nDim\n\n```python\nbatch = Dim(\"batch\")\ndynamic_shapes = {\"x1\": {0: dim}, \"x2\": {0: batch}}\n\n```\n\nFor eachDim, we can specify minimum and maximum values. We also allow\nspecifying relations betweenDims in univariate linear expressions:A*dim+B.\nThis allows users to specify more complex constraints like integer divisibility\nfor dynamic dimensions. These features allow for users to place explicit\nrestrictions on the dynamic behavior of theExportedProgramproduced.\nDim\nDim\nA*dim+B\nExportedProgram\n\n```python\ndx = Dim(\"dx\", min=4, max=256)\ndh = Dim(\"dh\", max=512)\ndynamic_shapes = {\n    \"x\": (dx, None),\n    \"y\": (2 * dx, dh),\n}\n\n```\n\nHowever,ConstraintViolationErrorswill be raised if the while tracing, we emit guards\nthat conflict with the relations or static/dynamic specifications given. For\nexample, in the above specification, the following is asserted:\nConstraintViolationErrors\nx.shape[0]is to have range[4,256], and related toy.shape[0]byy.shape[0]==2*x.shape[0].\nx.shape[0]\n[4,256]\ny.shape[0]\ny.shape[0]==2*x.shape[0]\nx.shape[1]is static.\nx.shape[1]\ny.shape[1]has range[0,512], and is unrelated to any other dimension.\ny.shape[1]\n[0,512]\nIf any of these assertions are found to be incorrect while tracing (ex.x.shape[0]is static, ory.shape[1]has a smaller range, ory.shape[0]!=2*x.shape[0]), then aConstraintViolationErrorwill be\nraised, and the user will need to change theirdynamic_shapesspecification.\nx.shape[0]\ny.shape[1]\ny.shape[0]!=2*x.shape[0]\nConstraintViolationError\ndynamic_shapes\n\n## Dim Hints#\n\nInstead of explicitly specifying dynamism usingDim(\"name\"), we can lettorch.exportinfer the ranges and relationships of the dynamic values usingDim.DYNAMIC. This is also a more convenient way to specify dynamism when you\ndon\u2019t know specificallyhowdynamic your dynamic values are.\nDim(\"name\")\ntorch.export\nDim.DYNAMIC\n\n```python\ndynamic_shapes = {\n    \"x\": (Dim.DYNAMIC, None),\n    \"y\": (Dim.DYNAMIC, Dim.DYNAMIC),\n}\n\n```\n\nWe can also specify min/max values forDim.DYNAMIC, which will serve as hints\nto export. But if while tracing export found the range to be different, it will\nautomatically update the range without raising an error. We also cannot specify\nrelationships between dynamic values. Instead, this will be inferred by export,\nand exposed to users through an inspection of assertions within the graph.  In\nthis method of specifying dynamism,ConstraintViolationErrorswillonlybe\nraised if the specified value is inferred to bestatic.\nDim.DYNAMIC\nConstraintViolationErrors\nAn even more convenient way to specify dynamism is to useDim.AUTO, which will\nbehave likeDim.DYNAMIC, but willnotraise an error if the dimension is\ninferred to be static. This is useful for when you have no idea what the dynamic\nvalues are, and want to export the program with a \u201cbest effort\u201d dynamic approach.\nDim.AUTO\nDim.DYNAMIC\n\n## ShapesCollection#\n\nWhen specifying which inputs are dynamic viadynamic_shapes, we must specify\nthe dynamism of every input. For example, given the following inputs:\ndynamic_shapes\n\n```python\nargs = {\"x\": tensor_x, \"others\": [tensor_y, tensor_z]}\n\n```\n\nwe would need to specify the dynamism oftensor_x,tensor_y, andtensor_zalong with the dynamic shapes:\ntensor_x\ntensor_y\ntensor_z\n\n```python\n# With named-Dims\ndim = torch.export.Dim(...)\ndynamic_shapes = {\"x\": {0: dim, 1: dim + 1}, \"others\": [{0: dim * 2}, None]}\n\ntorch.export(..., args, dynamic_shapes=dynamic_shapes)\n\n```\n\nHowever, this is particularly complicated as we need to specify thedynamic_shapesspecification in the same nested input structure as the input\narguments. Instead, an easier way to specify dynamic shapes is with the helper\nutilitytorch.export.ShapesCollection, where instead of specifying the\ndynamism of every single input, we can just assign directly which input\ndimensions are dynamic.\ndynamic_shapes\ntorch.export.ShapesCollection\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def forward(self, inp):\n        x = inp[\"x\"] * 1\n        y = inp[\"others\"][0] * 2\n        z = inp[\"others\"][1] * 3\n        return x, y, z\n\ntensor_x = torch.randn(3, 4, 8)\ntensor_y = torch.randn(6)\ntensor_z = torch.randn(6)\nargs = {\"x\": tensor_x, \"others\": [tensor_y, tensor_z]}\n\ndim = torch.export.Dim(\"dim\")\nsc = torch.export.ShapesCollection()\nsc[tensor_x] = (dim, dim + 1, 8)\nsc[tensor_y] = {0: dim * 2}\n\nprint(sc.dynamic_shapes(M(), (args,)))\nep = torch.export.export(M(), (args,), dynamic_shapes=sc)\nprint(ep)\n\n```\n\n\n```python\n{'inp': {'x': (Dim('dim', min=0), dim + 1, 8), 'others': [{0: 2*dim}, None]}}\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, inp_x: \"f32[s96, s96 + 1, 8]\", inp_others_0: \"f32[2*s96]\", inp_others_1: \"f32[6]\"):\n             # File: /tmp/ipykernel_210/1070110726.py:5 in forward, code: x = inp[\"x\"] * 1\n            mul: \"f32[s96, s96 + 1, 8]\" = torch.ops.aten.mul.Tensor(inp_x, 1);  inp_x = None\n            \n             # File: /tmp/ipykernel_210/1070110726.py:6 in forward, code: y = inp[\"others\"][0] * 2\n            mul_1: \"f32[2*s96]\" = torch.ops.aten.mul.Tensor(inp_others_0, 2);  inp_others_0 = None\n            \n             # File: /tmp/ipykernel_210/1070110726.py:7 in forward, code: z = inp[\"others\"][1] * 3\n            mul_2: \"f32[6]\" = torch.ops.aten.mul.Tensor(inp_others_1, 3);  inp_others_1 = None\n            return (mul, mul_1, mul_2)\n            \nGraph signature: \n    # inputs\n    inp_x: USER_INPUT\n    inp_others_0: USER_INPUT\n    inp_others_1: USER_INPUT\n    \n    # outputs\n    mul: USER_OUTPUT\n    mul_1: USER_OUTPUT\n    mul_2: USER_OUTPUT\n    \nRange constraints: {s96: VR[0, int_oo], s96 + 1: VR[1, int_oo], 2*s96: VR[0, int_oo]}\n\n```\n\n\n## AdditionalInputs#\n\nIn the case where you don\u2019t know how dynamic your inputs are, but you have an\nample set of testing or profiling data that can provide a fair sense of\nrepresentative inputs for a model, you can usetorch.export.AdditionalInputsin place ofdynamic_shapes. You can\nspecify all the possible inputs used to trace the program, andAdditionalInputswill infer which inputs are dynamic based on which input\nshapes are changing.\ntorch.export.AdditionalInputs\ndynamic_shapes\nAdditionalInputs\nExample:\n\n```python\nimport dataclasses\nimport torch\nimport torch.utils._pytree as pytree\n\n@dataclasses.dataclass\nclass D:\n    b: bool\n    i: int\n    f: float\n    t: torch.Tensor\n\npytree.register_dataclass(D)\n\nclass M(torch.nn.Module):\n    def forward(self, d: D):\n        return d.i + d.f + d.t\n\ninput1 = (D(True, 3, 3.0, torch.ones(3)),)\ninput2 = (D(True, 4, 3.0, torch.ones(4)),)\nai = torch.export.AdditionalInputs()\nai.add(input1)\nai.add(input2)\n\nprint(ai.dynamic_shapes(M(), input1))\nep = torch.export.export(M(), input1, dynamic_shapes=ai)\nprint(ep)\n\n```\n\n\n```python\n{'d': [None, _DimHint(type=<_DimHintType.DYNAMIC: 3>, min=None, max=None, _factory=True), None, (_DimHint(type=<_DimHintType.DYNAMIC: 3>, min=None, max=None, _factory=True),)]}\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, d_b, d_i: \"Sym(s37)\", d_f, d_t: \"f32[s99]\"):\n             # File: /tmp/ipykernel_210/829931439.py:16 in forward, code: return d.i + d.f + d.t\n            sym_float: \"Sym(ToFloat(s37))\" = torch.sym_float(d_i);  d_i = None\n            add: \"Sym(ToFloat(s37) + 3.0)\" = sym_float + 3.0;  sym_float = None\n            add_1: \"f32[s99]\" = torch.ops.aten.add.Tensor(d_t, add);  d_t = add = None\n            return (add_1,)\n            \nGraph signature: \n    # inputs\n    d_b: USER_INPUT\n    d_i: USER_INPUT\n    d_f: USER_INPUT\n    d_t: USER_INPUT\n    \n    # outputs\n    add_1: USER_OUTPUT\n    \nRange constraints: {s37: VR[0, int_oo], s99: VR[2, int_oo]}\n\n```\n\n\n## Serialization#\n\nTo save theExportedProgram, users can use thetorch.export.save()andtorch.export.load()APIs. The resulting file is a zipfile with a specific\nstructure. The details of the structure are defined in thePT2 Archive Spec.\nExportedProgram\ntorch.export.save()\ntorch.export.load()\nAn example:\n\n```python\nimport torch\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nexported_program = torch.export.export(MyModule(), (torch.randn(5),))\n\ntorch.export.save(exported_program, 'exported_program.pt2')\nsaved_exported_program = torch.export.load('exported_program.pt2')\n\n```\n\n\n## Export IR, Decompositions#\n\nThe graph produced bytorch.exportreturns a graph containing onlyATen operators, which are the basic unit of\ncomputation in PyTorch. As there are over\n3000 ATen operators, export provides a way to narrow down the operator set used\nin the graph based on certain characteristics, creating different IRs.\ntorch.export\nBy default, export produces the most generic IR which contains all ATen\noperators, including both functional and non-functional operators. A functional\noperator is one that does not contain any mutations or aliasing of the inputs.\nYou can find a list of all ATen operatorshereand you can inspect if an operator is functional by checkingop._schema.is_mutable.\nop._schema.is_mutable\nThis generic IR can be used to train in eager PyTorch Autograd.\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\nprint(ep_for_training.graph_module.print_readable(print_output=False))\n\n```\n\n\n```python\nclass GraphModule(torch.nn.Module):\n    def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"):\n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        conv2d: \"f32[1, 3, 3, 3]\" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias);  x = p_conv_weight = p_conv_bias = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:173 in forward, code: self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n        add_: \"i64[]\" = torch.ops.aten.add_.Tensor(b_bn_num_batches_tracked, 1);  b_bn_num_batches_tracked = add_ = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n        batch_norm: \"f32[1, 3, 3, 3]\" = torch.ops.aten.batch_norm.default(conv2d, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05, True);  conv2d = p_bn_weight = p_bn_bias = b_bn_running_mean = b_bn_running_var = None\n        return (batch_norm,)\n        \n\n```\n\nHowever, if you want to use the IR for inference, or decrease the amount of\noperators being used, you can lower the graph through theExportedProgram.run_decompositions()API. This method decomposes the\nATen operators into the ones specified in the decomposition table, and\nfunctionalizes the graph.\nExportedProgram.run_decompositions()\nBy specifying an empty set, we\u2019re only performing functionalization, and does\nnot do any additional decompositions. This results in an IR which contains ~2000\noperators (instead of the 3000 operators above), and is ideal for inference cases.\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\nwith torch.no_grad():\n    ep_for_inference = ep_for_training.run_decompositions(decomp_table={})\nprint(ep_for_inference.graph_module.print_readable(print_output=False))\n\n```\n\n\n```python\nclass GraphModule(torch.nn.Module):\n    def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"):\n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        conv2d: \"f32[1, 3, 3, 3]\" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias);  x = p_conv_weight = p_conv_bias = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:173 in forward, code: self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n        add: \"i64[]\" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1);  b_bn_num_batches_tracked = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n        _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(conv2d, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05);  conv2d = p_bn_weight = p_bn_bias = b_bn_running_mean = b_bn_running_var = None\n        getitem: \"f32[1, 3, 3, 3]\" = _native_batch_norm_legit_functional[0]\n        getitem_3: \"f32[3]\" = _native_batch_norm_legit_functional[3]\n        getitem_4: \"f32[3]\" = _native_batch_norm_legit_functional[4];  _native_batch_norm_legit_functional = None\n        return (getitem_3, getitem_4, add, getitem)\n        \n\n```\n\nAs we can see, the previously in-place operator,torch.ops.aten.add_.defaulthas now been replaced withtorch.ops.aten.add.default, a functional operator.\ntorch.ops.aten.add_.default\ntorch.ops.aten.add.default\nWe can also further lower this exported program to an operator set which only\ncontains theCoreATenOperatorSet<https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir>__,\nwhich is a collection of only ~180 operators. This IR is optimal for backends\nwho do not want to reimplement all ATen operators.\nCoreATenOperatorSet<https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir>\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\nwith torch.no_grad():\n    core_aten_ir = ep_for_training.run_decompositions(decomp_table=None)\nprint(core_aten_ir.graph_module.print_readable(print_output=False))\n\n```\n\n\n```python\nclass GraphModule(torch.nn.Module):\n    def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"):\n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        convolution: \"f32[1, 3, 3, 3]\" = torch.ops.aten.convolution.default(x, p_conv_weight, p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  x = p_conv_weight = p_conv_bias = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:173 in forward, code: self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n        add: \"i64[]\" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1);  b_bn_num_batches_tracked = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n        _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(convolution, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05);  convolution = p_bn_weight = p_bn_bias = b_bn_running_mean = b_bn_running_var = None\n        getitem: \"f32[1, 3, 3, 3]\" = _native_batch_norm_legit_functional[0]\n        getitem_3: \"f32[3]\" = _native_batch_norm_legit_functional[3]\n        getitem_4: \"f32[3]\" = _native_batch_norm_legit_functional[4];  _native_batch_norm_legit_functional = None\n        return (getitem_3, getitem_4, add, getitem)\n        \n\n```\n\nWe now see thattorch.ops.aten.conv2d.defaulthas been decomposed\nintotorch.ops.aten.convolution.default. This is becauseconvolutionis a more \u201ccore\u201d operator, as operations likeconv1dandconv2dcan be\nimplemented using the same op.\ntorch.ops.aten.conv2d.default\ntorch.ops.aten.convolution.default\nconvolution\nconv1d\nconv2d\nWe can also specify our own decomposition behaviors:\n\n```python\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\n\nmy_decomp_table = torch.export.default_decompositions()\n\ndef my_awesome_custom_conv2d_function(x, weight, bias, stride=[1, 1], padding=[0, 0], dilation=[1, 1], groups=1):\n    return 2 * torch.ops.aten.convolution(x, weight, bias, stride, padding, dilation, False, [0, 0], groups)\n\nmy_decomp_table[torch.ops.aten.conv2d.default] = my_awesome_custom_conv2d_function\nmy_ep = ep_for_training.run_decompositions(my_decomp_table)\nprint(my_ep.graph_module.print_readable(print_output=False))\n\n```\n\n\n```python\nclass GraphModule(torch.nn.Module):\n    def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"):\n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        convolution: \"f32[1, 3, 3, 3]\" = torch.ops.aten.convolution.default(x, p_conv_weight, p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  x = p_conv_weight = p_conv_bias = None\n        mul: \"f32[1, 3, 3, 3]\" = torch.ops.aten.mul.Tensor(convolution, 2);  convolution = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:173 in forward, code: self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n        add: \"i64[]\" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1);  b_bn_num_batches_tracked = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n        _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(mul, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05);  mul = p_bn_weight = p_bn_bias = b_bn_running_mean = b_bn_running_var = None\n        getitem: \"f32[1, 3, 3, 3]\" = _native_batch_norm_legit_functional[0]\n        getitem_3: \"f32[3]\" = _native_batch_norm_legit_functional[3]\n        getitem_4: \"f32[3]\" = _native_batch_norm_legit_functional[4];  _native_batch_norm_legit_functional = None\n        return (getitem_3, getitem_4, add, getitem)\n        \n\n```\n\nNotice that instead oftorch.ops.aten.conv2d.defaultbeing decomposed\nintotorch.ops.aten.convolution.default, it is now decomposed intotorch.ops.aten.convolution.defaultandtorch.ops.aten.mul.Tensor,\nwhich matches our custom decomposition rule.\ntorch.ops.aten.conv2d.default\ntorch.ops.aten.convolution.default\ntorch.ops.aten.convolution.default\ntorch.ops.aten.mul.Tensor\n\n## Limitations of torch.export#\n\nAstorch.exportis a one-shot process for capturing a computation graph from\na PyTorch program, it might ultimately run into untraceable parts of programs as\nit is nearly impossible to support tracing all PyTorch and Python features. In\nthe case oftorch.compile, an unsupported operation will cause a \u201cgraph\nbreak\u201d and the unsupported operation will be run with default Python evaluation.\nIn contrast,torch.exportwill require users to provide additional\ninformation or rewrite parts of their code to make it traceable.\ntorch.export\ntorch.compile\ntorch.export\nDraft-exportis a great resource for listing out\ngraphs breaks that will be encountered when tracing the program, along with\nadditional debug information to solve those errors.\nExportDBis also great resource for learning about the\nkinds of programs that are supported and unsupported, along with ways to rewrite\nprograms to make them traceable.\n\n## TorchDynamo unsupported#\n\nWhen usingtorch.exportwithstrict=True, this will use TorchDynamo to\nevaluate the program at the Python bytecode level to trace the program into a\ngraph. Compared to previous tracing frameworks, there will be significantly\nfewer rewrites required to make a program traceable, but there will still be\nsome Python features that are unsupported. An option to get past dealing with\nthis graph breaks is by usingnon-strict exportthrough changing thestrictflag\ntostrict=False.\ntorch.export\nstrict=True\nstrict\nstrict=False\n\n## Data/Shape-Dependent Control Flow#\n\nGraph breaks can also be encountered on data-dependent control flow (ifx.shape[0]>2) when shapes are not being specialized, as a tracing compiler cannot\npossibly deal with without generating code for a combinatorially exploding\nnumber of paths. In such cases, users will need to rewrite their code using\nspecial control flow operators. Currently, we supporttorch.condto express if-else like control flow (more coming soon!).\nifx.shape[0]>2\nYou can also refer to thistutorialfor more ways of addressing data-dependent errors.\n\n## Missing Fake/Meta Kernels for Operators#\n\nWhen tracing, a FakeTensor kernel (aka meta kernel) is required for all\noperators. This is used to reason about the input/output shapes for this\noperator.\nPlease see thistutorialfor more details.\nIn the unfortunate case where your model uses an ATen operator that is does not\nhave a FakeTensor kernel implementation yet, please file an issue.\n\n## Read More#\n\nAdditional Links for Export Users\nDeep Dive for PyTorch Developers",
  "url": "https://pytorch.org/docs/stable/export.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
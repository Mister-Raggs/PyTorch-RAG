{
  "doc_id": "93a3dfb7effc369dad36bffbea01c479",
  "source": "pytorch_docs",
  "title": "Autograd mechanics \u2014 PyTorch 2.9 documentation",
  "text": "\n## Autograd mechanics#\n\nCreated On: Jan 16, 2017 | Last Updated On: Jun 16, 2025\nThis note will present an overview of how autograd works and records the\noperations. It\u2019s not strictly necessary to understand all this, but we recommend\ngetting familiar with it, as it will help you write more efficient, cleaner\nprograms, and can aid you in debugging.\n\n## How autograd encodes the history#\n\nAutograd is a reverse automatic differentiation system.  Conceptually,\nautograd records a graph recording all of the operations that created\nthe data as you execute operations, giving you a directed acyclic graph\nwhose leaves are the input tensors and roots are the output tensors.\nBy tracing this graph from roots to leaves, you can automatically\ncompute the gradients using the chain rule.\nInternally, autograd represents this graph as a graph ofFunctionobjects (really expressions), which can beapply()ed to compute the result of\nevaluating the graph.  When computing the forward pass, autograd\nsimultaneously performs the requested computations and builds up a graph\nrepresenting the function that computes the gradient (the.grad_fnattribute of eachtorch.Tensoris an entry point into this graph).\nWhen the forward pass is completed, we evaluate this graph in the\nbackwards pass to compute the gradients.\nFunction\napply()\n.grad_fn\ntorch.Tensor\nAn important thing to note is that the graph is recreated from scratch at every\niteration, and this is exactly what allows for using arbitrary Python control\nflow statements, that can change the overall shape and size of the graph at\nevery iteration. You don\u2019t have to encode all possible paths before you\nlaunch the training - what you run is what you differentiate.\n\n## Saved tensors#\n\nSome operations need intermediary results to be saved during the forward pass\nin order to execute the backward pass. For example, the functionx\u21a6x2x\\mapsto x^2x\u21a6x2saves the inputxxxto compute the gradient.\nWhen defining a custom PythonFunction, you can usesave_for_backward()to save\ntensors during the forward pass andsaved_tensorsto retrieve them\nduring the backward pass. SeeExtending PyTorchfor more information.\nFunction\nsave_for_backward()\nsaved_tensors\nFor operations that PyTorch defines (e.g.torch.pow()), tensors are\nautomatically saved as needed. You can explore (for educational or debugging\npurposes) which tensors are saved by a certaingrad_fnby looking for its\nattributes starting with the prefix_saved.\ntorch.pow()\ngrad_fn\n_saved\n\n```python\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\nprint(x.equal(y.grad_fn._saved_self))  # True\nprint(x is y.grad_fn._saved_self)  # True\n\n```\n\nIn the previous code,y.grad_fn._saved_selfrefers to the same Tensor object asx.\nBut that may not always be the case. For instance:\ny.grad_fn._saved_self\n\n```python\nx = torch.randn(5, requires_grad=True)\ny = x.exp()\nprint(y.equal(y.grad_fn._saved_result))  # True\nprint(y is y.grad_fn._saved_result)  # False\n\n```\n\nUnder the hood, to prevent reference cycles, PyTorch haspackedthe tensor\nupon saving andunpackedit into a different tensor for reading. Here, the\ntensor you get from accessingy.grad_fn._saved_resultis a different tensor\nobject thany(but they still share the same storage).\ny.grad_fn._saved_result\ny\nWhether a tensor will be packed into a different tensor object depends on\nwhether it is an output of its owngrad_fn, which is an implementation detail\nsubject to change and that users should not rely on.\nYou can control how PyTorch does packing / unpacking withHooks for saved tensors.\n\n## Gradients for non-differentiable functions#\n\nThe gradient computation using Automatic Differentiation is only valid when each elementary function being used is differentiable.\nUnfortunately many of the functions we use in practice do not have this property (reluorsqrtat0, for example).\nTo try and reduce the impact of functions that are non-differentiable, we define the gradients of the elementary operations by applying the following rules in order:\nrelu\nsqrt\n0\nIf the function is differentiable and thus a gradient exists at the current point, use it.\nIf the function is convex (at least locally), use the sub-gradient of minimum norm.\nIf the function is concave (at least locally), use the super-gradient of minimum norm (consider-f(x)and apply the previous point).\nIf the function is defined, define the gradient at the current point by continuity (note thatinfis possible here, for example forsqrt(0)). If multiple values are possible, pick one arbitrarily.\ninf\nsqrt(0)\nIf the function is not defined (sqrt(-1),log(-1)or most functions when the input isNaN, for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will useNaNas the gradient, but for performance reasons, some functions will use other values (log(-1), for example).\nsqrt(-1)\nlog(-1)\nNaN\nNaN\nlog(-1)\nIf the function is not a deterministic mapping (i.e. it is not amathematical function), it will be marked as non-differentiable. This will make it error out in the backward if used on tensors that require grad outside of ano_gradenvironment.\nno_grad\n\n## Division by Zero in Autograd#\n\nWhen performing division by zero in PyTorch (e.g.,x/0), the forward pass will produceinfvalues following IEEE-754 floating point arithmetic. While theseinfvalues can be masked out before computing the final loss (e.g., via indexing or masking), the autograd system still tracks and differentiates through the full computation graph, including the division by zero operation.\nx/0\ninf\ninf\nDuring backpropagation, this can lead to problematic gradient expressions. For example:\n\n```python\nx = torch.tensor([1., 1.], requires_grad=True)\ndiv = torch.tensor([0., 1.])\n\ny = x / div          # Results in [inf, 1]\nmask = div != 0      # [False, True]\nloss = y[mask].sum()\nloss.backward()\nprint(x.grad)        # [nan, 1], not [0, 1]\n\n```\n\nIn this example, even though we only use the masked output (which excludes the division by zero), autograd still computes gradients through the full computation graph, including the division by zero operation. This results innangradients for the masked elements, which can cause training instability.\nnan\nTo avoid this issue, there are several recommended approaches:\nMask before division:\n\n```python\nx = torch.tensor([1., 1.], requires_grad=True)\ndiv = torch.tensor([0., 1.])\n\nmask = div != 0\nsafe = torch.zeros_like(x)\nsafe[mask] = x[mask] / div[mask]\nloss = safe.sum()\nloss.backward()      # Produces safe gradients [0, 1]\n\n```\n\nUse MaskedTensor (experimental API):\n\n```python\nfrom torch.masked import as_masked_tensor\n\nx = torch.tensor([1., 1.], requires_grad=True)\ndiv = torch.tensor([0., 1.])\n\ny = x / div\nmask = div != 0\nloss = as_masked_tensor(y, mask).sum()\nloss.backward()      # Cleanly handles \"undefined\" vs \"zero\" gradients\n\n```\n\nThe key principle is to prevent the division by zero operation from being recorded in the computation graph, rather than masking its results after the fact. This ensures that autograd only computes gradients through valid operations.\nThis behavior is important to keep in mind when working with operations that might produceinfornanvalues, as masking the outputs does not prevent the problematic gradients from being computed.\ninf\nnan\n\n## Locally disabling gradient computation#\n\nThere are several mechanisms available from Python to locally disable gradient\ncomputation:\nTo disable gradients across entire blocks of code, there are context managers\nlike no-grad mode and inference mode.\nFor more fine-grained exclusion of subgraphs from gradient computation,\nthere is setting therequires_gradfield of a tensor.\nrequires_grad\nBelow, in addition to discussing the mechanisms above, we also describe\nevaluation mode (nn.Module.eval()), a method that is not used\nto disable gradient computation but, because of its name, is often mixed up with the three.\nnn.Module.eval()\n\n## Settingrequires_grad#\n\nrequires_grad\nrequires_gradis a flag, defaulting to falseunless wrapped\nin ann.Parameter, that allows for fine-grained exclusion of\nsubgraphs from gradient computation. It takes effect in both the\nforward and backward passes:\nrequires_grad\nnn.Parameter\nDuring the forward pass, an operation is only recorded in the backward graph if\nat least one of its input tensors require grad.\nDuring the backward pass (.backward()), only leaf tensors withrequires_grad=Truewill have gradients accumulated into their.gradfields.\n.backward()\nrequires_grad=True\n.grad\nIt is important to note that even though every tensor has this flag,settingit only makes sense for leaf tensors (tensors that do not have agrad_fn, e.g., ann.Module\u2019s parameters).\nNon-leaf tensors (tensors that do havegrad_fn) are tensors that have a\nbackward graph associated with them. Thus their gradients will be needed\nas an intermediary result to compute the gradient for a leaf tensor that\nrequires grad. From this definition, it is clear that all non-leaf tensors\nwill automatically haverequire_grad=True.\ngrad_fn\nnn.Module\ngrad_fn\nrequire_grad=True\nSettingrequires_gradshould be the main way you control which parts\nof the model are part of the gradient computation, for example, if you need to\nfreeze parts of your pretrained model during model fine-tuning.\nrequires_grad\nTo freeze parts of your model, simply apply.requires_grad_(False)to\nthe parameters that you don\u2019t want updated. And as described above,\nsince computations that use these parameters as inputs would not be recorded in\nthe forward pass, they won\u2019t have their.gradfields updated in the backward\npass because they won\u2019t be part of the backward graph in the first place, as\ndesired.\n.requires_grad_(False)\n.grad\nBecause this is such a common pattern,requires_gradcan also be set at\nthe module level withnn.Module.requires_grad_().\nWhen applied to a module,.requires_grad_()takes effect on all\nof the module\u2019s parameters (which haverequires_grad=Trueby default).\nrequires_grad\nnn.Module.requires_grad_()\n.requires_grad_()\nrequires_grad=True\n\n## Grad Modes#\n\nApart from settingrequires_gradthere are also three grad modes that can\nbe selected from Python that can affect how computations in PyTorch are\nprocessed by autograd internally: default mode (grad mode), no-grad mode,\nand inference mode, all of which can be togglable via context managers and\ndecorators.\nrequires_grad\nMode\nExcludes operations from being recorded in backward graph\nSkips additional autograd tracking overhead\nTensors created while the mode is enabled can be used in grad-mode later\nExamples\ndefault\n\u2713\nForward pass\nno-grad\n\u2713\n\u2713\nOptimizer updates\ninference\n\u2713\n\u2713\nData processing, model evaluation\n\n## Default Mode (Grad Mode)#\n\nThe \u201cdefault mode\u201d is the mode we are implicitly in when no other modes like\nno-grad and inference mode are enabled. To be contrasted with\n\u201cno-grad mode\u201d the default mode is also sometimes called \u201cgrad mode\u201d.\nThe most important thing to know about the default mode is that it is the only\nmode in whichrequires_gradtakes effect.requires_gradis always overridden\nto beFalsein both the two other modes.\nrequires_grad\nrequires_grad\nFalse\n\n## No-grad Mode#\n\nComputations in no-grad mode behave as if none of the inputs require grad.\nIn other words, computations in no-grad mode are never recorded in the backward graph\neven if there are inputs that haverequire_grad=True.\nrequire_grad=True\nEnable no-grad mode when you need to perform operations that should not be\nrecorded by autograd, but you\u2019d still like to use the outputs of these\ncomputations in grad mode later. This context manager makes it convenient to\ndisable gradients for a block of code or function without\nhaving to temporarily set tensors to haverequires_grad=False, and then\nback toTrue.\nrequires_grad=False\nTrue\nFor example, no-grad mode might be useful when writing an optimizer: when\nperforming the training update you\u2019d like to update parameters\nin-place without the update being recorded by autograd.\nYou also intend to use the updated parameters for computations in\ngrad mode in the next forward pass.\nThe implementations intorch.nn.initalso\nrely on no-grad mode when initializing the parameters as to avoid\nautograd tracking when updating the initialized parameters in-place.\n\n## Inference Mode#\n\nInference mode is the extreme version of no-grad mode. Just like in no-grad\nmode, computations in inference mode are not recorded in the backward graph, but\nenabling inference mode will allow PyTorch to speed up your model even more.\nThis better runtime comes with a drawback: tensors created in inference mode\nwill not be able to be used in computations to be recorded by autograd after\nexiting inference mode.\nEnable inference mode when you are performing computations that do not have\ninteractions with autograd, AND you don\u2019t plan on using the tensors created\nin inference mode in any computation that is to be recorded by autograd later.\nIt is recommended that you try out inference mode in the parts of your code\nthat do not require autograd tracking (e.g., data processing and model evaluation).\nIf it works out of the box\nfor your use case it\u2019s a free performance win. If you run into errors after\nenabling inference mode, check that you are not using tensors created in\ninference mode in computations that are recorded by autograd after exiting inference\nmode. If you cannot avoid such use in your case, you can always switch back\nto no-grad mode.\nFor details on inference mode please seeInference Mode.\nFor implementation details of inference mode seeRFC-0011-InferenceMode.\n\n## Evaluation Mode (nn.Module.eval())#\n\nnn.Module.eval()\nEvaluation mode is not a mechanism to locally disable gradient computation.\nIt is included here anyway because it is sometimes confused to be such a mechanism.\nFunctionally,module.eval()(or equivalentlymodule.train(False)) are completely\northogonal to no-grad mode and inference mode. Howmodel.eval()affects\nyour model depends entirely on the specific modules used in your model and\nwhether they define any training-mode specific behavior.\nmodule.eval()\nmodule.train(False)\nmodel.eval()\nYou are responsible for callingmodel.eval()andmodel.train()if your\nmodel relies on modules such astorch.nn.Dropoutandtorch.nn.BatchNorm2dthat may behave\ndifferently depending on training mode, for example, to avoid updating your\nBatchNorm running statistics on validation data.\nmodel.eval()\nmodel.train()\ntorch.nn.Dropout\ntorch.nn.BatchNorm2d\nIt is recommended that you always usemodel.train()when\ntraining andmodel.eval()when evaluating your model (validation/testing) even\nif you aren\u2019t sure your model has training-mode specific behavior, because a\nmodule you are using might be updated to behave differently in training and\neval modes.\nmodel.train()\nmodel.eval()\n\n## In-place operations with autograd#\n\nSupporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nlower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.\nThere are two main reasons that limit the applicability of in-place operations:\nIn-place operations can potentially overwrite values required to compute\ngradients.\nEvery in-place operation requires the implementation to rewrite the\ncomputational graph. Out-of-place versions simply allocate new objects and\nkeep references to the old graph, while in-place operations, require\nchanging the creator of all inputs to theFunctionrepresenting\nthis operation. This can be tricky, especially if there are many Tensors\nthat reference the same storage (e.g. created by indexing or transposing),\nand in-place functions will raise an error if the storage of\nmodified inputs is referenced by any otherTensor.\nFunction\nTensor\n\n## In-place correctness checks#\n\nEvery tensor keeps a version counter, that is incremented every time it is\nmarked dirty in any operation. When a Function saves any tensors for backward,\na version counter of their containing Tensor is saved as well. Once you accessself.saved_tensorsit is checked, and if it is greater than the saved value\nan error is raised. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.\nself.saved_tensors\n\n## Multithreaded Autograd#\n\nThe autograd engine is responsible for running all the backward operations\nnecessary to compute the backward pass. This section will describe all the details\nthat can help you make the best use of it in a multithreaded environment. (This is\nrelevant only for PyTorch 1.6+ as the behavior in previous version was different.)\nUser could train their model with multithreading code (e.g. Hogwild training), and\ndoes not block on the concurrent backward computations, example code could be:\n\n```python\n# Define a train function to be used in different threads\ndef train_fn():\n    x = torch.ones(5, 5, requires_grad=True)\n    # forward\n    y = (x + 3) * (x + 4) * 0.5\n    # backward\n    y.sum().backward()\n    # potential optimizer update\n\n\n# User write their own threading code to drive the train_fn\nthreads = []\nfor _ in range(10):\n    p = threading.Thread(target=train_fn, args=())\n    p.start()\n    threads.append(p)\n\nfor p in threads:\n    p.join()\n\n```\n\nNote that some behaviors that user should be aware of:\n\n## Concurrency on CPU#\n\nWhen you runbackward()orgrad()via python or C++ API in multiple\nthreads on CPU, you are expecting to see extra concurrency instead of\nserializing all the backward calls in a specific order during execution\n(behavior before PyTorch 1.6).\nbackward()\ngrad()\n\n## Non-determinism#\n\nIf you are callingbackward()from multiple threads concurrently and have\nshared inputs (i.e. Hogwild CPU training), then non-determinism should be expected.\nThis can occur because parameters are automatically shared across threads,\nas such, multiple threads may access and try to accumulate the same.gradattribute during gradient accumulation. This is technically not safe, and\nit might result in race condition and the result might be invalid to use.\nbackward()\n.grad\nUsers developing multithreaded models featuring shared parameters should have the\nthreading model in mind and should understand the issues described above.\nThe functional APItorch.autograd.grad()may be used to calculate the\ngradients instead ofbackward()to avoid non-determinism.\ntorch.autograd.grad()\nbackward()\n\n## Graph retaining#\n\nIf part of the autograd graph is shared between threads, i.e. run first\npart of forward single thread, then run second part in multiple threads,\nthen the first part of graph is shared. In this case different threads\nexecutegrad()orbackward()on the same graph might have issue of\ndestroying the graph on the fly of one thread, and the other thread will\ncrash in this case. Autograd will error out to the user similar to what callbackward()twice with outretain_graph=True, and let the user know\nthey should useretain_graph=True.\ngrad()\nbackward()\nbackward()\nretain_graph=True\nretain_graph=True\n\n## Thread Safety on Autograd Node#\n\nSince Autograd allows the caller thread to drive its backward execution for\npotential parallelism, it\u2019s important that we ensure thread safety on CPU with\nparallelbackward()calls that share part/whole of the GraphTask.\nbackward()\nCustom Pythonautograd.Functions are automatically thread safe because of GIL.\nFor built-in C++ Autograd Nodes (e.g. AccumulateGrad, CopySlices) and customautograd::Functions, the Autograd Engine uses thread mutex locking to ensure\nthread safety on autograd Nodes that might have state write/read.\nautograd.Function\nautograd::Function\n\n## No thread safety on C++ hooks#\n\nAutograd relies on the user to write thread safe C++ hooks. If you want the hook\nto be correctly applied in multithreading environment, you will need to write\nproper thread locking code to ensure the hooks are thread safe.\n\n## Autograd for Complex Numbers#\n\nThe short version:\nWhen you use PyTorch to differentiate any functionf(z)f(z)f(z)with complex domain and/or codomain,\nthe gradients are computed under the assumption that the function is a part of a larger real-valued\nloss functiong(input)=Lg(input)=Lg(input)=L. The gradient computed is\u2202L\u2202z\u2217\\frac{\\partial L}{\\partial z^*}\u2202z\u2217\u2202L\u200b(note the conjugation of z), the negative of which is precisely the direction of steepest descent\nused in Gradient Descent algorithm. Thus, there is a viable path in making the existing optimizers\nwork out of the box with complex parameters.\nThis convention matches TensorFlow\u2019s convention for complex\ndifferentiation, but is different from JAX (which computes\u2202L\u2202z\\frac{\\partial L}{\\partial z}\u2202z\u2202L\u200b).\nIf you have a real-to-real function which internally uses complex\noperations, the convention here doesn\u2019t matter: you will always get\nthe same result that you would have gotten if it had been implemented\nwith only real operations.\nIf you are curious about the mathematical details, or want to know how\nto define complex derivatives in PyTorch, read on.\n\n## What are complex derivatives?#\n\nThe mathematical definition of complex-differentiability takes the\nlimit definition of a derivative and generalizes it to operate on\ncomplex numbers. Consider a functionf:C\u2192Cf: \u2102 \u2192 \u2102f:C\u2192C,\nwhereuuuandvvvare two variable real valued functions\nandjjjis the imaginary unit.\nUsing the derivative definition, we can write:\nIn order for this limit to exist, not only mustuuuandvvvmust be\nreal differentiable, butfffmust also satisfy the Cauchy-Riemannequations.  In\nother words: the limit computed for real and imaginary steps (hhh)\nmust be equal. This is a more restrictive condition.\nThe complex differentiable functions are commonly known as holomorphic\nfunctions. They are well behaved, have all the nice properties that\nyou\u2019ve seen from real differentiable functions, but are practically of no\nuse in the optimization world. For optimization problems, only real valued objective\nfunctions are used in the research community since complex numbers are not part of any\nordered field and so having complex valued loss does not make much sense.\nIt also turns out that no interesting real-valued objective fulfill the\nCauchy-Riemann equations. So the theory with holomorphic function cannot be\nused for optimization and most people therefore use the Wirtinger calculus.\n\n## Wirtinger Calculus comes into the picture \u2026#\n\nSo, we have this great theory of complex differentiability and\nholomorphic functions, and we can\u2019t use any of it at all, because many\nof the commonly used functions are not holomorphic. What\u2019s a poor\nmathematician to do? Well, Wirtinger observed that even iff(z)f(z)f(z)isn\u2019t holomorphic, one could rewrite it as a two variable functionf(z,z\u2217)f(z, z*)f(z,z\u2217)which is always holomorphic. This is because real and\nimaginary of the components ofzzzcan be expressed in terms ofzzzandz\u2217z^*z\u2217as:\nWirtinger calculus suggests to studyf(z,z\u2217)f(z, z^*)f(z,z\u2217)instead, which is\nguaranteed to be holomorphic iffffwas real differentiable (another\nway to think of it is as a change of coordinate system, fromf(x,y)f(x, y)f(x,y)tof(z,z\u2217)f(z, z^*)f(z,z\u2217).)  This function has partial derivatives\u2202\u2202z\\frac{\\partial }{\\partial z}\u2202z\u2202\u200band\u2202\u2202z\u2217\\frac{\\partial}{\\partial z^{*}}\u2202z\u2217\u2202\u200b.\nWe can use the chain rule to establish a\nrelationship between these partial derivatives and the partial\nderivatives w.r.t., the real and imaginary components ofzzz.\nFrom the above equations, we get:\nwhich is the classic definition of Wirtinger calculus that you would find onWikipedia.\nThere are a lot of beautiful consequences of this change.\nFor one, the Cauchy-Riemann equations translate into simply saying that\u2202f\u2202z\u2217=0\\frac{\\partial f}{\\partial z^*} = 0\u2202z\u2217\u2202f\u200b=0(that is to say, the functionfffcan be written\nentirely in terms ofzzz, without making reference toz\u2217z^*z\u2217).\nAnother important (and somewhat counterintuitive) result, as we\u2019ll see later, is that when we do optimization on a real-valued loss, the step we should\ntake while making variable update is given by\u2202Loss\u2202z\u2217\\frac{\\partial Loss}{\\partial z^*}\u2202z\u2217\u2202Loss\u200b(not\u2202Loss\u2202z\\frac{\\partial Loss}{\\partial z}\u2202z\u2202Loss\u200b).\nFor more reading, check out:https://arxiv.org/pdf/0906.4835.pdf\n\n## How is Wirtinger Calculus useful in optimization?#\n\nResearchers in audio and other fields, more commonly, use gradient\ndescent to optimize real valued loss functions with complex variables.\nTypically, these people treat the real and imaginary values as separate\nchannels that can be updated. For a step size\u03b1/2\\alpha/2\u03b1/2and lossLLL, we can write the following equations inR2\u211d^2R2:\nHow do these equations translate into complex spaceC\u2102C?\nSomething very interesting has happened: Wirtinger calculus tells us\nthat we can simplify the complex variable update formula above to only\nrefer to the conjugate Wirtinger derivative\u2202L\u2202z\u2217\\frac{\\partial L}{\\partial z^*}\u2202z\u2217\u2202L\u200b, giving us exactly the step we take in optimization.\nBecause the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative\nwhen you differentiate a function with a real valued loss.\n\n## How does PyTorch compute the conjugate Wirtinger derivative?#\n\nTypically, our derivative formulas take ingrad_outputas an input,\nrepresenting the incoming Vector-Jacobian product that we\u2019ve already\ncomputed, aka,\u2202L\u2202s\u2217\\frac{\\partial L}{\\partial s^*}\u2202s\u2217\u2202L\u200b, whereLLLis the loss of the entire computation (producing a real loss) andsssis the output of our function. The goal here is to compute\u2202L\u2202z\u2217\\frac{\\partial L}{\\partial z^*}\u2202z\u2217\u2202L\u200b, wherezzzis the input of\nthe function.  It turns out that in the case of real loss, we can\nget away withonlycalculating\u2202L\u2202s\u2217\\frac{\\partial L}{\\partial s^*}\u2202s\u2217\u2202L\u200b,\neven though the chain rule implies that we also need to\nhave access to\u2202L\u2202s\\frac{\\partial L}{\\partial s}\u2202s\u2202L\u200b.  If you want\nto skip this derivation, look at the last equation in this section\nand then skip to the next section.\nLet\u2019s continue working withf:C\u2192Cf: \u2102 \u2192 \u2102f:C\u2192Cdefined asf(z)=f(x+yj)=u(x,y)+v(x,y)jf(z) = f(x+yj) = u(x, y) + v(x, y)jf(z)=f(x+yj)=u(x,y)+v(x,y)j. As discussed above,\nautograd\u2019s gradient convention is centered around optimization for real\nvalued loss functions, so let\u2019s assumefffis a part of larger\nreal valued loss functionggg. Using chain rule, we can write:\nNow using Wirtinger derivative definition, we can write:\nIt should be noted here that sinceuuuandvvvare real\nfunctions, andLLLis real by our assumption thatfffis a\npart of a real valued function, we have:\ni.e.,\u2202L\u2202s\\frac{\\partial L}{\\partial s}\u2202s\u2202L\u200bequals tograd_output\u2217grad\\_output^*grad_output\u2217.\nSolving the above equations for\u2202L\u2202u\\frac{\\partial L}{\\partial u}\u2202u\u2202L\u200band\u2202L\u2202v\\frac{\\partial L}{\\partial v}\u2202v\u2202L\u200b, we get:\nSubstituting(3)in(1), we get:\nUsing(2), we get:\nThis last equation is the important one for writing your own gradients,\nas it decomposes our derivative formula into a simpler one that is easy\nto compute by hand.\n\n## How can I write my own derivative formula for a complex function?#\n\nThe above boxed equation gives us the general formula for all\nderivatives on complex functions.  However, we still need to\ncompute\u2202s\u2202z\\frac{\\partial s}{\\partial z}\u2202z\u2202s\u200band\u2202s\u2202z\u2217\\frac{\\partial s}{\\partial z^*}\u2202z\u2217\u2202s\u200b.\nThere are two ways you could do this:\nThe first way is to just use the definition of Wirtinger derivatives directly and calculate\u2202s\u2202z\\frac{\\partial s}{\\partial z}\u2202z\u2202s\u200band\u2202s\u2202z\u2217\\frac{\\partial s}{\\partial z^*}\u2202z\u2217\u2202s\u200bby\nusing\u2202s\u2202x\\frac{\\partial s}{\\partial x}\u2202x\u2202s\u200band\u2202s\u2202y\\frac{\\partial s}{\\partial y}\u2202y\u2202s\u200b(which you can compute in the normal way).\nThe second way is to use the change of variables trick and rewritef(z)f(z)f(z)as a two variable functionf(z,z\u2217)f(z, z^*)f(z,z\u2217), and compute\nthe conjugate Wirtinger derivatives by treatingzzzandz\u2217z^*z\u2217as independent variables. This is often easier; for example, if the function in question is holomorphic, onlyzzzwill be used (and\u2202s\u2202z\u2217\\frac{\\partial s}{\\partial z^*}\u2202z\u2217\u2202s\u200bwill be zero).\nLet\u2019s consider the functionf(z=x+yj)=c\u2217z=c\u2217(x+yj)f(z = x + yj) = c * z = c * (x+yj)f(z=x+yj)=c\u2217z=c\u2217(x+yj)as an example, wherec\u2208Rc \\in \u211dc\u2208R.\nUsing the first way to compute the Wirtinger derivatives, we have.\nUsing(4), andgrad_output = 1.0(which is the default grad output value used whenbackward()is called on a scalar output in PyTorch), we get:\nbackward()\nUsing the second way to compute Wirtinger derivatives, we directly get:\nAnd using(4)again, we get\u2202L\u2202z\u2217=c\\frac{\\partial L}{\\partial z^*} = c\u2202z\u2217\u2202L\u200b=c. As you can see, the second way involves lesser calculations, and comes\nin more handy for faster calculations.\n\n## What about cross-domain functions?#\n\nSome functions map from complex inputs to real outputs, or vice versa.\nThese functions form a special case of(4), which we can derive using the\nchain rule:\nForf:C\u2192Rf: \u2102 \u2192 \u211df:C\u2192R, we get:\nForf:R\u2192Cf: \u211d \u2192 \u2102f:R\u2192C, we get:\n\n## Hooks for saved tensors#\n\nYou can controlhow saved tensors are packed / unpackedby defining a pair ofpack_hook/unpack_hookhooks.  Thepack_hookfunction should take a tensor as its single argument\nbut can return any python object (e.g. another tensor, a tuple, or even a\nstring containing a filename). Theunpack_hookfunction takes as its single\nargument the output ofpack_hookand should return a tensor to be used in\nthe backward pass. The tensor returned byunpack_hookonly needs to have\nthe same content as the tensor passed as input topack_hook. In particular,\nany autograd-related metadata can be ignored as they will be overwritten during\nunpacking.\npack_hook\nunpack_hook\npack_hook\nunpack_hook\npack_hook\nunpack_hook\npack_hook\nAn example of such pair is:\n\n```python\nclass SelfDeletingTempFile():\n    def __init__(self):\n        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n\n    def __del__(self):\n        os.remove(self.name)\n\ndef pack_hook(tensor):\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(temp_file):\n    return torch.load(temp_file.name)\n\n```\n\nNotice that theunpack_hookshould not delete the temporary file because it\nmight be called multiple times: the temporary file should be alive for as long\nas the returnedSelfDeletingTempFileobject is alive.  In the above example,\nwe prevent leaking the temporary file by closing it when it is no longer needed\n(on deletion of theSelfDeletingTempFileobject).\nunpack_hook\nNote\nWe guarantee thatpack_hookwill only be called once butunpack_hookcan\nbe called as many times as the backward pass requires it and we expect it to\nreturn the same data each time.\npack_hook\nunpack_hook\nWarning\nPerforming inplace operations on the input of any of the functions is forbidden\nas they may lead to unexpected side-effects. PyTorch will throw an error if the\ninput to a pack hook is modified inplace but does not catch the case where the\ninput to an unpack hook is modified inplace.\n\n## Registering hooks for a saved tensor#\n\nYou can register a pair of hooks on a saved tensor by calling theregister_hooks()method on aSavedTensorobject. Those objects are exposed as attributes of agrad_fnand start with the_raw_saved_prefix.\nregister_hooks()\nSavedTensor\ngrad_fn\n_raw_saved_\n\n```python\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\ny.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)\n\n```\n\nThepack_hookmethod is called as soon as the pair is registered.\nTheunpack_hookmethod is called each time the saved tensor needs to be\naccessed, either by means ofy.grad_fn._saved_selfor during the backward\npass.\npack_hook\nunpack_hook\ny.grad_fn._saved_self\nWarning\nIf you maintain a reference to aSavedTensorafter the saved\ntensors have been released (i.e. after backward has been called), calling\nitsregister_hooks()is forbidden.\nPyTorch will throw an error most of the time but it may fail\nto do so in some cases and undefined behavior may arise.\nSavedTensor\nregister_hooks()\n\n## Registering default hooks for saved tensors#\n\nAlternatively, you can use the context-managersaved_tensors_hooksto register a pair of\nhooks which will be applied toallsaved tensors that are created in\nthat context.\nsaved_tensors_hooks\nExample:\n\n```python\n# Only save on disk tensors that have size >= 1000\nSAVE_ON_DISK_THRESHOLD = 1000\n\ndef pack_hook(x):\n    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n        return x.detach()\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(tensor_or_sctf):\n    if isinstance(tensor_or_sctf, torch.Tensor):\n        return tensor_or_sctf\n    return torch.load(tensor_or_sctf.name)\n\nclass Model(nn.Module):\n    def forward(self, x):\n        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n          # ... compute output\n          output = x\n        return output\n\nmodel = Model()\nnet = nn.DataParallel(model)\n\n```\n\nThe hooks defined with this context manager are thread-local.\nHence, the following code will not produce the desired effects because the hooks do not go\nthroughDataParallel.\n\n```python\n# Example what NOT to do\n\nnet = nn.DataParallel(model)\nwith torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n    output = net(input)\n\n```\n\nNote that using those hooks disables all the optimization in place to reduce\nTensor object creation. For example:\n\n```python\nwith torch.autograd.graph.saved_tensors_hooks(lambda x: x.detach(), lambda x: x):\n    x = torch.randn(5, requires_grad=True)\n    y = x * x\n\n```\n\nWithout the hooks,x,y.grad_fn._saved_selfandy.grad_fn._saved_otherall refer to the same tensor object.\nWith the hooks, PyTorch will pack and unpackxinto two new tensor objects\nthat share the same storage with the originalx(no copy performed).\nx\ny.grad_fn._saved_self\ny.grad_fn._saved_other\n\n## Backward Hooks execution#\n\nThis section will discuss when different hooks fire or don\u2019t fire.\nThen it will discuss the order in which they are fired.\nThe hooks that will be covered are: backward hooks registered to Tensor viatorch.Tensor.register_hook(), post-accumulate-grad hooks registered to\nTensor viatorch.Tensor.register_post_accumulate_grad_hook(), post-hooks\nregistered to Node viatorch.autograd.graph.Node.register_hook(), and\npre-hooks registered to Node viatorch.autograd.graph.Node.register_prehook().\ntorch.Tensor.register_hook()\ntorch.Tensor.register_post_accumulate_grad_hook()\ntorch.autograd.graph.Node.register_hook()\ntorch.autograd.graph.Node.register_prehook()\n\n## Whether a particular hook will be fired#\n\nHooks registered to a Tensor viatorch.Tensor.register_hook()are executed when gradients are being computed for that Tensor. (Note that this does not require\nthe Tensor\u2019s grad_fn to be executed. For example, if the Tensor is passed\nas part of theinputsargument totorch.autograd.grad(),\nthe Tensor\u2019s grad_fn may not be executed, but the hook register to that Tensor will always be executed.)\ntorch.Tensor.register_hook()\ninputs\ntorch.autograd.grad()\nHooks registered to a Tensor viatorch.Tensor.register_post_accumulate_grad_hook()are executed after the gradients have been accumulated for that Tensor, meaning the\nTensor\u2019s grad field has been set. Whereas hooks registered viatorch.Tensor.register_hook()are run as gradients are being computed, hooks registered viatorch.Tensor.register_post_accumulate_grad_hook()are only triggered once the Tensor\u2019s grad field is updated by autograd at the end of\nthe backward pass. Thus, post-accumulate-grad hooks can only be registered for leaf\nTensors. Registering a hook viatorch.Tensor.register_post_accumulate_grad_hook()on a non-leaf Tensor will error, even if you callbackward(retain_graph=True).\ntorch.Tensor.register_post_accumulate_grad_hook()\ntorch.Tensor.register_hook()\ntorch.Tensor.register_post_accumulate_grad_hook()\ntorch.Tensor.register_post_accumulate_grad_hook()\nHooks registered totorch.autograd.graph.Nodeusingtorch.autograd.graph.Node.register_hook()ortorch.autograd.graph.Node.register_prehook()are only fired if\nthe Node it was registered to is executed.\ntorch.autograd.graph.Node\ntorch.autograd.graph.Node.register_hook()\ntorch.autograd.graph.Node.register_prehook()\nWhether a particular Node is executed may depend on whether the backward pass was called withtorch.autograd.grad()ortorch.autograd.backward().\nSpecifically, you should be aware of these differences when you register a hook on a\nNode corresponding to a Tensor that you are passing totorch.autograd.grad()ortorch.autograd.backward()as part of theinputsargument.\ntorch.autograd.grad()\ntorch.autograd.backward()\ntorch.autograd.grad()\ntorch.autograd.backward()\ninputs\nIf you are usingtorch.autograd.backward(), all of the above mentioned hooks will be executed,\nwhether or not you specified theinputsargument. This is because.backward()executes all\nNodes, even if they correspond to a Tensor specified as an input.\n(Note that the execution of this additional Node corresponding to Tensors passed asinputsis usually unnecessary, but done anyway. This behavior is subject to change;\nyou should not depend on it.)\ntorch.autograd.backward()\ninputs\ninputs\nOn the other hand, if you are usingtorch.autograd.grad(), the backward hooks registered\nto Nodes that correspond to the Tensors passed toinputmay not be executed, because\nthose Nodes will not be executed unless there is another input that depends on the gradient\nresult of this Node.\ntorch.autograd.grad()\ninput\n\n## The order in which the different hooks are fired#\n\nThe order in which things happen are:\nhooks registered to Tensor are executed\npre-hooks registered to Node are executed (if Node is executed).\nthe.gradfield is updated for Tensors that retain_grad\n.grad\nNode is executed (subject to rules above)\nfor leaf Tensors that have.gradaccumulated, post-accumulate-grad hooks are executed\n.grad\npost-hooks registered to Node are executed (if Node is executed)\nIf multiple hooks of the same type are registered on the same Tensor or Node\nthey are executed in the order in which they are registered.\nHooks that are executed later can observe the modifications to the gradient made by\nearlier hooks.\n\n## Special hooks#\n\ntorch.autograd.graph.register_multi_grad_hook()is implemented using hooks registered\nto Tensors. Each individual Tensor hook is fired following the Tensor hook ordering\ndefined above and the registered multi-grad hook is called when the last Tensor gradient\nis computed.\ntorch.autograd.graph.register_multi_grad_hook()\ntorch.nn.modules.module.register_module_full_backward_hook()is implemented using hooks\nregistered to Node. As the forward is computed, hooks are registered to grad_fn corresponding\nto the inputs and outputs of the module. Because a module may take multiple inputs and return\nmultiple outputs, a dummy custom autograd Function is first applied to the inputs of the module\nbefore forward and the outputs of the module before the output of forward is returned to ensure\nthat those Tensors share a single grad_fn, which we can then attach our hooks to.\ntorch.nn.modules.module.register_module_full_backward_hook()\n\n## Behavior of Tensor hooks when Tensor is modified in-place#\n\nUsually hooks registered to a Tensor receive the gradient of the outputs with respect to that\nTensor, where the value of the Tensor is taken to be its value at the time backward is computed.\nHowever, if you register hooks to a Tensor, and then modify that Tensor in-place, hooks\nregistered before in-place modification similarly receive gradients of the outputs with\nrespect to the Tensor, but the value of the Tensor is taken to be its value before\nin-place modification.\nIf you prefer the behavior in the former case,\nyou should register them to the Tensor after all in-place modifications to it have been made.\nFor example:\n\n```python\nt = torch.tensor(1., requires_grad=True).sin()\nt.cos_()\nt.register_hook(fn)\nt.backward()\n\n```\n\nFurthermore, it can be helpful to know that under the hood,\nwhen hooks are registered to a Tensor, they actually become permanently bound to the grad_fn\nof that Tensor, so if that Tensor is then modified in-place,\neven though the Tensor now has a new grad_fn, hooks registered before it was\nmodified in-place will continue to be associated with the old grad_fn, e.g. they will\nfire when that Tensor\u2019s old grad_fn is reached in the graph by the autograd engine.",
  "url": "https://pytorch.org/docs/stable/notes/autograd.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
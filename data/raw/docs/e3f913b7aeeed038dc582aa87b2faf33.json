{
  "doc_id": "e3f913b7aeeed038dc582aa87b2faf33",
  "source": "pytorch_docs",
  "title": "Use torch._dynamo.nonstrict_trace \u2014 PyTorch 2.9 documentation",
  "text": "\n## Usetorch._dynamo.nonstrict_trace#\n\ntorch._dynamo.nonstrict_trace\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nSummary:\nUsenonstrict_traceto trace a function with non-strict tracing inside of atorch.compile\u2019d region.\nYou may wish to do this because the Dynamo graph breaks on something inside of the function\nand you are sure that the function is non-strict traceable.\nnonstrict_trace\ntorch.compile\nConsider the following scenario:\n\n```python\ndef get_magic_num():\n    # This explicit graph break call is meant to emulate any kind of Dynamo\n    # graph break, e.g., the function is implemented in C, or uses some python\n    # language feature Dynamo doesn't yet support.\n    torch._dynamo.graph_break()\n    return torch.tensor([42])\n@torch.compile(fullgraph=True)\ndef func(x):\n    n = get_magic_num()\n    return x + n\ntry:\n    func(torch.rand(10))\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nCall to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_316/2253748958.py\", line 9, in func\n    n = get_magic_num()\n  File \"/tmp/ipykernel_316/2253748958.py\", line 5, in get_magic_num\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\nIf we run the code above, we\u2019ll get an error from Dynamo, because it sees a graph break while the user specifiedfullgraph=True.\nfullgraph=True\nIn these situations, if a user still wants to keepfullgraph=True, they typically have several options:\nfullgraph=True\nThe graph break is due to a language feature Dynamo doesn\u2019t yet support.\nIn this case, the user either rewrites their code, or files an issue on GitHub.\nThe graph break is due to a call to a function implemented in C.\nIn this case, the user can try to use a custom op.\nThe user could also try providing a polyfill (a reference implementation in Python)\nso that Dynamo can trace through it.\nWorst case scenario \u2013 an internal compiler error. In this case, the user likely has to file an issue on GitHub.\nIn addition to all these options, PyTorch does provide an alternativetorch._dynamo.nonstrict_trace, if the function call that induced the graph break satisfies certain requirements:\ntorch._dynamo.nonstrict_trace\nThe requirements ofgeneral non-strict tracing.\nThe inputs and outputs must contain either basic types (e.g.,int,float,list,dict,torch.Tensor),\nor user-defined types that are registered totorch.utils._pytree.\nint\nfloat\nlist\ndict\ntorch.Tensor\ntorch.utils._pytree\nThe function must be defined outside thetorch.compile\u2019d region.\ntorch.compile\nAny non-input values read by the function will be treated as a constant\n(e.g., a global tensor), and will not be guarded on.\nWhen tracing through a call to atorch._dynamo.nonstrict_trace\u2019d function,torch.compileswitches tonon-strict tracing,\nand the FX graph will eventually contain all the relevant tensor operations which happened inside that function.\ntorch._dynamo.nonstrict_trace\ntorch.compile\nFor the example above, we can usetorch._dynamo.nonstrict_tracetoeliminatethe graph break:\ntorch._dynamo.nonstrict_tracetoeliminate\n\n```python\n@torch._dynamo.nonstrict_trace\ndef get_magic_num():\n    # This explicit graph break call is meant to emulate any kind of Dynamo\n    # graph break, e.g., the function is implemented in C, or uses some python\n    # language feature Dynamo doesn't yet support.\n    torch._dynamo.graph_break()\n    return torch.tensor([42])\n@torch.compile(fullgraph=True)\ndef func(x):\n    n = get_magic_num()\n    return x + n\nprint(func(torch.rand(10)))\n# No graph break and no error.\n\n```\n\n\n```python\ntensor([42.6048, 42.6620, 42.3730, 42.1969, 42.7525, 42.4637, 42.1544, 42.3485,\n        42.8316, 42.8360])\n\n```\n\nNote that one can use it inside atorch.compile\u2019d region as well:\ntorch.compile\n\n```python\ndef get_magic_num():\n    # This explicit graph break call is meant to emulate any kind of Dynamo\n    # graph break, e.g., the function is implemented in C, or uses some python\n    # language feature Dynamo doesn't yet support.\n    torch._dynamo.graph_break()\n    return torch.tensor([42])\n@torch.compile(fullgraph=True)\ndef func(x):\n    n = torch._dynamo.nonstrict_trace(get_magic_num)()\n    return x + n\nprint(func(torch.rand(10)))\n# No graph break and no error.\n\n```\n\n\n```python\ntensor([42.8445, 42.3965, 42.4669, 42.6729, 42.7607, 42.7630, 42.2933, 42.7238,\n        42.9972, 42.4853])\n\n```\n",
  "url": "https://pytorch.org/docs/stable/compile/programming_model.dynamo_nonstrict_trace.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
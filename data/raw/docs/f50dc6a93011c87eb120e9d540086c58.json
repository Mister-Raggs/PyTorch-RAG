{
  "doc_id": "f50dc6a93011c87eb120e9d540086c58",
  "source": "pytorch_docs",
  "title": "Named Tensors operator coverage \u2014 PyTorch 2.9 documentation",
  "text": "\n## Named Tensors operator coverage#\n\nCreated On: Oct 08, 2019 | Last Updated On: Jun 08, 2025\nPlease readNamed Tensorsfirst for an introduction to named tensors.\nThis document is a reference forname inference, a process that defines how\nnamed tensors:\nuse names to provide additional automatic runtime correctness checks\npropagate names from input tensors to output tensors\nBelow is a list of all operations that are supported with named tensors\nand their associated name inference rules.\nIf you don\u2019t see an operation listed here, but it would help your use case, pleasesearch if an issue has already been filedand if not,file one.\nWarning\nThe named tensor API is experimental and subject to change.\nAPI\nName inference rule\nTensor.abs(),torch.abs()\nTensor.abs()\ntorch.abs()\nKeeps input names\nTensor.abs_()\nTensor.abs_()\nKeeps input names\nTensor.acos(),torch.acos()\nTensor.acos()\ntorch.acos()\nKeeps input names\nTensor.acos_()\nTensor.acos_()\nKeeps input names\nTensor.add(),torch.add()\nTensor.add()\ntorch.add()\nUnifies names from inputs\nTensor.add_()\nTensor.add_()\nUnifies names from inputs\nTensor.addmm(),torch.addmm()\nTensor.addmm()\ntorch.addmm()\nContracts away dims\nTensor.addmm_()\nTensor.addmm_()\nContracts away dims\nTensor.addmv(),torch.addmv()\nTensor.addmv()\ntorch.addmv()\nContracts away dims\nTensor.addmv_()\nTensor.addmv_()\nContracts away dims\nTensor.align_as()\nTensor.align_as()\nSee documentation\nTensor.align_to()\nTensor.align_to()\nSee documentation\nTensor.all(),torch.all()\nTensor.all()\ntorch.all()\nNone\nTensor.any(),torch.any()\nTensor.any()\ntorch.any()\nNone\nTensor.asin(),torch.asin()\nTensor.asin()\ntorch.asin()\nKeeps input names\nTensor.asin_()\nTensor.asin_()\nKeeps input names\nTensor.atan(),torch.atan()\nTensor.atan()\ntorch.atan()\nKeeps input names\nTensor.atan2(),torch.atan2()\nTensor.atan2()\ntorch.atan2()\nUnifies names from inputs\nTensor.atan2_()\nTensor.atan2_()\nUnifies names from inputs\nTensor.atan_()\nTensor.atan_()\nKeeps input names\nTensor.bernoulli(),torch.bernoulli()\nTensor.bernoulli()\ntorch.bernoulli()\nKeeps input names\nTensor.bernoulli_()\nTensor.bernoulli_()\nNone\nTensor.bfloat16()\nTensor.bfloat16()\nKeeps input names\nTensor.bitwise_not(),torch.bitwise_not()\nTensor.bitwise_not()\ntorch.bitwise_not()\nKeeps input names\nTensor.bitwise_not_()\nTensor.bitwise_not_()\nNone\nTensor.bmm(),torch.bmm()\nTensor.bmm()\ntorch.bmm()\nContracts away dims\nTensor.bool()\nTensor.bool()\nKeeps input names\nTensor.byte()\nTensor.byte()\nKeeps input names\ntorch.cat()\ntorch.cat()\nUnifies names from inputs\nTensor.cauchy_()\nTensor.cauchy_()\nNone\nTensor.ceil(),torch.ceil()\nTensor.ceil()\ntorch.ceil()\nKeeps input names\nTensor.ceil_()\nTensor.ceil_()\nNone\nTensor.char()\nTensor.char()\nKeeps input names\nTensor.chunk(),torch.chunk()\nTensor.chunk()\ntorch.chunk()\nKeeps input names\nTensor.clamp(),torch.clamp()\nTensor.clamp()\ntorch.clamp()\nKeeps input names\nTensor.clamp_()\nTensor.clamp_()\nNone\nTensor.copy_()\nTensor.copy_()\nout function and in-place variants\nTensor.cos(),torch.cos()\nTensor.cos()\ntorch.cos()\nKeeps input names\nTensor.cos_()\nTensor.cos_()\nNone\nTensor.cosh(),torch.cosh()\nTensor.cosh()\ntorch.cosh()\nKeeps input names\nTensor.cosh_()\nTensor.cosh_()\nNone\nTensor.acosh(),torch.acosh()\nTensor.acosh()\ntorch.acosh()\nKeeps input names\nTensor.acosh_()\nTensor.acosh_()\nNone\nTensor.cpu()\nTensor.cpu()\nKeeps input names\nTensor.cuda()\nTensor.cuda()\nKeeps input names\nTensor.cumprod(),torch.cumprod()\nTensor.cumprod()\ntorch.cumprod()\nKeeps input names\nTensor.cumsum(),torch.cumsum()\nTensor.cumsum()\ntorch.cumsum()\nKeeps input names\nTensor.data_ptr()\nTensor.data_ptr()\nNone\nTensor.deg2rad(),torch.deg2rad()\nTensor.deg2rad()\ntorch.deg2rad()\nKeeps input names\nTensor.deg2rad_()\nTensor.deg2rad_()\nNone\nTensor.detach(),torch.detach()\nTensor.detach()\ntorch.detach()\nKeeps input names\nTensor.detach_()\nTensor.detach_()\nNone\nTensor.device,torch.device()\nTensor.device\ntorch.device()\nNone\nTensor.digamma(),torch.digamma()\nTensor.digamma()\ntorch.digamma()\nKeeps input names\nTensor.digamma_()\nTensor.digamma_()\nNone\nTensor.dim()\nTensor.dim()\nNone\nTensor.div(),torch.div()\nTensor.div()\ntorch.div()\nUnifies names from inputs\nTensor.div_()\nTensor.div_()\nUnifies names from inputs\nTensor.dot(),torch.dot()\nTensor.dot()\ntorch.dot()\nNone\nTensor.double()\nTensor.double()\nKeeps input names\nTensor.element_size()\nTensor.element_size()\nNone\ntorch.empty()\ntorch.empty()\nFactory functions\ntorch.empty_like()\ntorch.empty_like()\nFactory functions\nTensor.eq(),torch.eq()\nTensor.eq()\ntorch.eq()\nUnifies names from inputs\nTensor.erf(),torch.erf()\nTensor.erf()\ntorch.erf()\nKeeps input names\nTensor.erf_()\nTensor.erf_()\nNone\nTensor.erfc(),torch.erfc()\nTensor.erfc()\ntorch.erfc()\nKeeps input names\nTensor.erfc_()\nTensor.erfc_()\nNone\nTensor.erfinv(),torch.erfinv()\nTensor.erfinv()\ntorch.erfinv()\nKeeps input names\nTensor.erfinv_()\nTensor.erfinv_()\nNone\nTensor.exp(),torch.exp()\nTensor.exp()\ntorch.exp()\nKeeps input names\nTensor.exp_()\nTensor.exp_()\nNone\nTensor.expand()\nTensor.expand()\nKeeps input names\nTensor.expm1(),torch.expm1()\nTensor.expm1()\ntorch.expm1()\nKeeps input names\nTensor.expm1_()\nTensor.expm1_()\nNone\nTensor.exponential_()\nTensor.exponential_()\nNone\nTensor.fill_()\nTensor.fill_()\nNone\nTensor.flatten(),torch.flatten()\nTensor.flatten()\ntorch.flatten()\nSee documentation\nTensor.float()\nTensor.float()\nKeeps input names\nTensor.floor(),torch.floor()\nTensor.floor()\ntorch.floor()\nKeeps input names\nTensor.floor_()\nTensor.floor_()\nNone\nTensor.frac(),torch.frac()\nTensor.frac()\ntorch.frac()\nKeeps input names\nTensor.frac_()\nTensor.frac_()\nNone\nTensor.ge(),torch.ge()\nTensor.ge()\ntorch.ge()\nUnifies names from inputs\nTensor.get_device(),torch.get_device()\nTensor.get_device()\ntorch.get_device()\nNone\nTensor.grad\nTensor.grad\nNone\nTensor.gt(),torch.gt()\nTensor.gt()\ntorch.gt()\nUnifies names from inputs\nTensor.half()\nTensor.half()\nKeeps input names\nTensor.has_names()\nTensor.has_names()\nSee documentation\nTensor.index_fill(),torch.index_fill()\nTensor.index_fill()\ntorch.index_fill()\nKeeps input names\nTensor.index_fill_()\nTensor.index_fill_()\nNone\nTensor.int()\nTensor.int()\nKeeps input names\nTensor.is_contiguous()\nTensor.is_contiguous()\nNone\nTensor.is_cuda\nTensor.is_cuda\nNone\nTensor.is_floating_point(),torch.is_floating_point()\nTensor.is_floating_point()\ntorch.is_floating_point()\nNone\nTensor.is_leaf\nTensor.is_leaf\nNone\nTensor.is_pinned()\nTensor.is_pinned()\nNone\nTensor.is_shared()\nTensor.is_shared()\nNone\nTensor.is_signed(),torch.is_signed()\nTensor.is_signed()\ntorch.is_signed()\nNone\nTensor.is_sparse\nTensor.is_sparse\nNone\nTensor.is_sparse_csr\nTensor.is_sparse_csr\nNone\ntorch.is_tensor()\ntorch.is_tensor()\nNone\nTensor.item()\nTensor.item()\nNone\nTensor.itemsize\nTensor.itemsize\nNone\nTensor.kthvalue(),torch.kthvalue()\nTensor.kthvalue()\ntorch.kthvalue()\nRemoves dimensions\nTensor.le(),torch.le()\nTensor.le()\ntorch.le()\nUnifies names from inputs\nTensor.log(),torch.log()\nTensor.log()\ntorch.log()\nKeeps input names\nTensor.log10(),torch.log10()\nTensor.log10()\ntorch.log10()\nKeeps input names\nTensor.log10_()\nTensor.log10_()\nNone\nTensor.log1p(),torch.log1p()\nTensor.log1p()\ntorch.log1p()\nKeeps input names\nTensor.log1p_()\nTensor.log1p_()\nNone\nTensor.log2(),torch.log2()\nTensor.log2()\ntorch.log2()\nKeeps input names\nTensor.log2_()\nTensor.log2_()\nNone\nTensor.log_()\nTensor.log_()\nNone\nTensor.log_normal_()\nTensor.log_normal_()\nNone\nTensor.logical_not(),torch.logical_not()\nTensor.logical_not()\ntorch.logical_not()\nKeeps input names\nTensor.logical_not_()\nTensor.logical_not_()\nNone\nTensor.logsumexp(),torch.logsumexp()\nTensor.logsumexp()\ntorch.logsumexp()\nRemoves dimensions\nTensor.long()\nTensor.long()\nKeeps input names\nTensor.lt(),torch.lt()\nTensor.lt()\ntorch.lt()\nUnifies names from inputs\ntorch.manual_seed()\ntorch.manual_seed()\nNone\nTensor.masked_fill(),torch.masked_fill()\nTensor.masked_fill()\ntorch.masked_fill()\nKeeps input names\nTensor.masked_fill_()\nTensor.masked_fill_()\nNone\nTensor.masked_select(),torch.masked_select()\nTensor.masked_select()\ntorch.masked_select()\nAligns mask up to input and then unifies_names_from_input_tensors\nTensor.matmul(),torch.matmul()\nTensor.matmul()\ntorch.matmul()\nContracts away dims\nTensor.mean(),torch.mean()\nTensor.mean()\ntorch.mean()\nRemoves dimensions\nTensor.median(),torch.median()\nTensor.median()\ntorch.median()\nRemoves dimensions\nTensor.nanmedian(),torch.nanmedian()\nTensor.nanmedian()\ntorch.nanmedian()\nRemoves dimensions\nTensor.mm(),torch.mm()\nTensor.mm()\ntorch.mm()\nContracts away dims\nTensor.mode(),torch.mode()\nTensor.mode()\ntorch.mode()\nRemoves dimensions\nTensor.mul(),torch.mul()\nTensor.mul()\ntorch.mul()\nUnifies names from inputs\nTensor.mul_()\nTensor.mul_()\nUnifies names from inputs\nTensor.mv(),torch.mv()\nTensor.mv()\ntorch.mv()\nContracts away dims\nTensor.names\nTensor.names\nSee documentation\nTensor.narrow(),torch.narrow()\nTensor.narrow()\ntorch.narrow()\nKeeps input names\nTensor.nbytes\nTensor.nbytes\nNone\nTensor.ndim\nTensor.ndim\nNone\nTensor.ndimension()\nTensor.ndimension()\nNone\nTensor.ne(),torch.ne()\nTensor.ne()\ntorch.ne()\nUnifies names from inputs\nTensor.neg(),torch.neg()\nTensor.neg()\ntorch.neg()\nKeeps input names\nTensor.neg_()\nTensor.neg_()\nNone\ntorch.normal()\ntorch.normal()\nKeeps input names\nTensor.normal_()\nTensor.normal_()\nNone\nTensor.numel(),torch.numel()\nTensor.numel()\ntorch.numel()\nNone\ntorch.ones()\ntorch.ones()\nFactory functions\nTensor.pow(),torch.pow()\nTensor.pow()\ntorch.pow()\nUnifies names from inputs\nTensor.pow_()\nTensor.pow_()\nNone\nTensor.prod(),torch.prod()\nTensor.prod()\ntorch.prod()\nRemoves dimensions\nTensor.rad2deg(),torch.rad2deg()\nTensor.rad2deg()\ntorch.rad2deg()\nKeeps input names\nTensor.rad2deg_()\nTensor.rad2deg_()\nNone\ntorch.rand()\ntorch.rand()\nFactory functions\ntorch.rand()\ntorch.rand()\nFactory functions\ntorch.randn()\ntorch.randn()\nFactory functions\ntorch.randn()\ntorch.randn()\nFactory functions\nTensor.random_()\nTensor.random_()\nNone\nTensor.reciprocal(),torch.reciprocal()\nTensor.reciprocal()\ntorch.reciprocal()\nKeeps input names\nTensor.reciprocal_()\nTensor.reciprocal_()\nNone\nTensor.refine_names()\nTensor.refine_names()\nSee documentation\nTensor.register_hook()\nTensor.register_hook()\nNone\nTensor.register_post_accumulate_grad_hook()\nTensor.register_post_accumulate_grad_hook()\nNone\nTensor.rename()\nTensor.rename()\nSee documentation\nTensor.rename_()\nTensor.rename_()\nSee documentation\nTensor.requires_grad\nTensor.requires_grad\nNone\nTensor.requires_grad_()\nTensor.requires_grad_()\nNone\nTensor.resize_()\nTensor.resize_()\nOnly allow resizes that do not change shape\nTensor.resize_as_()\nTensor.resize_as_()\nOnly allow resizes that do not change shape\nTensor.round(),torch.round()\nTensor.round()\ntorch.round()\nKeeps input names\nTensor.round_()\nTensor.round_()\nNone\nTensor.rsqrt(),torch.rsqrt()\nTensor.rsqrt()\ntorch.rsqrt()\nKeeps input names\nTensor.rsqrt_()\nTensor.rsqrt_()\nNone\nTensor.select(),torch.select()\nTensor.select()\ntorch.select()\nRemoves dimensions\nTensor.short()\nTensor.short()\nKeeps input names\nTensor.sigmoid(),torch.sigmoid()\nTensor.sigmoid()\ntorch.sigmoid()\nKeeps input names\nTensor.sigmoid_()\nTensor.sigmoid_()\nNone\nTensor.sign(),torch.sign()\nTensor.sign()\ntorch.sign()\nKeeps input names\nTensor.sign_()\nTensor.sign_()\nNone\nTensor.sgn(),torch.sgn()\nTensor.sgn()\ntorch.sgn()\nKeeps input names\nTensor.sgn_()\nTensor.sgn_()\nNone\nTensor.sin(),torch.sin()\nTensor.sin()\ntorch.sin()\nKeeps input names\nTensor.sin_()\nTensor.sin_()\nNone\nTensor.sinh(),torch.sinh()\nTensor.sinh()\ntorch.sinh()\nKeeps input names\nTensor.sinh_()\nTensor.sinh_()\nNone\nTensor.asinh(),torch.asinh()\nTensor.asinh()\ntorch.asinh()\nKeeps input names\nTensor.asinh_()\nTensor.asinh_()\nNone\nTensor.size()\nTensor.size()\nNone\nTensor.softmax(),torch.softmax()\nTensor.softmax()\ntorch.softmax()\nKeeps input names\nTensor.split(),torch.split()\nTensor.split()\ntorch.split()\nKeeps input names\nTensor.sqrt(),torch.sqrt()\nTensor.sqrt()\ntorch.sqrt()\nKeeps input names\nTensor.sqrt_()\nTensor.sqrt_()\nNone\nTensor.squeeze(),torch.squeeze()\nTensor.squeeze()\ntorch.squeeze()\nRemoves dimensions\nTensor.std(),torch.std()\nTensor.std()\ntorch.std()\nRemoves dimensions\ntorch.std_mean()\ntorch.std_mean()\nRemoves dimensions\nTensor.stride()\nTensor.stride()\nNone\nTensor.sub(),torch.sub()\nTensor.sub()\ntorch.sub()\nUnifies names from inputs\nTensor.sub_()\nTensor.sub_()\nUnifies names from inputs\nTensor.sum(),torch.sum()\nTensor.sum()\ntorch.sum()\nRemoves dimensions\nTensor.tan(),torch.tan()\nTensor.tan()\ntorch.tan()\nKeeps input names\nTensor.tan_()\nTensor.tan_()\nNone\nTensor.tanh(),torch.tanh()\nTensor.tanh()\ntorch.tanh()\nKeeps input names\nTensor.tanh_()\nTensor.tanh_()\nNone\nTensor.atanh(),torch.atanh()\nTensor.atanh()\ntorch.atanh()\nKeeps input names\nTensor.atanh_()\nTensor.atanh_()\nNone\ntorch.tensor()\ntorch.tensor()\nFactory functions\nTensor.to()\nTensor.to()\nKeeps input names\nTensor.topk(),torch.topk()\nTensor.topk()\ntorch.topk()\nRemoves dimensions\nTensor.transpose(),torch.transpose()\nTensor.transpose()\ntorch.transpose()\nPermutes dimensions\nTensor.trunc(),torch.trunc()\nTensor.trunc()\ntorch.trunc()\nKeeps input names\nTensor.trunc_()\nTensor.trunc_()\nNone\nTensor.type()\nTensor.type()\nNone\nTensor.type_as()\nTensor.type_as()\nKeeps input names\nTensor.unbind(),torch.unbind()\nTensor.unbind()\ntorch.unbind()\nRemoves dimensions\nTensor.unflatten()\nTensor.unflatten()\nSee documentation\nTensor.uniform_()\nTensor.uniform_()\nNone\nTensor.var(),torch.var()\nTensor.var()\ntorch.var()\nRemoves dimensions\ntorch.var_mean()\ntorch.var_mean()\nRemoves dimensions\nTensor.zero_()\nTensor.zero_()\nNone\ntorch.zeros()\ntorch.zeros()\nFactory functions\n\n## Keeps input names#\n\nAll pointwise unary functions follow this rule as well as some other unary functions.\nCheck names: None\nPropagate names: input tensor\u2019s names are propagated to the output.\n\n```python\n>>> x = torch.randn(3, 3, names=('N', 'C'))\n>>> x.abs().names\n('N', 'C')\n\n```\n\n\n## Removes dimensions#\n\nAll reduction ops likesum()remove dimensions by reducing\nover the desired dimensions. Other operations likeselect()andsqueeze()remove dimensions.\nsum()\nselect()\nsqueeze()\nWherever one can pass an integer dimension index to an operator, one can also pass\na dimension name. Functions that take lists of dimension indices can also take in a\nlist of dimension names.\nCheck names: Ifdimordimsis passed in as a list of names,\ncheck that those names exist inself.\ndim\ndims\nself\nPropagate names: If the dimensions of the input tensor specified bydimordimsare not present in the output tensor, then the corresponding names\nof those dimensions do not appear inoutput.names.\ndim\ndims\noutput.names\n\n```python\n>>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.squeeze('N').names\n('C', 'H', 'W')\n\n>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.sum(['N', 'C']).names\n('H', 'W')\n\n# Reduction ops with keepdim=True don't actually remove dimensions.\n>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.sum(['N', 'C'], keepdim=True).names\n('N', 'C', 'H', 'W')\n\n```\n\n\n## Unifies names from inputs#\n\nAll binary arithmetic ops follow this rule. Operations that broadcast still\nbroadcast positionally from the right to preserve compatibility with unnamed\ntensors. To perform explicit broadcasting by names, useTensor.align_as().\nTensor.align_as()\nCheck names: All names must match positionally from the right. i.e., intensor+other,match(tensor.names[i],other.names[i])must be true for alliin(-min(tensor.dim(),other.dim())+1,-1].\ntensor+other\nmatch(tensor.names[i],other.names[i])\ni\n(-min(tensor.dim(),other.dim())+1,-1]\nCheck names: Furthermore, all named dimensions must be aligned from the right.\nDuring matching, if we match a named dimensionAwith an unnamed dimensionNone, thenAmust not appear in the tensor with the unnamed dimension.\nA\nNone\nA\nPropagate names: unify pairs of names from the right from both tensors to\nproduce output names.\nFor example,\n\n```python\n# tensor: Tensor[   N, None]\n# other:  Tensor[None,    C]\n>>> tensor = torch.randn(3, 3, names=('N', None))\n>>> other = torch.randn(3, 3, names=(None, 'C'))\n>>> (tensor + other).names\n('N', 'C')\n\n```\n\nCheck names:\nmatch(tensor.names[-1],other.names[-1])isTrue\nmatch(tensor.names[-1],other.names[-1])\nTrue\nmatch(tensor.names[-2],tensor.names[-2])isTrue\nmatch(tensor.names[-2],tensor.names[-2])\nTrue\nBecause we matchedNoneintensorwith'C',\ncheck to make sure'C'doesn\u2019t exist intensor(it does not).\nNone\ntensor\n'C'\n'C'\ntensor\nCheck to make sure'N'doesn\u2019t exists inother(it does not).\n'N'\nother\nFinally, the output names are computed with[unify('N',None),unify(None,'C')]=['N','C']\n[unify('N',None),unify(None,'C')]=['N','C']\nMore examples:\n\n```python\n# Dimensions don't match from the right:\n# tensor: Tensor[N, C]\n# other:  Tensor[   N]\n>>> tensor = torch.randn(3, 3, names=('N', 'C'))\n>>> other = torch.randn(3, names=('N',))\n>>> (tensor + other).names\nRuntimeError: Error when attempting to broadcast dims ['N', 'C'] and dims\n['N']: dim 'C' and dim 'N' are at the same position from the right but do\nnot match.\n\n# Dimensions aren't aligned when matching tensor.names[-1] and other.names[-1]:\n# tensor: Tensor[N, None]\n# other:  Tensor[      N]\n>>> tensor = torch.randn(3, 3, names=('N', None))\n>>> other = torch.randn(3, names=('N',))\n>>> (tensor + other).names\nRuntimeError: Misaligned dims when attempting to broadcast dims ['N'] and\ndims ['N', None]: dim 'N' appears in a different position from the right\nacross both lists.\n\n```\n\nNote\nIn both of the last examples, it is possible to align the tensors by names\nand then perform the addition. UseTensor.align_as()to align\ntensors by name orTensor.align_to()to align tensors to a custom\ndimension ordering.\nTensor.align_as()\nTensor.align_to()\n\n## Permutes dimensions#\n\nSome operations, likeTensor.t(), permute the order of dimensions. Dimension names\nare attached to individual dimensions so they get permuted as well.\nTensor.t()\nIf the operator takes in positional indexdim, it is also able to take a dimension\nname asdim.\ndim\ndim\nCheck names: Ifdimis passed as a name, check that it exists in the tensor.\ndim\nPropagate names: Permute dimension names in the same way as the dimensions that are\nbeing permuted.\n\n```python\n>>> x = torch.randn(3, 3, names=('N', 'C'))\n>>> x.transpose('N', 'C').names\n('C', 'N')\n\n```\n\n\n## Contracts away dims#\n\nMatrix multiply functions follow some variant of this. Let\u2019s go throughtorch.mm()first and then generalize the rule for batch matrix multiplication.\ntorch.mm()\nFortorch.mm(tensor,other):\ntorch.mm(tensor,other)\nCheck names: None\nPropagate names: result names are(tensor.names[-2],other.names[-1]).\n(tensor.names[-2],other.names[-1])\n\n```python\n>>> x = torch.randn(3, 3, names=('N', 'D'))\n>>> y = torch.randn(3, 3, names=('in', 'out'))\n>>> x.mm(y).names\n('N', 'out')\n\n```\n\nInherently, a matrix multiplication performs a dot product over two dimensions,\ncollapsing them. When two tensors are matrix-multiplied, the contracted dimensions\ndisappear and do not show up in the output tensor.\ntorch.mv(),torch.dot()work in a similar way: name inference does not\ncheck input names and removes the dimensions that are involved in the dot product:\ntorch.mv()\ntorch.dot()\n\n```python\n>>> x = torch.randn(3, 3, names=('N', 'D'))\n>>> y = torch.randn(3, names=('something',))\n>>> x.mv(y).names\n('N',)\n\n```\n\nNow, let\u2019s take a look attorch.matmul(tensor,other). Assume thattensor.dim()>=2andother.dim()>=2.\ntorch.matmul(tensor,other)\ntensor.dim()>=2\nother.dim()>=2\nCheck names: Check that the batch dimensions of the inputs are aligned and broadcastable.\nSeeUnifies names from inputsfor what it means for the inputs to be aligned.\nPropagate names: result names are obtained by unifying the batch dimensions and removing\nthe contracted dimensions:unify(tensor.names[:-2],other.names[:-2])+(tensor.names[-2],other.names[-1]).\nunify(tensor.names[:-2],other.names[:-2])+(tensor.names[-2],other.names[-1])\nExamples:\n\n```python\n# Batch matrix multiply of matrices Tensor['C', 'D'] and Tensor['E', 'F'].\n# 'A', 'B' are batch dimensions.\n>>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))\n>>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F'))\n>>> torch.matmul(x, y).names\n('A', 'B', 'C', 'F')\n\n```\n\nFinally, there are fusedaddversions of many matmul functions. i.e.,addmm()andaddmv(). These are treated as composing name inference for i.e.mm()and\nname inference foradd().\nadd\naddmm()\naddmv()\nmm()\nadd()\n\n## Factory functions#\n\nFactory functions now take a newnamesargument that associates a name\nwith each dimension.\nnames\n\n```python\n>>> torch.zeros(2, 3, names=('N', 'C'))\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], names=('N', 'C'))\n\n```\n\n\n## out function and in-place variants#\n\nA tensor specified as anout=tensor has the following behavior:\nout=\nIf it has no named dimensions, then the names computed from the operation\nget propagated to it.\nIf it has any named dimensions, then the names computed from the operation\nmust be exactly equal to the existing names. Otherwise, the operation errors.\nAll in-place methods modify inputs to have names equal to the computed names\nfrom name inference. For example:\n\n```python\n>>> x = torch.randn(3, 3)\n>>> y = torch.randn(3, 3, names=('N', 'C'))\n>>> x.names\n(None, None)\n\n>>> x += y\n>>> x.names\n('N', 'C')\n\n```\n",
  "url": "https://pytorch.org/docs/stable/name_inference.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "b97472067199cbbe4d84d1845911f60c",
  "source": "pytorch_docs",
  "title": "PyTorch 2.0 NNModule Support \u2014 PyTorch 2.9 documentation",
  "text": "\n## PyTorch 2.0 NNModule Support#\n\nCreated On: Apr 06, 2023 | Last Updated On: Jun 10, 2025\nAuthor:Will Constable\ntorch.compilehas special handling for torch.nn.Module objects, tracing them differently than it traces\narbitrary python classes, with the intent of producing faster code by making assumptions about the structure.\ntorch.compile\nThis doc describes some of the tradeoffs or edge cases that come up due to this specialization.\n\n## NNModule Hooks Support#\n\nPreviously,torch.compilehad no support for hooks on nn.Modules, and if hooks were registered\nthey would simply be ignored in the compiled program. Indeed many users do not\nuse nn.Module hooks at all, or only use them for debug workflows, but there are valid use cases\nfor composing nn.Module hooks withtorch.compile.\ntorch.compile\ntorch.compile\nHooks that are orchestrated via nn.Module.callimplementation include_forward_pre_hooks,forward_hooks,_backward_pre_hooks, and_backward_hooks, and will be referred to as \u2018call hooks\u2019.\nThese hooks are partially supported bytorch.compilewith limitations described below.\n_forward_pre_hooks\nforward_hooks\n_backward_pre_hooks\n_backward_hooks\ntorch.compile\nAnother category of hooks includes_state_dict_hooksand itspreandload_variants, and are still\nunsupported bytorch.compile.\n_state_dict_hooks\npre\nload_\ntorch.compile\n\n## nn.Module.__call__Hooks Usage and limitations#\n\nnn.Module.__call__\nBy default,torch.compilewill trace the contents ofnn.Module.__call__which means it will encounter\nand run forward/pre-forward hooks.  If you install hooks before callingtorch.compileand then do not remove\nor alter the hooks later, your use case should be supported by default.\ntorch.compile\nnn.Module.__call__\ntorch.compile\nBackward/Pre-backward hooks are generally also supported, with similar caveats: currently graph-breaks in dynamo\noccur when accessing backward_hooks dicts, which is probably avoiable with some work.  Graph-breaks also impact the\ntiming of firing backward hooks, since graph-segments are run as autograd-functions which produce all their grads at\nthe same time.  Assuming it were possible for dynamo to not graph-break on the presence of backward-hooks, we would\nstill expect the backward hooks for a series of modules to all fire together after the whole compiled graph\u2019s backward\nran.\nhooks on \u2018allowed modules\u2019torch.compiletreats common modules such as torch.conv, as well as modules that are difficult to trace, specially\nby allowing them to be called opaquely in the dynamo graph instead of traced into by dynamo.  For such modules, hooks\ncurrently trigger a graph-break so that the affected modules run outside of dynamo.  Depending on the model, this could\nintroduce a significant performance regression, and additional work is required to improve this support.\ntorch.compile\nskip_nnmodule_hook_guardsBy default,torch._dynamo.config.skip_nnmodule_hook_guardsis set to True, meaning no guards will be installed\non each nn.Module hook dictionary, improving runtime by reducing guard execution time, at the cost of not noticing\nif any hook dict is changed after compilation.\ntorch._dynamo.config.skip_nnmodule_hook_guards\nIf you want to be able to remove or modify hooks after compilation and havetorch.compilereact appropriately\n(by recompiling), then you need to setskip_nnmodule_hook_guards=Falseand expect a runtime penalty for the added\nguards.\ntorch.compile\nskip_nnmodule_hook_guards=False\nTODO: confirm if backward/pre_backward hooks are working or not and document accordingly\n\n## state_dict Hooks#\n\nState dict hooks have not yet been supported intorch.compile.\ntorch.compile\nTODO: warn_once if graph-breaking on hooks.  warn_once to point to this doc if hooks are present.",
  "url": "https://pytorch.org/docs/stable/torch.compiler_nn_module.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
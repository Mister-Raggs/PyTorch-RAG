{
  "doc_id": "e638bdafc8b2a8a15f33ff56c8cb19ed",
  "source": "pytorch_docs",
  "title": "torch.nested \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.nested#\n\nCreated On: Mar 02, 2022 | Last Updated On: Jun 14, 2025\n\n## Introduction#\n\nWarning\nThe PyTorch API of nested tensors is in prototype stage and will change in the near future.\nNested tensors allow for ragged-shaped data to be contained within and operated upon as a\nsingle tensor. Such data is stored underneath in an efficient packed representation, while exposing\na standard PyTorch tensor interface for applying operations.\nA common application of nested tensors is for expressing batches of variable-length sequential data\npresent in various domains, such as varying sentence lengths, image sizes, and audio / video clip\nlengths. Traditionally, such data has been handled by padding sequences to that of the max length\nwithin a batch, performing computation on the padded form, and subsequently masking to remove\npadding. This is inefficient and error-prone, and nested tensors exist to address these problems.\nThe API for calling operations on a nested tensor is no different from that of a regulartorch.Tensor, allowing for seamless integration with existing models, with the main\ndifference beingconstruction of the inputs.\ntorch.Tensor\nAs this is a prototype feature, the set ofoperations supportedis\nlimited, but growing. We welcome issues, feature requests, and contributions.\nMore information on contributing can be foundin this Readme.\n\n## Construction#\n\nNote\nThere are two forms of nested tensors present within PyTorch, distinguished by layout as\nspecified during construction. Layout can be one oftorch.stridedortorch.jagged.\nWe recommend utilizing thetorch.jaggedlayout whenever possible. While it currently only\nsupports a single ragged dimension, it has better op coverage, receives active development, and\nintegrates well withtorch.compile. These docs adhere to this recommendation and refer to\nnested tensors with thetorch.jaggedlayout as \u201cNJTs\u201d for brevity throughout.\ntorch.strided\ntorch.jagged\ntorch.jagged\ntorch.compile\ntorch.jagged\nConstruction is straightforward and involves passing a list of tensors to thetorch.nested.nested_tensorconstructor. A nested tensor with thetorch.jaggedlayout\n(AKA an \u201cNJT\u201d) supports a single ragged dimension. This constructor will copy the input tensors\ninto a packed, contiguous block of memory according to the layout described in thedata_layout_\nsection below.\ntorch.nested.nested_tensor\ntorch.jagged\ndata_layout\n\n```python\n>>> a, b = torch.arange(3), torch.arange(5) + 3\n>>> a\ntensor([0, 1, 2])\n>>> b\ntensor([3, 4, 5, 6, 7])\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> print([component for component in nt])\n[tensor([0, 1, 2]), tensor([3, 4, 5, 6, 7])]\n\n```\n\nEach tensor in the list must have the same number of dimensions, but the shapes can otherwise vary\nalong a single dimension. If the dimensionalities of the input components don\u2019t match, the\nconstructor throws an error.\n\n```python\n>>> a = torch.randn(50, 128) # 2D tensor\n>>> b = torch.randn(2, 50, 128) # 3D tensor\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n...\nRuntimeError: When constructing a nested tensor, all tensors in list must have the same dim\n\n```\n\nDuring construction, dtype, device, and whether gradients are required can be chosen via the\nusual keyword arguments.\n\n```python\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32, device=\"cuda\", requires_grad=True)\n>>> print([component for component in nt])\n[tensor([0., 1., 2.], device='cuda:0',\n       grad_fn=<UnbindBackwardAutogradNestedTensor0>), tensor([3., 4., 5., 6., 7.], device='cuda:0',\n       grad_fn=<UnbindBackwardAutogradNestedTensor0>)]\n\n```\n\ntorch.nested.as_nested_tensorcan be used to preserve autograd history from the tensors passed\nto the constructor. When this constructor is utilized, gradients will flow through the nested tensor\nback into the original components. Note that this constructor still copies the input components into\na packed, contiguous block of memory.\ntorch.nested.as_nested_tensor\n\n```python\n>>> a = torch.randn(12, 512, requires_grad=True)\n>>> b = torch.randn(23, 512, requires_grad=True)\n>>> nt = torch.nested.as_nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.sum().backward()\n>>> a.grad\ntensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])\n>>> b.grad\ntensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])\n\n```\n\nThe above functions all create contiguous NJTs, where a chunk of memory is allocated to store\na packed form of the underlying components (see thedata_layout_ section below for more\ndetails).\ndata_layout\nIt is also possible to create a non-contiguous NJT view over a pre-existing dense tensor\nwith padding, avoiding the memory allocation and copying.torch.nested.narrow()is the tool\nfor accomplishing this.\ntorch.nested.narrow()\n\n```python\n>>> padded = torch.randn(3, 5, 4)\n>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)\n>>> nt = torch.nested.narrow(padded, dim=1, start=0, length=seq_lens, layout=torch.jagged)\n>>> nt.shape\ntorch.Size([3, j1, 4])\n>>> nt.is_contiguous()\nFalse\n\n```\n\nNote that the nested tensor acts as a view over the original padded dense tensor, referencing the\nsame memory without copying / allocation. Operation support for non-contiguous NJTs is somewhat more\nlimited, so if you run into support gaps, it\u2019s always possible to convert to a contiguous NJT\nusingcontiguous().\ncontiguous()\n\n## Data Layout and Shape#\n\nFor efficiency, nested tensors generally pack their tensor components into a contiguous chunk of\nmemory and maintain additional metadata to specify batch item boundaries. For thetorch.jaggedlayout, the contiguous chunk of memory is stored in thevaluescomponent, with theoffsetscomponent delineating batch item boundaries for the ragged dimension.\ntorch.jagged\nvalues\noffsets\n\nIt\u2019s possible to directly access the underlying NJT components when necessary.\n\n```python\n>>> a = torch.randn(50, 128) # text 1\n>>> b = torch.randn(32, 128) # text 2\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.values().shape  # note the \"packing\" of the ragged dimension; no padding needed\ntorch.Size([82, 128])\n>>> nt.offsets()\ntensor([ 0, 50, 82])\n\n```\n\nIt can also be useful to construct an NJT from the jaggedvaluesandoffsetsconstituents directly; thetorch.nested.nested_tensor_from_jagged()constructor serves\nthis purpose.\nvalues\noffsets\ntorch.nested.nested_tensor_from_jagged()\n\n```python\n>>> values = torch.randn(82, 128)\n>>> offsets = torch.tensor([0, 50, 82], dtype=torch.int64)\n>>> nt = torch.nested.nested_tensor_from_jagged(values=values, offsets=offsets)\n\n```\n\nAn NJT has a well-defined shape with dimensionality 1 greater than that of its components. The\nunderlying structure of the ragged dimension is represented by a symbolic value (j1in the\nexample below).\nj1\n\n```python\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.dim()\n3\n>>> nt.shape\ntorch.Size([2, j1, 128])\n\n```\n\nNJTs must have the same ragged structure to be compatible with each other. For example, to run a\nbinary operation involving two NJTs, the ragged structures must match (i.e. they must have the\nsame ragged shape symbol in their shapes). In the details, each symbol corresponds with an exactoffsetstensor, so both NJTs must have the sameoffsetstensor to be compatible with\neach other.\noffsets\noffsets\n\n```python\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt2 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt1.offsets() is nt2.offsets()\nFalse\n>>> nt3 = nt1 + nt2\nRuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)\n\n```\n\nIn the above example, even though the conceptual shapes of the two NJTs are the same, they don\u2019t\nshare a reference to the sameoffsetstensor, so their shapes differ, and they are not\ncompatible. We recognize that this behavior is unintuitive and are working hard to relax this\nrestriction for the beta release of nested tensors. For a workaround, see theTroubleshootingsection of this document.\noffsets\nIn addition to theoffsetsmetadata, NJTs can also compute and cache the minimum and maximum\nsequence lengths for its components, which can be useful for invoking particular kernels (e.g. SDPA).\nThere are currently no public APIs for accessing these, but this will change for the beta release.\noffsets\n\n## Supported Operations#\n\nThis section contains a list of common operations over nested tensors that you may find useful.\nIt is not comprehensive, as there are on the order of a couple thousand ops within PyTorch. While\na sizeable subset of these are supported for nested tensors today, full support is a large task.\nThe ideal state for nested tensors is full support of all PyTorch operations that are available\nfor non-nested tensors. To help us accomplish this, please consider:\nRequesting particular ops needed for your use casehereto help us prioritize.\nContributing! It\u2019s not too hard to add nested tensor support for a given PyTorch op; see\ntheContributionssection below for details.\n\n## Viewing nested tensor constituents#\n\nunbind()allows you to retrieve a view of the nested tensor\u2019s constituents.\nunbind()\n\n```python\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(3, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt.unbind()\n(tensor([[-0.9916, -0.3363, -0.2799],\n        [-2.3520, -0.5896, -0.4374]]), tensor([[-2.0969, -1.0104,  1.4841],\n        [ 2.0952,  0.2973,  0.2516],\n        [ 0.9035,  1.3623,  0.2026]]))\n>>> nt.unbind()[0] is not a\nTrue\n>>> nt.unbind()[0].mul_(3)\ntensor([[ 3.6858, -3.7030, -4.4525],\n        [-2.3481,  2.0236,  0.1975]])\n>>> nt.unbind()\n(tensor([[-2.9747, -1.0089, -0.8396],\n        [-7.0561, -1.7688, -1.3122]]), tensor([[-2.0969, -1.0104,  1.4841],\n        [ 2.0952,  0.2973,  0.2516],\n        [ 0.9035,  1.3623,  0.2026]]))\n\n```\n\nNote thatnt.unbind()[0]is not a copy, but rather a slice of the underlying memory, which\nrepresents the first entry or constituent of the nested tensor.\nnt.unbind()[0]\ntorch.nested.to_padded_tensor()converts an NJT to a padded dense tensor with the specified\npadding value. The ragged dimension will be padded out to the size of the maximum sequence length.\ntorch.nested.to_padded_tensor()\n\n```python\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(6, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> padded = torch.nested.to_padded_tensor(nt, padding=4.2)\n>>> padded\ntensor([[[ 1.6107,  0.5723,  0.3913],\n         [ 0.0700, -0.4954,  1.8663],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000]],\n        [[-0.0479, -0.7610, -0.3484],\n         [ 1.1345,  1.0556,  0.3634],\n         [-1.7122, -0.5921,  0.0540],\n         [-0.5506,  0.7608,  2.0606],\n         [ 1.5658, -1.1934,  0.3041],\n         [ 0.1483, -1.1284,  0.6957]]])\n\n```\n\nThis can be useful as an escape hatch to work around NJT support gaps, but ideally such\nconversions should be avoided when possible for optimal memory usage and performance, as the\nmore efficient nested tensor layout does not materialize padding.\nThe reverse conversion can be accomplished usingtorch.nested.narrow(), which applies\nragged structure to a given dense tensor to produce an NJT. Note that by default, this operation\ndoes not copy the underlying data, and thus the output NJT is generally non-contiguous. It may be\nuseful to explicitly callcontiguous()here if a contiguous NJT is desired.\ntorch.nested.narrow()\ncontiguous()\n\n```python\n>>> padded = torch.randn(3, 5, 4)\n>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)\n>>> nt = torch.nested.narrow(padded, dim=1, length=seq_lens, layout=torch.jagged)\n>>> nt.shape\ntorch.Size([3, j1, 4])\n>>> nt = nt.contiguous()\n>>> nt.shape\ntorch.Size([3, j2, 4])\n\n```\n\n\n## Shape manipulations#\n\nNested tensors support a wide array of operations for shape manipulation, including views.\n\n```python\n>>> a = torch.randn(2, 6)\n>>> b = torch.randn(4, 6)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt.shape\ntorch.Size([2, j1, 6])\n>>> nt.unsqueeze(-1).shape\ntorch.Size([2, j1, 6, 1])\n>>> nt.unflatten(-1, [2, 3]).shape\ntorch.Size([2, j1, 2, 3])\n>>> torch.cat([nt, nt], dim=2).shape\ntorch.Size([2, j1, 12])\n>>> torch.stack([nt, nt], dim=2).shape\ntorch.Size([2, j1, 2, 6])\n>>> nt.transpose(-1, -2).shape\ntorch.Size([2, 6, j1])\n\n```\n\n\n## Attention mechanisms#\n\nAs variable-length sequences are common inputs to attention mechanisms, nested tensors support\nimportant attention operatorsScaled Dot Product Attention (SDPA)andFlexAttention.\nSeeherefor usage examples of NJT with SDPA andherefor usage examples of NJT with FlexAttention.\n\n## Usage with torch.compile#\n\nNJTs are designed to be used withtorch.compile()for optimal performance, and we always\nrecommend utilizingtorch.compile()with NJTs when possible. NJTs work out-of-the-box and\ngraph-break-free both when passed as inputs to a compiled function or module OR when\ninstantiated in-line within the function.\ntorch.compile()\ntorch.compile()\nNote\n\n```python\nIf you're not able to utilize ``torch.compile()`` for your use case, performance and memory\nusage may still benefit from the use of NJTs, but it's not as clear-cut whether this will be\nthe case. It is important that the tensors being operated on are large enough so the\nperformance gains are not outweighed by the overhead of python tensor subclasses.\n\n```\n\n\n```python\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(4, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> def f(x): return x.sin() + 1\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output = compiled_f(nt)\n>>> output.shape\ntorch.Size([2, j1, 3])\n>>> def g(values, offsets): return torch.nested.nested_tensor_from_jagged(values, offsets) * 2.\n...\n>>> compiled_g = torch.compile(g, fullgraph=True)\n>>> output2 = compiled_g(nt.values(), nt.offsets())\n>>> output2.shape\ntorch.Size([2, j1, 3])\n\n```\n\nNote that NJTs supportDynamic Shapesto avoid unnecessary recompiles with changing ragged structure.\n\n```python\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(4, 3)\n>>> c = torch.randn(5, 3)\n>>> d = torch.randn(6, 3)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt2 = torch.nested.nested_tensor([c, d], layout=torch.jagged)\n>>> def f(x): return x.sin() + 1\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output1 = compiled_f(nt1)\n>>> output2 = compiled_f(nt2)  # NB: No recompile needed even though ragged structure differs\n\n```\n\nIf you run into problems or arcane errors when utilizing NJT +torch.compile, please file a\nPyTorch issue. Full subclass support withintorch.compileis a long-term effort and there may\nbe some rough edges at this time.\ntorch.compile\ntorch.compile\n\n## Troubleshooting#\n\nThis section contains common errors that you may run into when utilizing nested tensors, alongside\nthe reason for these errors and suggestions for how to address them.\n\n## Unimplemented ops#\n\nThis error is becoming rarer as nested tensor op support grows, but it\u2019s still possible to hit it\ntoday given that there are a couple thousand ops within PyTorch.\n\n```python\n    NotImplementedError: aten.view_as_real.default\n\n```\n\nThe error is straightforward; we haven\u2019t gotten around to adding op support for this particular op\nyet. If you\u2019d like, you cancontributean implementation yourself OR simplyrequestthat we add support for this op\nin a future PyTorch release.\n\n## Ragged structure incompatibility#\n\n\n```python\n    RuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)\n\n```\n\nThis error occurs when calling an op that operates over multiple NJTs with incompatible ragged\nstructures. Currently, it is required that input NJTs have the exact sameoffsetsconstituent\nin order to have the same symbolic ragged structure symbol (e.g.j1).\noffsets\nj1\nAs a workaround for this situation, it is possible to construct NJTs from thevaluesandoffsetscomponents directly. With both NJTs referencing the sameoffsetscomponents, they\nare considered to have the same ragged structure and are thus compatible.\nvalues\noffsets\noffsets\n\n```python\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt2 = torch.nested.nested_tensor_from_jagged(values=torch.randn(82, 128), offsets=nt1.offsets())\n>>> nt3 = nt1 + nt2\n>>> nt3.shape\ntorch.Size([2, j1, 128])\n\n```\n\n\n## Data dependent operation within torch.compile#\n\n\n```python\n    torch._dynamo.exc.Unsupported: data dependent operator: aten._local_scalar_dense.default; to enable, set torch._dynamo.config.capture_scalar_outputs = True\n\n```\n\nThis error occurs when calling an op that does data-dependent operation within torch.compile; this\ncommonly occurs for ops that need to examine the values of the NJT\u2019soffsetsto determine the\noutput shape. For example:\noffsets\n\n```python\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> def f(nt): return nt.chunk(2, dim=0)[0]\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output = compiled_f(nt)\n\n```\n\nIn this example, callingchunk()on the batch dimension of the NJT requires examination of the\nNJT\u2019soffsetsdata to delineate batch item boundaries within the packed ragged dimension. As a\nworkaround, there are a couple torch.compile flags that can be set:\nchunk()\noffsets\n\n```python\n>>> torch._dynamo.config.capture_dynamic_output_shape_ops = True\n>>> torch._dynamo.config.capture_scalar_outputs = True\n\n```\n\nIf, after setting these, you still see data-dependent operator errors, please file an issue with\nPyTorch. This area oftorch.compile()is still in heavy development and certain aspects of\nNJT support may be incomplete.\ntorch.compile()\n\n## Contributions#\n\nIf you\u2019d like to contribute to nested tensor development, one of the most impactful ways to do\nso is to add nested tensor support for a currently-unsupported PyTorch op. This process generally\nconsists of a couple simple steps:\nDetermine the name of the op to add; this should be something likeaten.view_as_real.default.\nThe signature for this op can be found inaten/src/ATen/native/native_functions.yaml.\naten.view_as_real.default\naten/src/ATen/native/native_functions.yaml\nRegister an op implementation intorch/nested/_internal/ops.py, following the pattern\nestablished there for other ops. Use the signature fromnative_functions.yamlfor schema\nvalidation.\ntorch/nested/_internal/ops.py\nnative_functions.yaml\nThe most common way to implement an op is to unwrap the NJT into its constituents, redispatch the\nop on the underlyingvaluesbuffer, and propagate the relevant NJT metadata (includingoffsets) to a new output NJT. If the output of the op is expected to have a different shape\nfrom the input, newoffsets, etc. metadata must be computed.\nvalues\noffsets\noffsets\nWhen an op is applied over the batch or ragged dimension, these tricks can help quickly get a\nworking implementation:\nFornon-batchwiseoperation, anunbind()-based fallback should work.\nunbind()\nFor operation on the ragged dimension, consider converting to padded dense with a properly-selected\npadding value that won\u2019t negatively bias the output, running the op, and converting back to NJT.\nWithintorch.compile, these conversions can be fused to avoid materializing the padded\nintermediate.\ntorch.compile\n\n## Detailed Docs for Construction and Conversion Functions#\n\nConstructs a nested tensor with no autograd history (also known as a \u201cleaf tensor\u201d, seeAutograd mechanics) fromtensor_lista list of tensors.\ntensor_list\ntensor_list(List[array_like]) \u2013 a list of tensors, or anything that can be passed to torch.tensor,\ndimensionality.(where each elementofthe list has the same) \u2013\ndtype(torch.dtype, optional) \u2013 the desired type of returned nested tensor.\nDefault: if None, sametorch.dtypeas leftmost tensor in the list.\ntorch.dtype\ntorch.dtype\nlayout(torch.layout, optional) \u2013 the desired layout of returned nested tensor.\nOnly strided and jagged layouts are supported. Default: if None, the strided layout.\ntorch.layout\ndevice(torch.device, optional) \u2013 the desired device of returned nested tensor.\nDefault: if None, sametorch.deviceas leftmost tensor in the list\ntorch.device\ntorch.device\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned nested tensor. Default:False.\nFalse\npin_memory(bool,optional) \u2013 If set, returned nested tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default:False.\nFalse\nTensor\nExample:\n\n```python\n>>> a = torch.arange(3, dtype=torch.float, requires_grad=True)\n>>> b = torch.arange(5, dtype=torch.float, requires_grad=True)\n>>> nt = torch.nested.nested_tensor([a, b], requires_grad=True)\n>>> nt.is_leaf\nTrue\n\n```\n\nConstructs a jagged layout nested tensor from the given jagged components. The jagged layout\nconsists of a required values buffer with the jagged dimension packed into a single dimension.\nThe offsets / lengths metadata determines how this dimension is split into batch elements\nand are expected to be allocated on the same device as the values buffer.\noffsets: Indices within the packed dimension splitting it into heterogeneously-sized\nbatch elements. Example: [0, 2, 3, 6] indicates that a packed jagged dim of size 6\nshould be conceptually split into batch elements of length [2, 1, 3]. Note that both the\nbeginning and ending offsets are required for kernel convenience (i.e. shape batch_size + 1).\nlengths: Lengths of the individual batch elements; shape == batch_size. Example: [2, 1, 3]\nindicates that a packed jagged dim of size 6 should be conceptually split into batch\nelements of length [2, 1, 3].\nNote that it can be useful to provide both offsets and lengths. This describes a nested tensor\nwith \u201choles\u201d, where the offsets indicate the start position of each batch item and the length\nspecifies the total number of elements (see example below).\nThe returned jagged layout nested tensor will be a view of the input values tensor.\nvalues(torch.Tensor) \u2013 The underlying buffer in the shape of\n(sum_B(*), D_1, \u2026, D_N). The jagged dimension is packed into a single dimension,\nwith the offsets / lengths metadata used to distinguish batch elements.\ntorch.Tensor\noffsets(optionaltorch.Tensor) \u2013 Offsets into the jagged dimension of shape B + 1.\ntorch.Tensor\nlengths(optionaltorch.Tensor) \u2013 Lengths of the batch elements of shape B.\ntorch.Tensor\njagged_dim(optional python:int) \u2013 Indicates which dimension in values is the packed jagged\ndimension. Must be >= 1 as the batch dimension (dim=0) cannot be ragged.\nIf None, this is set to dim=1 (i.e. the dimension immediately following the batch dimension). Default: None\nmin_seqlen(optional python:int) \u2013 If set, uses the specified value as the cached minimum sequence\nlength for the returned nested tensor. This can be a useful alternative to computing\nthis value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\nmax_seqlen(optional python:int) \u2013 If set, uses the specified value as the cached maximum sequence\nlength for the returned nested tensor. This can be a useful alternative to computing\nthis value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\nTensor\nExample:\n\n```python\n>>> values = torch.randn(12, 5)\n>>> offsets = torch.tensor([0, 3, 5, 6, 10, 12])\n>>> nt = nested_tensor_from_jagged(values, offsets)\n>>> # 3D shape with the middle dimension jagged\n>>> nt.shape\ntorch.Size([5, j2, 5])\n>>> # Length of each item in the batch:\n>>> offsets.diff()\ntensor([3, 2, 1, 4, 2])\n\n>>> values = torch.randn(6, 5)\n>>> offsets = torch.tensor([0, 2, 3, 6])\n>>> lengths = torch.tensor([1, 1, 2])\n>>> # NT with holes\n>>> nt = nested_tensor_from_jagged(values, offsets, lengths)\n>>> a, b, c = nt.unbind()\n>>> # Batch item 1 consists of indices [0, 1)\n>>> torch.equal(a, values[0:1, :])\nTrue\n>>> # Batch item 2 consists of indices [2, 3)\n>>> torch.equal(b, values[2:3, :])\nTrue\n>>> # Batch item 3 consists of indices [3, 5)\n>>> torch.equal(c, values[3:5, :])\nTrue\n\n```\n\nConstructs a nested tensor preserving autograd history from a tensor or a list / tuple of\ntensors.\nIf a nested tensor is passed, it will be returned directly unless the device / dtype / layout\ndiffer. Note that converting device / dtype will result in a copy, while converting layout\nis not currently supported by this function.\nIf a non-nested tensor is passed, it is treated as a batch of constituents of consistent size.\nA copy will be incurred if the passed device / dtype differ from those of the input OR if\nthe input is non-contiguous. Otherwise, the input\u2019s storage will be used directly.\nIf a tensor list is provided, tensors in the list are always copied during construction of\nthe nested tensor.\nts(TensororList[Tensor] orTuple[Tensor]) \u2013 a tensor to treat as a nested tensor OR a\nlist / tuple of tensors with the same ndim\ndtype(torch.dtype, optional) \u2013 the desired type of returned nested tensor.\nDefault: if None, sametorch.dtypeas leftmost tensor in the list.\ntorch.dtype\ntorch.dtype\ndevice(torch.device, optional) \u2013 the desired device of returned nested tensor.\nDefault: if None, sametorch.deviceas leftmost tensor in the list\ntorch.device\ntorch.device\nlayout(torch.layout, optional) \u2013 the desired layout of returned nested tensor.\nOnly strided and jagged layouts are supported. Default: if None, the strided layout.\ntorch.layout\nTensor\nExample:\n\n```python\n>>> a = torch.arange(3, dtype=torch.float, requires_grad=True)\n>>> b = torch.arange(5, dtype=torch.float, requires_grad=True)\n>>> nt = torch.nested.as_nested_tensor([a, b])\n>>> nt.is_leaf\nFalse\n>>> fake_grad = torch.nested.nested_tensor([torch.ones_like(a), torch.zeros_like(b)])\n>>> nt.backward(fake_grad)\n>>> a.grad\ntensor([1., 1., 1.])\n>>> b.grad\ntensor([0., 0., 0., 0., 0.])\n>>> c = torch.randn(3, 5, requires_grad=True)\n>>> nt2 = torch.nested.as_nested_tensor(c)\n\n```\n\nReturns a new (non-nested) Tensor by padding theinputnested tensor.\nThe leading entries will be filled with the nested data,\nwhile the trailing entries will be padded.\ninput\nWarning\nto_padded_tensor()always copies the underlying data,\nsince the nested and the non-nested tensors differ in memory layout.\nto_padded_tensor()\npadding(float) \u2013 The padding value for the trailing entries.\noutput_size(Tuple[int]) \u2013 The size of the output tensor.\nIf given, it must be large enough to contain all nested data;\nelse, will infer by taking the max size of each nested sub-tensor along each dimension.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> nt = torch.nested.nested_tensor([torch.randn((2, 5)), torch.randn((3, 4))])\nnested_tensor([\n  tensor([[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276],\n          [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995]]),\n  tensor([[-1.8546, -0.7194, -0.2918, -0.1846],\n          [ 0.2773,  0.8793, -0.5183, -0.6447],\n          [ 1.8009,  1.8468, -0.9832, -1.5272]])\n])\n>>> pt_infer = torch.nested.to_padded_tensor(nt, 0.0)\ntensor([[[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276],\n         [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995],\n         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n        [[-1.8546, -0.7194, -0.2918, -0.1846,  0.0000],\n         [ 0.2773,  0.8793, -0.5183, -0.6447,  0.0000],\n         [ 1.8009,  1.8468, -0.9832, -1.5272,  0.0000]]])\n>>> pt_large = torch.nested.to_padded_tensor(nt, 1.0, (2, 4, 6))\ntensor([[[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276,  1.0000],\n         [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995,  1.0000],\n         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]],\n        [[-1.8546, -0.7194, -0.2918, -0.1846,  1.0000,  1.0000],\n         [ 0.2773,  0.8793, -0.5183, -0.6447,  1.0000,  1.0000],\n         [ 1.8009,  1.8468, -0.9832, -1.5272,  1.0000,  1.0000],\n         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]])\n>>> pt_small = torch.nested.to_padded_tensor(nt, 2.0, (2, 2, 2))\nRuntimeError: Value in output_size is less than NestedTensor padded size. Truncation is not supported.\n\n```\n\nConstructs a nested tensor given a strided tensor input and a strided mask, the resulting jagged layout nested tensor\nwill have values retain values where the mask is equal to True. The dimensionality of the mask is preserved and is\nrepresented with the offsets, this is unlikemasked_select()where the output is collapsed to a 1D tensor.\nmasked_select()\nArgs:\ntensor (torch.Tensor): a strided tensor from which the jagged layout nested tensor is constructed from.\nmask (torch.Tensor): a strided mask tensor which is applied to the tensor input\ntorch.Tensor\ntorch.Tensor\nExample:\n\n```python\n>>> tensor = torch.randn(3, 3)\n>>> mask = torch.tensor([[False, False, True], [True, False, True], [False, False, True]])\n>>> nt = torch.nested.masked_select(tensor, mask)\n>>> nt.shape\ntorch.Size([3, j4])\n>>> # Length of each item in the batch:\n>>> nt.offsets().diff()\ntensor([1, 2, 1])\n\n>>> tensor = torch.randn(6, 5)\n>>> mask = torch.tensor([False])\n>>> nt = torch.nested.masked_select(tensor, mask)\n>>> nt.shape\ntorch.Size([6, j5])\n>>> # Length of each item in the batch:\n>>> nt.offsets().diff()\ntensor([0, 0, 0, 0, 0, 0])\n\n```\n\nTensor\nConstructs a nested tensor (which might be a view) fromtensor, a strided tensor. This follows\nsimilar semantics to torch.Tensor.narrow, where in thedim-th dimension the new nested tensor\nshows only the elements in the interval[start, start+length). As nested representations\nallow for a differentstartandlengthat each \u2018row\u2019 of that dimension,startandlengthcan also be tensors of shapetensor.shape[0].\ntensor\ndim\nstart\nlength\nThere\u2019s some differences depending on the layout you use for the nested tensor. If using strided layout,\ntorch.narrow will do a copy of the narrowed data into a contiguous NT with strided layout, while\njagged layout narrow() will create a non-contiguous view of your original strided tensor. This particular\nrepresentation is really useful for representing kv-caches in Transformer models, as specialized\nSDPA kernels can deal with format easily, resulting in performance improvements.\ntensor(torch.Tensor) \u2013 a strided tensor, which will be used as the underlying data\nfor the nested tensor if using the jagged layout or will be copied for the strided layout.\ntorch.Tensor\ndim(int) \u2013 the dimension where narrow will be applied. Onlydim=1is supported for the\njagged layout, while strided supports all dim\nstart(Union[int,torch.Tensor]) \u2013 starting element for the narrow operation\ntorch.Tensor\nlength(Union[int,torch.Tensor]) \u2013 number of elements taken during the narrow op\ntorch.Tensor\nlayout(torch.layout, optional) \u2013 the desired layout of returned nested tensor.\nOnly strided and jagged layouts are supported. Default: if None, the strided layout.\ntorch.layout\nTensor\nExample:\n\n```python\n>>> starts = torch.tensor([0, 1, 2, 3, 4], dtype=torch.int64)\n>>> lengths = torch.tensor([3, 2, 2, 1, 5], dtype=torch.int64)\n>>> narrow_base = torch.randn(5, 10, 20)\n>>> nt_narrowed = torch.nested.narrow(narrow_base, 1, starts, lengths, layout=torch.jagged)\n>>> nt_narrowed.is_contiguous()\nFalse\n\n```\n\nSee also\nAccelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile",
  "url": "https://pytorch.org/docs/stable/nested.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
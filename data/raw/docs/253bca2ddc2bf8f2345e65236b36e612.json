{
  "doc_id": "253bca2ddc2bf8f2345e65236b36e612",
  "source": "pytorch_docs",
  "title": "torch.masked \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.masked#\n\nCreated On: Aug 15, 2022 | Last Updated On: Jun 17, 2025\n\n## Introduction#\n\n\n## Motivation#\n\nWarning\nThe PyTorch API of masked tensors is in the prototype stage and may or may not change in the future.\nMaskedTensor serves as an extension totorch.Tensorthat provides the user with the ability to:\ntorch.Tensor\nuse any masked semantics (e.g. variable length tensors, nan* operators, etc.)\ndifferentiate between 0 and NaN gradients\nvarious sparse applications (see tutorial below)\n\u201cSpecified\u201d and \u201cunspecified\u201d have a long history in PyTorch without formal semantics and certainly without\nconsistency; indeed, MaskedTensor was born out of a build up of issues that the vanillatorch.Tensorclass could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for\nsaid \u201cspecified\u201d and \u201cunspecified\u201d values in PyTorch where they are a first class citizen instead of an afterthought.\nIn turn, this should further unlocksparsity\u2019spotential,\nenable safer and more consistent operators, and provide a smoother and more intuitive experience\nfor users and developers alike.\ntorch.Tensor\n\n## What is a MaskedTensor?#\n\nA MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us\nwhich entries from the input should be included or ignored.\nBy way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray)\nand take the max:\nOn top is the vanilla tensor example while the bottom is MaskedTensor where all the 0\u2019s are masked out.\nThis clearly yields a different result depending on whether we have the mask, but this flexible structure\nallows the user to systematically ignore any elements they\u2019d like during computation.\nThere are already a number of existing tutorials that we\u2019ve written to help users onboard, such as:\nOverview \u2013 the place to start for new users, discusses how to use MaskedTensors and why they\u2019re useful\nSparsity \u2013 MaskedTensor supports sparse COO and CSR data and mask Tensors\nAdagrad sparse semantics \u2013 a practical example of how MaskedTensor can simplify sparse semantics and implementations\nAdvanced semantics \u2013 discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations), differences with NumPy\u2019s MaskedArray, and reduction semantics\n\n## Supported Operators#\n\n\n## Unary Operators#\n\nUnary operators are operators that only contain only a single input.\nApplying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index,\nwe apply the operator, otherwise we\u2019ll continue to mask out the data.\nThe available unary operators are:\nabs\n\nabs\nComputes the absolute value of each element ininput.\ninput\nabsolute\n\nabsolute\nAlias fortorch.abs()\ntorch.abs()\nacos\n\nacos\nComputes the inverse cosine of each element ininput.\ninput\narccos\n\narccos\nAlias fortorch.acos().\ntorch.acos()\nacosh\n\nacosh\nReturns a new tensor with the inverse hyperbolic cosine of the elements ofinput.\ninput\narccosh\n\narccosh\nAlias fortorch.acosh().\ntorch.acosh()\nangle\n\nangle\nComputes the element-wise angle (in radians) of the giveninputtensor.\ninput\nasin\n\nasin\nReturns a new tensor with the arcsine of the elements ofinput.\ninput\narcsin\n\narcsin\nAlias fortorch.asin().\ntorch.asin()\nasinh\n\nasinh\nReturns a new tensor with the inverse hyperbolic sine of the elements ofinput.\ninput\narcsinh\n\narcsinh\nAlias fortorch.asinh().\ntorch.asinh()\natan\n\natan\nReturns a new tensor with the arctangent of the elements ofinput.\ninput\narctan\n\narctan\nAlias fortorch.atan().\ntorch.atan()\natanh\n\natanh\nReturns a new tensor with the inverse hyperbolic tangent of the elements ofinput.\ninput\narctanh\n\narctanh\nAlias fortorch.atanh().\ntorch.atanh()\nbitwise_not\n\nbitwise_not\nComputes the bitwise NOT of the given input tensor.\nceil\n\nceil\nReturns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.\ninput\nclamp\n\nclamp\nClamps all elements ininputinto the range[min,max].\ninput\nmin\nmax\nclip\n\nclip\nAlias fortorch.clamp().\ntorch.clamp()\nconj_physical\n\nconj_physical\nComputes the element-wise conjugate of the giveninputtensor.\ninput\ncos\n\ncos\nReturns a new tensor with the cosine  of the elements ofinput.\ninput\ncosh\n\ncosh\nReturns a new tensor with the hyperbolic cosine  of the elements ofinput.\ninput\ndeg2rad\n\ndeg2rad\nReturns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.\ninput\ndigamma\n\ndigamma\nAlias fortorch.special.digamma().\ntorch.special.digamma()\nerf\n\nerf\nAlias fortorch.special.erf().\ntorch.special.erf()\nerfc\n\nerfc\nAlias fortorch.special.erfc().\ntorch.special.erfc()\nerfinv\n\nerfinv\nAlias fortorch.special.erfinv().\ntorch.special.erfinv()\nexp\n\nexp\nReturns a new tensor with the exponential of the elements of the input tensorinput.\ninput\nexp2\n\nexp2\nAlias fortorch.special.exp2().\ntorch.special.exp2()\nexpm1\n\nexpm1\nAlias fortorch.special.expm1().\ntorch.special.expm1()\nfix\n\nfix\nAlias fortorch.trunc()\ntorch.trunc()\nfloor\n\nfloor\nReturns a new tensor with the floor of the elements ofinput, the largest integer less than or equal to each element.\ninput\nfrac\n\nfrac\nComputes the fractional portion of each element ininput.\ninput\nlgamma\n\nlgamma\nComputes the natural logarithm of the absolute value of the gamma function oninput.\ninput\nlog\n\nlog\nReturns a new tensor with the natural logarithm of the elements ofinput.\ninput\nlog10\n\nlog10\nReturns a new tensor with the logarithm to the base 10 of the elements ofinput.\ninput\nlog1p\n\nlog1p\nReturns a new tensor with the natural logarithm of (1 +input).\ninput\nlog2\n\nlog2\nReturns a new tensor with the logarithm to the base 2 of the elements ofinput.\ninput\nlogit\n\nlogit\nAlias fortorch.special.logit().\ntorch.special.logit()\ni0\n\ni0\nAlias fortorch.special.i0().\ntorch.special.i0()\nisnan\n\nisnan\nReturns a new tensor with boolean elements representing if each element ofinputis NaN or not.\ninput\nnan_to_num\n\nnan_to_num\nReplacesNaN, positive infinity, and negative infinity values ininputwith the values specified bynan,posinf, andneginf, respectively.\nNaN\ninput\nnan\nposinf\nneginf\nneg\n\nneg\nReturns a new tensor with the negative of the elements ofinput.\ninput\nnegative\n\nnegative\nAlias fortorch.neg()\ntorch.neg()\npositive\n\npositive\nReturnsinput.\ninput\npow\n\npow\nTakes the power of each element ininputwithexponentand returns a tensor with the result.\ninput\nexponent\nrad2deg\n\nrad2deg\nReturns a new tensor with each of the elements ofinputconverted from angles in radians to degrees.\ninput\nreciprocal\n\nreciprocal\nReturns a new tensor with the reciprocal of the elements ofinput\ninput\nround\n\nround\nRounds elements ofinputto the nearest integer.\ninput\nrsqrt\n\nrsqrt\nReturns a new tensor with the reciprocal of the square-root of each of the elements ofinput.\ninput\nsigmoid\n\nsigmoid\nAlias fortorch.special.expit().\ntorch.special.expit()\nsign\n\nsign\nReturns a new tensor with the signs of the elements ofinput.\ninput\nsgn\n\nsgn\nThis function is an extension of torch.sign() to complex tensors.\nsignbit\n\nsignbit\nTests if each element ofinputhas its sign bit set or not.\ninput\nsin\n\nsin\nReturns a new tensor with the sine of the elements ofinput.\ninput\nsinc\n\nsinc\nAlias fortorch.special.sinc().\ntorch.special.sinc()\nsinh\n\nsinh\nReturns a new tensor with the hyperbolic sine of the elements ofinput.\ninput\nsqrt\n\nsqrt\nReturns a new tensor with the square-root of the elements ofinput.\ninput\nsquare\n\nsquare\nReturns a new tensor with the square of the elements ofinput.\ninput\ntan\n\ntan\nReturns a new tensor with the tangent of the elements ofinput.\ninput\ntanh\n\ntanh\nReturns a new tensor with the hyperbolic tangent of the elements ofinput.\ninput\ntrunc\n\ntrunc\nReturns a new tensor with the truncated integer values of the elements ofinput.\ninput\nThe available inplace unary operators are all of the aboveexcept:\nangle\n\nangle\nComputes the element-wise angle (in radians) of the giveninputtensor.\ninput\npositive\n\npositive\nReturnsinput.\ninput\nsignbit\n\nsignbit\nTests if each element ofinputhas its sign bit set or not.\ninput\nisnan\n\nisnan\nReturns a new tensor with boolean elements representing if each element ofinputis NaN or not.\ninput\n\n## Binary Operators#\n\nAs you may have seen in the tutorial,MaskedTensoralso has binary operations implemented with the caveat\nthat the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you\nneed support for a particular operator or have proposed semantics for how they should behave instead, please open\nan issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users\nknow exactly what is going on and are being intentional about their decisions with masked semantics.\nMaskedTensor\nThe available binary operators are:\nadd\n\nadd\nAddsother, scaled byalpha, toinput.\nother\nalpha\ninput\natan2\n\natan2\nElement-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.\narctan2\n\narctan2\nAlias fortorch.atan2().\ntorch.atan2()\nbitwise_and\n\nbitwise_and\nComputes the bitwise AND ofinputandother.\ninput\nother\nbitwise_or\n\nbitwise_or\nComputes the bitwise OR ofinputandother.\ninput\nother\nbitwise_xor\n\nbitwise_xor\nComputes the bitwise XOR ofinputandother.\ninput\nother\nbitwise_left_shift\n\nbitwise_left_shift\nComputes the left arithmetic shift ofinputbyotherbits.\ninput\nother\nbitwise_right_shift\n\nbitwise_right_shift\nComputes the right arithmetic shift ofinputbyotherbits.\ninput\nother\ndiv\n\ndiv\nDivides each element of the inputinputby the corresponding element ofother.\ninput\nother\ndivide\n\ndivide\nAlias fortorch.div().\ntorch.div()\nfloor_divide\n\nfloor_divide\n\nfmod\n\nfmod\nApplies C++'sstd::fmodentrywise.\nlogaddexp\n\nlogaddexp\nLogarithm of the sum of exponentiations of the inputs.\nlogaddexp2\n\nlogaddexp2\nLogarithm of the sum of exponentiations of the inputs in base-2.\nmul\n\nmul\nMultipliesinputbyother.\ninput\nother\nmultiply\n\nmultiply\nAlias fortorch.mul().\ntorch.mul()\nnextafter\n\nnextafter\nReturn the next floating-point value afterinputtowardsother, elementwise.\ninput\nother\nremainder\n\nremainder\nComputesPython's modulus operationentrywise.\nsub\n\nsub\nSubtractsother, scaled byalpha, frominput.\nother\nalpha\ninput\nsubtract\n\nsubtract\nAlias fortorch.sub().\ntorch.sub()\ntrue_divide\n\ntrue_divide\nAlias fortorch.div()withrounding_mode=None.\ntorch.div()\nrounding_mode=None\neq\n\neq\nComputes element-wise equality\nne\n\nne\nComputesinput\u2260other\\text{input} \\neq \\text{other}input\ue020=otherelement-wise.\nle\n\nle\nComputesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.\nge\n\nge\nComputesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.\ngreater\n\ngreater\nAlias fortorch.gt().\ntorch.gt()\ngreater_equal\n\ngreater_equal\nAlias fortorch.ge().\ntorch.ge()\ngt\n\ngt\nComputesinput>other\\text{input} > \\text{other}input>otherelement-wise.\nless_equal\n\nless_equal\nAlias fortorch.le().\ntorch.le()\nlt\n\nlt\nComputesinput<other\\text{input} < \\text{other}input<otherelement-wise.\nless\n\nless\nAlias fortorch.lt().\ntorch.lt()\nmaximum\n\nmaximum\nComputes the element-wise maximum ofinputandother.\ninput\nother\nminimum\n\nminimum\nComputes the element-wise minimum ofinputandother.\ninput\nother\nfmax\n\nfmax\nComputes the element-wise maximum ofinputandother.\ninput\nother\nfmin\n\nfmin\nComputes the element-wise minimum ofinputandother.\ninput\nother\nnot_equal\n\nnot_equal\nAlias fortorch.ne().\ntorch.ne()\nThe available inplace binary operators are all of the aboveexcept:\nlogaddexp\n\nlogaddexp\nLogarithm of the sum of exponentiations of the inputs.\nlogaddexp2\n\nlogaddexp2\nLogarithm of the sum of exponentiations of the inputs in base-2.\nequal\n\nequal\nTrueif two tensors have the same size and elements,Falseotherwise.\nTrue\nFalse\nfmin\n\nfmin\nComputes the element-wise minimum ofinputandother.\ninput\nother\nminimum\n\nminimum\nComputes the element-wise minimum ofinputandother.\ninput\nother\nfmax\n\nfmax\nComputes the element-wise maximum ofinputandother.\ninput\nother\n\n## Reductions#\n\nThe following reductions are available (with autograd support). For more information, theOverviewtutorial\ndetails some examples of reductions, while theAdvanced semanticstutorial\nhas some further in-depth discussions about how we decided on certain reduction semantics.\nsum\n\nsum\nReturns the sum of all elements in theinputtensor.\ninput\nmean\n\nmean\n\namin\n\namin\nReturns the minimum value of each slice of theinputtensor in the given dimension(s)dim.\ninput\ndim\namax\n\namax\nReturns the maximum value of each slice of theinputtensor in the given dimension(s)dim.\ninput\ndim\nargmin\n\nargmin\nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension\nargmax\n\nargmax\nReturns the indices of the maximum value of all elements in theinputtensor.\ninput\nprod\n\nprod\nReturns the product of all elements in theinputtensor.\ninput\nall\n\nall\nTests if all elements ininputevaluate toTrue.\ninput\nnorm\n\nnorm\nReturns the matrix norm or vector norm of a given tensor.\nvar\n\nvar\nCalculates the variance over the dimensions specified bydim.\ndim\nstd\n\nstd\nCalculates the standard deviation over the dimensions specified bydim.\ndim\n\n## View and select functions#\n\nWe\u2019ve included a number of view and select functions as well; intuitively, these operators will apply to\nboth the data and the mask and then wrap the result in aMaskedTensor. For a quick example,\nconsiderselect():\nMaskedTensor\nselect()\n\n```python\n    >>> data = torch.arange(12, dtype=torch.float).reshape(3, 4)\n    >>> data\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11.]])\n    >>> mask = torch.tensor([[True, False, False, True], [False, True, False, False], [True, True, True, True]])\n    >>> mt = masked_tensor(data, mask)\n    >>> data.select(0, 1)\n    tensor([4., 5., 6., 7.])\n    >>> mask.select(0, 1)\n    tensor([False,  True, False, False])\n    >>> mt.select(0, 1)\n    MaskedTensor(\n      [      --,   5.0000,       --,       --]\n    )\n\n```\n\nThe following ops are currently supported:\natleast_1d\n\natleast_1d\nReturns a 1-dimensional view of each input tensor with zero dimensions.\nbroadcast_tensors\n\nbroadcast_tensors\nBroadcasts the given tensors according toBroadcasting semantics.\nbroadcast_to\n\nbroadcast_to\nBroadcastsinputto the shapeshape.\ninput\nshape\ncat\n\ncat\nConcatenates the given sequence of tensors intensorsin the given dimension.\ntensors\nchunk\n\nchunk\nAttempts to split a tensor into the specified number of chunks.\ncolumn_stack\n\ncolumn_stack\nCreates a new tensor by horizontally stacking the tensors intensors.\ntensors\ndsplit\n\ndsplit\nSplitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.\ninput\nindices_or_sections\nflatten\n\nflatten\nFlattensinputby reshaping it into a one-dimensional tensor.\ninput\nhsplit\n\nhsplit\nSplitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.\ninput\nindices_or_sections\nhstack\n\nhstack\nStack tensors in sequence horizontally (column wise).\nkron\n\nkron\nComputes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.\ninput\nother\nmeshgrid\n\nmeshgrid\nCreates grids of coordinates specified by the 1D inputs inattr:tensors.\nnarrow\n\nnarrow\nReturns a new tensor that is a narrowed version ofinputtensor.\ninput\nnn.functional.unfold\nnn.functional.unfold\nExtract sliding local blocks from a batched input tensor.\nravel\n\nravel\nReturn a contiguous flattened tensor.\nselect\n\nselect\nSlices theinputtensor along the selected dimension at the given index.\ninput\nsplit\n\nsplit\nSplits the tensor into chunks.\nstack\n\nstack\nConcatenates a sequence of tensors along a new dimension.\nt\n\nt\nExpectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.\ninput\ntranspose\n\ntranspose\nReturns a tensor that is a transposed version ofinput.\ninput\nvsplit\n\nvsplit\nSplitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.\ninput\nindices_or_sections\nvstack\n\nvstack\nStack tensors in sequence vertically (row wise).\nTensor.expand\nTensor.expand\nReturns a new view of theselftensor with singleton dimensions expanded to a larger size.\nself\nTensor.expand_as\nTensor.expand_as\nExpand this tensor to the same size asother.\nother\nTensor.reshape\nTensor.reshape\nReturns a tensor with the same data and number of elements asselfbut with the specified shape.\nself\nTensor.reshape_as\nTensor.reshape_as\nReturns this tensor as the same shape asother.\nother\nTensor.unfold\nTensor.unfold\nReturns a view of the original tensor which contains all slices of sizesizefromselftensor in the dimensiondimension.\nsize\nself\ndimension\nTensor.view\nTensor.view\nReturns a new tensor with the same data as theselftensor but of a differentshape.\nself\nshape",
  "url": "https://pytorch.org/docs/stable/masked.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
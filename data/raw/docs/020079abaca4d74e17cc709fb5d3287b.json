{
  "doc_id": "020079abaca4d74e17cc709fb5d3287b",
  "source": "pytorch_docs",
  "title": "FullyShardedDataParallel \u2014 PyTorch 2.9 documentation",
  "text": "\n## FullyShardedDataParallel#\n\nCreated On: Feb 02, 2022 | Last Updated On: Jun 11, 2025\nA wrapper for sharding module parameters across data parallel workers.\nThis is inspired byXu et al.as\nwell as the ZeRO Stage 3 fromDeepSpeed.\nFullyShardedDataParallel is commonly shortened to FSDP.\nExample:\n\n```python\n>>> import torch\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> torch.cuda.set_device(device_id)\n>>> sharded_module = FSDP(my_module)\n>>> optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)\n>>> x = sharded_module(x, y=3, z=torch.Tensor([1]))\n>>> loss = x.sum()\n>>> loss.backward()\n>>> optim.step()\n\n```\n\nUsing FSDP involves wrapping your module and then initializing your\noptimizer after. This is required since FSDP changes the parameter\nvariables.\nWhen setting up FSDP, you need to consider the destination CUDA\ndevice. If the device has an ID (dev_id), you have three options:\ndev_id\nPlace the module on that device\nSet the device usingtorch.cuda.set_device(dev_id)\ntorch.cuda.set_device(dev_id)\nPassdev_idinto thedevice_idconstructor argument.\ndev_id\ndevice_id\nThis ensures that the FSDP instance\u2019s compute device is the\ndestination device. For option 1 and 3, the FSDP initialization\nalways occurs on GPU. For option 2, the FSDP initialization\nhappens on module\u2019s current device, which may be a CPU.\nIf you\u2019re using thesync_module_states=Trueflag, you need to\nensure that the module is on a GPU or use thedevice_idargument to specify a CUDA device that FSDP will move the module\nto in the FSDP constructor. This is necessary becausesync_module_states=Truerequires GPU communication.\nsync_module_states=True\ndevice_id\nsync_module_states=True\nFSDP also takes care of moving input tensors to the forward method\nto the GPU compute device, so you don\u2019t need to manually move them\nfrom CPU.\nForuse_orig_params=True,ShardingStrategy.SHARD_GRAD_OPexposes the unsharded\nparameters, not the sharded parameters after forward, unlikeShardingStrategy.FULL_SHARD. If you want\nto inspect the gradients, you can use thesummon_full_paramsmethod withwith_grads=True.\nuse_orig_params=True\nShardingStrategy.SHARD_GRAD_OP\nShardingStrategy.FULL_SHARD\nsummon_full_params\nwith_grads=True\nWithlimit_all_gathers=True, you may see a gap in the FSDP\npre-forward where the CPU thread is not issuing any kernels. This is\nintentional and shows the rate limiter in effect. Synchronizing the CPU\nthread in that way prevents over-allocating memory for subsequent\nall-gathers, and it should not actually delay GPU kernel execution.\nlimit_all_gathers=True\nFSDP replaces managed modules\u2019 parameters withtorch.Tensorviews during forward and backward computation for autograd-related\nreasons. If your module\u2019s forward relies on saved references to\nthe parameters instead of reacquiring the references each\niteration, then it will not see FSDP\u2019s newly created views,\nand autograd will not work correctly.\ntorch.Tensor\nFinally, when usingsharding_strategy=ShardingStrategy.HYBRID_SHARDwith the sharding process group being intra-node and the\nreplication process group being inter-node, settingNCCL_CROSS_NIC=1can help improve the all-reduce times over\nthe replication process group for some cluster setups.\nsharding_strategy=ShardingStrategy.HYBRID_SHARD\nNCCL_CROSS_NIC=1\nLimitations\nThere are several limitations to be aware of when using FSDP:\nFSDP currently does not support gradient accumulation outsideno_sync()when using CPU offloading. This is because FSDP\nuses the newly-reduced gradient instead of accumulating with any\nexisting gradient, which can lead to incorrect results.\nno_sync()\nFSDP does not support running the forward pass of a submodule\nthat is contained in an FSDP instance. This is because the\nsubmodule\u2019s parameters will be sharded, but the submodule itself\nis not an FSDP instance, so its forward pass will not all-gather\nthe full parameters appropriately.\nFSDP does not work with double backwards due to the way it\nregisters backward hooks.\nFSDP has some constraints when freezing parameters.\nForuse_orig_params=False, each FSDP instance must manage\nparameters that are all frozen or all non-frozen. Foruse_orig_params=True, FSDP supports mixing frozen and\nnon-frozen parameters, but it\u2019s recommended to avoid doing so to\nprevent higher than expected gradient memory usage.\nuse_orig_params=False\nuse_orig_params=True\nAs of PyTorch 1.12, FSDP offers limited support for shared\nparameters. If enhanced shared parameter support is needed for\nyour use case, please post inthis issue.\nYou should avoid modifying the parameters between forward and\nbackward without using thesummon_full_paramscontext, as\nthe modifications may not persist.\nsummon_full_params\nmodule(nn.Module) \u2013 This is the module to be wrapped with FSDP.\nprocess_group(Optional[Union[ProcessGroup,Tuple[ProcessGroup,ProcessGroup]]]) \u2013 This is the process group over which the model is sharded and thus\nthe one used for FSDP\u2019s all-gather and reduce-scatter collective\ncommunications. IfNone, then FSDP uses the default process\ngroup. For hybrid sharding strategies such asShardingStrategy.HYBRID_SHARD, users can pass in a tuple of\nprocess groups, representing the groups over which to shard and\nreplicate, respectively. IfNone, then FSDP constructs process\ngroups for the user to shard intra-node and replicate inter-node.\n(Default:None)\nNone\nShardingStrategy.HYBRID_SHARD\nNone\nNone\nsharding_strategy(Optional[ShardingStrategy]) \u2013 This configures the sharding strategy, which may trade off memory\nsaving and communication overhead. SeeShardingStrategyfor details. (Default:FULL_SHARD)\nShardingStrategy\nFULL_SHARD\ncpu_offload(Optional[CPUOffload]) \u2013 This configures CPU offloading. If this is set toNone, then\nno CPU offloading happens. SeeCPUOffloadfor details.\n(Default:None)\nNone\nCPUOffload\nNone\nauto_wrap_policy(Optional[Union[Callable[[nn.Module,bool,int],bool],ModuleWrapPolicy,CustomPolicy]]) \u2013This specifies a policy to apply FSDP to submodules ofmodule,\nwhich is needed for communication and computation overlap and thus\naffects performance. IfNone, then FSDP only applies tomodule, and users should manually apply FSDP to parent modules\nthemselves (proceeding bottom-up). For convenience, this acceptsModuleWrapPolicydirectly, which allows users to specify the\nmodule classes to wrap (e.g. the transformer block). Otherwise,\nthis should be a callable that takes in three argumentsmodule:nn.Module,recurse:bool, andnonwrapped_numel:intand should return aboolspecifying\nwhether the passed-inmoduleshould have FSDP applied ifrecurse=Falseor if the traversal should continue into the\nmodule\u2019s subtree ifrecurse=True. Users may add additional\narguments to the callable. Thesize_based_auto_wrap_policyintorch.distributed.fsdp.wrap.pygives an example callable that\napplies FSDP to a module if the parameters in its subtree exceed\n100M numel. We recommend printing the model after applying FSDP\nand adjusting as needed.Example:>>>defcustom_auto_wrap_policy(>>>module:nn.Module,>>>recurse:bool,>>>nonwrapped_numel:int,>>># Additional custom arguments>>>min_num_params:int=int(1e8),>>>)->bool:>>>returnnonwrapped_numel>=min_num_params>>># Configure a custom `min_num_params`>>>my_auto_wrap_policy=functools.partial(custom_auto_wrap_policy,min_num_params=int(1e5))\nThis specifies a policy to apply FSDP to submodules ofmodule,\nwhich is needed for communication and computation overlap and thus\naffects performance. IfNone, then FSDP only applies tomodule, and users should manually apply FSDP to parent modules\nthemselves (proceeding bottom-up). For convenience, this acceptsModuleWrapPolicydirectly, which allows users to specify the\nmodule classes to wrap (e.g. the transformer block). Otherwise,\nthis should be a callable that takes in three argumentsmodule:nn.Module,recurse:bool, andnonwrapped_numel:intand should return aboolspecifying\nwhether the passed-inmoduleshould have FSDP applied ifrecurse=Falseor if the traversal should continue into the\nmodule\u2019s subtree ifrecurse=True. Users may add additional\narguments to the callable. Thesize_based_auto_wrap_policyintorch.distributed.fsdp.wrap.pygives an example callable that\napplies FSDP to a module if the parameters in its subtree exceed\n100M numel. We recommend printing the model after applying FSDP\nand adjusting as needed.\nmodule\nNone\nmodule\nModuleWrapPolicy\nmodule:nn.Module\nrecurse:bool\nnonwrapped_numel:int\nbool\nmodule\nrecurse=False\nrecurse=True\nsize_based_auto_wrap_policy\ntorch.distributed.fsdp.wrap.py\nExample:\n\n```python\n>>> def custom_auto_wrap_policy(\n>>>     module: nn.Module,\n>>>     recurse: bool,\n>>>     nonwrapped_numel: int,\n>>>     # Additional custom arguments\n>>>     min_num_params: int = int(1e8),\n>>> ) -> bool:\n>>>     return nonwrapped_numel >= min_num_params\n>>> # Configure a custom `min_num_params`\n>>> my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=int(1e5))\n\n```\n\nbackward_prefetch(Optional[BackwardPrefetch]) \u2013 This configures explicit backward prefetching of all-gathers. IfNone, then FSDP does not backward prefetch, and there is no\ncommunication and computation overlap in the backward pass. SeeBackwardPrefetchfor details. (Default:BACKWARD_PRE)\nNone\nBackwardPrefetch\nBACKWARD_PRE\nmixed_precision(Optional[MixedPrecision]) \u2013 This configures native mixed precision for FSDP. If this is set toNone, then no mixed precision is used. Otherwise, parameter,\nbuffer, and gradient reduction dtypes can be set. SeeMixedPrecisionfor details. (Default:None)\nNone\nMixedPrecision\nNone\nignored_modules(Optional[Iterable[torch.nn.Module]]) \u2013 Modules whose\nown parameters and child modules\u2019 parameters and buffers are\nignored by this instance. None of the modules directly inignored_modulesshould beFullyShardedDataParallelinstances, and any child modules that are already-constructedFullyShardedDataParallelinstances will not be ignored if\nthey are nested under this instance. This argument may be used to\navoid sharding specific parameters at module granularity when using anauto_wrap_policyor if parameters\u2019 sharding is not managed by\nFSDP. (Default:None)\nignored_modules\nFullyShardedDataParallel\nFullyShardedDataParallel\nauto_wrap_policy\nNone\nparam_init_fn(Optional[Callable[[nn.Module],None]]) \u2013ACallable[torch.nn.Module]->Nonethat\nspecifies how modules that are currently on the meta device should\nbe initialized onto an actual device. As of v1.12, FSDP detects\nmodules with parameters or buffers on meta device viais_metaand either appliesparam_init_fnif specified or callsnn.Module.reset_parameters()otherwise. For both cases, the\nimplementation shouldonlyinitialize the parameters/buffers of\nthe module, not those of its submodules. This is to avoid\nre-initialization. In addition, FSDP also supports deferred\ninitialization via torchdistX\u2019s (pytorch/torchdistX)deferred_init()API, where the deferred modules are initialized\nby callingparam_init_fnif specified or torchdistX\u2019s defaultmaterialize_module()otherwise. Ifparam_init_fnis\nspecified, then it is applied to all meta-device modules, meaning\nthat it should probably case on the module type. FSDP calls the\ninitialization function before parameter flattening and sharding.Example:>>>module=MyModule(device=\"meta\")>>>defmy_init_fn(module:nn.Module):>>># E.g. initialize depending on the module type>>>...>>>fsdp_model=FSDP(module,param_init_fn=my_init_fn,auto_wrap_policy=size_based_auto_wrap_policy)>>>print(next(fsdp_model.parameters()).device)# current CUDA device>>># With torchdistX>>>module=deferred_init.deferred_init(MyModule,device=\"cuda\")>>># Will initialize via deferred_init.materialize_module().>>>fsdp_model=FSDP(module,auto_wrap_policy=size_based_auto_wrap_policy)\nACallable[torch.nn.Module]->Nonethat\nspecifies how modules that are currently on the meta device should\nbe initialized onto an actual device. As of v1.12, FSDP detects\nmodules with parameters or buffers on meta device viais_metaand either appliesparam_init_fnif specified or callsnn.Module.reset_parameters()otherwise. For both cases, the\nimplementation shouldonlyinitialize the parameters/buffers of\nthe module, not those of its submodules. This is to avoid\nre-initialization. In addition, FSDP also supports deferred\ninitialization via torchdistX\u2019s (pytorch/torchdistX)deferred_init()API, where the deferred modules are initialized\nby callingparam_init_fnif specified or torchdistX\u2019s defaultmaterialize_module()otherwise. Ifparam_init_fnis\nspecified, then it is applied to all meta-device modules, meaning\nthat it should probably case on the module type. FSDP calls the\ninitialization function before parameter flattening and sharding.\nCallable[torch.nn.Module]->None\nis_meta\nparam_init_fn\nnn.Module.reset_parameters()\ndeferred_init()\nparam_init_fn\nmaterialize_module()\nparam_init_fn\nExample:\n\n```python\n>>> module = MyModule(device=\"meta\")\n>>> def my_init_fn(module: nn.Module):\n>>>     # E.g. initialize depending on the module type\n>>>     ...\n>>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)\n>>> print(next(fsdp_model.parameters()).device) # current CUDA device\n>>> # With torchdistX\n>>> module = deferred_init.deferred_init(MyModule, device=\"cuda\")\n>>> # Will initialize via deferred_init.materialize_module().\n>>> fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)\n\n```\n\ndevice_id(Optional[Union[int,torch.device]]) \u2013 Anintortorch.devicegiving the CUDA device on which FSDP\ninitialization takes place, including the module initialization\nif needed and the parameter sharding. This should be specified to\nimprove initialization speed ifmoduleis on CPU. If the\ndefault CUDA device was set (e.g. viatorch.cuda.set_device),\nthen the user may passtorch.cuda.current_deviceto this.\n(Default:None)\nint\ntorch.device\nmodule\ntorch.cuda.set_device\ntorch.cuda.current_device\nNone\nsync_module_states(bool) \u2013 IfTrue, then each FSDP module will\nbroadcast module parameters and buffers from rank 0 to ensure that\nthey are replicated across ranks (adding communication overhead to\nthis constructor). This can help loadstate_dictcheckpoints\nviaload_state_dictin a memory efficient way. SeeFullStateDictConfigfor an example of this. (Default:False)\nTrue\nstate_dict\nload_state_dict\nFullStateDictConfig\nFalse\nforward_prefetch(bool) \u2013 IfTrue, then FSDPexplicitlyprefetches\nthe next forward-pass all-gather before the current forward\ncomputation. This is only useful for CPU-bound workloads, in which\ncase issuing the next all-gather earlier may improve overlap. This\nshould only be used for static-graph models since the prefetching\nfollows the first iteration\u2019s execution order. (Default:False)\nTrue\nFalse\nlimit_all_gathers(bool) \u2013 IfTrue, then FSDP explicitly\nsynchronizes the CPU thread to ensure GPU memory usage from onlytwoconsecutive FSDP instances (the current instance running\ncomputation and the next instance whose all-gather is prefetched).\nIfFalse, then FSDP allows the CPU thread to issue all-gathers\nwithout any extra synchronization. (Default:True) We often\nrefer to this feature as the \u201crate limiter\u201d. This flag should only\nbe set toFalsefor specific CPU-bound workloads with low\nmemory pressure in which case the CPU thread can aggressively issue\nall kernels without concern for the GPU memory usage.\nTrue\nFalse\nTrue\nFalse\nuse_orig_params(bool) \u2013 Setting this toTruehas FSDP usemodule\u2018s original parameters. FSDP exposes those original\nparameters to the user viann.Module.named_parameters()instead of FSDP\u2019s internalFlatParameters. This means\nthat the optimizer step runs on the original parameters, enabling\nper-original-parameter hyperparameters. FSDP preserves the original\nparameter variables and manipulates their data between unsharded\nand sharded forms, where they are always views into the underlying\nunsharded or shardedFlatParameter, respectively. With the\ncurrent algorithm, the sharded form is always 1D, losing the\noriginal tensor structure. An original parameter may have all,\nsome, or none of its data present for a given rank. In the none\ncase, its data will be like a size-0 empty tensor. Users should not\nauthor programs relying on what data is present for a given\noriginal parameter in its sharded form.Trueis required to\nusetorch.compile(). Setting this toFalseexposes FSDP\u2019s\ninternalFlatParameters to the user viann.Module.named_parameters(). (Default:False)\nTrue\nmodule\nnn.Module.named_parameters()\nFlatParameter\nFlatParameter\nTrue\ntorch.compile()\nFalse\nFlatParameter\nnn.Module.named_parameters()\nFalse\nignored_states(Optional[Iterable[torch.nn.Parameter]],Optional[Iterable[torch.nn.Module]]) \u2013 Ignored parameters or modules that will not be managed by this FSDP\ninstance, meaning that the parameters are not sharded and their\ngradients are not reduced across ranks. This argument unifies with\nthe existingignored_modulesargument, and we may deprecateignored_modulessoon. For backward compatibility, we keep bothignored_statesandignored_modules`, but FSDP only allows one\nof them to be specified as notNone.\nignored_modules\nignored_modules\nignored_states\nNone\ndevice_mesh(Optional[DeviceMesh]) \u2013 DeviceMesh can be used as an alternative to\nprocess_group. When device_mesh is passed, FSDP will use the underlying process\ngroups for all-gather and reduce-scatter collective communications. Therefore,\nthese two args need to be mutually exclusive. For hybrid sharding strategies such asShardingStrategy.HYBRID_SHARD, users can pass in a 2D DeviceMesh instead\nof a tuple of process groups. For 2D FSDP + TP, users are required to pass in\ndevice_mesh instead of process_group. For more DeviceMesh info, please visit:https://pytorch.org/tutorials/recipes/distributed_device_mesh.html\nShardingStrategy.HYBRID_SHARD\nApplyfnrecursively to every submodule (as returned by.children()) as well as self.\nfn\n.children()\nTypical use includes initializing the parameters of a model (see alsotorch.nn.init).\nCompared totorch.nn.Module.apply, this version additionally gathers\nthe full parameters before applyingfn. It should not be called from\nwithin anothersummon_full_paramscontext.\ntorch.nn.Module.apply\nfn\nsummon_full_params\nfn(Module-> None) \u2013 function to be applied to each submodule\nModule\nself\nModule\n\nCheck if this instance is a root FSDP module.\nbool\nClip the gradient norm of all parameters.\nThe norm is computed over all parameters\u2019 gradients as viewed as a single vector, and the\ngradients are modified in-place.\nmax_norm(floatorint) \u2013 max norm of the gradients\nnorm_type(floatorint) \u2013 type of the used p-norm. Can be'inf'for infinity norm.\n'inf'\nTotal norm of the parameters (viewed as a single vector).\nTensor\nIf every FSDP instance usesNO_SHARD, meaning that no\ngradients are sharded across ranks, then you may directly usetorch.nn.utils.clip_grad_norm_().\nNO_SHARD\ntorch.nn.utils.clip_grad_norm_()\nIf at least some FSDP instance uses a sharded strategy (i.e.\none other thanNO_SHARD), then you should use this method\ninstead oftorch.nn.utils.clip_grad_norm_()since this method\nhandles the fact that gradients are sharded across ranks.\nNO_SHARD\ntorch.nn.utils.clip_grad_norm_()\nThe total norm returned will have the \u201clargest\u201d dtype across\nall parameters/gradients as defined by PyTorch\u2019s type promotion\nsemantics. For example, ifallparameters/gradients use a low\nprecision dtype, then the returned norm\u2019s dtype will be that low\nprecision dtype, but if there exists at least one parameter/\ngradient using FP32, then the returned norm\u2019s dtype will be FP32.\nWarning\nThis needs to be called on all ranks since it uses\ncollective communications.\nFlatten a sharded optimizer state-dict.\nThe API is similar toshard_full_optim_state_dict(). The only\ndifference is that the inputsharded_optim_state_dictshould be\nreturned fromsharded_optim_state_dict(). Therefore, there will\nbe all-gather calls on each rank to gatherShardedTensors.\nshard_full_optim_state_dict()\nsharded_optim_state_dict\nsharded_optim_state_dict()\nShardedTensor\nsharded_optim_state_dict(Dict[str,Any]) \u2013 Optimizer state dict\ncorresponding to the unflattened parameters and holding the\nsharded optimizer state.\nmodel(torch.nn.Module) \u2013 Refer toshard_full_optim_state_dict().\nshard_full_optim_state_dict()\noptim(torch.optim.Optimizer) \u2013 Optimizer formodel\u2018s\nparameters.\nmodel\nRefer toshard_full_optim_state_dict().\nshard_full_optim_state_dict()\ndict[str,Any]\nRun the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.\nAny\nReturn all nested FSDP instances.\nThis possibly includesmoduleitself and only includes FSDP root modules ifroot_only=True.\nmodule\nroot_only=True\nmodule(torch.nn.Module) \u2013 Root module, which may or may not be anFSDPmodule.\nFSDP\nroot_only(bool) \u2013 Whether to return only FSDP root modules.\n(Default:False)\nFalse\nFSDP modules that are nested in\nthe inputmodule.\nmodule\nList[FullyShardedDataParallel]\nReturn the full optimizer state-dict.\nConsolidates the full optimizer state on rank 0 and returns it\nas adictfollowing the convention oftorch.optim.Optimizer.state_dict(), i.e. with keys\"state\"and\"param_groups\". The flattened parameters inFSDPmodules\ncontained inmodelare mapped back to their unflattened parameters.\ndict\ntorch.optim.Optimizer.state_dict()\n\"state\"\n\"param_groups\"\nFSDP\nmodel\nThis needs to be called on all ranks since it uses\ncollective communications. However, ifrank0_only=True, then\nthe state dict is only populated on rank 0, and all other ranks\nreturn an emptydict.\nrank0_only=True\ndict\nUnliketorch.optim.Optimizer.state_dict(), this method\nuses full parameter names as keys instead of parameter IDs.\ntorch.optim.Optimizer.state_dict()\nLike intorch.optim.Optimizer.state_dict(), the tensors\ncontained in the optimizer state dict are not cloned, so there may\nbe aliasing surprises. For best practices, consider saving the\nreturned optimizer state dict immediately, e.g. usingtorch.save().\ntorch.optim.Optimizer.state_dict()\ntorch.save()\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\nwere passed into the optimizeroptim.\nFullyShardedDataParallel\noptim\noptim(torch.optim.Optimizer) \u2013 Optimizer formodel\u2018s\nparameters.\nmodel\noptim_input(Optional[Union[List[Dict[str,Any]],Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizeroptimrepresenting either alistof parameter groups or an iterable of parameters;\nifNone, then this method assumes the input wasmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default:None)\noptim\nlist\nNone\nmodel.parameters()\nNone\nrank0_only(bool) \u2013 IfTrue, saves the populateddictonly on rank 0; ifFalse, saves it on all ranks. (Default:True)\nTrue\ndict\nFalse\nTrue\ngroup(dist.ProcessGroup) \u2013 Model\u2019s process group orNoneif using\nthe default process group. (Default:None)\nNone\nNone\nAdictcontaining the optimizer state formodel\u2018s original unflattened parameters and including keys\n\u201cstate\u201d and \u201cparam_groups\u201d following the convention oftorch.optim.Optimizer.state_dict(). Ifrank0_only=True,\nthen nonzero ranks return an emptydict.\ndict\nmodel\ntorch.optim.Optimizer.state_dict()\nrank0_only=True\ndict\nDict[str, Any]\nGet the state_dict_type and the corresponding configurations for the FSDP modules rooted atmodule.\nmodule\nThe target module does not have to be an FSDP module.\nAStateDictSettingscontaining the state_dict_type and\nstate_dict / optim_state_dict configs that are currently set.\nStateDictSettings\nAssertionError` if the StateDictSettings for differen\u2013\nFSDP submodules differ.\u2013\nStateDictSettings\nReturn the wrapped module.\nReturn an iterator over module buffers, yielding both the name of the buffer and the buffer itself.\nIntercepts buffer names and removes all occurrences of the FSDP-specific flattened buffer prefix\nwhen inside thesummon_full_params()context manager.\nsummon_full_params()\nIterator[tuple[str,torch.Tensor]]\nReturn an iterator over module parameters, yielding both the name of the parameter and the parameter itself.\nIntercepts parameter names and removes all occurrences of the FSDP-specific flattened parameter prefix\nwhen inside thesummon_full_params()context manager.\nsummon_full_params()\nIterator[tuple[str,torch.nn.parameter.Parameter]]\nDisable gradient synchronizations across FSDP instances.\nWithin this context, gradients will be accumulated in module\nvariables, which will later be synchronized in the first\nforward-backward pass after exiting the context. This should only be\nused on the root FSDP instance and will recursively apply to all\nchildren FSDP instances.\nNote\nThis likely results in higher memory usage because FSDP will\naccumulate the full model gradients (instead of gradient shards)\nuntil the eventual sync.\nNote\nWhen used with CPU offloading, the gradients will not be\noffloaded to CPU when inside the context manager. Instead, they\nwill only be offloaded right after the eventual sync.\nGenerator\nTransform the state-dict of an optimizer corresponding to a sharded model.\nThe given state-dict can be transformed to one of three types:\n1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.\nFor full optimizer state_dict, all states are unflattened and not sharded.\nRank0 only and CPU only can be specified viastate_dict_type()to\navoid OOM.\nstate_dict_type()\nFor sharded optimizer state_dict, all states are unflattened but sharded.\nCPU only can be specified viastate_dict_type()to further save\nmemory.\nstate_dict_type()\nFor local state_dict, no transformation will be performed. But a state\nwill be converted from nn.Tensor to ShardedTensor to represent its sharding\nnature (this is not supported yet).\nExample:\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     model, optim, optim_state_dict\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n\n```\n\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\nwere passed into the optimizeroptim.\nFullyShardedDataParallel\noptim\noptim(torch.optim.Optimizer) \u2013 Optimizer formodel\u2018s\nparameters.\nmodel\noptim_state_dict(Dict[str,Any]) \u2013 the target optimizer state_dict to\ntransform. If the value is None, optim.state_dict() will be used. (\nDefault:None)\nNone\ngroup(dist.ProcessGroup) \u2013 Model\u2019s process group across which parameters\nare sharded orNoneif using the default process group. (\nDefault:None)\nNone\nNone\nAdictcontaining the optimizer state formodel. The sharding of the optimizer state is based onstate_dict_type.\ndict\nmodel\nstate_dict_type\nDict[str, Any]\nConvert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.\nGiven aoptim_state_dictthat is transformed throughoptim_state_dict(), it gets converted to the flattened optimizer\nstate_dict that can be loaded tooptimwhich is the optimizer formodel.modelmust be sharded by FullyShardedDataParallel.\noptim_state_dict\noptim_state_dict()\noptim\nmodel\nmodel\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> original_osd = optim.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(\n>>>     model,\n>>>     optim,\n>>>     optim_state_dict=original_osd\n>>> )\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     model, optim, optim_state_dict\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n\n```\n\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\nwere passed into the optimizeroptim.\nFullyShardedDataParallel\noptim\noptim(torch.optim.Optimizer) \u2013 Optimizer formodel\u2018s\nparameters.\nmodel\noptim_state_dict(Dict[str,Any]) \u2013 The optimizer states to be loaded.\nis_named_optimizer(bool) \u2013 Is this optimizer a NamedOptimizer or\nKeyedOptimizer. Only set to True ifoptimis TorchRec\u2019s\nKeyedOptimizer or torch.distributed\u2019s NamedOptimizer.\noptim\nload_directly(bool) \u2013 If this is set to True, this API will also\ncall optim.load_state_dict(result) before returning the result.\nOtherwise, users are responsible to calloptim.load_state_dict()(Default:False)\noptim.load_state_dict()\nFalse\ngroup(dist.ProcessGroup) \u2013 Model\u2019s process group across which parameters\nare sharded orNoneif using the default process group. (\nDefault:None)\nNone\nNone\ndict[str,Any]\nRegister a communication hook.\nThis is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates\ngradients across multiple workers.\nThis hook can be used to implement several algorithms likeGossipGradand gradient compression\nwhich involve different communication strategies for\nparameter syncs while training withFullyShardedDataParallel.\nFullyShardedDataParallel\nWarning\nFSDP communication hook should be registered before running an initial forward pass\nand only once.\nstate(object) \u2013Passed to the hook to maintain any state information during the training process.\nExamples include error feedback in gradient compression,\npeers to communicate with next inGossipGrad, etc.\nIt is locally stored by each worker\nand shared by all the gradient tensors on the worker.\nPassed to the hook to maintain any state information during the training process.\nExamples include error feedback in gradient compression,\npeers to communicate with next inGossipGrad, etc.\nIt is locally stored by each worker\nand shared by all the gradient tensors on the worker.\nhook(Callable) \u2013 Callable, which has one of the following signatures:\n1)hook:Callable[torch.Tensor]->None:\nThis function takes in a Python tensor, which represents\nthe full, flattened, unsharded gradient with respect to all variables\ncorresponding to the model this FSDP unit is wrapping\n(that are not wrapped by other FSDP sub-units).\nIt then performs all necessary processing and returnsNone;\n2)hook:Callable[torch.Tensor,torch.Tensor]->None:\nThis function takes in two Python tensors, the first one represents\nthe full, flattened, unsharded gradient with respect to all variables\ncorresponding to the model this FSDP unit is wrapping\n(that are not wrapped by other FSDP sub-units). The latter\nrepresents a pre-sized tensor to store a chunk of a sharded gradient after\nreduction.\nIn both cases, callable performs all necessary processing and returnsNone.\nCallables with signature 1 are expected to handle gradient communication for aNO_SHARDcase.\nCallables with signature 2 are expected to handle gradient communication for sharded cases.\nhook:Callable[torch.Tensor]->None\nNone\nhook:Callable[torch.Tensor,torch.Tensor]->None\nNone\nRe-keys the optimizer state dictoptim_state_dictto use the key typeoptim_state_key_type.\noptim_state_dict\noptim_state_key_type\nThis can be used to achieve compatibility between optimizer state dicts from models with FSDP\ninstances and ones without.\nTo re-key an FSDP full optimizer state dict (i.e. fromfull_optim_state_dict()) to use parameter IDs and be loadable to\na non-wrapped model:\nfull_optim_state_dict()\n\n```python\n>>> wrapped_model, wrapped_optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)\n>>> nonwrapped_optim.load_state_dict(rekeyed_osd)\n\n```\n\nTo re-key a normal optimizer state dict from a non-wrapped model to be\nloadable to a wrapped model:\n\n```python\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> osd = nonwrapped_optim.state_dict()\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)\n>>> wrapped_model, wrapped_optim = ...\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)\n>>> wrapped_optim.load_state_dict(sharded_osd)\n\n```\n\nThe optimizer state dict re-keyed using the\nparameter keys specified byoptim_state_key_type.\noptim_state_key_type\nDict[str, Any]\nScatter the full optimizer state dict from rank 0 to all other ranks.\nReturns the sharded optimizer state dict on each rank.\nThe return value is the same asshard_full_optim_state_dict(), and on rank\n0, the first argument should be the return value offull_optim_state_dict().\nshard_full_optim_state_dict()\nfull_optim_state_dict()\nExample:\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim, new_group = ...\n>>> sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)\n>>> new_optim.load_state_dict(sharded_osd)\n\n```\n\nNote\nBothshard_full_optim_state_dict()andscatter_full_optim_state_dict()may be used to get the\nsharded optimizer state dict to load. Assuming that the full\noptimizer state dict resides in CPU memory, the former requires\neach rank to have the full dict in CPU memory, where each rank\nindividually shards the dict without any communication, while the\nlatter requires only rank 0 to have the full dict in CPU memory,\nwhere rank 0 moves each shard to GPU memory (for NCCL) and\ncommunicates it to ranks appropriately. Hence, the former has\nhigher aggregate CPU memory cost, while the latter has higher\ncommunication cost.\nshard_full_optim_state_dict()\nscatter_full_optim_state_dict()\nfull_optim_state_dict(Optional[Dict[str,Any]]) \u2013 Optimizer state\ndict corresponding to the unflattened parameters and holding\nthe full non-sharded optimizer state if on rank 0; the argument\nis ignored on nonzero ranks.\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\ncorrespond to the optimizer state infull_optim_state_dict.\nFullyShardedDataParallel\nfull_optim_state_dict\noptim_input(Optional[Union[List[Dict[str,Any]],Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer representing either alistof parameter groups or an iterable of parameters;\nifNone, then this method assumes the input wasmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default:None)\nlist\nNone\nmodel.parameters()\nNone\noptim(Optional[torch.optim.Optimizer]) \u2013 Optimizer that will load\nthe state dict returned by this method. This is the preferred\nargument to use overoptim_input. (Default:None)\noptim_input\nNone\ngroup(dist.ProcessGroup) \u2013 Model\u2019s process group orNoneif\nusing the default process group. (Default:None)\nNone\nNone\nThe full optimizer state dict now remapped to\nflattened parameters instead of unflattened parameters and\nrestricted to only include this rank\u2019s part of the optimizer state.\nDict[str, Any]\nSet thestate_dict_typeof all the descendant FSDP modules of the target module.\nstate_dict_type\nAlso takes (optional) configuration for the model\u2019s and optimizer\u2019s state dict.\nThe target module does not have to be a FSDP module. If the target\nmodule is a FSDP module, itsstate_dict_typewill also be changed.\nstate_dict_type\nNote\nThis API should be called for only the top-level (root)\nmodule.\nNote\nThis API enables users to transparently use the conventionalstate_dictAPI to take model checkpoints in cases where the\nroot FSDP module is wrapped by anothernn.Module. For example,\nthe following will ensurestate_dictis called on all non-FSDP\ninstances, while dispatching intosharded_state_dictimplementation\nfor FSDP:\nstate_dict\nnn.Module\nstate_dict\nExample:\n\n```python\n>>> model = DDP(FSDP(...))\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>>     state_dict_config = ShardedStateDictConfig(offload_to_cpu=True),\n>>>     optim_state_dict_config = OptimStateDictConfig(offload_to_cpu=True),\n>>> )\n>>> param_state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n\n```\n\nmodule(torch.nn.Module) \u2013 Root module.\nstate_dict_type(StateDictType) \u2013 the desiredstate_dict_typeto set.\nstate_dict_type\nstate_dict_config(Optional[StateDictConfig]) \u2013 the configuration for the\ntargetstate_dict_type.\nstate_dict_type\noptim_state_dict_config(Optional[OptimStateDictConfig]) \u2013 the configuration\nfor the optimizer state dict.\nA StateDictSettings that include the previous state_dict type and\nconfiguration for the module.\nStateDictSettings\nShard a full optimizer state-dict.\nRemaps the state infull_optim_state_dictto flattened parameters instead of unflattened\nparameters and restricts to only this rank\u2019s part of the optimizer state.\nThe first argument should be the return value offull_optim_state_dict().\nfull_optim_state_dict\nfull_optim_state_dict()\nExample:\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)\n>>> torch.save(full_osd, PATH)\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim = ...\n>>> full_osd = torch.load(PATH)\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)\n>>> new_optim.load_state_dict(sharded_osd)\n\n```\n\nNote\nBothshard_full_optim_state_dict()andscatter_full_optim_state_dict()may be used to get the\nsharded optimizer state dict to load. Assuming that the full\noptimizer state dict resides in CPU memory, the former requires\neach rank to have the full dict in CPU memory, where each rank\nindividually shards the dict without any communication, while the\nlatter requires only rank 0 to have the full dict in CPU memory,\nwhere rank 0 moves each shard to GPU memory (for NCCL) and\ncommunicates it to ranks appropriately. Hence, the former has\nhigher aggregate CPU memory cost, while the latter has higher\ncommunication cost.\nshard_full_optim_state_dict()\nscatter_full_optim_state_dict()\nfull_optim_state_dict(Dict[str,Any]) \u2013 Optimizer state dict\ncorresponding to the unflattened parameters and holding the\nfull non-sharded optimizer state.\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\ncorrespond to the optimizer state infull_optim_state_dict.\nFullyShardedDataParallel\nfull_optim_state_dict\noptim_input(Optional[Union[List[Dict[str,Any]],Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer representing either alistof parameter groups or an iterable of parameters;\nifNone, then this method assumes the input wasmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default:None)\nlist\nNone\nmodel.parameters()\nNone\noptim(Optional[torch.optim.Optimizer]) \u2013 Optimizer that will load\nthe state dict returned by this method. This is the preferred\nargument to use overoptim_input. (Default:None)\noptim_input\nNone\nThe full optimizer state dict now remapped to\nflattened parameters instead of unflattened parameters and\nrestricted to only include this rank\u2019s part of the optimizer state.\nDict[str, Any]\nReturn the optimizer state-dict in its sharded form.\nThe API is similar tofull_optim_state_dict()but this API chunks\nall non-zero-dimension states toShardedTensorto save memory.\nThis API should only be used when the modelstate_dictis derived\nwith the context managerwithstate_dict_type(SHARDED_STATE_DICT):.\nfull_optim_state_dict()\nShardedTensor\nstate_dict\nwithstate_dict_type(SHARDED_STATE_DICT):\nFor the detailed usage, refer tofull_optim_state_dict().\nfull_optim_state_dict()\nWarning\nThe returned state dict containsShardedTensorand\ncannot be directly used by the regularoptim.load_state_dict.\nShardedTensor\noptim.load_state_dict\ndict[str,Any]\nSet thestate_dict_typeof all the descendant FSDP modules of the target module.\nstate_dict_type\nThis context manager has the same functions asset_state_dict_type(). Read the document ofset_state_dict_type()for the detail.\nset_state_dict_type()\nset_state_dict_type()\nExample:\n\n```python\n>>> model = DDP(FSDP(...))\n>>> with FSDP.state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>> ):\n>>>     checkpoint = model.state_dict()\n\n```\n\nmodule(torch.nn.Module) \u2013 Root module.\nstate_dict_type(StateDictType) \u2013 the desiredstate_dict_typeto set.\nstate_dict_type\nstate_dict_config(Optional[StateDictConfig]) \u2013 the modelstate_dictconfiguration for the targetstate_dict_type.\nstate_dict\nstate_dict_type\noptim_state_dict_config(Optional[OptimStateDictConfig]) \u2013 the optimizerstate_dictconfiguration for the targetstate_dict_type.\nstate_dict\nstate_dict_type\nGenerator\nExpose full params for FSDP instances with this context manager.\nCan be usefulafterforward/backward for a model to get\nthe params for additional processing or checking. It can take a non-FSDP\nmodule and will summon full params for all contained FSDP modules as\nwell as their children, depending on therecurseargument.\nrecurse\nNote\nThis can be used on inner FSDPs.\nNote\nThis cannotbe used within a forward or backward pass. Nor\ncan forward and backward be started from within this context.\nNote\nParameters will revert to their local shards after the context\nmanager exits, storage behavior is the same as forward.\nNote\nThe full parameters can be modified, but only the portion\ncorresponding to the local param shard will persist after the\ncontext manager exits (unlesswriteback=False, in which case\nchanges will be discarded). In the case where FSDP does not shard\nthe parameters, currently only whenworld_size==1, orNO_SHARDconfig, the modification is persisted regardless ofwriteback.\nwriteback=False\nworld_size==1\nNO_SHARD\nwriteback\nNote\nThis method works on modules which are not FSDP themselves but\nmay contain multiple independent FSDP units. In that case, the given\narguments will apply to all contained FSDP units.\nWarning\nNote thatrank0_only=Truein conjunction withwriteback=Trueis not currently supported and will raise an\nerror. This is because model parameter shapes would be different\nacross ranks within the context, and writing to them can lead to\ninconsistency across ranks when the context is exited.\nrank0_only=True\nwriteback=True\nWarning\nNote thatoffload_to_cpuandrank0_only=Falsewill\nresult in full parameters being redundantly copied to CPU memory for\nGPUs that reside on the same machine, which may incur the risk of\nCPU OOM. It is recommended to useoffload_to_cpuwithrank0_only=True.\noffload_to_cpu\nrank0_only=False\noffload_to_cpu\nrank0_only=True\nrecurse(bool,Optional) \u2013 recursively summon all params for nested\nFSDP instances (default: True).\nwriteback(bool,Optional) \u2013 ifFalse, modifications to params are\ndiscarded after the context manager exits;\ndisabling this can be slightly more efficient (default: True)\nFalse\nrank0_only(bool,Optional) \u2013 ifTrue, full parameters are\nmaterialized on only global rank 0. This means that within the\ncontext, only rank 0 will have full parameters and the other\nranks will have sharded parameters. Note that settingrank0_only=Truewithwriteback=Trueis not supported,\nas model parameter shapes will be different across ranks\nwithin the context, and writing to them can lead to\ninconsistency across ranks when the context is exited.\nTrue\nrank0_only=True\nwriteback=True\noffload_to_cpu(bool,Optional) \u2013 IfTrue, full parameters are\noffloaded to CPU. Note that this offloading currently only\noccurs if the parameter is sharded (which is only not the case\nfor world_size = 1 orNO_SHARDconfig). It is recommended\nto useoffload_to_cpuwithrank0_only=Trueto avoid\nredundant copies of model parameters being offloaded to the same CPU memory.\nTrue\nNO_SHARD\noffload_to_cpu\nrank0_only=True\nwith_grads(bool,Optional) \u2013 IfTrue, gradients are also\nunsharded with the parameters. Currently, this is only\nsupported when passinguse_orig_params=Trueto the FSDP\nconstructor andoffload_to_cpu=Falseto this method.\n(Default:False)\nTrue\nuse_orig_params=True\noffload_to_cpu=False\nFalse\nGenerator\nThis configures explicit backward prefetching, which improves throughput by\nenabling communication and computation overlap in the backward pass at the\ncost of slightly increased memory usage.\nBACKWARD_PRE: This enables the most overlap but increases memory\nusage the most. This prefetches the next set of parametersbeforethe\ncurrent set of parameters\u2019 gradient computation. This overlaps thenext\nall-gatherand thecurrent gradient computation, and at the peak, it\nholds the current set of parameters, next set of parameters, and current\nset of gradients in memory.\nBACKWARD_PRE\nBACKWARD_POST: This enables less overlap but requires less memory\nusage. This prefetches the next set of parametersafterthe current\nset of parameters\u2019 gradient computation. This overlaps thecurrent\nreduce-scatterand thenext gradient computation, and it frees the\ncurrent set of parameters before allocating memory for the next set of\nparameters, only holding the next set of parameters and current set of\ngradients in memory at the peak.\nBACKWARD_POST\nFSDP\u2019sbackward_prefetchargument acceptsNone, which disables\nthe backward prefetching altogether. This has no overlap and does not\nincrease memory usage. In general, we do not recommend this setting since\nit may degrade throughput significantly.\nbackward_prefetch\nNone\nFor more technical context: For a single process group using NCCL backend,\nany collectives, even if issued from different streams, contend for the\nsame per-device NCCL stream, which implies that the relative order in which\nthe collectives are issued matters for overlapping. The two backward\nprefetching values correspond to different issue orders.\nThis specifies the sharding strategy to be used for distributed training byFullyShardedDataParallel.\nFullyShardedDataParallel\nFULL_SHARD: Parameters, gradients, and optimizer states are sharded.\nFor the parameters, this strategy unshards (via all-gather) before the\nforward, reshards after the forward, unshards before the backward\ncomputation, and reshards after the backward computation. For gradients,\nit synchronizes and shards them (via reduce-scatter) after the backward\ncomputation. The sharded optimizer states are updated locally per rank.\nFULL_SHARD\nSHARD_GRAD_OP: Gradients and optimizer states are sharded during\ncomputation, and additionally, parameters are sharded outside\ncomputation. For the parameters, this strategy unshards before the\nforward, does not reshard them after the forward, and only reshards them\nafter the backward computation. The sharded optimizer states are updated\nlocally per rank. Insideno_sync(), the parameters are not resharded\nafter the backward computation.\nSHARD_GRAD_OP\nno_sync()\nNO_SHARD: Parameters, gradients, and optimizer states are not sharded\nbut instead replicated across ranks similar to PyTorch\u2019sDistributedDataParallelAPI. For gradients, this strategy\nsynchronizes them (via all-reduce) after the backward computation. The\nunsharded optimizer states are updated locally per rank.\nNO_SHARD\nDistributedDataParallel\nHYBRID_SHARD: ApplyFULL_SHARDwithin a node, and replicate parameters across\nnodes. This results in reduced communication volume as expensive all-gathers and\nreduce-scatters are only done within a node, which can be more performant for medium\n-sized models.\nHYBRID_SHARD\nFULL_SHARD\n_HYBRID_SHARD_ZERO2: ApplySHARD_GRAD_OPwithin a node, and replicate parameters across\nnodes. This is likeHYBRID_SHARD, except this may provide even higher throughput\nsince the unsharded parameters are not freed after the forward pass, saving the\nall-gathers in the pre-backward.\n_HYBRID_SHARD_ZERO2\nSHARD_GRAD_OP\nHYBRID_SHARD\nThis configures FSDP-native mixed precision training.\nparam_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for model\nparameters during forward and backward and thus the dtype for\nforward and backward computation. Outside forward and backward, theshardedparameters are kept in full precision (e.g. for the\noptimizer step), and for model checkpointing, the parameters are\nalways saved in full precision. (Default:None)\nNone\nreduce_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\ngradient reduction (i.e. reduce-scatter or all-reduce). If this isNonebutparam_dtypeis notNone, then this takes on\ntheparam_dtypevalue, still running gradient reduction in low\nprecision. This is permitted to differ fromparam_dtype, e.g.\nto force gradient reduction to run in full precision. (Default:None)\nNone\nparam_dtype\nNone\nparam_dtype\nparam_dtype\nNone\nbuffer_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\nbuffers. FSDP does not shard buffers. Rather, FSDP casts them tobuffer_dtypein the first forward pass and keeps them in that\ndtype thereafter. For model checkpointing, the buffers are saved\nin full precision except forLOCAL_STATE_DICT. (Default:None)\nbuffer_dtype\nLOCAL_STATE_DICT\nNone\nkeep_low_precision_grads(bool) \u2013 IfFalse, then FSDP upcasts\ngradients to full precision after the backward pass in preparation\nfor the optimizer step. IfTrue, then FSDP keeps the gradients\nin the dtype used for gradient reduction, which can save memory if\nusing a custom optimizer that supports running in low precision.\n(Default:False)\nFalse\nTrue\nFalse\ncast_forward_inputs(bool) \u2013 IfTrue, then this FSDP module casts\nits forward args and kwargs toparam_dtype. This is to ensure\nthat parameter and input dtypes match for forward computation, as\nrequired by many ops. This may need to be set toTruewhen only\napplying mixed precision to some but not all FSDP modules, in which\ncase a mixed-precision FSDP submodule needs to recast its inputs.\n(Default:False)\nTrue\nparam_dtype\nTrue\nFalse\ncast_root_forward_inputs(bool) \u2013 IfTrue, then the root FSDP module\ncasts its forward args and kwargs toparam_dtype, overriding\nthe value ofcast_forward_inputs. For non-root FSDP modules,\nthis does not do anything. (Default:True)\nTrue\nparam_dtype\ncast_forward_inputs\nTrue\n_module_classes_to_ignore(collections.abc.Sequence[type[torch.nn.modules.module.Module]]) \u2013 (Sequence[Type[nn.Module]]): This specifies\nmodule classes to ignore for mixed precision when using anauto_wrap_policy: Modules of these classes will have FSDP\napplied to them separately with mixed precision disabled (meaning\nthat the final FSDP construction would deviate from the specified\npolicy). Ifauto_wrap_policyis not specified, then this does\nnot do anything. This API is experimental and subject to change.\n(Default:(_BatchNorm,))\nauto_wrap_policy\nauto_wrap_policy\n(_BatchNorm,)\nNote\nThis API is experimental and subject to change.\nNote\nOnly floating point tensors are cast to their specified dtypes.\nNote\nInsummon_full_params, parameters are forced to full\nprecision, but buffers are not.\nsummon_full_params\nNote\nLayer norm and batch norm accumulate infloat32even when\ntheir inputs are in a low precision likefloat16orbfloat16.\nDisabling FSDP\u2019s mixed precision for those norm modules only means that\nthe affine parameters are kept infloat32. However, this incurs\nseparate all-gathers and reduce-scatters for those norm modules, which\nmay be inefficient, so if the workload permits, the user should prefer\nto still apply mixed precision to those modules.\nfloat32\nfloat16\nbfloat16\nfloat32\nNote\nBy default, if the user passes a model with any_BatchNormmodules and specifies anauto_wrap_policy, then the batch norm\nmodules will have FSDP applied to them separately with mixed precision\ndisabled. See the_module_classes_to_ignoreargument.\n_BatchNorm\nauto_wrap_policy\n_module_classes_to_ignore\nNote\nMixedPrecisionhascast_root_forward_inputs=Trueandcast_forward_inputs=Falseby default. For the root FSDP instance,\nitscast_root_forward_inputstakes precedence over itscast_forward_inputs. For non-root FSDP instances, theircast_root_forward_inputsvalues are ignored. The default setting is\nsufficient for the typical case where each FSDP instance has the sameMixedPrecisionconfiguration and only needs to cast inputs to theparam_dtypeat the beginning of the model\u2019s forward pass.\nMixedPrecision\ncast_root_forward_inputs=True\ncast_forward_inputs=False\ncast_root_forward_inputs\ncast_forward_inputs\ncast_root_forward_inputs\nMixedPrecision\nparam_dtype\nNote\nFor nested FSDP instances with differentMixedPrecisionconfigurations, we recommend setting individualcast_forward_inputsvalues to configure casting inputs or not before each instance\u2019s\nforward. In such a case, since the casts happen before each FSDP\ninstance\u2019s forward, a parent FSDP instance should have its non-FSDP\nsubmodules run before its FSDP submodules to avoid the activation dtype\nbeing changed due to a differentMixedPrecisionconfiguration.\nMixedPrecision\ncast_forward_inputs\nMixedPrecision\nExample:\n\n```python\n>>> model = nn.Sequential(nn.Linear(3, 3), nn.Linear(3, 3))\n>>> model[1] = FSDP(\n>>>     model[1],\n>>>     mixed_precision=MixedPrecision(param_dtype=torch.float16, cast_forward_inputs=True),\n>>> )\n>>> model = FSDP(\n>>>     model,\n>>>     mixed_precision=MixedPrecision(param_dtype=torch.bfloat16, cast_forward_inputs=True),\n>>> )\n\n```\n\nThe above shows a working example. On the other hand, ifmodel[1]were replaced withmodel[0], meaning that the submodule using\ndifferentMixedPrecisionran its forward first, thenmodel[1]would incorrectly seefloat16activations instead ofbfloat16ones.\nmodel[1]\nmodel[0]\nMixedPrecision\nmodel[1]\nfloat16\nbfloat16\nThis configures CPU offloading.\noffload_params(bool) \u2013 This specifies whether to offload parameters to\nCPU when not involved in computation. IfTrue, then this\noffloads gradients to CPU as well, meaning that the optimizer step\nruns on CPU.\nTrue\nStateDictConfigis the base class for allstate_dictconfiguration\nclasses. Users should instantiate a child class (e.g.FullStateDictConfig) in order to configure settings for the\ncorrespondingstate_dicttype supported by FSDP.\nStateDictConfig\nstate_dict\nFullStateDictConfig\nstate_dict\noffload_to_cpu(bool) \u2013 IfTrue, then FSDP offloads the state dict\nvalues to CPU, and ifFalse, then FSDP keeps them on GPU.\n(Default:False)\nTrue\nFalse\nFalse\nFullStateDictConfigis a config class meant to be used withStateDictType.FULL_STATE_DICT. We recommend enabling bothoffload_to_cpu=Trueandrank0_only=Truewhen saving full state\ndicts to save GPU memory and CPU memory, respectively. This config class\nis meant to be used via thestate_dict_type()context manager as\nfollows:\nFullStateDictConfig\nStateDictType.FULL_STATE_DICT\noffload_to_cpu=True\nrank0_only=True\nstate_dict_type()\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> fsdp = FSDP(model, auto_wrap_policy=...)\n>>> cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n>>> with FSDP.state_dict_type(fsdp, StateDictType.FULL_STATE_DICT, cfg):\n>>>     state = fsdp.state_dict()\n>>> # `state` will be empty on non rank 0 and contain CPU tensors on rank 0.\n>>> # To reload checkpoint for inference, finetuning, transfer learning, etc:\n>>> model = model_fn()  # Initialize model in preparation for wrapping with FSDP\n>>> if dist.get_rank() == 0:\n>>> # Load checkpoint only on rank 0 to avoid memory redundancy\n>>>     state_dict = torch.load(\"my_checkpoint.pt\")\n>>>     model.load_state_dict(state_dict)\n>>> # All ranks initialize FSDP module as usual. `sync_module_states` argument\n>>> # communicates loaded checkpoint states from rank 0 to rest of the world.\n>>> fsdp = FSDP(\n...     model,\n...     device_id=torch.cuda.current_device(),\n...     auto_wrap_policy=...,\n...     sync_module_states=True,\n... )\n>>> # After this point, all ranks have FSDP model with loaded checkpoint.\n\n```\n\nrank0_only(bool) \u2013 IfTrue, then only rank 0 saves the full state\ndict, and nonzero ranks save an empty dict. IfFalse, then all\nranks save the full state dict. (Default:False)\nTrue\nFalse\nFalse\nShardedStateDictConfigis a config class meant to be used withStateDictType.SHARDED_STATE_DICT.\nShardedStateDictConfig\nStateDictType.SHARDED_STATE_DICT\n_use_dtensor(bool) \u2013 IfTrue, then FSDP saves the state dict values\nasDTensor, and ifFalse, then FSDP saves them asShardedTensor. (Default:False)\nTrue\nDTensor\nFalse\nShardedTensor\nFalse\nWarning\n_use_dtensoris a private field ofShardedStateDictConfigand it is used by FSDP to determine the type of state dict values. Users should not\nmanually modify_use_dtensor.\n_use_dtensor\nShardedStateDictConfig\n_use_dtensor\nOptimStateDictConfigis the base class for alloptim_state_dictconfiguration classes.  Users should instantiate a child class (e.g.FullOptimStateDictConfig) in order to configure settings for the\ncorrespondingoptim_state_dicttype supported by FSDP.\nOptimStateDictConfig\noptim_state_dict\nFullOptimStateDictConfig\noptim_state_dict\noffload_to_cpu(bool) \u2013 IfTrue, then FSDP offloads the state dict\u2019s\ntensor values to CPU, and ifFalse, then FSDP keeps them on the\noriginal device (which is GPU unless parameter CPU offloading is\nenabled). (Default:True)\nTrue\nFalse\nTrue\nrank0_only(bool) \u2013 IfTrue, then only rank 0 saves the full state\ndict, and nonzero ranks save an empty dict. IfFalse, then all\nranks save the full state dict. (Default:False)\nTrue\nFalse\nFalse\nShardedOptimStateDictConfigis a config class meant to be used withStateDictType.SHARDED_STATE_DICT.\nShardedOptimStateDictConfig\nStateDictType.SHARDED_STATE_DICT\n_use_dtensor(bool) \u2013 IfTrue, then FSDP saves the state dict values\nasDTensor, and ifFalse, then FSDP saves them asShardedTensor. (Default:False)\nTrue\nDTensor\nFalse\nShardedTensor\nFalse\nWarning\n_use_dtensoris a private field ofShardedOptimStateDictConfigand it is used by FSDP to determine the type of state dict values. Users should not\nmanually modify_use_dtensor.\n_use_dtensor\nShardedOptimStateDictConfig\n_use_dtensor",
  "url": "https://pytorch.org/docs/stable/fsdp.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
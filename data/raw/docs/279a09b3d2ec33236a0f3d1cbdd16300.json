{
  "doc_id": "279a09b3d2ec33236a0f3d1cbdd16300",
  "source": "pytorch_docs",
  "title": "Getting Started \u2014 PyTorch 2.9 documentation",
  "text": "\n## Getting Started#\n\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\nBefore you read this section, make sure to read thetorch.compiler\nlet\u2019s start by looking at a simpletorch.compileexample that demonstrates\nhow to usetorch.compilefor inference. This example demonstrates thetorch.cos()andtorch.sin()features which are examples of pointwise\noperators as they operate element by element on a vector. This example might\nnot show significant performance gains but should help you form an intuitive\nunderstanding of how you can usetorch.compilein your own programs.\ntorch.compile\ntorch.compile\ntorch.cos()\ntorch.sin()\ntorch.compile\nNote\nTo run this script, you need to have at least one GPU on your machine.\nIf you do not have a GPU, you can remove the.to(device=\"cuda:0\")code\nin the snippet below and it will run on CPU. You can also set device toxpu:0to run on Intel\u00ae GPUs.\n.to(device=\"cuda:0\")\nxpu:0\n\n```python\nimport torch\ndef fn(x):\n   a = torch.cos(x)\n   b = torch.sin(a)\n   return b\nnew_fn = torch.compile(fn, backend=\"inductor\")\ninput_tensor = torch.randn(10000).to(device=\"cuda:0\")\na = new_fn(input_tensor)\n\n```\n\nA more famous pointwise operator you might want to use would\nbe something liketorch.relu(). Pointwise ops in eager mode are\nsuboptimal because each one would need to read a tensor from the\nmemory, make some changes, and then write back those changes. The single\nmost important optimization that inductor performs is fusion. In the\nexample above we can turn 2 reads (x,a) and\n2 writes (a,b) into 1 read (x) and 1 write (b), which\nis crucial especially for newer GPUs where the bottleneck is memory\nbandwidth (how quickly you can send data to a GPU) rather than compute\n(how quickly your GPU can crunch floating point operations).\ntorch.relu()\nx\na\na\nb\nx\nb\nAnother major optimization that inductor provides is automatic\nsupport for CUDA graphs.\nCUDA graphs help eliminate the overhead from launching individual\nkernels from a Python program which is especially relevant for newer GPUs.\nTorchDynamo supports many different backends, but TorchInductor specifically works\nby generatingTritonkernels. Let\u2019s save\nour example above into a file calledexample.py. We can inspect the code\ngenerated Triton kernels by runningTORCH_COMPILE_DEBUG=1pythonexample.py.\nAs the script executes, you should seeDEBUGmessages printed to the\nterminal. Closer to the end of the log, you should see a path to a folder\nthat containstorchinductor_<your_username>. In that folder, you can find\ntheoutput_code.pyfile that contains the generated kernel code similar to\nthe following:\nexample.py\nTORCH_COMPILE_DEBUG=1pythonexample.py\nDEBUG\ntorchinductor_<your_username>\noutput_code.py\n\n```python\n@pointwise(size_hints=[16384], filename=__file__, triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': 0, 'constants': {}, 'mutated_arg_names': [], 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n@triton.jit\ndef triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n   xnumel = 10000\n   xoffset = tl.program_id(0) * XBLOCK\n   xindex = xoffset + tl.arange(0, XBLOCK)[:]\n   xmask = xindex < xnumel\n   x0 = xindex\n   tmp0 = tl.load(in_ptr0 + (x0), xmask, other=0.0)\n   tmp1 = tl.cos(tmp0)\n   tmp2 = tl.sin(tmp1)\n   tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp2, xmask)\n\n```\n\nNote\nThe above code snippet is an example. Depending on your hardware,\nyou might see different code generated.\nAnd you can verify that fusing thecosandsindid actually occur\nbecause thecosandsinoperations occur within a single Triton kernel\nand the temporary variables are held in registers with very fast access.\ncos\nsin\ncos\nsin\nRead more on Triton\u2019s performancehere. Because the code is written\nin Python, it\u2019s fairly easy to understand even if you have not written all that\nmany CUDA kernels.\nNext, let\u2019s try a real model like resnet50 from the PyTorch\nhub.\n\n```python\nimport torch\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(1,3,64,64))\n\n```\n\nAnd that is not the only available backend, you can run in a REPLtorch.compiler.list_backends()to see all the available backends. Try out thecudagraphsnext as inspiration.\ntorch.compiler.list_backends()\ncudagraphs\n\n## Using a pretrained model#\n\nPyTorch users frequently leverage pretrained models fromtransformersorTIMMand one of\nthe design goals is TorchDynamo and TorchInductor is to work out of the box with\nany model that people would like to author.\nLet\u2019s download a pretrained model directly from the HuggingFace hub and optimize\nit:\n\n```python\nimport torch\nfrom transformers import BertTokenizer, BertModel\n# Copy pasted from here https://huggingface.co/bert-base-uncased\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\").to(device=\"cuda:0\")\nmodel = torch.compile(model, backend=\"inductor\") # This is the only line of code that we changed\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt').to(device=\"cuda:0\")\noutput = model(**encoded_input)\n\n```\n\nIf you remove theto(device=\"cuda:0\")from the model andencoded_input, then Triton will generate C++ kernels that will be\noptimized for running on your CPU. You can inspect both Triton or C++\nkernels for BERT. They are more complex than the trigonometry\nexample we tried above but you can similarly skim through it and see if you\nunderstand how PyTorch works.\nto(device=\"cuda:0\")\nencoded_input\nSimilarly, let\u2019s try out a TIMM example:\n\n```python\nimport timm\nimport torch\nmodel = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(64,3,7,7))\n\n```\n\n\n## Next Steps#\n\nIn this section, we have reviewed a few inference examples and developed a\nbasic understanding of how torch.compile works. Here is what you check out next:\ntorch.compile tutorial on training\ntorch.compiler API reference\nTorchDynamo APIs for fine-grained tracing",
  "url": "https://pytorch.org/docs/stable/torch.compiler_get_started.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
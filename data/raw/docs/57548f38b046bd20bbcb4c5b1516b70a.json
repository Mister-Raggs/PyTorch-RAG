{
  "doc_id": "57548f38b046bd20bbcb4c5b1516b70a",
  "source": "pytorch_docs",
  "title": "torch.cuda.make_graphed_callables \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.cuda.make_graphed_callables#\n\nAccept callables (functions ornn.Modules) and returns graphed versions.\nnn.Module\nEach graphed callable\u2019s forward pass runs its source callable\u2019s\nforward CUDA work as a CUDA graph inside a single autograd node.\nThe graphed callable\u2019s forward pass also appends\na backward node to the autograd graph. During backward, this node runs the\ncallable\u2019s backward work as a CUDA graph.\nTherefore, each graphed callable should be a drop-in replacement for its source callable\nin an autograd-enabled training loop.\nSeePartial-network capturefor detailed use and constraints.\nIf you pass a tuple of several callables, their captures will use the same memory pool.\nSeeGraph memory managementfor when this is appropriate.\ncallables(torch.nn.ModuleorPython function, ortupleofthese) \u2013 Callable or callables to graph.\nSeeGraph memory managementfor when passing a tuple of callables\nis appropriate.  If you pass a tuple of callables, their order in the tuple must be the same order\nthey\u2019ll run in the live workload.\nsample_args(tupleofTensors, ortupleoftuplesofTensors) \u2013 Samples args for each callable.\nIf a single callable was passed,sample_argsmust be a single tuple of argument Tensors.\nIf a tuple of callables was passed,sample_argsmust be tuple of tuples of argument Tensors.\nsample_args\nsample_args\nnum_warmup_iters(int) \u2013 The number of warmup iterations. Currently,DataDistributedParallelneeds\n11 iterations for warm up. Default:3.\nDataDistributedParallel\n3\nallow_unused_input(bool) \u2013 If False, specifying inputs that were not used when computing outputs\n(and therefore their grad is always zero) is an error. Defaults to False.\npool(optional) \u2013 Token (returned bygraph_pool_handle()orother_Graph_instance.pool()) that hints this graph may share memory\nwith the indicated pool.  SeeGraph memory management.\ngraph_pool_handle()\nother_Graph_instance.pool()\nNote\nTherequires_gradstate of each Tensor insample_argsmust match the state\nthat\u2019s expected for the corresponding real input in the training loop.\nrequires_grad\nsample_args\nWarning\nThis API is in beta and may change in future releases.\nWarning\nsample_argsfor each callable must contain only Tensors. Other types are not allowed.\nsample_args\nWarning\nReturned callables do not support higher order differentiation (e.g., double backward).\nWarning\nIn anyModulepassed tomake_graphed_callables(), only parameters\nmay be trainable. Buffers must haverequires_grad=False.\nModule\nmake_graphed_callables()\nrequires_grad=False\nWarning\nAfter you pass atorch.nn.Modulethroughmake_graphed_callables(),\nyou may not add or remove any of that Module\u2019s parameters or buffers.\ntorch.nn.Module\nmake_graphed_callables()\nWarning\ntorch.nn.Modules passed tomake_graphed_callables()must not have module hooks\nregistered on them at the time they are passed. However, registering hooks on modulesafterpassing them\nthroughmake_graphed_callables()is allowed.\ntorch.nn.Module\nmake_graphed_callables()\nmake_graphed_callables()\nWarning\nWhen running a graphed callable, you must pass its arguments in the same order and format\nthey appeared in that callable\u2019ssample_args.\nsample_args\nWarning\nThe automatic mixed precision is supported inmake_graphed_callables()only with disabled\ncaching. The context managertorch.cuda.amp.autocast()must havecache_enabled=False.\nmake_graphed_callables()",
  "url": "https://pytorch.org/docs/stable/generated/torch.cuda.make_graphed_callables.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "9239fc26eaeec45fba009134f8515e0d",
  "source": "pytorch_docs",
  "title": "Control Flow - Cond \u2014 PyTorch 2.9 documentation",
  "text": "\n## Control Flow - Cond#\n\nCreated On: Oct 03, 2023 | Last Updated On: Jun 13, 2025\ntorch.condis a structured control flow operator. It can be used to specify if-else like control flow\nand can logically be seen as implemented as follows.\ntorch.cond\n\n```python\ndef cond(\n    pred: Union[bool, torch.Tensor],\n    true_fn: Callable,\n    false_fn: Callable,\n    operands: Tuple[torch.Tensor]\n):\n    if pred:\n        return true_fn(*operands)\n    else:\n        return false_fn(*operands)\n\n```\n\nIts unique power lies in its ability of expressingdata-dependent control flow: it lowers to a conditional\noperator (torch.ops.higher_order.cond), which preserves predicate, true function and false functions.\nThis unlocks great flexibility in writing and deploying models that change model architecture based on\nthevalueorshapeof inputs or intermediate outputs of tensor operations.\ntorch.ops.higher_order.cond\nWarning\ntorch.condis a prototype feature in PyTorch. It has limited support for input and output types and\ndoesn\u2019t support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\nRead more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\ntorch.cond\n\n## Examples#\n\nBelow is an example that uses cond to branch based on input shape:\n\n```python\n    import torch\n\n    def true_fn(x: torch.Tensor):\n        return x.cos() + x.sin()\n\n    def false_fn(x: torch.Tensor):\n        return x.sin()\n\n    class DynamicShapeCondPredicate(torch.nn.Module):\n        \"\"\"\n        A basic usage of cond based on dynamic shape predicate.\n        \"\"\"\n\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, x: torch.Tensor) -> torch.Tensor:\n            def true_fn(x: torch.Tensor):\n                return x.cos()\n\n            def false_fn(x: torch.Tensor):\n                return x.sin()\n\n            return torch.cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\n    dyn_shape_mod = DynamicShapeCondPredicate()\n\n```\n\nWe can eagerly run the model and expect the results vary based on input shape:\n\n```python\n    inp = torch.randn(3)\n    inp2 = torch.randn(5)\n    assert torch.equal(dyn_shape_mod(inp), false_fn(inp))\n    assert torch.equal(dyn_shape_mod(inp2), true_fn(inp2))\n\n```\n\nWe can export the model for further transformations and deployment:\n\n```python\n    inp = torch.randn(4, 3)\n    dim_batch = torch.export.Dim(\"batch\", min=2)\n    ep = torch.export.export(DynamicShapeCondPredicate(), (inp,), {}, dynamic_shapes={\"x\": {0: dim_batch}})\n    print(ep)\n\n```\n\nThis gives us an exported program as shown below:\n\n```python\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[s0, 3]):\n            sym_size: Sym(s0) = torch.ops.aten.sym_size.int(arg0_1, 0)\n            gt: Sym(s0 > 4) = sym_size > 4;  sym_size = None\n            true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            conditional: f32[s0, 3] = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n            return (conditional,)\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                cos: f32[s0, 3] = torch.ops.aten.cos.default(arg0_1)\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                add: f32[s0, 3] = torch.ops.aten.add.Tensor(cos, sin);  cos = sin = None\n                return add\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                return sin\n\n```\n\nNotice thattorch.condis lowered totorch.ops.higher_order.cond, its predicate becomes a Symbolic expression over the shape of input,\nand branch functions becomes two sub-graph attributes of the top level graph module.\ntorch.cond\ntorch.ops.higher_order.cond\nHere is another example that showcases how to express a data-dependent control flow:\n\n```python\n    class DataDependentCondPredicate(torch.nn.Module):\n        \"\"\"\n        A basic usage of cond based on data dependent predicate.\n        \"\"\"\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, x: torch.Tensor) -> torch.Tensor:\n            return torch.cond(x.sum() > 4.0, true_fn, false_fn, (x,))\n\n```\n\nThe exported program we get after export:\n\n```python\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[s0, 3]):\n            sum_1: f32[] = torch.ops.aten.sum.default(arg0_1)\n            gt: b8[] = torch.ops.aten.gt.Scalar(sum_1, 4.0);  sum_1 = None\n\n            true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            conditional: f32[s0, 3] = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n            return (conditional,)\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                cos: f32[s0, 3] = torch.ops.aten.cos.default(arg0_1)\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                add: f32[s0, 3] = torch.ops.aten.add.Tensor(cos, sin);  cos = sin = None\n                return add\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                return sin\n\n```\n\n\n## Invariants of torch.ops.higher_order.cond#\n\nThere are several useful invariants fortorch.ops.higher_order.cond:\ntorch.ops.higher_order.cond\nFor predicate:\nDynamicness of predicate is preserved (e.g.gtshown in the above example)\ngt\nIf the predicate in user-program is constant (e.g. a python bool constant), thepredof the operator will be a constant.\npred\nFor branches:\nThe input and output signature will be a flattened tuple.\nThey aretorch.fx.GraphModule.\ntorch.fx.GraphModule\nClosures in original function becomes explicit inputs. No closures.\nNo mutations on inputs or globals are allowed.\nFor operands:\nIt will also be a flat tuple.\nNesting oftorch.condin user program becomes nested graph modules.\ntorch.cond\n\n## API Reference#\n\nConditionally appliestrue_fnorfalse_fn.\nWarning\ntorch.condis a prototype feature in PyTorch. It has limited support for input and output types and\ndoesn\u2019t support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\nRead more about feature classification at:https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\ncondis structured control flow operator. That is, it is like a Python if-statement,\nbut has restrictions ontrue_fn,false_fn, andoperandsthat enable it to be\ncapturable using torch.compile and torch.export.\nAssuming the constraints oncond\u2019s arguments are met,condis equivalent to the following:\n\n```python\ndef cond(pred, true_branch, false_branch, operands):\n    if pred:\n        return true_branch(*operands)\n    else:\n        return false_branch(*operands)\n\n```\n\npred(Union[bool,torch.Tensor]) \u2013 A boolean expression or a tensor with one element,\nindicating which branch function to apply.\ntrue_fn(Callable) \u2013 A callable function (a -> b) that is within the\nscope that is being traced.\nfalse_fn(Callable) \u2013 A callable function (a -> b) that is within the\nscope that is being traced. The true branch and false branch must\nhave consistent input and outputs, meaning the inputs have to be\nthe same, and the outputs have to be the same type and shape. Int\noutput is also allowed. We\u2019ll make the output dynamic by turning it\ninto a symint.\noperands(Tupleofpossibly nested dict/list/tupleoftorch.Tensor) \u2013 A tuple of inputs to the\ntrue/false functions. It can be empty if true_fn/false_fn doesn\u2019t require input. Defaults to ().\nAny\nExample:\n\n```python\ndef true_fn(x: torch.Tensor):\n    return x.cos()\n\n\ndef false_fn(x: torch.Tensor):\n    return x.sin()\n\n\nreturn cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\n```\n\nThe conditional statement (akapred) must meet one of the following constraints:\nIt\u2019s atorch.Tensorwith only one element, and torch.bool dtype\nIt\u2019s a boolean expression, e.g.x.shape[0] > 10orx.dim() > 1 and x.shape[1] > 10\nThe branch function (akatrue_fn/false_fn) must meet all of the following constraints:\nThe function signature must match with operands.\nThe function must return a tensor with the same metadata, e.g. shape,\ndtype, etc.\nThe function cannot have in-place mutations on inputs or global variables.\n(Note: in-place tensor operations such asadd_for intermediate results\nare allowed in a branch)",
  "url": "https://pytorch.org/docs/stable/cond.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
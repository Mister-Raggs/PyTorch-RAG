{
  "doc_id": "c3b96c3b6b4e80c80af767705a3a04f8",
  "source": "pytorch_docs",
  "title": "PyTorch 2.0 Performance Dashboard \u2014 PyTorch 2.9 documentation",
  "text": "\n## PyTorch 2.0 Performance Dashboard#\n\nCreated On: May 04, 2023 | Last Updated On: Jun 10, 2025\nAuthor:Bin BaoandHuy Do\nPyTorch 2.0\u2019s performance is tracked nightly on thisdashboard.\nThe performance collection runs on 12 GCP A100 nodes every night. Each node contains a 40GB A100 Nvidia GPU and\na 6-core 2.2GHz Intel Xeon CPU. The corresponding CI workflow file can be foundhere.\n\n## How to read the dashboard?#\n\nThe landing page shows tables for all three benchmark suites we measure,TorchBench,Huggingface, andTIMM,\nand graphs for one benchmark suite with the default setting. For example, the default graphs currently show the AMP\ntraining performance trend in the past 7 days forTorchBench. Droplists on the top of that page can be\nselected to view tables and graphs with different options. In addition to the pass rate, there are 3 key\nperformance metrics reported there:Geometricmeanspeedup,Meancompilationtime, andPeakmemoryfootprintcompressionratio.\nBothGeometricmeanspeedupandPeakmemoryfootprintcompressionratioare compared against\nthe PyTorch eager performance, and the larger the better. Each individual performance number on those tables can be clicked,\nwhich will bring you to a view with detailed numbers for all the tests in that specific benchmark suite.\nTorchBench\nHuggingface\nTIMM\nTorchBench\nGeometricmeanspeedup\nMeancompilationtime\nPeakmemoryfootprintcompressionratio\nGeometricmeanspeedup\nPeakmemoryfootprintcompressionratio\n\n## What is measured on the dashboard?#\n\nAll the dashboard tests are defined in thisfunction.\nThe exact test configurations are subject to change, but at the moment, we measure both inference and training\nperformance with AMP precision on the three benchmark suites. We also measure different settings of TorchInductor,\nincludingdefault,with_cudagraphs(default+cudagraphs), anddynamic(default+dynamic_shapes).\ndefault\nwith_cudagraphs(default+cudagraphs)\ndynamic(default+dynamic_shapes)\n\n## Can I check if my PR affects TorchInductor\u2019s performance on the dashboard before merging?#\n\nIndividual dashboard runs can be triggered manually by clicking theRunworkflowbuttonhereand submitting with your PR\u2019s branch selected. This will kick off a whole dashboard run with your PR\u2019s changes.\nOnce it is done, you can check the results by selecting the corresponding branch name and commit ID\non the performance dashboard UI. Be aware that this is an expensive CI run. With the limited\nresources, please use this functionality wisely.\nRunworkflow\n\n## How can I run any performance test locally?#\n\nThe exact command lines used during a complete dashboard run can be found in any recent CI run logs.\nTheworkflow pageis a good place to look for logs from some of the recent runs.\nIn those logs, you can search for lines likepythonbenchmarks/dynamo/huggingface.py--performance--cold-start-latency--inference--amp--backendinductor--disable-cudagraphs--devicecudaand run them locally if you have a GPU working with PyTorch 2.0.pythonbenchmarks/dynamo/huggingface.py-hwill give you a detailed explanation on options of the benchmarking script.\npythonbenchmarks/dynamo/huggingface.py--performance--cold-start-latency--inference--amp--backendinductor--disable-cudagraphs--devicecuda\npythonbenchmarks/dynamo/huggingface.py-h",
  "url": "https://pytorch.org/docs/stable/torch.compiler_performance_dashboard.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
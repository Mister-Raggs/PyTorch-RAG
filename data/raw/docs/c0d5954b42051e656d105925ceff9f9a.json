{
  "doc_id": "c0d5954b42051e656d105925ceff9f9a",
  "source": "pytorch_docs",
  "title": "torch.fx \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.fx#\n\nCreated On: Dec 15, 2020 | Last Updated On: Jul 15, 2025\n\n## Overview#\n\nFX is a toolkit for developers to use to transformnn.Moduleinstances. FX consists of three main components: asymbolic tracer,anintermediate representation, andPython code generation. A\ndemonstration of these components in action:\nnn.Module\n\n```python\nimport torch\n\n\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced: torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph():\n    %x : [num_users=1] = placeholder[target=x]\n    %param : [num_users=1] = get_attr[target=param]\n    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n    %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n    %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n    return clamp\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n    param = self.param\n    add = x + param;  x = param = None\n    linear = self.linear(add);  add = None\n    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n    return clamp\n\"\"\"\n\n```\n\nThesymbolic tracerperforms \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non these Proxies are recorded. More information about symbolic tracing\ncan be found in thesymbolic_trace()andTracerdocumentation.\nsymbolic_trace()\nTracer\nTheintermediate representationis the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nortorch.nn.Moduleinstances), and return values. More information\nabout the IR can be found in the documentation forGraph. The\nIR is the format on which transformations are applied.\ntorch.nn.Module\nGraph\nPython code generationis what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up inGraphModule, which is atorch.nn.Moduleinstance that holds aGraphas well as aforwardmethod generated from the Graph.\nGraphModule\ntorch.nn.Module\nGraph\nforward\nTaken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX!\nSeveral example transformations can be found at theexamplesrepository.\n\n## Writing Transformations#\n\nWhat is an FX transform? Essentially, it\u2019s a function that looks like this.\n\n```python\n\nimport torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n                tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)\n\n```\n\nYour transform will take in atorch.nn.Module, acquire aGraphfrom it, do some modifications, and return a newtorch.nn.Module. You should think of thetorch.nn.Modulethat your FX\ntransform returns as identical to a regulartorch.nn.Module\u2013 you can pass it to another\nFX transform, or you can run it. Ensuring that the inputs and outputs of your FX transform are atorch.nn.Modulewill allow for composability.\ntorch.nn.Module\nGraph\ntorch.nn.Module\ntorch.nn.Module\ntorch.nn.Module\ntorch.nn.Module\nNote\nIt is also possible to modify an existingGraphModuleinstead of\ncreating a new one, like so:\nGraphModule\n\n```python\nimport torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm\n\n```\n\nNote that you MUST callGraphModule.recompile()to bring the generatedforward()method on theGraphModulein sync with the modifiedGraph.\nGraphModule.recompile()\nforward()\nGraphModule\nGraph\nGiven that you\u2019ve passed in atorch.nn.Modulethat has been traced into aGraph, there are now two primary approaches you can take to building a newGraph.\ntorch.nn.Module\nGraph\nGraph\n\n## A Quick Primer on Graphs#\n\nFull treatment of the semantics of graphs can be found in theGraphdocumentation, but we are going to cover the basics here. AGraphis\na data structure that represents a method on aGraphModule. The\ninformation that this requires is:\nGraph\nGraph\nGraphModule\nWhat are the inputs to the method?\nWhat are the operations that run inside the method?\nWhat is the output (i.e. return) value from the method?\nAll three of these concepts are represented withNodeinstances.\nLet\u2019s see what we mean by that with a short example:\nNode\n\n```python\n\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()\n\n```\n\nHere we define a moduleMyModulefor demonstration purposes, instantiate it,\nsymbolically trace it, then call theGraph.print_tabular()method to print\nout a table showing the nodes of thisGraph:\nMyModule\nGraph.print_tabular()\nGraph\nopcode\nname\ntarget\nargs\nkwargs\nplaceholder\nx\nx\n()\n{}\nget_attr\nlinear_weight\nlinear.weight\n()\n{}\ncall_function\nadd_1\n\n(x, linear_weight)\n{}\ncall_module\nlinear_1\nlinear\n(add_1,)\n{}\ncall_method\nrelu_1\nrelu\n(linear_1,)\n{}\ncall_function\nsum_1\n<built-in method sum \u2026>\n(relu_1,)\n{\u2018dim\u2019: -1}\ncall_function\ntopk_1\n<built-in method topk \u2026>\n(sum_1, 3)\n{}\noutput\noutput\noutput\n(topk_1,)\n{}\nWe can use this information to answer the questions we posed above.\nWhat are the inputs to the method? In FX, method inputs are specified\nvia specialplaceholdernodes. In this case, we have a singleplaceholdernode with atargetofx, meaning we have\na single (non-self) argument named x.\nplaceholder\nplaceholder\ntarget\nx\nWhat are the operations within the method? Theget_attr,call_function,call_module, andcall_methodnodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in theNodedocumentation.\nget_attr\ncall_function\ncall_module\ncall_method\nNode\nWhat is the return value of the method? The return value in aGraphis specified by a specialoutputnode.\nGraph\noutput\nGiven that we now know the basics of how code is represented in\nFX, we can now explore how we would edit aGraph.\nGraph\n\n## Graph Manipulation#\n\nOne approach to building this newGraphis to directly manipulate your old\none. To aid in this, we can simply take theGraphwe obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replacetorch.add()calls withtorch.mul()calls.\nGraph\nGraph\ntorch.add()\ntorch.mul()\n\n```python\n\nimport torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n                tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                    # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)\n\n```\n\nWe can also do more involvedGraphrewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in theGraphdocumentation. An\nexample of using these APIs to append atorch.relu()call\ncan be found below.\nGraph\nGraph\ntorch.relu()\n\n```python\n\n# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)\n\n```\n\nFor simple transformations that only consist of substitutions, you can also\nmake use of thesubgraph rewriter.\nFX also provides another level of automation on top of direct graph manipulation.\nThereplace_pattern()API is essentially a \u201cfind/replace\u201d tool for editingGraph\\s. It allows you to specify apatternandreplacementfunction\nand it will trace through those functions, find instances of the group of operations\nin thepatterngraph, and replace those instances with copies of thereplacementgraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex.\nreplace_pattern()\nGraph\npattern\nreplacement\npattern\nreplacement\nReplace one\nop\nConv/Batch Norm\nfusion\nreplace_pattern: Basic usage\nQuantization\nInvert Transformation\n\n## Proxy/Retracing#\n\nAnother way of manipulatingGraph\\s is by reusing theProxymachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform everyF.relu(x)call into(x>0)*x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after theF.relu, and then clean up the originalF.relu. However, we can automate this process by usingProxyobjects to automatically record operations into theGraph.\nGraph\nProxy\nF.relu(x)\n(x>0)*x\nF.relu\nF.relu\nProxy\nGraph\nTo use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code withProxyobjects as arguments.\nTheseProxyobjects will capture the operations that are performed\non them and append them to theGraph.\nProxy\nProxy\nGraph\n\n```python\n\n# Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n                tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    tracer = torch.fx.proxy.GraphAppendingTracer(new_graph)\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)\n\n```\n\nIn addition to avoiding explicit graph manipulation, usingProxy\\s\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. Note that while callingProxywe also\npassed a tracer pointing to the underlying variablegraph. This is done so\nif in case the operations in graph are n-ary (e.g. add is a binary operator)\nthe call toProxydoes not create multiple instances of a graph\ntracer which can lead to unexpected runtime errors. We recommend this method\nof usingProxyespecially when the underlying operators can not be\nsafely assumed to be unary.\nProxy\nProxy\ngraph\nProxy\nProxy\nA worked example of usingProxy\\s forGraphmanipulation\ncan be foundhere.\nProxy\nGraph\n\n## The Interpreter Pattern#\n\nA useful code organizational pattern in FX is to loop over all theNode\\s\nin aGraphand execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing withProxy\\s. For example, suppose we want to run aGraphModuleand record thetorch.Tensorshape and dtype\nproperties on the nodes as we see them at runtime. That might look like:\nNode\nGraph\nProxy\nGraphModule\ntorch.Tensor\n\n```python\n\nimport torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistent target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)\n\n```\n\nAs you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\ntheInterpreterclass, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides.\nInterpreter\nIn addition to executing operations, we can also generate a newGraphby feedingProxyvalues through an interpreter.\nSimilarly, we provide theTransformerclass to encompass\nthis pattern.Transformerbehaves similarly toInterpreter, but instead of calling therunmethod to\nget a concrete output value from the Module, you would call theTransformer.transform()method to return a newGraphModulewhich was subject to any transformation rules\nyou installed as overridden methods.\nGraph\nProxy\nTransformer\nTransformer\nInterpreter\nrun\nTransformer.transform()\nGraphModule\nShapePropagation\nPerformance Profiler\n\n## Debugging#\n\n\n## Introduction#\n\nOften in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code.\nIf you\u2019re not familiar with debuggers, please see the auxiliary sectionAvailable Debuggers.\n\n## Common Pitfalls in Transform Authoring#\n\nNondeterministicsetiteration order. In Python, thesetdatatype is\nunordered. Usingsetto contain collections of objects likeNode\\ s,\nfor example, can cause unexpected nondeterminism. An example is iterating\nover a set ofNodes to insert them into aGraph. Because thesetdata type is unordered, the ordering of the operations in the output\nprogram will be nondeterministic and can change across program invocations.\nThe recommended alternative is to use adictdata type, which isinsertion orderedas of Python 3.7 (and as of cPython 3.6). Adictcan be used equivalently\nto a set by storing values to be deduplicated in the keys of thedict.\nset\nset\nset\nNode\nNode\nGraph\nset\ndict\ndict\ndict\n\n## Checking Correctness of Modules#\n\nBecause the output of most deep learning modules consists of floating\npointtorch.Tensorinstances, checking for equivalence between\nthe results of twotorch.nn.Moduleis not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample:\ntorch.Tensor\ntorch.nn.Module\n\n```python\n\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"\n\n```\n\nHere, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the==equality operator. However, this is not well-defined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (seeherefor more\ndetails). We can usetorch.allclose()instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold:\n==\ntorch.allclose()\n\n```python\nassert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n\n```\n\nThis is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.\n\n## Debugging the Generated Code#\n\nBecause FX generates theforward()function onGraphModule\\s, using\ntraditional debugging techniques likeprintstatements orpdbis\nnot as straightforward. Luckily, we have several techniques we can use\nfor debugging the generated code.\nforward()\nGraphModule\nprint\npdb\npdb\nInvokepdbto step into the running program. Although the code that\nrepresents theGraphis not in any source file, we can still step\ninto it manually usingpdbwhen the forward pass is invoked.\npdb\nGraph\npdb\n\n```python\n\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)\n\n```\n\nIf you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code withpdb. In that case, one\napproach is to simply copy-paste the generatedforwardpass into\nyour code and examine it from there.\npdb\nforward\n\n```python\n\n# Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()\n\n```\n\nto_folder\nGraphModule\nGraphModule.to_folder()is a method inGraphModulethat allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as inPrint the Generated Code,\nit may be easier to examine modules and parameters usingto_folder.\nGraphModule.to_folder()\nGraphModule\nto_folder\n\n```python\n\nm = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()\n\n```\n\nAfter running the above example, we can then look at the code withinfoo/module.pyand modify it as desired (e.g. addingprintstatements or usingpdb) to debug the generated code.\nfoo/module.py\nprint\npdb\n\n## Debugging the Transformation#\n\nNow that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\ntheLimitations of Symbolic Tracingsection in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during ourGraphModuletransformation. There may be a quick answer inWriting Transformations, but, if not, there are several ways to\nexamine our traced module:\nGraphModule\n\n```python\n\n# Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add = x + y;  x = y = None\n    return add\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph():\n    %x : [num_users=1] = placeholder[target=x]\n    %y : [num_users=1] = placeholder[target=y]\n    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})\n    return add\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args    kwargs\n-------------  ------  -----------------------  ------  --------\nplaceholder    x       x                        ()      {}\nplaceholder    y       y                        ()      {}\ncall_function  add     <built-in function add>  (x, y)  {}\noutput         output  output                   (add,)  {}\n\"\"\"\n\n```\n\nUsing the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger likepdbcan be a good\nnext step.\npdb\nGoing off of the example above, consider the following code:\n\n```python\n\n# Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)\n\n```\n\nUsing the above example, let\u2019s say that the call toprint(traced)showed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start apdbsession. We can see\nwhat\u2019s happening during the transform by breaking ontransform_graph(traced), then pressingsto \u201cstep into\u201d the call\ntotransform_graph(traced).\nprint(traced)\npdb\ntransform_graph(traced)\ns\ntransform_graph(traced)\nWe may also have good luck by editing theprint_tabularmethod to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019sinput_nodesandusers.)\nprint_tabular\ninput_nodes\nusers\n\n## Available Debuggers#\n\nThe most common Python debugger ispdb. You can start\nyour program in \u201cdebug mode\u201d withpdbby typingpython-mpdbFILENAME.pyinto the command line, whereFILENAMEis the name of the file you want to debug. After that, you can use thepdbdebugger commandsto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (bLINE-NUMBER) when you startpdb, then callcto\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (usingsorn) to get to the part\nof the code you want to examine. Alternatively, you can writeimportpdb;pdb.set_trace()before the line you want to break at.\nIf you addpdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just typepythonFILENAME.pyinto the command line instead ofpython-mpdbFILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials onpdbonline, including RealPython\u2019s\u201cPython Debugging With Pdb\u201d.\npdb\npython-mpdbFILENAME.py\nFILENAME\npdb\nbLINE-NUMBER\npdb\nc\ns\nn\nimportpdb;pdb.set_trace()\npdb.set_trace()\npythonFILENAME.py\npython-mpdbFILENAME.py\npdb\nIDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) usepdbby pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper aroundpdb).\npdb\npdb\n\n## Limitations of Symbolic Tracing#\n\nFX uses a system ofsymbolic tracing(a.k.asymbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system istracingin that it executes the program (really atorch.nn.Moduleor function) to record operations. It issymbolicin that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxyin FX parlance).\ntorch.nn.Module\nProxy\nAlthough symbolic tracing works for most neural net code, it has some\nlimitations.\n\n## Dynamic Control Flow#\n\nThe main limitation of symbolic tracing is it does not currently supportdynamic control flow. That is, loops orifstatements where the\ncondition may depend on the input values of the program.\nif\nFor example, let\u2019s examine the following program:\n\n```python\n\ndef func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n    <...>\n    File \"dyn.py\", line 6, in func_to_trace\n    if x.sum() > 0:\n    File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n    File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"\n\n```\n\nThe condition to theifstatement relies on the value ofx.sum(),\nwhich relies on the value ofx, a function input. Sincexcan change (i.e. if you pass a new input tensor to the traced\nfunction), this isdynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens.\nif\nx.sum()\nx\nx\n\n## Static Control Flow#\n\nOn the other hand, so-calledstatic control flowis supported. Static\ncontrol flow is loops orifstatements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:\nif\n\n```python\n\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"\n\n```\n\nThe if-statementifself.do_activationdoes not depend on any\nfunction inputs, thus it is static.do_activationcan be considered\nto be a hyper-parameter, and the traces of different instances ofMyModulewith different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.\nifself.do_activation\ndo_activation\nMyModule\nMany instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues toModuleattributes or by binding concrete values to arguments\nduring symbolic tracing:\nModule\n\n```python\n\ndef f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\nfx.symbolic_trace(f, concrete_args={'flag': True})\n\n```\n\nIn the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (seeCustomizing Tracing with the Tracer class) or function (seewrap()) rather than tracing through them.\nwrap()\n\n## Non-torchFunctions#\n\ntorch\nFX uses__torch_function__as the mechanism by which it intercepts\ncalls (see thetechnical\noverviewfor more information about this). Some functions, such as builtin Python\nfunctions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example:\n__torch_function__\nmath\n__torch_function__\n\n```python\n\nimport torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n    <...>\n    File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n    File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"\n\n```\n\nThe error tells us that the built-in functionlenis not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using thewrap()API:\nlen\nwrap()\n\n```python\n\ntorch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"\n\n```\n\n\n## Customizing Tracing with theTracerclass#\n\nTracer\nTheTracerclass is the class that underlies the\nimplementation ofsymbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:\nTracer\nsymbolic_trace\n\n```python\n\nclass MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)\n\n```\n\n\n## Leaf Modules#\n\nLeaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standardtorch.nnmodule instances. For example:\ntorch.nn\n\n```python\n\nclass MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"\n\n```\n\nThe set of leaf modules can be customized by overridingTracer.is_leaf_module().\nTracer.is_leaf_module()\n\n## Miscellanea#\n\nTensor constructors (e.g.torch.zeros,torch.ones,torch.rand,torch.randn,torch.sparse_coo_tensor)\nare currently not traceable.\ntorch.zeros\ntorch.ones\ntorch.rand\ntorch.randn\ntorch.sparse_coo_tensor\nThe deterministic constructors (zeros,ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,ones_likeorzeros_likemay be a viable substitute.\nzeros\nones\nones_like\nzeros_like\nNondeterministic constructors (rand,randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wraptorch.randnin atorch.fx.wrapfunction and call that instead.\nrand\nrandn\ntorch.randn\ntorch.fx.wrap\n\n```python\n\n@torch.fx.wrap\ndef torch_randn(x, shape):\n    return torch.randn(shape)\n\ndef f(x):\n    return x + torch_randn(x, 5)\nfx.symbolic_trace(f)\n\n```\n\nThis behavior may be fixed in a future release.\nType annotations\nPython 3-style type annotations (e.g.func(x:torch.Tensor,y:int)->torch.Tensor) are supported\nand will be preserved by symbolic tracing.\nfunc(x:torch.Tensor,y:int)->torch.Tensor\nPython 2-style comment type annotations#type:(torch.Tensor,int)->torch.Tensorare not currently\nsupported.\n#type:(torch.Tensor,int)->torch.Tensor\nAnnotations on local names within a function are not currently\nsupported.\nGotcha aroundtrainingflag and submodules\ntraining\nWhen using functionals liketorch.nn.functional.dropout, it will be common for the training argument to be passed in asself.training. During FX tracing, this will likely be baked in as a constant value.\ntorch.nn.functional.dropout\nself.training\n\n```python\n\nimport torch\nimport torch.fx\n\nclass DropoutRepro(torch.nn.Module):\n    def forward(self, x):\n    return torch.nn.functional.dropout(x, training=self.training)\n\n\ntraced = torch.fx.symbolic_trace(DropoutRepro())\nprint(traced.code)\n\"\"\"\ndef forward(self, x):\n    dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False);  x = None\n    return dropout\n\"\"\"\n\ntraced.eval()\n\nx = torch.randn(5, 3)\ntorch.testing.assert_close(traced(x), x)\n\"\"\"\nAssertionError: Tensor-likes are not close!\n\nMismatched elements: 15 / 15 (100.0%)\nGreatest absolute difference: 1.6207983493804932 at index (0, 2) (up to 1e-05 allowed)\nGreatest relative difference: 1.0 at index (0, 0) (up to 0.0001 allowed)\n\"\"\"\n\n```\n\nHowever, when the standardnn.Dropout()submodule is used, the training flag is encapsulated and\u2013because of the preservation of thenn.Moduleobject model\u2013can be changed.\nnn.Dropout()\nnn.Module\n\n```python\n\nclass DropoutRepro2(torch.nn.Module):\n    def __init__(self):\n    super().__init__()\n    self.drop = torch.nn.Dropout()\n\n    def forward(self, x):\n    return self.drop(x)\n\ntraced = torch.fx.symbolic_trace(DropoutRepro2())\nprint(traced.code)\n\"\"\"\ndef forward(self, x):\n    drop = self.drop(x);  x = None\n    return drop\n\"\"\"\n\ntraced.eval()\n\nx = torch.randn(5, 3)\ntorch.testing.assert_close(traced(x), x)\n\n```\n\nBecause of this difference, consider marking modules that interact with thetrainingflag dynamically as leaf modules.\ntraining\n\n## API Reference#\n\nSymbolic tracing API\nGiven annn.Moduleor function instanceroot, this function will return aGraphModuleconstructed by recording operations seen while tracing throughroot.\nnn.Module\nroot\nGraphModule\nroot\nconcrete_argsallows you to partially specialize your function, whether it\u2019s to remove control flow or data structures.\nconcrete_args\nFor example:\n\n```python\ndef f(a, b):\n    if b == True:\n        return a\n    else:\n        return a * 2\n\n```\n\nFX can typically not trace through this due to the presence of control\nflow. However, we can useconcrete_argsto specialize on the value ofbto trace through this:\n\n```python\nf = fx.symbolic_trace(f, concrete_args={\"b\": False})\nassert f(3, False) == 6\n\n```\n\nNote that although you can still pass in different values ofb, they will be ignored.\nWe can also useconcrete_argsto eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass infx.PHfor values that shouldn\u2019t be\nspecialized. For example:\n\n```python\ndef f(x):\n    out = 0\n    for v in x.values():\n        out += v\n    return out\n\n\nf = fx.symbolic_trace(\n    f, concrete_args={\"x\": {\"a\": fx.PH, \"b\": fx.PH, \"c\": fx.PH}}\n)\nassert f({\"a\": 1, \"b\": 2, \"c\": 4}) == 7\n\n```\n\nroot(Union[torch.nn.Module,Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation.\nconcrete_args(Optional[Dict[str,any]]) \u2013 Inputs to be partially specialized\na Module created from the recorded operations fromroot.\nroot\nGraphModule\nNote\nBackwards-compatibility for this API is guaranteed.\nThis function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through:\n\n```python\n# foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\n\ntorch.fx.wrap(\"my_custom_function\")\n\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)\n\n```\n\nThis function can also equivalently be used as a decorator:\n\n```python\n# foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y\n\n```\n\nA wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through.\nfn_or_name(Union[str,Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called\nNote\nBackwards-compatibility for this API is guaranteed.\nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has agraphattribute, as well ascodeandforwardattributes generated\nfrom thatgraph.\ngraph\ncode\nforward\ngraph\nWarning\nWhengraphis reassigned,codeandforwardwill be automatically\nregenerated. However, if you edit the contents of thegraphwithout reassigning\nthegraphattribute itself, you must callrecompile()to update the generated\ncode.\ngraph\ncode\nforward\ngraph\ngraph\nrecompile()\nNote\nBackwards-compatibility for this API is guaranteed.\nConstruct a GraphModule.\nroot(Union[torch.nn.Module,Dict[str,Any]) \u2013rootcan either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case thatrootis a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019targetfield will be copied over from the respective place\nwithinroot\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case thatrootis a dict, the qualified name found in a Node\u2019stargetwill be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy.\nroot\nroot\ntarget\nroot\nroot\ntarget\ngraph(Graph) \u2013graphcontains the nodes this GraphModule should use for code generation\ngraph\nclass_name(str) \u2013namedenotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating fromGraphModule. It may be helpful to set this\ntoroot\u2019s original name or a name that makes sense within the context of your transform.\nname\nGraphModule\nroot\nNote\nBackwards-compatibility for this API is guaranteed.\nAdds the given submodule toself.\nself\nThis installs empty Modules where none exist yet if they are\nsubpaths oftarget.\ntarget\ntarget(str) \u2013 The fully-qualified string name of the new submodule\n(See example innn.Module.get_submodulefor how to\nspecify a fully-qualified string.)\nnn.Module.get_submodule\nm(Module) \u2013 The submodule itself; the actual object we want to\ninstall in the current Module\nWhether or not the submodule could be inserted. Forthis method to return True, each object in the chain\ndenoted bytargetmust either a) not exist yet,\nor b) reference annn.Module(not a parameter or\nother attribute)\nthis method to return True, each object in the chain\ndenoted bytargetmust either a) not exist yet,\nor b) reference annn.Module(not a parameter or\nother attribute)\ntarget\nnn.Module\nbool\nNote\nBackwards-compatibility for this API is guaranteed.\nReturn the Python code generated from theGraphunderlying thisGraphModule.\nGraph\nGraphModule\nDeletes all unused submodules fromself.\nself\nA Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via acall_modulenode\n3. It has a non-Module attribute that is used from aget_attrnode\ncall_module\nget_attr\nThis method can be called to clean up annn.Modulewithout\nmanually callingdelete_submoduleon each unused submodule.\nnn.Module\ndelete_submodule\nNote\nBackwards-compatibility for this API is guaranteed.\nDeletes the given submodule fromself.\nself\nThe module will not be deleted iftargetis not a valid\ntarget.\ntarget\ntarget(str) \u2013 The fully-qualified string name of the new submodule\n(See example innn.Module.get_submodulefor how to\nspecify a fully-qualified string.)\nnn.Module.get_submodule\nWhether or not the target string referenced asubmodule we want to delete. A return value ofFalsemeans that thetargetwas not a valid reference to\na submodule.\nsubmodule we want to delete. A return value ofFalsemeans that thetargetwas not a valid reference to\na submodule.\nFalse\ntarget\nbool\nNote\nBackwards-compatibility for this API is guaranteed.\nReturn theGraphunderlying thisGraphModule\nGraph\nGraphModule\nReturn the Python code generated for current GraphModule and its children GraphModules\nWarning\nThis API is experimental and isNOTbackward-compatible.\nRecompile this GraphModule from itsgraphattribute. This should be\ncalled after editing the containedgraph, otherwise the generated\ncode of thisGraphModulewill be out of date.\ngraph\ngraph\nGraphModule\nNote\nBackwards-compatibility for this API is guaranteed.\nPythonCode\nfolder\nmodule_name\nimported withfrom<folder>import<module_name>\nfrom<folder>import<module_name>\nArgs:\nfolder (Union[str, os.PathLike]): The folder to write the code out to\nModule\nwriting out the code\nWarning\nThis API is experimental and isNOTbackward-compatible.\nGraphis the main data structure used in the FX Intermediate Representation.\nIt consists of a series ofNodes, each representing callsites (or other\nsyntactic constructs). The list ofNodes, taken together, constitute a\nvalid Python function.\nGraph\nNode\nNode\nFor example, the following code\n\n```python\nimport torch\nimport torch.fx\n\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(\n            torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3\n        )\n\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\n```\n\nWill produce the following Graph:\n\n```python\nprint(gm.graph)\n\n```\n\n\n```python\ngraph(x):\n    %linear_weight : [num_users=1] = self.linear.weight\n    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [num_users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [num_users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [num_users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [num_users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1\n\n```\n\nFor the semantics of operations represented in theGraph, please seeNode.\nGraph\nNode\nNote\nBackwards-compatibility for this API is guaranteed.\nConstruct an empty Graph.\nNote\nBackwards-compatibility for this API is guaranteed.\nInsert acall_functionNodeinto theGraph. Acall_functionnode\nrepresents a call to a Python callable, specified bythe_function.\ncall_function\nNode\nGraph\ncall_function\nthe_function\nthe_function(Callable[...,Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of thebuiltinsoroperatornamespaces.\nbuiltins\noperator\nargs(Optional[Tuple[Argument,...]]) \u2013 The positional arguments to be passed\nto the called function.\nkwargs(Optional[Dict[str,Argument]]) \u2013 The keyword arguments to be passed\nto the called function\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nname(Optional[str]) \u2013 The name of the node. If not specified, set to None\nThe newly created and insertedcall_functionnode.\ncall_function\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node().\nGraph.create_node()\nNote\nBackwards-compatibility for this API is guaranteed.\nInsert acall_methodNodeinto theGraph. Acall_methodnode\nrepresents a call to a given method on the 0th element ofargs.\ncall_method\nNode\nGraph\ncall_method\nargs\nmethod_name(str) \u2013 The name of the method to apply to the self argument.\nFor example, if args[0] is aNoderepresenting aTensor,\nthen to callrelu()on thatTensor, passrelutomethod_name.\nNode\nTensor\nrelu()\nTensor\nrelu\nmethod_name\nargs(Optional[Tuple[Argument,...]]) \u2013 The positional arguments to be passed\nto the called method. Note that thisshouldinclude aselfargument.\nself\nkwargs(Optional[Dict[str,Argument]]) \u2013 The keyword arguments to be passed\nto the called method\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nThe newly created and insertedcall_methodnode.\ncall_method\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node().\nGraph.create_node()\nNote\nBackwards-compatibility for this API is guaranteed.\nInsert acall_moduleNodeinto theGraph. Acall_modulenode\nrepresents a call to the forward() function of aModulein theModulehierarchy.\ncall_module\nNode\nGraph\ncall_module\nModule\nModule\nmodule_name(str) \u2013 The qualified name of theModulein theModulehierarchy to be called. For example, if the tracedModulehas a\nsubmodule namedfoo, which has a submodule namedbar, the\nqualified namefoo.barshould be passed asmodule_nameto\ncall that module.\nModule\nModule\nModule\nfoo\nbar\nfoo.bar\nmodule_name\nargs(Optional[Tuple[Argument,...]]) \u2013 The positional arguments to be passed\nto the called method. Note that this shouldnotinclude aselfargument.\nself\nkwargs(Optional[Dict[str,Argument]]) \u2013 The keyword arguments to be passed\nto the called method\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nThe newly-created and insertedcall_modulenode.\ncall_module\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node().\nGraph.create_node()\nNote\nBackwards-compatibility for this API is guaranteed.\nCreate aNodeand add it to theGraphat the current insert-point.\nNote that the current insert-point can be set viaGraph.inserting_before()andGraph.inserting_after().\nNode\nGraph\nGraph.inserting_before()\nGraph.inserting_after()\nop(str) \u2013 the opcode for this Node. One of \u2018call_function\u2019, \u2018call_method\u2019, \u2018get_attr\u2019,\n\u2018call_module\u2019, \u2018placeholder\u2019, or \u2018output\u2019. The semantics of these opcodes are\ndescribed in theGraphdocstring.\nGraph\nargs(Optional[Tuple[Argument,...]]) \u2013 is a tuple of arguments to this node.\nkwargs(Optional[Dict[str,Argument]]) \u2013 the kwargs of this Node\nname(Optional[str]) \u2013 an optional string name for theNode.\nThis will influence the name of the value assigned to in the\nPython generated code.\nNode\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nThe newly-created and inserted node.\nNode\nNote\nBackwards-compatibility for this API is guaranteed.\nRemove all dead code from the graph, based on each node\u2019s number of\nusers, and whether the nodes have any side effects. The graph must be\ntopologically sorted before calling.\nis_impure_node(Optional[Callable[[Node],bool]]) \u2013 A function that returns\nNone(whether a node is impure. If this is) \u2013\nto(then the default behavior is) \u2013\nNode.is_impure.(use) \u2013\nWhether the graph was changed as a result of the pass.\nbool\nExample:\nBefore dead code is eliminated,afroma = x + 1below has no users\nand thus can be eliminated from the graph without having an effect.\n\n```python\ndef forward(self, x):\n    a = x + 1\n    return x + self.attr_1\n\n```\n\nAfter dead code is eliminated,a = x + 1has been removed, and the rest\nofforwardremains.\n\n```python\ndef forward(self, x):\n    return x + self.attr_1\n\n```\n\nWarning\nDead code elimination has some heuristics to avoid removing\nside-effectful nodes (see Node.is_impure) but in general coverage\nis very bad, so you should assume that this method is not sound\nto call unless you know that your FX graph consists entirely\nof functional operations or you supply your own custom\nfunction for detecting side-effectful nodes.\nNote\nBackwards-compatibility for this API is guaranteed.\nErases aNodefrom theGraph. Throws an exception if\nthere are still users of that node in theGraph.\nNode\nGraph\nGraph\nto_erase(Node) \u2013 TheNodeto erase from theGraph.\nNode\nGraph\nNote\nBackwards-compatibility for this API is guaranteed.\nAllows for fast query of nodes\nop(str) \u2013 the name of the operation\ntarget(Optional[Target]) \u2013 the target of the node. For call_function,\nthe target is required. For other ops, the target is optional.\nsort(bool) \u2013 whether to return nodes in the order they appear on\non the graph.\nIterable of nodes with the requested op and target.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInsert aget_attrnode into the Graph. Aget_attrNoderepresents the\nfetch of an attribute from theModulehierarchy.\nget_attr\nget_attr\nNode\nModule\nqualified_name(str) \u2013 the fully-qualified name of the attribute to be retrieved.\nFor example, if the traced Module has a submodule namedfoo, which has a\nsubmodule namedbar, which has an attribute namedbaz, the qualified\nnamefoo.bar.bazshould be passed asqualified_name.\nfoo\nbar\nbaz\nfoo.bar.baz\nqualified_name\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nThe newly-created and insertedget_attrnode.\nget_attr\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node.\nGraph.create_node\nNote\nBackwards-compatibility for this API is guaranteed.\nCopy all nodes from a given graph intoself.\nself\ng(Graph) \u2013 The source graph from which to copy Nodes.\nval_map(Dict[Node,Node]) \u2013 a dictionary that will be populated with a mapping\nfrom nodes ingto nodes inself. Note thatval_mapcan be passed\nin with values in it already to override copying of certain values.\ng\nself\nval_map\nThe value inselfthat is now equivalent to the output value ing,\nifghad anoutputnode.Noneotherwise.\nself\ng\ng\noutput\nNone\nOptional[Union[tuple[\u2018Argument\u2019, \u2026],Sequence[Argument],Mapping[str, Argument],slice,range,Node,str,int,float,bool,complex,dtype,Tensor,device,memory_format,layout,OpOverload,SymInt,SymBool,SymFloat]]\nNote\nBackwards-compatibility for this API is guaranteed.\nWhen used within a \u2018with\u2019 statement, this will temporary set the insert point and\nthen restore it when the with statement exits:\n\n```python\nwith g.inserting_after(n):\n    ...  # inserting after node n\n...  # insert point restored to what it was previously\ng.inserting_after(n)  #  set the insert point permanently\n\n```\n\nArgs:\nthe beginning of the entire graph.\nA resource manager that will restore the insert point on__exit__.\n__exit__\nNote\nBackwards-compatibility for this API is guaranteed.\nWhen used within a \u2018with\u2019 statement, this will temporary set the insert point and\nthen restore it when the with statement exits:\n\n```python\nwith g.inserting_before(n):\n    ...  # inserting before node n\n...  # insert point restored to what it was previously\ng.inserting_before(n)  #  set the insert point permanently\n\n```\n\nArgs:\nthe beginning of the entire graph.\nA resource manager that will restore the insert point on__exit__.\n__exit__\nNote\nBackwards-compatibility for this API is guaranteed.\nRuns various checks on this Graph to make sure it is well-formed. In\nparticular:\n- Checks Nodes have correct ownership (owned by this graph)\n- Checks Nodes appear in topological order\n- If this Graph has an owning GraphModule, checks that targets\nexist in that GraphModule\nNote\nBackwards-compatibility for this API is guaranteed.\nCopy a node from one graph into another.arg_transformneeds to transform arguments from\nthe graph of node to the graph of self. Example:\narg_transform\n\n```python\n# Copying all the nodes in `g` into `new_graph`\ng: torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n: value_remap[n])\n\n```\n\nnode(Node) \u2013 The node to copy intoself.\nself\narg_transform(Callable[[Node],Argument]) \u2013 A function that transformsNodearguments in node\u2019sargsandkwargsinto the\nequivalent argument inself. In the simplest case, this should\nretrieve a value out of a table mapping Nodes in the original\ngraph toself.\nNode\nargs\nkwargs\nself\nself\nNode\nNote\nBackwards-compatibility for this API is guaranteed.\nGet the list of Nodes that constitute this Graph.\nNote that thisNodelist representation is a doubly-linked list. Mutations\nduring iteration (e.g. delete a Node, add a Node) are safe.\nNode\nA doubly-linked list of Nodes. Note thatreversedcan be called on\nthis list to switch iteration order.\nreversed\nRegister a transformer function when python code is generated\na function that returns a code transformer to be registered.\nThis function is called byon_generate_codeto obtain the\ncode transformer.\nThis function is also given as its input the currently\nregistered code transformer (or None if nothing is registered),\nin case it is not desirable to overwrite it. This is useful to\nchain code transformers together.\na context manager that when used in awithstatement, to automatically\nrestore the previously registered code transformer.\nExample:\n\n```python\ngm: fx.GraphModule = ...\n\n\n# This is a code transformer we want to register. This code\n# transformer prepends a pdb import and trace statement at the very\n# beginning of the generated torch.fx code to allow for manual\n# debugging with the PDB library.\ndef insert_pdb(body):\n    return [\"import pdb; pdb.set_trace()\\n\", *body]\n\n\n# Registers `insert_pdb`, and overwrites the current registered\n# code transformer (given by `_` to the lambda):\ngm.graph.on_generate_code(lambda _: insert_pdb)\n\n# Or alternatively, registers a code transformer which first\n# runs `body` through existing registered transformer, then\n# through `insert_pdb`:\ngm.graph.on_generate_code(\n    lambda current_trans: (\n        lambda body: insert_pdb(\n            current_trans(body) if current_trans else body\n        )\n    )\n)\n\ngm.recompile()\ngm(*inputs)  # drops into pdb\n\n```\n\nThis function can also be used as a context manager, with the benefit to\nautomatically restores the previously registered code transformer:\n\n```python\n# ... continue from previous example\n\nwith gm.graph.on_generate_code(lambda _: insert_pdb):\n    # do more stuff with `gm`...\n    gm.recompile()\n    gm(*inputs)  # drops into pdb\n\n# now previous code transformer is restored (but `gm`'s code with pdb\n# remains - that means you can run `gm` with pdb here too, until you\n# run next `recompile()`).\n\n```\n\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInsert anoutputNodeinto theGraph. Anoutputnode represents\nareturnstatement in Python code.resultis the value that should\nbe returned.\noutput\nNode\nGraph\noutput\nreturn\nresult\nresult(Argument) \u2013 The value to be returned.\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node.\nGraph.create_node\nNote\nBackwards-compatibility for this API is guaranteed.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nNode\nInsert aplaceholdernode into the Graph. Aplaceholderrepresents\na function input.\nplaceholder\nplaceholder\nname(str) \u2013 A name for the input value. This corresponds to the name\nof the positional argument to the function thisGraphrepresents.\nGraph\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. This is needed in some\ncases for proper code generation (e.g. when the function is used\nsubsequently in TorchScript compilation).\ndefault_value(Any) \u2013 The default value this function argument should take\non. NOTE: to allow forNoneas a default value,inspect.Signature.emptyshould be passed as this argument to specify that the parameter does _not_\nhave a default value.\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node.\nGraph.create_node\nNote\nBackwards-compatibility for this API is guaranteed.\nPrints the intermediate representation of the graph in tabular\nformat. Note that this API requires thetabulatemodule to be\ninstalled.\ntabulate\nNote\nBackwards-compatibility for this API is guaranteed.\nProcesses args so that they can be passed to the FX graph.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nTurn thisGraphinto valid Python code.\nGraph\nroot_module(str) \u2013 The name of the root module on which to look-up\nqualified name targets. This is usually \u2018self\u2019.\nsrc: the Python source code representing the object\nglobals: a dictionary of global names insrc-> the objects that they reference.\nA PythonCode object, consisting of two fields\nNote\nBackwards-compatibility for this API is guaranteed.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nNodeis the data structure that represents individual operations within\naGraph. For the most part, Nodes represent callsites to various entities,\nsuch as operators, methods, and Modules (some exceptions include nodes that\nspecify function inputs and outputs). EachNodehas a function specified\nby itsopproperty. TheNodesemantics for each value ofopare as follows:\nNode\nGraph\nNode\nop\nNode\nop\nplaceholderrepresents a function input. Thenameattribute specifies the name this value will take on.targetis similarly the name of the argument.argsholds either: 1) nothing, or 2) a single argument\ndenoting the default parameter of the function input.kwargsis don\u2019t-care. Placeholders correspond to\nthe function parameters (e.g.x) in the graph printout.\nplaceholder\nname\ntarget\nargs\nkwargs\nx\nget_attrretrieves a parameter from the module hierarchy.nameis similarly the name the result of the\nfetch is assigned to.targetis the fully-qualified name of the parameter\u2019s position in the module hierarchy.argsandkwargsare don\u2019t-care\nget_attr\nname\ntarget\nargs\nkwargs\ncall_functionapplies a free function to some values.nameis similarly the name of the value to assign\nto.targetis the function to be applied.argsandkwargsrepresent the arguments to the function,\nfollowing the Python calling convention\ncall_function\nname\ntarget\nargs\nkwargs\ncall_moduleapplies a module in the module hierarchy\u2019sforward()method to given arguments.nameis\nas previous.targetis the fully-qualified name of the module in the module hierarchy to call.argsandkwargsrepresent the arguments to invoke the module on,excluding the self argument.\ncall_module\nforward()\nname\ntarget\nargs\nkwargs\ncall_methodcalls a method on a value.nameis as similar.targetis the string name of the method\nto apply to theselfargument.argsandkwargsrepresent the arguments to invoke the module on,including the self argument\ncall_method\nname\ntarget\nself\nargs\nkwargs\noutputcontains the output of the traced function in itsargs[0]attribute. This corresponds to the \u201creturn\u201d statement\nin the Graph printout.\noutput\nargs[0]\nNote\nBackwards-compatibility for this API is guaranteed.\nReturn all Nodes that are inputs to this Node. This is equivalent to\niterating overargsandkwargsand only collecting the values that\nare Nodes.\nargs\nkwargs\nList ofNodesthat appear in theargsandkwargsof thisNode, in that order.\nNodes\nargs\nkwargs\nNode\nInsertxafter this node in the list of nodes in the graph.\nEquivalent toself.next.prepend(x)\nx\nself.next.prepend(x)\nx(Node) \u2013 The node to put after this node. Must be a member of the same graph.\nNote\nBackwards-compatibility for this API is guaranteed.\nThe tuple of arguments to thisNode. The interpretation of arguments\ndepends on the node\u2019s opcode. See theNodedocstring for more\ninformation.\nNode\nNode\nAssignment to this property is allowed. All accounting of uses and users\nis updated automatically on assignment.\nReturn a descriptive string representation ofself.\nself\nThis method can be used with no arguments as a debugging\nutility.\nThis function is also used internally in the__str__method\nofGraph. Together, the strings inplaceholder_namesandmaybe_return_typenamemake up the signature of the\nautogeneratedforwardfunction in this Graph\u2019s surrounding\nGraphModule.placeholder_namesandmaybe_return_typenameshould not be used otherwise.\n__str__\nGraph\nplaceholder_names\nmaybe_return_typename\nforward\nplaceholder_names\nmaybe_return_typename\nplaceholder_names(Optional[list[str]]) \u2013 A list that will store formatted strings\nrepresenting the placeholders in the generatedforwardfunction. Internal use only.\nforward\nmaybe_return_typename(Optional[list[str]]) \u2013 A single-element list that will store\na formatted string representing the output of the\ngeneratedforwardfunction. Internal use only.\nforward\ninclude_tensor_metadata(bool) \u2013 Whether to include tensor metadata\nIf 1) we\u2019re usingformat_nodeas an internal helperin the__str__method ofGraph, and 2)selfis a placeholder Node, returnNone. Otherwise,\nreturn a  descriptive string representation of the\ncurrent Node.\nformat_node\nin the__str__method ofGraph, and 2)selfis a placeholder Node, returnNone. Otherwise,\nreturn a  descriptive string representation of the\ncurrent Node.\n__str__\nGraph\nself\nNone\nstr\nNote\nBackwards-compatibility for this API is guaranteed.\nInsert an positional argument to the argument list with given index.\nidx(int) \u2013 The index of the element inself.argsto be inserted before.\nself.args\narg(Argument) \u2013 The new argument value to insert intoargs\nargs\nNote\nBackwards-compatibility for this API is guaranteed.\nReturns whether this op is impure, i.e. if its op is a placeholder or\noutput, or if a call_function or call_module which is impure.\nimpure_random(bool) \u2013 Whether to treat rand op as impure.\nIf the op is impure or not.\nbool\nWarning\nThis API is experimental and isNOTbackward-compatible.\nThe dict of keyword arguments to thisNode. The interpretation of arguments\ndepends on the node\u2019s opcode. See theNodedocstring for more\ninformation.\nNode\nNode\nAssignment to this property is allowed. All accounting of uses and users\nis updated automatically on assignment.\nReturns the nextNodein the linked list of Nodes.\nNode\nThe nextNodein the linked list of Nodes.\nNode\nReturns normalized arguments to Python targets. This means thatargs/kwargswill be matched up to the module/functional\u2019s\nsignature and return exclusively kwargs in positional order\nifnormalize_to_only_use_kwargsis true.\nAlso populates default values. Does not support positional-only\nparameters or varargs parameters.\nSupports module calls.\nMay requirearg_typesandkwarg_typesin order to disambiguate overloads.\nroot(torch.nn.Module) \u2013 Module upon which to resolve module targets.\narg_types(Optional[Tuple[Any]]) \u2013 Tuple of arg types for the args\nkwarg_types(Optional[Dict[str,Any]]) \u2013 Dict of arg types for the kwargs\nnormalize_to_only_use_kwargs(bool) \u2013 Whether to normalize to only use kwargs.\nReturns NamedTuple ArgsKwargsPair, orNoneif not successful.\nOptional[ArgsKwargsPair]\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInsert x before this node in the list of nodes in the graph. Example:\n\n```python\nBefore: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n\n```\n\nx(Node) \u2013 The node to put before this node. Must be a member of the same graph.\nNote\nBackwards-compatibility for this API is guaranteed.\nReturns the previousNodein the linked list of Nodes.\nNode\nThe previousNodein the linked list of Nodes.\nNode\nReplace all uses ofselfin the Graph with the Nodereplace_with.\nself\nreplace_with\nreplace_with(Node) \u2013 The node to replace all uses ofselfwith.\nself\ndelete_user_cb(Callable) \u2013 Callback that is called to determine\nwhether a given user of the self node should be removed.\npropagate_meta(bool) \u2013 Whether or not to copy all properties\non the .meta field of the original node onto the replacement node.\nFor safety, this is only valid to do if the replacement node\ndoesn\u2019t already have an existing .meta field.\nThe list of Nodes on which this change was made.\nlist[\u2018Node\u2019]\nNote\nBackwards-compatibility for this API is guaranteed.\nLoop through input nodes ofself, and replace all instances ofold_inputwithnew_input.\nself\nold_input\nnew_input\nold_input(Node) \u2013 The old input node to be replaced.\nnew_input(Node) \u2013 The new input node to replaceold_input.\nold_input\nNote\nBackwards-compatibility for this API is guaranteed.\nReturn the Python stack trace that was recorded during tracing, if any.\nWhen traced with fx.Tracer, this property is usually populated byTracer.create_proxy. To record stack traces during tracing for debug purposes,\nsetrecord_stack_traces = Trueon theTracerinstance.\nWhen traced with dynamo, this property will be populated by default byOutputGraph.create_proxy.\nstack_trace would have the innermost frame at the end of the string.\nUpdate an existing positional argument to contain the new valuearg. After calling,self.args[idx]==arg.\narg\nself.args[idx]==arg\nidx(int) \u2013 The index intoself.argsof the element to update\nself.args\narg(Argument) \u2013 The new argument value to write intoargs\nargs\nNote\nBackwards-compatibility for this API is guaranteed.\nUpdate an existing keyword argument to contain the new valuearg. After calling,self.kwargs[key]==arg.\narg\nself.kwargs[key]==arg\nkey(str) \u2013 The key inself.kwargsof the element to update\nself.kwargs\narg(Argument) \u2013 The new argument value to write intokwargs\nkwargs\nNote\nBackwards-compatibility for this API is guaranteed.\nTraceris the class that implements the symbolic tracing functionality\noftorch.fx.symbolic_trace. A call tosymbolic_trace(m)is equivalent\ntoTracer().trace(m).\nTracer\ntorch.fx.symbolic_trace\nsymbolic_trace(m)\nTracer().trace(m)\nTracer can be subclassed to override various behaviors of the tracing\nprocess. The different behaviors that can be overridden are described\nin the docstrings of the methods on this class.\nNote\nBackwards-compatibility for this API is guaranteed.\nMethod that specifies the behavior of thisTracerwhen it encounters\na call to annn.Moduleinstance.\nTracer\nnn.Module\nBy default, the behavior is to check if the called module is a leaf module\nviais_leaf_module. If it is, emit acall_modulenode referring tomin theGraph. Otherwise, call theModulenormally, tracing through\nthe operations in itsforwardfunction.\nis_leaf_module\ncall_module\nm\nGraph\nModule\nforward\nThis method can be overridden to\u2013for example\u2013create nested traced\nGraphModules, or any other behavior you would want while tracing acrossModuleboundaries.\nModule\nm(Module) \u2013 The module for which a call is being emitted\nforward(Callable) \u2013 The forward() method of theModuleto be invoked\nModule\nargs(Tuple) \u2013 args of the module callsite\nkwargs(Dict) \u2013 kwargs of the module callsite\nThe return value from the Module call. In the case that acall_modulenode was emitted, this is aProxyvalue. Otherwise, it is whatever\nvalue was returned from theModuleinvocation.\ncall_module\nProxy\nModule\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nA method to specify the behavior of tracing when preparing values to\nbe used as arguments to nodes in theGraph.\nGraph\nBy default, the behavior includes:\nIterate through collection types (e.g. tuple, list, dict) and recursively\ncallcreate_argson the elements.\ncreate_args\nGiven a Proxy object, return a reference to the underlying IRNode\nNode\nGiven a non-Proxy Tensor object, emit IR for various cases:\nFor a Parameter, emit aget_attrnode referring to that Parameter\nget_attr\nFor a non-Parameter Tensor, store the Tensor away in a special\nattribute referring to that attribute.\nThis method can be overridden to support more types.\na(Any) \u2013 The value to be emitted as anArgumentin theGraph.\nArgument\nGraph\nThe valueaconverted into the appropriateArgument\na\nArgument\nArgument\nNote\nBackwards-compatibility for this API is guaranteed.\nCreateplaceholdernodes corresponding to the signature of therootModule. This method introspects root\u2019s signature and emits those\nnodes accordingly, also supporting*argsand**kwargs.\nplaceholder\nroot\n*args\n**kwargs\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInserts a graph node given target, args, kwargs, and name.\nThis method can be overridden to do extra checking, validation, or\nmodification of values used in node creation. For example, one might\nwant to disallow in-place operations from being recorded.\nNote\nBackwards-compatibility for this API is guaranteed.\nNode\nCreate a Node from the given arguments, then return the Node\nwrapped in a Proxy object.\nIf kind = \u2018placeholder\u2019, then we\u2019re creating a Node that\nrepresents the parameter of a function. If we need to encode\na default parameter, we use theargstuple.argsis\notherwise empty forplaceholderNodes.\nargs\nargs\nplaceholder\nNote\nBackwards-compatibility for this API is guaranteed.\nGets a fresh name for a prefix and returns it. This function ensures\nthat it will not clash with an existing attribute on the graph.\nNote\nBackwards-compatibility for this API is guaranteed.\nstr\nMethod that specifies the behavior of thisTracerwhen we call getattr\non a call to annn.Moduleinstance.\nTracer\nnn.Module\nBy default, the behavior is to return a proxy value for the attribute. It\nalso stores the proxy value in theparameter_proxy_cache, so that future\ncalls will reuse the proxy rather than creating a new one.\nparameter_proxy_cache\nThis method can be overridden to \u2013for example\u2013 not return proxies when\nquerying parameters.\nattr(str) \u2013 The name of the attribute being queried\nattr_val(Any) \u2013 The value of the attribute\nparameter_proxy_cache(Dict[str,Any]) \u2013 A cache of attr names to proxies\nThe return value from the getattr call.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nA method to specify whether a givennn.Moduleis a \u201cleaf\u201d module.\nnn.Module\nLeaf modules are the atomic units that appear in\nthe IR, referenced bycall_modulecalls. By default,\nModules in the PyTorch standard library namespace (torch.nn)\nare leaf modules. All other modules are traced through and\ntheir constituent ops are recorded, unless specified otherwise\nvia this parameter.\ncall_module\nm(Module) \u2013 The module being queried about\nmodule_qualified_name(str) \u2013 The path to root of this module. For example,\nif you have a module hierarchy where submodulefoocontains\nsubmodulebar, which contains submodulebaz, that module will\nappear with the qualified namefoo.bar.bazhere.\nfoo\nbar\nbaz\nfoo.bar.baz\nbool\nNote\nBackwards-compatibility for this API is guaranteed.\nwhen used in control flow.  Normally we don\u2019t know what to do because\nwe don\u2019t know the value of the proxy, but a custom tracer can attach more\ninformation to the graph node using create_node and can choose to return an iterator.\nNote\nBackwards-compatibility for this API is guaranteed.\nIterator\nThis is what happens when ** is called on a proxy. This should return an\niterator it ** is suppose to work in your custom tracer.\nNote\nBackwards-compatibility for this API is guaranteed.\nAny\nHelper method to find the qualified name ofmodin the Module hierarchy\nofroot. For example, ifroothas a submodule namedfoo, which has\na submodule namedbar, passingbarinto this function will return\nthe string \u201cfoo.bar\u201d.\nmod\nroot\nroot\nfoo\nbar\nbar\nmod(str) \u2013 TheModuleto retrieve the qualified name for.\nModule\nstr\nNote\nBackwards-compatibility for this API is guaranteed.\nNote\nBackwards-compatibility for this API is guaranteed.\nProxy\nwhen used in control flow.  Normally we don\u2019t know what to do because\nwe don\u2019t know the value of the proxy, but a custom tracer can attach more\ninformation to the graph node using create_node and can choose to return a value.\nNote\nBackwards-compatibility for this API is guaranteed.\nbool\nTracerootand return the corresponding FXGraphrepresentation.rootcan either be annn.Moduleinstance or a Python callable.\nroot\nGraph\nroot\nnn.Module\nNote that after this call,self.rootmay be different from therootpassed\nin here. For example, when a free function is passed totrace(), we will\ncreate annn.Moduleinstance to use as the root and add embedded constants\nto.\nself.root\nroot\ntrace()\nnn.Module\nroot(Union[Module,Callable]) \u2013 Either aModuleor a function to be\ntraced through. Backwards-compatibility for this parameter is\nguaranteed.\nModule\nconcrete_args(Optional[Dict[str,any]]) \u2013 Concrete arguments that should\nnot be treated as Proxies. This parameter is experimental and\nits backwards-compatibility isNOTguaranteed.\nAGraphrepresenting the semantics of the passed-inroot.\nGraph\nroot\nGraph\nNote\nBackwards-compatibility for this API is guaranteed.\nProxyobjects areNodewrappers that flow through the\nprogram during symbolic tracing and record all the operations\n(torchfunction calls, method calls, operators) that they touch\ninto the growing FX Graph.\nProxy\nNode\ntorch\nIf you\u2019re doing graph transforms, you can wrap your ownProxymethod around a rawNodeso that you can use the overloaded\noperators to add additional things to aGraph.\nProxy\nNode\nGraph\nProxyobjects cannot be iterated. In other words, the symbolic\ntracer will throw an error if aProxyis used in a loop or as\nan*args/**kwargsfunction argument.\nProxy\nProxy\n*args\n**kwargs\nThere are two main ways around this:\n1. Factor out the untraceable logic into a top-level function and\nusefx.wrapon it.\n2. If the control flow is static (i.e. the loop trip count is\nbased on some hyperparameter), the code can be kept in its original\nposition and refactored into something like:\nfx.wrap\n\n```python\nfor i in range(self.some_hyperparameter):\n    indexed_item = proxied_value[i]\n\n```\n\nFor a more detailed description into the Proxy internals, check out\nthe \u201cProxy\u201d section intorch/fx/README.md\nNote\nBackwards-compatibility for this API is guaranteed.\nAn Interpreter executes an FX graph Node-by-Node. This pattern\ncan be useful for many things, including writing code\ntransformations as well as analysis passes.\nMethods in the Interpreter class can be overridden to customize\nthe behavior of execution. The map of overridable methods\nin terms of call hierarchy:\n\n```python\nrun()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()\n\n```\n\nExample\nSuppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclass Interpreter like so:\ntorch.neg\ntorch.sigmoid\nTensor\n\n```python\nclass NegSigmSwapInterpreter(Interpreter):\n    def call_function(\n        self, target: Target, args: Tuple, kwargs: Dict\n    ) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(target, args, kwargs)\n\n    def call_method(self, target: Target, args: Tuple, kwargs: Dict) -> Any:\n        if target == \"neg\":\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(target, args, kwargs)\n\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_close(result, torch.neg(input).sigmoid())\n\n```\n\nmodule(torch.nn.Module) \u2013 The module to be executed\ngarbage_collect_values(bool) \u2013 Whether to delete values after their last\nuse within the Module\u2019s execution. This ensures optimal memory usage during\nexecution. This can be disabled to, for example, examine all of the intermediate\nvalues in the execution by looking at theInterpreter.envattribute.\nInterpreter.env\ngraph(Optional[Graph]) \u2013 If passed, the interpreter will execute this\ngraph instead ofmodule.graph, using the providedmoduleargument to satisfy any requests for state.\nNote\nBackwards-compatibility for this API is guaranteed.\nRunmodulevia interpretation and return the result.  This uses the \u201cboxed\u201d\ncalling convention, where you pass a list of arguments, which will be cleared\nby the interpreter.  This ensures that input tensors are promptly deallocated.\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute acall_functionnode and return the result.\ncall_function\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nAny\nAny: The value returned by the function invocation\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute acall_methodnode and return the result.\ncall_method\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nAny\nAny: The value returned by the method invocation\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute acall_modulenode and return the result.\ncall_module\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nAny\nAny: The value returned by the module invocation\nNote\nBackwards-compatibility for this API is guaranteed.\nFetch the concrete values ofargsandkwargsof nodenfrom the current execution environment.\nargs\nkwargs\nn\nn(Node) \u2013 The node for whichargsandkwargsshould be fetched.\nargs\nkwargs\nargsandkwargswith concrete values forn.\nargs\nkwargs\nn\nTuple[Tuple, Dict]\nNote\nBackwards-compatibility for this API is guaranteed.\nFetch an attribute from theModulehierarchy ofself.module.\nModule\nself.module\ntarget(str) \u2013 The fully-qualified name of the attribute to fetch\nThe value of the attribute.\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute aget_attrnode. Will retrieve an attribute\nvalue from theModulehierarchy ofself.module.\nget_attr\nModule\nself.module\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nThe value of the attribute that was retrieved\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nRecursively descend throughargsand look up the concrete value\nfor eachNodein the current execution environment.\nargs\nNode\nargs(Argument) \u2013 Data structure within which to look up concrete values\nn(Node) \u2013 Node to whichargsbelongs. This is only used for error reporting.\nargs\nOptional[Union[tuple[\u2018Argument\u2019, \u2026],Sequence[Argument],Mapping[str, Argument],slice,range,Node,str,int,float,bool,complex,dtype,Tensor,device,memory_format,layout,OpOverload,SymInt,SymBool,SymFloat]]\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute anoutputnode. This really just retrieves\nthe value referenced by theoutputnode and returns it.\noutput\noutput\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nThe return value referenced by the output node\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute aplaceholdernode. Note that this is stateful:Interpretermaintains an internal iterator over\narguments passed torunand this method returns\nnext() on that iterator.\nplaceholder\nInterpreter\nrun\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nThe argument value that was retrieved.\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nRunmodulevia interpretation and return the result.\n*args\u2013 The arguments to the Module to run, in positional order\ninitial_env(Optional[Dict[Node,Any]]) \u2013 An optional starting environment for execution.\nThis is a dict mappingNodeto any value. This can be used, for example, to\npre-populate results for certainNodesso as to do only partial evaluation within\nthe interpreter.\nenable_io_processing(bool) \u2013 If true, we process the inputs and outputs with graph\u2019s process_inputs and\nprocess_outputs function first before using them.\nThe value returned from executing the Module\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nRun a specific nodenand return the result.\nCalls into placeholder, get_attr, call_function,\ncall_method, call_module, or output depending\nonnode.op\nn\nnode.op\nn(Node) \u2013 The Node to execute\nThe result of executingn\nn\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nTransformeris a special type of interpreter that produces a\nnewModule. It exposes atransform()method that returns\nthe transformedModule.Transformerdoes not require\narguments to run, asInterpreterdoes.Transformerworks\nentirely symbolically.\nTransformer\nModule\ntransform()\nModule\nTransformer\nInterpreter\nTransformer\nExample\nSuppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclassTransformerlike so:\ntorch.neg\ntorch.sigmoid\nTensor\nTransformer\n\n```python\nclass NegSigmSwapXformer(Transformer):\n    def call_function(\n        self,\n        target: \"Target\",\n        args: Tuple[Argument, ...],\n        kwargs: Dict[str, Any],\n    ) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(target, args, kwargs)\n\n    def call_method(\n        self,\n        target: \"Target\",\n        args: Tuple[Argument, ...],\n        kwargs: Dict[str, Any],\n    ) -> Any:\n        if target == \"neg\":\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(target, args, kwargs)\n\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed: torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_close(transformed(input), torch.neg(input).sigmoid())\n\n```\n\nmodule(GraphModule) \u2013 TheModuleto be transformed.\nModule\nNote\nBackwards-compatibility for this API is guaranteed.\nNote\nBackwards-compatibility for this API is guaranteed.\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nAny\nExecute aget_attrnode. InTransformer, this is\noverridden to insert a newget_attrnode into the output\ngraph.\nget_attr\nTransformer\nget_attr\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nProxy\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute aplaceholdernode. InTransformer, this is\noverridden to insert a newplaceholderinto the output\ngraph.\nplaceholder\nTransformer\nplaceholder\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nProxy\nNote\nBackwards-compatibility for this API is guaranteed.\nTransformself.moduleand return the transformedGraphModule.\nself.module\nGraphModule\nNote\nBackwards-compatibility for this API is guaranteed.\nGraphModule\nMatches all possible non-overlapping sets of operators and their\ndata dependencies (pattern) in the Graph of a GraphModule\n(gm), then replaces each of these matched subgraphs with another\nsubgraph (replacement).\npattern\ngm\nreplacement\ngm(GraphModule) \u2013 The GraphModule that wraps the Graph to operate on\npattern(Union[Callable,GraphModule]) \u2013 The subgraph to match ingmfor replacement\ngm\nreplacement(Union[Callable,GraphModule]) \u2013 The subgraph to replacepatternwith\npattern\nA list ofMatchobjects representing the places\nin the original graph thatpatternwas matched to. The list\nis empty if there are no matches.Matchis defined as:classMatch(NamedTuple):# Node from which the match was foundanchor:Node# Maps nodes in the pattern subgraph to nodes in the larger graphnodes_map:Dict[Node,Node]\nA list ofMatchobjects representing the places\nin the original graph thatpatternwas matched to. The list\nis empty if there are no matches.Matchis defined as:\nMatch\npattern\nMatch\n\n```python\nclass Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n\n```\n\nList[Match]\nExamples:\n\n```python\nimport torch\nfrom torch.fx import symbolic_trace, subgraph_rewriter\n\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x, w1, w2):\n        m1 = torch.cat([w1, w2]).sum()\n        m2 = torch.cat([w1, w2]).sum()\n        return x + torch.max(m1) + torch.max(m2)\n\n\ndef pattern(w1, w2):\n    return torch.cat([w1, w2])\n\n\ndef replacement(w1, w2):\n    return torch.stack([w1, w2])\n\n\ntraced_module = symbolic_trace(M())\n\nsubgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n\n```\n\nThe above code will first matchpatternin theforwardmethod oftraced_module. Pattern-matching is done based on\nuse-def relationships, not node names. For example, if you hadp=torch.cat([a,b])inpattern, you could matchm=torch.cat([a,b])in the originalforwardfunction,\ndespite the variable names being different (pvsm).\npattern\nforward\ntraced_module\np=torch.cat([a,b])\npattern\nm=torch.cat([a,b])\nforward\np\nm\nThereturnstatement inpatternis matched based on its\nvalue only; it may or may not match to thereturnstatement in\nthe larger graph. In other words, the pattern doesn\u2019t have to extend\nto the end of the larger graph.\nreturn\npattern\nreturn\nWhen the pattern is matched, it will be removed from the larger\nfunction and replaced byreplacement. If there are multiple\nmatches forpatternin the larger function, each non-overlapping\nmatch will be replaced. In the case of a match overlap, the first\nfound match in the set of overlapping matches will be replaced.\n(\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node\nis the parameter that appears directly afterself, while the\nlast Node is whatever the function returns.)\nreplacement\npattern\nself\nOne important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\nand the parameters of thereplacementCallable must match\nthe pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn\u2019t usex, so it shouldn\u2019t specifyxas a parameter.\nAs an example of the second rule, consider replacing\npattern\nreplacement\nforward\nx,w1,w2\npattern\nw1,w2\npattern\nx\nx\n\n```python\ndef pattern(x, y):\n    return torch.neg(x) + torch.relu(y)\n\n```\n\nwith\n\n```python\ndef replacement(x, y):\n    return torch.relu(x)\n\n```\n\nIn this case,replacementneeds the same number of parameters\naspattern(bothxandy), even though the parameteryisn\u2019t used inreplacement.\nreplacement\npattern\nx\ny\ny\nreplacement\nAfter callingsubgraph_rewriter.replace_pattern, the generated\nPython code looks like this:\nsubgraph_rewriter.replace_pattern\n\n```python\ndef forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2\n\n```\n\nNote\nBackwards-compatibility for this API is guaranteed.",
  "url": "https://pytorch.org/docs/stable/fx.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "1cf9af912783ad06aef0795aee5fdb75",
  "source": "pytorch_docs",
  "title": "torch.func \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.func#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\ntorch.func, previously known as \u201cfunctorch\u201d, isJAX-likecomposable function transforms for PyTorch.\nNote\nThis library is currently inbeta.\nWhat this means is that the features generally work (unless otherwise documented)\nand we (the PyTorch team) are committed to bringing this library forward. However, the APIs\nmay change under user feedback and we don\u2019t have full coverage over PyTorch operations.\nIf you have suggestions on the API or use-cases you\u2019d like to be covered, please\nopen a GitHub issue or reach out. We\u2019d love to hear about how you\u2019re using the library.\n\n## What are composable function transforms?#\n\nA \u201cfunction transform\u201d is a higher-order function that accepts a numerical function\nand returns a new function that computes a different quantity.\ntorch.funchas auto-differentiation transforms (grad(f)returns a function that\ncomputes the gradient off), a vectorization/batching transform (vmap(f)returns a function that computesfover batches of inputs), and others.\ntorch.func\ngrad(f)\nf\nvmap(f)\nf\nThese function transforms can compose with each other arbitrarily. For example,\ncomposingvmap(grad(f))computes a quantity called per-sample-gradients that\nstock PyTorch cannot efficiently compute today.\nvmap(grad(f))\n\n## Why composable function transforms?#\n\nThere are a number of use cases that are tricky to do in PyTorch today:\ncomputing per-sample-gradients (or other per-sample quantities)\nrunning ensembles of models on a single machine\nefficiently batching together tasks in the inner-loop of MAML\nefficiently computing Jacobians and Hessians\nefficiently computing batched Jacobians and Hessians\nComposingvmap(),grad(), andvjp()transforms allows us to express the above without designing a separate subsystem for each.\nThis idea of composable function transforms comes from theJAX framework.\nvmap()\ngrad()\nvjp()\n\n## Read More#\n",
  "url": "https://pytorch.org/docs/stable/func.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
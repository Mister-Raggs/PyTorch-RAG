{
  "doc_id": "9e7902a2ada27e4bdf4b51879c761594",
  "source": "pytorch_docs",
  "title": "Tensor Views \u2014 PyTorch 2.9 documentation",
  "text": "\n## Tensor Views#\n\nCreated On: Feb 28, 2020 | Last Updated On: Feb 26, 2025\nPyTorch allows a tensor to be aViewof an existing tensor. View tensor shares the same underlying data\nwith its base tensor. SupportingViewavoids explicit data copy, thus allows us to do fast and memory efficient\nreshaping, slicing and element-wise operations.\nView\nView\nFor example, to get a view of an existing tensort, you can callt.view(...).\nt\nt.view(...)\n\n```python\n>>> t = torch.rand(4, 4)\n>>> b = t.view(2, 8)\n>>> t.storage().data_ptr() == b.storage().data_ptr()  # `t` and `b` share the same underlying data.\nTrue\n# Modifying view tensor changes base tensor as well.\n>>> b[0][0] = 3.14\n>>> t[0][0]\ntensor(3.14)\n\n```\n\nSince views share underlying data with its base tensor, if you edit the data\nin the view, it will be reflected in the base tensor as well.\nTypically a PyTorch op returns a new tensor as output, e.g.add().\nBut in case of view ops, outputs are views of input tensors to avoid unnecessary data copy.\nNo data movement occurs when creating a view, view tensor just changes the way\nit interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor.\nUsers should pay additional attention as contiguity might have implicit performance impact.transpose()is a common example.\nadd()\ntranspose()\n\n```python\n>>> base = torch.tensor([[0, 1],[2, 3]])\n>>> base.is_contiguous()\nTrue\n>>> t = base.transpose(0, 1)  # `t` is a view of `base`. No data movement happened here.\n# View tensors might be non-contiguous.\n>>> t.is_contiguous()\nFalse\n# To get a contiguous tensor, call `.contiguous()` to enforce\n# copying data when `t` is not contiguous.\n>>> c = t.contiguous()\n\n```\n\nFor reference, here\u2019s a full list of view ops in PyTorch:\nBasic slicing and indexing op, e.g.tensor[0,2:,1:7:2]returns a view of basetensor, see note below.\ntensor[0,2:,1:7:2]\ntensor\nadjoint()\nadjoint()\nas_strided()\nas_strided()\ndetach()\ndetach()\ndiagonal()\ndiagonal()\nexpand()\nexpand()\nexpand_as()\nexpand_as()\nmovedim()\nmovedim()\nnarrow()\nnarrow()\npermute()\npermute()\nselect()\nselect()\nsqueeze()\nsqueeze()\ntranspose()\ntranspose()\nt()\nt()\nT\nT\nH\nH\nmT\nmT\nmH\nmH\nreal\n\nreal\nimag\n\nimag\nview_as_real()\nview_as_real()\nunflatten()\nunflatten()\nunfold()\nunfold()\nunsqueeze()\nunsqueeze()\nview()\nview()\nview_as()\nview_as()\nunbind()\nunbind()\nsplit()\nsplit()\nhsplit()\nhsplit()\nvsplit()\nvsplit()\ntensor_split()\ntensor_split()\nsplit_with_sizes()\nsplit_with_sizes()\nswapaxes()\nswapaxes()\nswapdims()\nswapdims()\nchunk()\nchunk()\nindices()(sparse tensor only)\nindices()\nvalues()(sparse tensor only)\nvalues()\nNote\nWhen accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors\nthat basic indexing returns views, while advanced indexing returns a copy.\nAssignment via either basic or advanced indexing is in-place. See more examples inNumpy indexing documentation.\nIt\u2019s also worth mentioning a few ops with special behaviors:\nreshape(),reshape_as()andflatten()can return either a view or new tensor, user code shouldn\u2019t rely on whether it\u2019s view or not.\nreshape()\nreshape_as()\nflatten()\ncontiguous()returnsitselfif input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data.\ncontiguous()\nFor a more detailed walk-through of PyTorch internal implementation,\nplease refer toezyang\u2019s blogpost about PyTorch Internals.",
  "url": "https://pytorch.org/docs/stable/tensor_view.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
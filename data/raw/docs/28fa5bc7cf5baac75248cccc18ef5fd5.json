{
  "doc_id": "28fa5bc7cf5baac75248cccc18ef5fd5",
  "source": "pytorch_docs",
  "title": "torch.special \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.special#\n\nCreated On: Mar 04, 2021 | Last Updated On: Jun 18, 2025\nThe torch.special module, modeled after SciPy\u2019sspecialmodule.\n\n## Functions#\n\nAiry functionAi(input)\\text{Ai}\\left(\\text{input}\\right)Ai(input).\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nBessel function of the first kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nBessel function of the first kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nBessel function of the second kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nBessel function of the second kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the first kindTn(input)T_{n}(\\text{input})Tn\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Ifn<6n < 6n<6or\u2223input\u2223>1|\\text{input}| > 1\u2223input\u2223>1the recursion:\nis evaluated. Otherwise, the explicit trigonometric formula:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the second kindUn(input)U_{n}(\\text{input})Un\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,2\u00d7input2 \\times \\text{input}2\u00d7inputis returned. Ifn<6n < 6n<6or\u2223input\u2223>1|\\text{input}| > 1\u2223input\u2223>1, the recursion:\nis evaluated. Otherwise, the explicit trigonometric formula:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the third kindVn\u2217(input)V_{n}^{\\ast}(\\text{input})Vn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the fourth kindWn\u2217(input)W_{n}^{\\ast}(\\text{input})Wn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nComputes the logarithmic derivative of the gamma function oninput.\ninput(Tensor) \u2013 the tensor to compute the digamma function on\nout(Tensor,optional) \u2013 the output tensor.\nNote\nThis function is similar to SciPy\u2019sscipy.special.digamma.\nNote\nFrom PyTorch 1.8 onwards, the digamma function returns-Inffor0.\nPreviously it returnedNaNfor0.\nExample:\n\n```python\n>>> a = torch.tensor([1, 0.5])\n>>> torch.special.digamma(a)\ntensor([-0.5772, -1.9635])\n\n```\n\nComputes the entropy oninput(as defined below), elementwise.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.arange(-0.5, 1, 0.5)\n>>> a\ntensor([-0.5000,  0.0000,  0.5000])\n>>> torch.special.entr(a)\ntensor([  -inf, 0.0000, 0.3466])\n\n```\n\nComputes the error function ofinput. The error function is defined as follows:\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])\n\n```\n\nComputes the complementary error function ofinput.\nThe complementary error function is defined as follows:\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])\n\n```\n\nComputes the scaled complementary error function for each element ofinput.\nThe scaled complementary error function is defined as follows:\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.erfcx(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 5.0090, 0.0561])\n\n```\n\nComputes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as:\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])\n\n```\n\nComputes the base two exponential function ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\ntensor([ 1.,  2.,  8., 16.])\n\n```\n\nComputes the expit (also known as the logistic sigmoid function) of the elements ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> t = torch.randn(4)\n>>> t\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n>>> torch.special.expit(t)\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\n\n```\n\nComputes the exponential of the elements minus 1\nofinput.\ninput\nNote\nThis function provides greater precision than exp(x) - 1 for small values of x.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])\n\n```\n\nComputes the regularized lower incomplete gamma function:\nwhere bothinputi\\text{input}_iinputi\u200bandotheri\\text{other}_iotheri\u200bare weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative thenouti=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5)in the equation above is the gamma function,\nSeetorch.special.gammaincc()andtorch.special.gammaln()for related functions.\ntorch.special.gammaincc()\ntorch.special.gammaln()\nSupportsbroadcasting to a common shapeand float inputs.\nNote\nThe backward pass with respect toinputis not yet supported.\nPlease open an issue on PyTorch\u2019s Github to request it.\ninput\ninput(Tensor) \u2013 the first non-negative input tensor\nother(Tensor) \u2013 the second non-negative input tensor\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.special.gammaincc(a1, a2)\ntensor([0.3528, 0.5665, 0.7350])\ntensor([0.3528, 0.5665, 0.7350])\n>>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)\ntensor([1., 1., 1.])\n\n```\n\nComputes the regularized upper incomplete gamma function:\nwhere bothinputi\\text{input}_iinputi\u200bandotheri\\text{other}_iotheri\u200bare weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative thenouti=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5)in the equation above is the gamma function,\nSeetorch.special.gammainc()andtorch.special.gammaln()for related functions.\ntorch.special.gammainc()\ntorch.special.gammaln()\nSupportsbroadcasting to a common shapeand float inputs.\nNote\nThe backward pass with respect toinputis not yet supported.\nPlease open an issue on PyTorch\u2019s Github to request it.\ninput\ninput(Tensor) \u2013 the first non-negative input tensor\nother(Tensor) \u2013 the second non-negative input tensor\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.special.gammaincc(a1, a2)\ntensor([0.6472, 0.4335, 0.2650])\n>>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)\ntensor([1., 1., 1.])\n\n```\n\nComputes the natural logarithm of the absolute value of the gamma function oninput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.special.gammaln(a)\ntensor([ 0.5724,  0.0000, -0.1208])\n\n```\n\nPhysicist\u2019s Hermite polynomialHn(input)H_{n}(\\text{input})Hn\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Otherwise, the recursion:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nProbabilist\u2019s Hermite polynomialHen(input)He_{n}(\\text{input})Hen\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Otherwise, the recursion:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nComputes the zeroth order modified Bessel function of the first kind for each element ofinput.\ninput\ninput(Tensor) \u2013 the input tensor\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.i0(torch.arange(5, dtype=torch.float32))\ntensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])\n\n```\n\nComputes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\n\n```\n\nComputes the first order modified Bessel function of the first kind (as defined below)\nfor each element ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.i1(torch.arange(5, dtype=torch.float32))\ntensor([0.0000, 0.5652, 1.5906, 3.9534, 9.7595])\n\n```\n\nComputes the exponentially scaled first order modified Bessel function of the first kind (as defined below)\nfor each element ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.i1e(torch.arange(5, dtype=torch.float32))\ntensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788])\n\n```\n\nLaguerre polynomialLn(input)L_{n}(\\text{input})Ln\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Otherwise, the recursion:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nLegendre polynomialPn(input)P_{n}(\\text{input})Pn\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Otherwise, the recursion:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nAlias fortorch.log1p().\ntorch.log1p()\nComputes the log of the area under the standard Gaussian probability density function,\nintegrated from minus infinity toinput, elementwise.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.log_ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))\ntensor([-6.6077 -3.7832 -1.841  -0.6931 -0.1728 -0.023  -0.0014])\n\n```\n\nComputes softmax followed by a logarithm.\nWhile mathematically equivalent to log(softmax(x)), doing these two\noperations separately is slower and numerically unstable. This function\nis computed as:\ninput(Tensor) \u2013 input\ndim(int) \u2013 A dimension along which log_softmax will be computed.\ndtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is cast todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.\ntorch.dtype\ndtype\nExample:\n\n```python\n>>> t = torch.ones(2, 2)\n>>> torch.special.log_softmax(t, 0)\ntensor([[-0.6931, -0.6931],\n        [-0.6931, -0.6931]])\n\n```\n\nReturns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN.\ninput\ninput\ninput\ninput\ninput(Tensor) \u2013 the input tensor.\neps(float,optional) \u2013 the epsilon for input clamp bound. Default:None\nNone\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.rand(5)\n>>> a\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\n>>> torch.special.logit(a, eps=1e-6)\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\n\n```\n\nAlias fortorch.logsumexp().\ntorch.logsumexp()\nModified Bessel function of the first kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nModified Bessel function of the first kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nModified Bessel function of the second kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nModified Bessel function of the second kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nComputes themultivariate log-gamma functionwith dimensionpppelement-wise, given by\nwhereC=log\u2061(\u03c0)\u22c5p(p\u22121)4C = \\log(\\pi) \\cdot \\frac{p (p - 1)}{4}C=log(\u03c0)\u22c54p(p\u22121)\u200band\u0393(\u2212)\\Gamma(-)\u0393(\u2212)is the Gamma function.\nAll elements must be greater thanp\u221212\\frac{p - 1}{2}2p\u22121\u200b, otherwise the behavior is undefined.\ninput(Tensor) \u2013 the tensor to compute the multivariate log-gamma function\np(int) \u2013 the number of dimensions\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.empty(2, 3).uniform_(1, 2)\n>>> a\ntensor([[1.6835, 1.8474, 1.1929],\n        [1.0475, 1.7162, 1.4180]])\n>>> torch.special.multigammaln(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n        [1.0311, 0.3901, 0.5049]])\n\n```\n\nComputes the area under the standard Gaussian probability density function,\nintegrated from minus infinity toinput, elementwise.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))\ntensor([0.0013, 0.0228, 0.1587, 0.5000, 0.8413, 0.9772, 0.9987])\n\n```\n\nComputes the argument, x, for which the area under the Gaussian probability density function\n(integrated from minus infinity to x) is equal toinput, elementwise.\ninput\nNote\nAlso known as quantile function for Normal Distribution.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.ndtri(torch.tensor([0, 0.25, 0.5, 0.75, 1]))\ntensor([   -inf, -0.6745,  0.0000,  0.6745,     inf])\n\n```\n\nComputes thenthn^{th}nthderivative of the digamma function oninput.n\u22650n \\geq 0n\u22650is called the order of the polygamma function.\ninput\nNote\nThis function is implemented only for nonnegative integersn\u22650n \\geq 0n\u22650.\nn(int) \u2013 the order of the polygamma function\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.tensor([1, 0.5])\n>>> torch.special.polygamma(1, a)\ntensor([1.64493, 4.9348])\n>>> torch.special.polygamma(2, a)\ntensor([ -2.4041, -16.8288])\n>>> torch.special.polygamma(3, a)\ntensor([ 6.4939, 97.4091])\n>>> torch.special.polygamma(4, a)\ntensor([ -24.8863, -771.4742])\n\n```\n\nAlias fortorch.special.digamma().\ntorch.special.digamma()\nAlias fortorch.round().\ntorch.round()\nScaled modified Bessel function of the second kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nScaled modified Bessel function of the second kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the first kindTn\u2217(input)T_{n}^{\\ast}(\\text{input})Tn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the second kindUn\u2217(input)U_{n}^{\\ast}(\\text{input})Un\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the third kindVn\u2217(input)V_{n}^{\\ast}(\\text{input})Vn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the fourth kindWn\u2217(input)W_{n}^{\\ast}(\\text{input})Wn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nComputes the normalized sinc ofinput.\ninput.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> t = torch.randn(4)\n>>> t\ntensor([ 0.2252, -0.2948,  1.0267, -1.1566])\n>>> torch.special.sinc(t)\ntensor([ 0.9186,  0.8631, -0.0259, -0.1300])\n\n```\n\nComputes the softmax function.\nSoftmax is defined as:\nSoftmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}Softmax(xi\u200b)=\u2211j\u200bexp(xj\u200b)exp(xi\u200b)\u200b\nIt is applied to all slices along dim, and will re-scale them so that the elements\nlie in the range[0, 1]and sum to 1.\ninput(Tensor) \u2013 input\ndim(int) \u2013 A dimension along which softmax will be computed.\ndtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is cast todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.\ntorch.dtype\ndtype\n\n```python\n>>> t = torch.ones(2, 2)\n>>> torch.special.softmax(t, 0)\ntensor([[0.5000, 0.5000],\n        [0.5000, 0.5000]])\n\n```\n\nSpherical Bessel function of the first kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nComputesinput*log1p(other)with the following cases.\ninput*log1p(other)\nSimilar to SciPy\u2019sscipy.special.xlog1py.\ninput(NumberorTensor) \u2013 Multiplier\nother(NumberorTensor) \u2013 Argument\nNote\nAt least one ofinputorothermust be a tensor.\ninput\nother\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])\n\n```\n\nComputesinput*log(other)with the following cases.\ninput*log(other)\nSimilar to SciPy\u2019sscipy.special.xlogy.\ninput(NumberorTensor) \u2013 Multiplier\nother(NumberorTensor) \u2013 Argument\nNote\nAt least one ofinputorothermust be a tensor.\ninput\nother\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlogy(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlogy(x, y)\ntensor([1.0986, 1.3863, 0.0000])\n>>> torch.special.xlogy(x, 4)\ntensor([1.3863, 2.7726, 4.1589])\n>>> torch.special.xlogy(2, y)\ntensor([2.1972, 1.3863, 0.0000])\n\n```\n\nComputes the Hurwitz zeta function, elementwise.\ninput(Tensor) \u2013 the input tensor corresponding tox.\nother(Tensor) \u2013 the input tensor corresponding toq.\nNote\nThe Riemann zeta function corresponds to the case whenq = 1\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> x = torch.tensor([2., 4.])\n>>> torch.special.zeta(x, 1)\ntensor([1.6449, 1.0823])\n>>> torch.special.zeta(x, torch.tensor([1., 2.]))\ntensor([1.6449, 0.0823])\n>>> torch.special.zeta(2, torch.tensor([1., 2.]))\ntensor([1.6449, 0.6449])\n\n```\n",
  "url": "https://pytorch.org/docs/stable/special.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
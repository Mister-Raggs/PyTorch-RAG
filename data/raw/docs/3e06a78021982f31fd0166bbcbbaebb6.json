{
  "doc_id": "3e06a78021982f31fd0166bbcbbaebb6",
  "source": "pytorch_docs",
  "title": "Broadcasting semantics \u2014 PyTorch 2.9 documentation",
  "text": "\n## Broadcasting semantics#\n\nCreated On: Apr 27, 2017 | Last Updated On: Jan 31, 2021\nMany PyTorch operations support NumPy\u2019s broadcasting semantics.\nSeehttps://numpy.org/doc/stable/user/basics.broadcasting.htmlfor details.\nIn short, if a PyTorch operation supports broadcast, then its Tensor arguments can be\nautomatically expanded to be of equal sizes (without making copies of the data).\n\n## General semantics#\n\nTwo tensors are \u201cbroadcastable\u201d if the following rules hold:\nEach tensor has at least one dimension.\nWhen iterating over the dimension sizes, starting at the trailing dimension,\nthe dimension sizes must either be equal, one of them is 1, or one of them\ndoes not exist.\nFor Example:\n\n```python\n>>> x=torch.empty(5,7,3)\n>>> y=torch.empty(5,7,3)\n# same shapes are always broadcastable (i.e. the above rules always hold)\n\n>>> x=torch.empty((0,))\n>>> y=torch.empty(2,2)\n# x and y are not broadcastable, because x does not have at least 1 dimension\n\n# can line up trailing dimensions\n>>> x=torch.empty(5,3,4,1)\n>>> y=torch.empty(  3,1,1)\n# x and y are broadcastable.\n# 1st trailing dimension: both have size 1\n# 2nd trailing dimension: y has size 1\n# 3rd trailing dimension: x size == y size\n# 4th trailing dimension: y dimension doesn't exist\n\n# but:\n>>> x=torch.empty(5,2,4,1)\n>>> y=torch.empty(  3,1,1)\n# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n\n```\n\nIf two tensorsx,yare \u201cbroadcastable\u201d, the resulting tensor size\nis calculated as follows:\nx\ny\nIf the number of dimensions ofxandyare not equal, prepend 1\nto the dimensions of the tensor with fewer dimensions to make them equal length.\nx\ny\nThen, for each dimension size, the resulting dimension size is the max of the sizes ofxandyalong that dimension.\nx\ny\nFor Example:\n\n```python\n# can line up trailing dimensions to make reading easier\n>>> x=torch.empty(5,1,4,1)\n>>> y=torch.empty(  3,1,1)\n>>> (x+y).size()\ntorch.Size([5, 3, 4, 1])\n\n# but not necessary:\n>>> x=torch.empty(1)\n>>> y=torch.empty(3,1,7)\n>>> (x+y).size()\ntorch.Size([3, 1, 7])\n\n>>> x=torch.empty(5,2,4,1)\n>>> y=torch.empty(3,1,1)\n>>> (x+y).size()\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n```\n\n\n## In-place semantics#\n\nOne complication is that in-place operations do not allow the in-place tensor to change shape\nas a result of the broadcast.\nFor Example:\n\n```python\n>>> x=torch.empty(5,3,4,1)\n>>> y=torch.empty(3,1,1)\n>>> (x.add_(y)).size()\ntorch.Size([5, 3, 4, 1])\n\n# but:\n>>> x=torch.empty(1,3,1)\n>>> y=torch.empty(3,1,7)\n>>> (x.add_(y)).size()\nRuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.\n\n```\n\n\n## Backwards compatibility#\n\nPrior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes,\nas long as the number of elements in each tensor was equal.  The pointwise operation would then be carried\nout by viewing each tensor as 1-dimensional.  PyTorch now supports broadcasting and the \u201c1-dimensional\u201d\npointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are\nnot broadcastable, but have the same number of elements.\nNote that the introduction of broadcasting can cause backwards incompatible changes in the case where\ntwo tensors do not have the same shape, but are broadcastable and have the same number of elements.\nFor Example:\n\n```python\n>>> torch.add(torch.ones(4,1), torch.randn(4))\n\n```\n\nwould previously produce a Tensor with size: torch.Size([4,1]), but now produces a Tensor with size: torch.Size([4,4]).\nIn order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist,\nyou may settorch.utils.backcompat.broadcast_warning.enabledtoTrue, which will generate a python warning\nin such cases.\nFor Example:\n\n```python\n>>> torch.utils.backcompat.broadcast_warning.enabled=True\n>>> torch.add(torch.ones(4,1), torch.ones(4))\n__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.\nChanging behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.\n\n```\n",
  "url": "https://pytorch.org/docs/stable/notes/broadcasting.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
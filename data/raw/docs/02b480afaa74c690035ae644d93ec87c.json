{
  "doc_id": "02b480afaa74c690035ae644d93ec87c",
  "source": "pytorch_docs",
  "title": "tlparse / TORCH_TRACE \u2014 PyTorch 2.9 documentation",
  "text": "\n## tlparse / TORCH_TRACE#\n\nCreated On: Jul 29, 2025 | Last Updated On: Jul 29, 2025\ntlparse /TORCH_TRACEare a pair of tools that produce compilation reports that looklike this.\nTORCH_TRACE\nTraces are fairly straightforward to collect. To collect a trace, run your model like so:\n\n```python\nTORCH_TRACE=\"/tmp/tracedir\" python foo.py\npip install tlparse\ntlparse /tmp/tracedir\n\n```\n\nThis approach works even if you are running a distributed job, providing a trace for each rank.\nIt will open your browser with HTML similar to what\u2019s generated above.\nIf you are making a bug report for a complicated problem that you don\u2019t have a standalone reproduction for,\nyou can still greatly assist PyTorch developers by attaching the trace log generated in/tmp/tracedir.\n/tmp/tracedir\nWarning\nThe trace log contains all of your model code.\nDo not share the trace log if the model you are working on is sensitive. The trace log does NOT contain weights.\nThe output oftlparseis primarily aimed for PyTorch developers,\nand the log format is easy to upload and share on GitHub.\nHowever,  as a non-PyTorch developer, you can still extract useful information from it.\nWe recommend starting with the inline help text in the report, which explains its contents.\nHere are some insights you can gain from atlparse:\ntlparse\ntlparse\nWhat model code was compiled by looking at the stack trie?\nThis is especially useful if you\u2019re not familiar with the codebase being compiled!\nHow many graph breaks / distinct compilation regions are there?\n(Each distinct compile is its own color coded block like[0/0]).\nFrames that are potentially graph-broken are light green[2/4].\nIf there are a lot of frames, that is suspicious, and suggests that you had some catastrophic graph breaks,\nor maybe your code isn\u2019t a good match fortorch.compile.\ntorch.compile\nHow many times did I recompile a particular frame? Something that recompiled a lot will look like:[10/0][10/1][10/2]- if something is being recompiled a lot, that is very suspicious and worth looking into, even if it isn\u2019t the root cause of your problem.\nWas there a compilation error? Frames that errored will look like[0/1].\nWhat intermediate compiler products did I generate for a given frame?\nFor example, you can look at the high-level generated FX graph or the generated Triton code.\nIs there relevant information for a particular frame? You can find these incompilation_metrics.\ncompilation_metrics\n\n## TORCH_LOGS#\n\nYou can use theTORCH_LOGSenvironment variable to selectively enable parts of thetorch.compilestack to log.TORCH_LOGSis in fact the source of logs fortlparse. The format of theTORCH_LOGSenvironment variable looks like this:\nTORCH_LOGS\ntorch.compile\nTORCH_LOGS\ntlparse\nTORCH_LOGS\n\n```python\nTORCH_LOGS=\"<option1>,<option2>,...\" python foo.py\n\n```\n\nYou can also programmatically set logging options usingtorch._logging.set_logs:\ntorch._logging.set_logs\n\n```python\nimport logging\ntorch._logging.set_logs(graph_breaks=True, dynamic=logging.DEBUG)\n\n```\n\nThe most useful options are:\ngraph_breaks: logs locations of graph breaks in user code and the reason for the graph break\ngraph_breaks\nguards: logs guards that are generated\nguards\nrecompiles: logs which function recompiled and the guards that failed, leading to the recompilation\nrecompiles\ndynamic: logs related to dynamic shapes\ndynamic\noutput_code: logs the code generated by Inductor\noutput_code\nSome more helpfulTORCH_LOGSoptions include:\nTORCH_LOGS\nOption\nDescription\n+all\nOutput debug logs from alltorch.compilecomponents\ntorch.compile\n+dynamo\nOutput debug logs from TorchDynamo\n+aot\nOutput debug logs from AOTAutograd\n+inductor\nOutput debug logs from TorchInductor\ndynamic\nOutput logs from dynamic shapes\ngraph_code\nOutput the Python code for the FX graph that Dynamo generated\ngraph_sizes\nOutput the tensor sizes of the FX graph that Dynamo generated\ntrace_bytecode\nOutput the bytecode instructions that Dynamo is tracing through and the symbolic interpreter stack Dynamo is keeping track of\ntrace_source\nOutput the line of code in the original source that Dynamo is currently tracing through\nbytecode\nOutput Dynamo-generated bytecode\nguards\nOutput generated guards\nrecompiles\nOutput recompilation reasons (only the first guard check that fails)\nrecompiles_verbose\nOutput all guard checks that fail when a recompilation occurs\naot_graphs\nOutput graph generated by AOTAutograd\naot_joint_graphs\nOutput the joint forward-backward graph generated by AOTAutograd\noutput_code\nOutput code generated by Inductor\nkernel_code\nOutput code generated by Inductor on a per-kernel basis\nschedule\nOutput Inductor scheduling logs\nperf_hints\nOutput Inductor perf hint logs\nfusion\nOutput Inductor fusion logs\nFor the full list of options, seetorch._loggingandtorch._logging.set_logs.\n\n## tlparse vs. TORCH_LOGS#\n\nGenerally, we suggest first usingtlparsewhen encountering issues.tlparseis ideal for debugging large models and gaining a high-level overview of how your model was compiled.\nOn the other hand,TORCH_LOGSis preferred for small examples and fine-grained debugging detail,\nwhen we already have an idea of whichtorch.compilecomponent is causing the problem.\ntlparse\ntlparse\nTORCH_LOGS\ntorch.compile",
  "url": "https://pytorch.org/docs/stable/compile/programming_model.observability.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "2d748e1eb180400a0189d9fb3bf4f830",
  "source": "pytorch_docs",
  "title": "TorchDynamo APIs for fine-grained tracing \u2014 PyTorch 2.9 documentation",
  "text": "\n## TorchDynamo APIs for fine-grained tracing#\n\nCreated On: Jul 28, 2023 | Last Updated On: Jun 16, 2025\nNote\nIn this documenttorch.compiler.compileandtorch.compileare used interchangeably.\nBoth versions will work in your code.\ntorch.compiler.compile\ntorch.compile\ntorch.compileperforms TorchDynamo tracing on the whole user model.\nHowever, it is possible that a small part of the model code cannot be\nhandled bytorch.compiler. In this case, you might want to disable\nthe compiler on that particular portion, while running compilation on\nthe rest of the model. This section describe the existing APIs that\nuse to define parts of your code in which you want to skip compilation\nand the relevant use cases.\ntorch.compile\ntorch.compiler\nThe API that you can use to define portions of the code on which you can\ndisable compilation are listed in the following table:\nAPI\nDescription\nWhen to use?\ntorch.compiler.disable\ntorch.compiler.disable\nDisables Dynamo on the decorated function as well as recursively invoked functions.\nExcellent for unblocking a user, if a small portion of the model cannot be handled withtorch.compile.\ntorch.compile\ntorch._dynamo.disallow_in_graph\ntorch._dynamo.disallow_in_graph\nDisallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.nnThis is suitable for the ops, whiletorch.compiler.disableis suitable for decorating functions.\ntorch.compiler.disable\nThis API is excellent for both debugging and unblocking if a custom op liketorch.ops.fbgemm.*is causing issues with thetorch.compilefunction.\ntorch.ops.fbgemm.*\ntorch.compile\ntorch.compile.allow_in_graph\ntorch.compile.allow_in_graph\nThe annotated callable goes as is in the TorchDynamo graph. For example, a black-box for TorchDynamo Dynamo.nnNote that AOT Autograd will trace through it, so theallow_in_graphis only a Dynamo-level concept.\nallow_in_graph\nThis API is useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks orautograd.Function. However, each usage ofallow_in_graphmust be carefully screened(no graph breaks, no closures).\nautograd.Function\nallow_in_graph\ntorch._dynamo.graph_break\ntorch._dynamo.graph_break\nAdds a graph break. The code before and after the graph break goes through TorchDynamo.\nRarely useful for deployment- If you think you need this, most probably you need eitherdisableordisallow_in_graph.\ndisable\ndisallow_in_graph\ntorch.compiler.is_compiling\ntorch.compiler.is_compiling\nIndicates whether a graph is executed/traced as part of torch.compile() or torch.export().\ntorch.compiler.is_dynamo_compiling\ntorch.compiler.is_dynamo_compiling\nIndicates whether a graph is traced via TorchDynamo. It\u2019s stricter than torch.compiler.is_compiling() flag, as it would only be set to True when TorchDynamo is used.\ntorch.compiler.is_exporting\ntorch.compiler.is_exporting\nIndicates whether a graph is traced via export. It\u2019s stricter than torch.compiler.is_compiling() flag, as it would only be set to True when torch.export is used.\n\n## torch.compiler.disable#\n\ntorch.compiler.disable\ntorch.compiler.disabledisables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame.\ntorch.compiler.disable\nTorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the functionfncalls functionsa_fnandb_fn. Anda_fncallsaa_fnandab_fn. When you use the PyTorch eager mode rather thantorch.compile, these function frames run as is. Withtorch.compile, TorchDynamo intercepts each of these function frames (indicated by the green color):\nfn\na_fn\nb_fn\na_fn\naa_fn\nab_fn\ntorch.compile\ntorch.compile\nLet\u2019s imagine, that functiona_fnis causing troubles withtorch.compile.\nAnd this is a non-critical portion of the model. You can usecompiler.disableon functiona_fn. As shown above, TorchDynamo will stop looking at frames\noriginating from thea_fncall (white color indicates original Python behavior).\na_fn\ntorch.compile\ncompiler.disable\na_fn\na_fn\nTo skip compilation, you can decorate the offending function with@torch.compiler.disable.\n@torch.compiler.disable\nYou can also use the non-decorator syntax if you don\u2019t want to change the source\ncode\nHowever, we recommend that you avoid this style if possible. Here, you have to\ntake care that all users of the original function are now using the patched\nversion.\n\n## torch._dynamo.disallow_in_graph#\n\ntorch._dynamo.disallow_in_graph\ntorch._dynamo.disallow_in_graphdisallows an operator but not the function\nto be present in the TorchDynamo extracted graph. Note that this is suitable\nfor operators and not general functions as in the case of_dynamo.disable.\ntorch._dynamo.disallow_in_graph\n_dynamo.disable\nLet\u2019s imagine you compile your model with PyTorch. TorchDynamo is able to\nextract a graph, but then you see the downstream compiler failing. For example,\nthe meta kernel is missing, or some Autograd dispatch key is set incorrectly\nfor a particular operator. Then you can mark that operator asdisallow_in_graph, and TorchDynamo will cause a graph break and run that\noperator by using the PyTorch eager mode.\ndisallow_in_graph\nThe catch is that you will have to find the corresponding Dynamo level operator,\nand not the ATen level operator. See more in the Limitations section of the doc.\nWarning\ntorch._dynamo.disallow_in_graphis a global flag. If you are comparing\ndifferent backend compilers, you might have to callallow_in_graphfor\nthe disallowed operator when switching to the other compiler.\ntorch._dynamo.disallow_in_graph\nallow_in_graph\n\n## torch.compiler.allow_in_graph#\n\ntorch.compiler.allow_in_graph\ntorch.compiler.allow_in_graphis useful when the relevant function frame\nhas some known hard-to-support TorchDynamo feature, such as hooks andautograd.Function, and you are confident that downstream PyTorch components\nsuch as AOTAutograd can safely trace through the decorated function. When a\nfunction is decorated withallow_in_graph, TorchDynamo treats it as a\nblack-box and puts it as is in the generated graph.\ntorch.compiler.allow_in_graph\nautograd.Function\nallow_in_graph\nWarning\nallow_in_graphskips TorchDynamo completely on the decorated function\nomitting all TorchDynamo safety checks, including graph breaks, handling\nclosures, and others. Useallow_in_graphwith caution. PyTorch downstream\ncomponents, such as AOTAutograd rely on TorchDynamo to handle complex Python\nfeatures, butallow_in_graphbypasses TorchDynamo. Usingallow_in_graphcould lead to soundness and hard-to-debug issues.\nallow_in_graph\nallow_in_graph\nallow_in_graph\nallow_in_graph\n\n## Limitations#\n\nAll the existing APIs are applied at the TorchDynamo level. Therefore, these\nAPIs have visibility to only what TorchDynamo sees. This can lead to confusing\nscenarios.\nFor example,torch._dynamo.disallow_in_graphwill not work for ATen operators\nbecause they are visible to AOT Autograd. For example,torch._dynamo.disallow_in_graph(torch.ops.aten.add)will not work in the\nabove example.\ntorch._dynamo.disallow_in_graph\ntorch._dynamo.disallow_in_graph(torch.ops.aten.add)",
  "url": "https://pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
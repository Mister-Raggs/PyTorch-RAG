{
  "doc_id": "d69fd0ab69d7a41ed024b6dc156d31ed",
  "source": "pytorch_docs",
  "title": "Rendezvous \u2014 PyTorch 2.9 documentation",
  "text": "\n## Rendezvous#\n\nCreated On: May 04, 2021 | Last Updated On: May 22, 2024\nIn the context of Torch Distributed Elastic we use the termrendezvousto\nrefer to a particular functionality that combines adistributed\nsynchronizationprimitive withpeer discovery.\nIt is used by Torch Distributed Elastic to gather participants of a training\njob (i.e. nodes) such that they all agree on the same list of participants and\neveryone\u2019s roles, as well as make a consistent collective decision on when\ntraining can begin/resume.\nTorch Distributed Elastic rendezvous provides the following critical\nfunctionalities:\nBarrier:\nNodes performing rendezvous will all block until the rendezvous is considered\ncomplete - this happens when at leastmintotal number of nodes have joined\nthe rendezvous barrier (for the same job). This also implies the barrier is not\nnecessarily of fixed size.\nmin\nThere\u2019s an additional small waiting time after reachingminnumber of\nnodes - this is used to ensure the rendezvous is not completed \u201ctoo quickly\u201d\n(which could potentially exclude additional nodes attempting to join at\napproximately the same time).\nmin\nIfmaxnumber of nodes is gathered at the barrier, the rendezvous is\ncompleted immediately.\nmax\nThere\u2019s also an overall timeout which causes the rendezvous to fail ifminnumber of nodes is never reached - this is meant to be a simple fail-safe to\nhelp release partially allocated job resources, in case there\u2019s a problem with\nthe resource manager, and is meant to be interpreted as non-retryable.\nmin\nExclusivity:\nA simple distributed barrier would not be sufficient, as we also need to ensure\nthat only one group of nodes exists at any given time (for a given job). In\nother words, new nodes (i.e. joining late) should not be able to form a parallel\nindependent group of workers for the same job.\nTorch Distributed Elastic rendezvous ensures that if a group of nodes has\nalready completed a rendezvous (and hence might already be training), then\nadditional \u201clate\u201d nodes attempting to rendezvous will only announce themselves\nas waiting, and will have to wait until the (previously completed) existing\nrendezvous is destroyed first.\nConsistency:\nWhen a rendezvous is completed, all its members will agree on the job membership\nand everyone\u2019s role in it. This role is represented using an integer, called\nrank, that is between between 0 and world size.\nNote that ranks arenot stable, in the sense that the same node can be\nassigned a different rank in the next (re-)rendezvous.\nFault-tolerance:\nTorch Distributed Elastic rendezvous is designed to tolerate node failures\nduring the rendezvous process. Should a process crash (or lose network\nconnectivity, etc), between joining the rendezvous and it being completed, then\na re-rendezvous with remaining healthy nodes will happen automatically.\nA node can also failafterit has completed (orhas been observedby other\nnodes to have completed) the rendezvous - this scenario will be handled by the\nTorch Distributed Elastictrain_loopinstead (where it will also trigger a\nre-rendezvous).\ntrain_loop\nShared key-value store:\nWhen the rendezvous is completed, a shared key-value store is created and\nreturned. This store implements atorch.distributed.StoreAPI (seedistributed communication docs).\ntorch.distributed.Store\nThis store is only shared by the members of the completed rendezvous. It\nis intended to be used by Torch Distributed Elastic to exchange information\nnecessary to initialize job control and data-planes.\nWaiting workers and rendezvous closing:\nTorch Distributed Elastic rendezvous handler object provides additional\nfunctionalities, which are technically not part of the rendezvous process:\nQuerying how many workers arrived late at the barrier, who can participate innextrendezvous.\nSetting the rendezvousclosedto signal all nodes not to participate in\nnext rendezvous.\nDynamicRendezvousHandler:\nTorch Distributed Elastic comes with theDynamicRendezvousHandlerclass that implements the rendezvous mechanism described above. It is a backend-\nagnostic type that expects a particularRendezvousBackendinstance\nto be specified during construction.\nDynamicRendezvousHandler\nRendezvousBackend\nTorch distributed users can either implement their own backend type or use one\nof the following implementations that come with PyTorch:\nC10dRendezvousBackend: Uses a C10d store (by defaultTCPStore) as the rendezvous backend. The main advantage of using a C10d\nstore is that it requires no 3rd-party dependency (such as etcd) to establish\na rendezvous.\nC10dRendezvousBackend\nTCPStore\nEtcdRendezvousBackend: Supersedes the legacyEtcdRendezvousHandlerclass. Passing anEtcdRendezvousBackendinstance toDynamicRendezvousHandleris functionally equivalent to\ninstantiating anEtcdRendezvousHandler.\nEtcdRendezvousBackend\nEtcdRendezvousHandler\nEtcdRendezvousBackend\nDynamicRendezvousHandler\nEtcdRendezvousHandler\n\n```python\nstore = TCPStore(\"localhost\")\n\nbackend = C10dRendezvousBackend(store, \"my_run_id\")\n\nrdzv_handler = DynamicRendezvousHandler.from_backend(\n    run_id=\"my_run_id\", store=store, backend=backend, min_nodes=2, max_nodes=4\n)\n\n```\n\nBelow is a state diagram describing how rendezvous works.\n\n## Registry#\n\nHold the parameters to construct aRendezvousHandler.\nRendezvousHandler\nbackend(str) \u2013 The name of the backend to use to handle the rendezvous.\nendpoint(str) \u2013 The endpoint of the rendezvous, usually in form <hostname>[:<port>].\nrun_id(str) \u2013 The id of the rendezvous.\nmin_nodes(int) \u2013 The minimum number of nodes to admit to the rendezvous.\nmax_nodes(int) \u2013 The maximum number of nodes to admit to the rendezvous.\nlocal_addr(Optional[str]) \u2013 The address of the local node.\n**kwargs\u2013 Additional parameters for the specified backend.\nReturn the value forkeyifkeyexists, elsedefault.\nkey\nkey\ndefault\nAny\nReturn the value forkeyas abool.\nkey\nbool\nOptional[bool]\nReturn the value forkeyas anint.\nkey\nint\nOptional[int]\nRepresent a registry ofRendezvousHandlerbackends.\nRendezvousHandler\n\n## Handler#\n\nMain rendezvous interface.\nNote\nDistributed Torch users normallydo notneed to implement their ownRendezvousHandler. An implementation based on C10d Store is already\nprovided, and is recommended for most users.\nRendezvousHandler\nReturn the name of the rendezvous backend.\nstr\nReturn the run id of the rendezvous.\nThe run id is a user-defined id that uniquely identifies an instance of\na distributed application. It typically maps to a job id and is used to\nallow nodes to join the correct distributed application.\nstr\nCheck whether the rendezvous has been closed.\nA closed rendezvous means all future attempts to re-rendezvous within\nsame job will fail.\nis_closed()andset_closed()have semantics of eventual\npropagation and should not be used for synchronization. The intention is\nthat if at least one node decides the job is finished, it will close the\nrendezvous, and other nodes will soon observe this and stop running as\nwell.\nis_closed()\nset_closed()\nbool\nMain entry-point into the rendezvous barrier.\nBlocks until the rendezvous is complete and the current process is\nincluded in the formed worker group, or a timeout occurs, or the\nrendezvous was marked closed.\nInstance ofRendezvousInfo.\nRendezvousInfo\nRendezvousClosedError\u2013 The rendezvous is closed.\nRendezvousConnectionError\u2013 The connection to the rendezvous backend has failed.\nRendezvousStateError\u2013 The rendezvous state is corrupt.\nRendezvousTimeoutError\u2013 The rendezvous did not complete on time.\nRendezvousInfo\nReturn the number of nodes who arrived late at the rendezvous\nbarrier, hence were not included in the current worker group.\nCallers should periodically call this method to check whether new\nnodes are waiting to join the job and if so admit them by callingnext_rendezvous()(re-rendezvous).\nnext_rendezvous()\nint\nMark the rendezvous as closed.\nClose all resources that were open for the rendezvous.\nExample:\n\n```python\nrdzv_handler = ...\ntry:\n    store, rank, world_size = rdzv_handler.next_rendezvous()\nfinally:\n    rdzv_handler.shutdown()\n\n```\n\nbool\nIndicates that store reference returned bynext_rendezvous()can be shared with user\napplications and will be available during application lifecycle.\nnext_rendezvous()\nRendezvous handler impl will share store details as instance ofRendezvousStoreInfo.\nApplications as a convention useMASTER_ADDR/MASTER_PORTenv variables to lookup the store.\nRendezvousStoreInfo\n\n## Dataclasses#\n\nHolds the information about the rendezvous.\nStore address and port that can be used to bootstrap trainer distributed comms\nFactory method, finds unused new port on rank0 host and addr/port info with all ranks.\nIf master_addr/master_port is knowns (useful when sharing existing tcp store server) use the constructor.\nrank(int) \u2013 rank of the current node\nstore(Store) \u2013 store to use for rendezvous\nlocal_addr(Optional[str]) \u2013 address of the current node, if not provided will be resolved from hostname\nserver_port(Optional[int]) \u2013 port of the TCPStore server, when the TCPStore is shared.\nRendezvousStoreInfo\n\n## Exceptions#\n\nRepresents the base type for rendezvous errors.\nRaised when a rendezvous is closed.\nRaised when a rendezvous did not complete on time.\nRaised when the connection to a rendezvous backend has failed.\nRaised when the state of a rendezvous is corrupt.\nRaised when node wasn\u2019t not included in rendezvous and gracefully exits.\nException is a mechanism to exit the stack, however does not mean a failure.\n\n## Implementations#\n\n\n## Dynamic Rendezvous#\n\nCreate a newDynamicRendezvousHandlerfrom the specified parameters.\nDynamicRendezvousHandler\nstore(Store) \u2013 The C10d store to return as part of the rendezvous.\nbackend(RendezvousBackend) \u2013 The backend to use to hold the rendezvous state.\nDynamicRendezvousHandler\nParameter\nDescription\njoin_timeout\nThe total time, in seconds, within which the\nrendezvous is expected to complete. Defaults to 600\nseconds.\nlast_call_timeout\nAn additional wait amount, in seconds, before\ncompleting the rendezvous once the minimum number of\nnodes has been reached. Defaults to 30 seconds.\nclose_timeout\nThe time, in seconds, within which the rendezvous is\nexpected to close after a call toRendezvousHandler.set_closed()orRendezvousHandler.shutdown(). Defaults to\n30 seconds.\nRendezvousHandler.set_closed()\nRendezvousHandler.shutdown()\nheartbeat\nThe time, in seconds, within which a keep-alive\nheartbeat is expected to complete\nRepresent a handler that sets up a rendezvous among a set of nodes.\nCreate a newDynamicRendezvousHandler.\nDynamicRendezvousHandler\nrun_id(str) \u2013 The run id of the rendezvous.\nstore(Store) \u2013 The C10d store to return as part of the rendezvous.\nbackend(RendezvousBackend) \u2013 The backend to use to hold the rendezvous state.\nmin_nodes(int) \u2013 The minimum number of nodes to admit to the rendezvous.\nmax_nodes(int) \u2013 The maximum number of nodes to admit to the rendezvous.\nlocal_addr(Optional[str]) \u2013 The local node address.\ntimeout(Optional[RendezvousTimeout]) \u2013 The timeout configuration of the rendezvous.\nkeep_alive_interval(int) \u2013 The amount of time a node waits before sending a heartbeat to keep\nit alive in the rendezvous.\nkeep_alive_max_attempt(int) \u2013 The maximum number of failed heartbeat attempts after which a node\nis considered dead.\nRepresent a backend that holds the rendezvous state.\nGet the rendezvous state.\nA tuple of the encoded rendezvous state and its fencing token orNoneif no state is found in the backend.\nNone\nRendezvousConnectionError\u2013 The connection to the backend has failed.\nRendezvousStateError\u2013 The rendezvous state is corrupt.\nOptional[tuple[bytes,Any]]\nGet the name of the backend.\nSet the rendezvous state.\nThe new rendezvous state is set conditionally:\nIf the specifiedtokenmatches the fencing token stored in the\nbackend, the state will be updated. The new state will be returned\nto the caller along with its fencing token.\ntoken\nIf the specifiedtokendoes not match the fencing token stored\nin the backend, the state won\u2019t be updated; instead the existing\nstate along with its fencing token will be returned to the caller.\ntoken\nIf the specifiedtokenisNone, the new state will be set\nonly if there is no existing state in the backend. Either the new\nstate or the existing state along with its fencing token will be\nreturned to the caller.\ntoken\nNone\nstate(bytes) \u2013 The encoded rendezvous state.\ntoken(Optional[Any]) \u2013 An optional fencing token that was retrieved by a previous call\ntoget_state()orset_state().\nget_state()\nset_state()\nA tuple of the serialized rendezvous state, its fencing token, and\na boolean value indicating whether our set attempt succeeded.\nRendezvousConnectionError\u2013 The connection to the backend has failed.\nRendezvousStateError\u2013 The rendezvous state is corrupt.\nOptional[tuple[bytes,Any,bool]]\nHold the timeout configuration of a rendezvous.\njoin(Optional[timedelta]) \u2013 The time within which the rendezvous is expected to complete.\nlast_call(Optional[timedelta]) \u2013 An additional wait amount before completing the rendezvous once the\nrendezvous has the minimum number of required participants.\nclose(Optional[timedelta]) \u2013 The time within which the rendezvous is expected to close after a\ncall toRendezvousHandler.set_closed()orRendezvousHandler.shutdown().\nRendezvousHandler.set_closed()\nRendezvousHandler.shutdown()\nheartbeat(Optional[timedelta]) \u2013 The time within which a keep-alive heartbeat is expected to\ncomplete.\nGet the close timeout.\nGet the keep-alive heartbeat timeout.\nGet the join timeout.\nGet the last call timeout.\nCreate a newC10dRendezvousBackendfrom the specified parameters.\nC10dRendezvousBackend\nParameter\nDescription\nstore_type\nThe type of the C10d store. The currently supported types\nare \u201ctcp\u201d and \u201cfile\u201d which correspond totorch.distributed.TCPStoreandtorch.distributed.FileStore, respectively.\nDefaults to \u201ctcp\u201d.\ntorch.distributed.TCPStore\ntorch.distributed.FileStore\nread_timeout\nThe read timeout, in seconds, for store operations.\nDefaults to 60 seconds.\nNote this only applies totorch.distributed.TCPStore. It is not relevant\ntotorch.distributed.FileStorewhich does not\ntake in timeout as a parameter.\ntorch.distributed.TCPStore\ntorch.distributed.FileStore\nis_host\nA boolean value indicating whether this backend instance\nwill host the C10d store. If not specified it will be\ninferred heuristically by matching the hostname or the IP\naddress of this machine against the specified rendezvous\nendpoint. Defaults toNone.\nNone\nNote that this configuration option only applies totorch.distributed.TCPStore. In normal\ncircumstances you can safely skip it; the only time when\nit is needed is if its value cannot be correctly\ndetermined (e.g. the rendezvous endpoint has a CNAME as\nthe hostname or does not match the FQDN of the machine).\ntorch.distributed.TCPStore\ntuple[torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend,torch.distributed.distributed_c10d.Store]\nRepresents a C10d-backed rendezvous backend.\nstore(Store) \u2013 Thetorch.distributed.Storeinstance to use to\ncommunicate with the C10d store.\ntorch.distributed.Store\nrun_id(str) \u2013 The run id of the rendezvous.\nSee base class.\nOptional[tuple[bytes,Any]]\nSee base class.\nSee base class.\nOptional[tuple[bytes,Any,bool]]\nCreate a newEtcdRendezvousBackendfrom the specified parameters.\nEtcdRendezvousBackend\nParameter\nDescription\nread_timeout\nThe read timeout, in seconds, for etcd operations.\nDefaults to 60 seconds.\nprotocol\nThe protocol to use to communicate with etcd. Valid\nvalues are \u201chttp\u201d and \u201chttps\u201d. Defaults to \u201chttp\u201d.\nssl_cert\nThe path to the SSL client certificate to use along with\nHTTPS. Defaults toNone.\nNone\nssl_cert_key\nThe path to the private key of the SSL client certificate\nto use along with HTTPS. Defaults toNone.\nNone\nca_cert\nThe path to the rool SSL authority certificate. Defaults\ntoNone.\nNone\ntuple[torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend,torch.distributed.distributed_c10d.Store]\nRepresents an etcd-based rendezvous backend.\nclient(Client) \u2013 Theetcd.Clientinstance to use to communicate with etcd.\netcd.Client\nrun_id(str) \u2013 The run id of the rendezvous.\nkey_prefix(Optional[str]) \u2013 The path under which to store the rendezvous state in etcd.\nttl(Optional[int]) \u2013 The TTL of the rendezvous state. If not specified, defaults to two hours.\nSee base class.\nOptional[tuple[bytes,Any]]\nSee base class.\nSee base class.\nOptional[tuple[bytes,Any,bool]]\n\n## Etcd Rendezvous (Legacy)#\n\nWarning\nTheDynamicRendezvousHandlerclass supersedes theEtcdRendezvousHandlerclass, and is recommended for most users.EtcdRendezvousHandleris in\nmaintenance mode and will be deprecated in the future.\nDynamicRendezvousHandler\nEtcdRendezvousHandler\nEtcdRendezvousHandler\nImplements atorch.distributed.elastic.rendezvous.RendezvousHandlerinterface\nbacked bytorch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.EtcdRendezvousHandleruses a URL to configure the type of rendezvous to\nuse and to pass implementation specific configurations to the rendezvous\nmodule. The basic etcd rendezvous configuration URL looks like the following\ntorch.distributed.elastic.rendezvous.RendezvousHandler\ntorch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous\nEtcdRendezvousHandler\n\n```python\netcd://<etcd_address>:<port>/<job_id>?min_workers=<min_workers>&max_workers=<max_workers>  # noqa: W605\n\n-- example --\n\netcd://localhost:2379/1234?min_workers=1&max_workers=3\n\n```\n\nThe URL above is interpreted as follows:\nUse the rendezvous handler that is registered with theetcdscheme\netcd\nTheetcdendpoint to use islocalhost:2379\netcd\nlocalhost:2379\njob_id==1234is used as the prefix in etcd (this allows one to\nshare a common etcd server for multiple jobs so long as thejob_idsare guaranteed to be unique). Note that the job id can be\nany string (e.g. does not need to be a number) as long as it is\nunique.\njob_id==1234\njob_ids\nmin_workers=1andmax_workers=3specifies a range for\nmembership size - Torch Distributed Elastic starts running the job as\nlong as the cluster size is greater than or equal tomin_workersand admits up tomax_workersinto the cluster.\nmin_workers=1\nmax_workers=3\nmin_workers\nmax_workers\nBelow are a full list of the parameters that can be passed to etcd\nrendezvous:\nParameter\nDescription\nmin_workers\nminimum number of\nworkers for the\nrendezvous to be valid\nmax_workers\nmaximum number of\nworkers to admit\ntimeout\ntotal timeout within\nwhich next_rendezvous is\nexpected to succeed\n(default 600s)\nlast_call_timeout\nadditional wait amount\n(\u201clast call\u201d) after min\nnumber of workers has\nbeen reached (defaults\nto 30s)\netcd_prefix\npath prefix (from etcd\nroot), inside which all\netcd nodes will be\ncreated (defaults to/torchelastic/p2p)\n/torchelastic/p2p\n\n## Etcd Store#\n\nTheEtcdStoreis the C10dStoreinstance type returned bynext_rendezvous()when etcd is used as the rendezvous backend.\nEtcdStore\nStore\nnext_rendezvous()\nImplement a c10 Store interface by piggybacking on the rendezvous etcd instance.\nThis is the store object returned byEtcdRendezvous.\nEtcdRendezvous\nAtomically increment a value by an integer amount.\nThe integer is represented as a string using base 10. If key is not present,\na default value of0will be assumed.\n0\nthe new (incremented) value\nint\nCheck if all of the keys are immediately present (without waiting).\nbool\nGet a value by key, possibly doing a blocking wait.\nIf key is not immediately present, will do a blocking wait\nfor at mosttimeoutduration or until the key is published.\ntimeout\nvalue(bytes)\n(bytes)\nLookupError - If key still not published after timeout\u2013\nbytes\nWrite a key/value pair intoEtcdStore.\nEtcdStore\nBoth key and value may be either Pythonstrorbytes.\nstr\nbytes\nWait until all of the keys are published, or until timeout.\nLookupError - if timeout occurs\u2013\n\n## Etcd Server#\n\nTheEtcdServeris a convenience class that makes it easy for you to\nstart and stop an etcd server on a subprocess. This is useful for testing\nor single-node (multi-worker) deployments where manually setting up an\netcd server on the side is cumbersome.\nEtcdServer\nWarning\nFor production and multi-node deployments please consider\nproperly deploying a highly available etcd server as this is\nthe single point of failure for your distributed jobs.\nNote\ntested on etcd server v3.4.3.\nStarts and stops a local standalone etcd server on a random free\nport. Useful for single node, multi-worker launches or testing,\nwhere a sidecar etcd server is more convenient than having to\nseparately setup an etcd server.\nThis class registers a termination handler to shutdown the etcd\nsubprocess on exit. This termination handler is NOT a substitute for\ncalling thestop()method.\nstop()\nThe following fallback mechanism is used to find the etcd binary:\nUses env var TORCHELASTIC_ETCD_BINARY_PATH\nUses<thisfileroot>/bin/etcdif one exists\n<thisfileroot>/bin/etcd\nUsesetcdfromPATH\netcd\nPATH\nUsage\n\n```python\nserver = EtcdServer(\"/usr/bin/etcd\", 2379, \"/tmp/default.etcd\")\nserver.start()\nclient = server.get_client()\n# use client\nserver.stop()\n\n```\n\netcd_binary_path\u2013 path of etcd server binary (see above for fallback path)",
  "url": "https://pytorch.org/docs/stable/elastic/rendezvous.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "2135a7d1f705bad69b6099f2fb7722e2",
  "source": "pytorch_docs",
  "title": "Frequently Asked Questions \u2014 PyTorch 2.9 documentation",
  "text": "\n## Frequently Asked Questions#\n\nCreated On: Feb 15, 2018 | Last Updated On: Aug 05, 2021\n\n## My model reports \u201ccuda runtime error(2): out of memory\u201d#\n\nAs the error message suggests, you have run out of memory on your\nGPU.  Since we often deal with large amounts of data in PyTorch,\nsmall mistakes can rapidly cause your program to use up all of your\nGPU; fortunately, the fixes in these cases are often simple.\nHere are a few common things to check:\nDon\u2019t accumulate history across your training loop.By default, computations involving variables that require gradients\nwill keep history.  This means that you should avoid using such\nvariables in computations which will live beyond your training loops,\ne.g., when tracking statistics. Instead, you should detach the variable\nor access its underlying data.\nSometimes, it can be non-obvious when differentiable variables can\noccur.  Consider the following training loop (abridged fromsource):\n\n```python\ntotal_loss = 0\nfor i in range(10000):\n    optimizer.zero_grad()\n    output = model(input)\n    loss = criterion(output)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss\n\n```\n\nHere,total_lossis accumulating history across your training loop, sincelossis a differentiable variable with autograd history. You can fix this by\nwritingtotal_loss += float(loss)instead.\ntotal_loss\nloss\nOther instances of this problem:1.\nDon\u2019t hold onto tensors and variables you don\u2019t need.If you assign a Tensor or Variable to a local, Python will not\ndeallocate until the local goes out of scope.  You can free\nthis reference by usingdelx.  Similarly, if you assign\na Tensor or Variable to a member variable of an object, it will\nnot deallocate until the object goes out of scope.  You will\nget the best memory usage if you don\u2019t hold onto temporaries\nyou don\u2019t need.\ndelx\nThe scopes of locals can be larger than you expect.  For example:\n\n```python\nfor i in range(5):\n    intermediate = f(input[i])\n    result += g(intermediate)\noutput = h(result)\nreturn output\n\n```\n\nHere,intermediateremains live even whilehis executing,\nbecause its scope extrudes past the end of the loop.  To free it\nearlier, you shoulddelintermediatewhen you are done with it.\nintermediate\nh\ndelintermediate\nAvoid running RNNs on sequences that are too large.The amount of memory required to backpropagate through an RNN scales\nlinearly with the length of the RNN input; thus, you will run out of memory\nif you try to feed an RNN a sequence that is too long.\nThe technical term for this phenomenon isbackpropagation through time,\nand there are plenty of references for how to implement truncated\nBPTT, including in theword language modelexample; truncation is handled by therepackagefunction as described inthis forum post.\nrepackage\nDon\u2019t use linear layers that are too large.A linear layernn.Linear(m,n)usesO(nm)O(nm)O(nm)memory: that is to say,\nthe memory requirements of the weights\nscales quadratically with the number of features.  It is very easy\ntoblow through your memorythis way (and remember that you will need at least twice the size of the\nweights, since you also need to store the gradients.)\nnn.Linear(m,n)\nConsider checkpointing.You can trade-off memory for compute by usingcheckpoint.\n\n## My GPU memory isn\u2019t freed properly#\n\nPyTorch uses a caching memory allocator to speed up memory allocations. As a\nresult, the values shown innvidia-smiusually don\u2019t reflect the true\nmemory usage. SeeMemory managementfor more details about GPU\nmemory management.\nnvidia-smi\nIf your GPU memory isn\u2019t freed even after Python quits, it is very likely that\nsome Python subprocesses are still alive. You may find them viaps-elf|greppythonand manually kill them withkill-9[pid].\nps-elf|greppython\nkill-9[pid]\n\n## My out of memory exception handler can\u2019t allocate memory#\n\nYou may have some code that tries to recover from out of memory errors.\n\n```python\ntry:\n    run_model(batch_size)\nexcept RuntimeError: # Out of memory\n    for _ in range(batch_size):\n        run_model(1)\n\n```\n\nBut find that when you do run out of memory, your recovery code can\u2019t allocate\neither. That\u2019s because the python exception object holds a reference to the\nstack frame where the error was raised. Which prevents the original tensor\nobjects from being freed. The solution is to move you OOM recovery code outside\nof theexceptclause.\nexcept\n\n```python\noom = False\ntry:\n    run_model(batch_size)\nexcept RuntimeError: # Out of memory\n    oom = True\n\nif oom:\n    for _ in range(batch_size):\n        run_model(1)\n\n```\n\n\n## My data loader workers return identical random numbers#\n\nYou are likely using other libraries to generate random numbers in the dataset\nand worker subprocesses are started viafork. Seetorch.utils.data.DataLoader\u2019s documentation for how to\nproperly set up random seeds in workers with itsworker_init_fnoption.\nfork\ntorch.utils.data.DataLoader\nworker_init_fn\n\n## My recurrent network doesn\u2019t work with data parallelism#\n\nThere is a subtlety in using thepacksequence->recurrentnetwork->unpacksequencepattern in aModulewithDataParallelordata_parallel(). Input to each theforward()on\neach device will only be part of the entire input. Because the unpack operationtorch.nn.utils.rnn.pad_packed_sequence()by default only pads up to the\nlongest input it sees, i.e., the longest on that particular device, size\nmismatches will happen when results are gathered together. Therefore, you can\ninstead take advantage of thetotal_lengthargument ofpad_packed_sequence()to make sure that theforward()calls return sequences of same length. For example, you can\nwrite:\npacksequence->recurrentnetwork->unpacksequence\nModule\nDataParallel\ndata_parallel()\nforward()\ntorch.nn.utils.rnn.pad_packed_sequence()\ntotal_length\npad_packed_sequence()\nforward()\n\n```python\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass MyModule(nn.Module):\n    # ... __init__, other methods, etc.\n\n    # padded_input is of shape [B x T x *] (batch_first mode) and contains\n    # the sequences sorted by lengths\n    #   B is the batch size\n    #   T is max sequence length\n    def forward(self, padded_input, input_lengths):\n        total_length = padded_input.size(1)  # get the max sequence length\n        packed_input = pack_padded_sequence(padded_input, input_lengths,\n                                            batch_first=True)\n        packed_output, _ = self.my_lstm(packed_input)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True,\n                                        total_length=total_length)\n        return output\n\n\nm = MyModule().cuda()\ndp_m = nn.DataParallel(m)\n\n```\n\nAdditionally, extra care needs to be taken when batch dimension is dim1(i.e.,batch_first=False) with data parallelism. In this case, the first\nargument of pack_padded_sequencepadding_inputwill be of shape[TxBx*]and should be scattered along dim1, but the second argumentinput_lengthswill be of shape[B]and should be scattered along dim0. Extra code to manipulate the tensor shapes will be needed.\n1\nbatch_first=False\npadding_input\n[TxBx*]\n1\ninput_lengths\n[B]\n0",
  "url": "https://pytorch.org/docs/stable/notes/faq.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
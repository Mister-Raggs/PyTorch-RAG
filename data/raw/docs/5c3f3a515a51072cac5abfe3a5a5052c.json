{
  "doc_id": "5c3f3a515a51072cac5abfe3a5a5052c",
  "source": "pytorch_docs",
  "title": "torch.library \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.library#\n\nCreated On: Jun 13, 2022 | Last Updated On: Aug 13, 2025\ntorch.library is a collection of APIs for extending PyTorch\u2019s core library\nof operators. It contains utilities for testing custom operators, creating new\ncustom operators, and extending operators defined with PyTorch\u2019s C++ operator\nregistration APIs (e.g. aten operators).\nFor a detailed guide on effectively using these APIs, please seePyTorch Custom Operators Landing Pagefor more details on how to effectively use these APIs.\n\n## Testing custom ops#\n\nUsetorch.library.opcheck()to test custom ops for incorrect usage of the\nPython torch.library and/or C++ TORCH_LIBRARY APIs. Also, if your operator supports\ntraining, usetorch.autograd.gradcheck()to test that the gradients are\nmathematically correct.\ntorch.library.opcheck()\ntorch.autograd.gradcheck()\nGiven an operator and some sample arguments, tests if the operator is\nregistered correctly.\nThat is, when you use the torch.library/TORCH_LIBRARY APIs to create a\ncustom op, you specified metadata (e.g. mutability info) about the custom op\nand these APIs require that the functions you pass them satisfy certain\nproperties (e.g. no data pointer access in the fake/meta/abstract kernel)opchecktests these metadata and properties.\nopcheck\nConcretely, we test the following:\ntest_schema: If the schema matches the implementation of\nthe operator. For example: if the schema specifies a Tensor is mutated,\nthen we check the implementation mutates the Tensor. If the schema\nspecifies that we return a new Tensor, then we check that the\nimplementation returns a new Tensor (instead of an existing one or\na view of an existing one).\ntest_autograd_registration: If the operator supports training\n(autograd): we check that its autograd formula is registered via\ntorch.library.register_autograd or a manual registration to one\nor more DispatchKey::Autograd keys. Any other DispatchKey-based\nregistrations may lead to undefined behavior.\ntest_faketensor: If the operator has a FakeTensor kernel\n(and if it is correct). The FakeTensor kernel is necessary (\nbut not sufficient) for the operator to work with PyTorch compilation\nAPIs (torch.compile/export/FX). We check that a FakeTensor kernel\n(also sometimes known as a meta kernel) was registered for the\noperator and that it is correct. This test takes the result of\nrunning the operator on real tensors and the result of running\nthe operator on FakeTensors and checks that they have the same\nTensor metadata (sizes/strides/dtype/device/etc).\ntest_aot_dispatch_dynamic: If the operator has correct behavior\nwith PyTorch compilation APIs (torch.compile/export/FX).\nThis checks that the outputs (and gradients, if applicable) are the\nsame under eager-mode PyTorch and torch.compile.\nThis test is a superset oftest_faketensorand is an e2e test;\nother things it tests are that the operator supports\nfunctionalization and that the backward pass (if it exists) also\nsupports FakeTensor and functionalization.\ntest_faketensor\nFor best results, please callopcheckmultiple times with a\nrepresentative set of inputs. If your operator supports\nautograd, please useopcheckwith inputs withrequires_grad=True;\nif your operator supports multiple devices (e.g. CPU and CUDA), please\nuseopcheckwith inputs on all supported devices.\nopcheck\nopcheck\nrequires_grad=True\nopcheck\nop(Union[OpOverload,OpOverloadPacket,CustomOpDef]) \u2013 The operator. Must either be a function decorated withtorch.library.custom_op()or an OpOverload/OpOverloadPacket\nfound in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)\ntorch.library.custom_op()\nargs(tuple[Any,...]) \u2013 The args to the operator\nkwargs(Optional[dict[str,Any]]) \u2013 The kwargs to the operator\ntest_utils(Union[str,Sequence[str]]) \u2013 Tests that we should run. Default: all of them.\nExample: (\u201ctest_schema\u201d, \u201ctest_faketensor\u201d)\nraise_exception(bool) \u2013 If we should raise an exception on the first\nerror. If False, we will return a dict with information\non if each test passed or not.\nrtol(Optional[float]) \u2013 Relative tolerance for floating point comparisons.\nIf specifiedatolmust also be specified.\nIf omitted, default values based on thedtypeare selected\n(see the table intorch.testing.assert_close()).\natol\ndtype\ntorch.testing.assert_close()\natol(Optional[float]) \u2013 Absolute tolerance for floating point comparisons.\nIf specifiedrtolmust also be specified.\nIf omitted, default values based on thedtypeare selected\n(see the table intorch.testing.assert_close()).\nrtol\ndtype\ntorch.testing.assert_close()\ndict[str,str]\nWarning\nopcheck andtorch.autograd.gradcheck()test different things;\nopcheck tests if your usage of torch.library APIs is correct whiletorch.autograd.gradcheck()tests if your autograd formula is\nmathematically correct. Use both to test custom ops that support\ngradient computation.\ntorch.autograd.gradcheck()\ntorch.autograd.gradcheck()\nExample\n\n```python\n>>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\n>>> def numpy_mul(x: Tensor, y: float) -> Tensor:\n>>>     x_np = x.numpy(force=True)\n>>>     z_np = x_np * y\n>>>     return torch.from_numpy(z_np).to(x.device)\n>>>\n>>> @numpy_mul.register_fake\n>>> def _(x, y):\n>>>     return torch.empty_like(x)\n>>>\n>>> def setup_context(ctx, inputs, output):\n>>>     y, = inputs\n>>>     ctx.y = y\n>>>\n>>> def backward(ctx, grad):\n>>>     return grad * ctx.y, None\n>>>\n>>> numpy_mul.register_autograd(backward, setup_context=setup_context)\n>>>\n>>> sample_inputs = [\n>>>     (torch.randn(3), 3.14),\n>>>     (torch.randn(2, 3, device='cuda'), 2.718),\n>>>     (torch.randn(1, 10, requires_grad=True), 1.234),\n>>>     (torch.randn(64, 64, device='cuda', requires_grad=True), 90.18),\n>>> ]\n>>>\n>>> for args in sample_inputs:\n>>>     torch.library.opcheck(numpy_mul, args)\n\n```\n\n\n## Creating new custom ops in Python#\n\nUsetorch.library.custom_op()to create new custom ops.\ntorch.library.custom_op()\nWraps a function into custom operator.\nReasons why you may want to create a custom op include:\n- Wrapping a third-party library or custom kernel to work with PyTorch\nsubsystems like Autograd.\n- Preventing torch.compile/export/FX tracing from peeking inside your function.\nThis API is used as a decorator around a function (please see examples).\nThe provided function must have type hints; these are needed to interface\nwith PyTorch\u2019s various subsystems.\nname(str) \u2013 A name for the custom op that looks like \u201c{namespace}::{name}\u201d,\ne.g. \u201cmylib::my_linear\u201d. The name is used as the op\u2019s stable identifier\nin PyTorch subsystems (e.g. torch.export, FX graphs).\nTo avoid name collisions, please use your project name as the namespace;\ne.g. all custom ops in pytorch/fbgemm use \u201cfbgemm\u201d as the namespace.\nmutates_args(Iterable[str] or\"unknown\") \u2013 The names of args that the function mutates.\nThis MUST be accurate, otherwise, the behavior is undefined. If \u201cunknown\u201d,\nit pessimistically assumes that all inputs to the operator are being mutated.\ndevice_types(None|str|Sequence[str]) \u2013 The device type(s) the function\nis valid for. If no device type is provided, then the function\nis used as the default implementation for all device types.\nExamples: \u201ccpu\u201d, \u201ccuda\u201d.\nWhen registering a device-specific implementation for an operator that accepts no Tensors,\nwe require the operator to have a \u201cdevice: torch.device argument\u201d.\nschema(None|str) \u2013 A schema string for the operator. If None\n(recommended) we\u2019ll infer a schema for the operator from its type\nannotations. We recommend letting us infer a schema unless you\nhave a specific reason not to.\nExample: \u201c(Tensor x, int y) -> (Tensor, Tensor)\u201d.\nUnion[Callable[[Callable[[\u2026],object]],CustomOpDef],CustomOpDef]\nNote\nWe recommend not passing in aschemaarg and instead letting us infer\nit from the type annotations. It is error-prone to write your own schema.\nYou may wish to provide your own schema if our interpretation of\nthe type annotation is not what you want.\nFor more info on how to write a schema string, seehere\nschema\n\n```python\n>>> import torch\n>>> from torch import Tensor\n>>> from torch.library import custom_op\n>>> import numpy as np\n>>>\n>>> @custom_op(\"mylib::numpy_sin\", mutates_args=())\n>>> def numpy_sin(x: Tensor) -> Tensor:\n>>>     x_np = x.cpu().numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np).to(device=x.device)\n>>>\n>>> x = torch.randn(3)\n>>> y = numpy_sin(x)\n>>> assert torch.allclose(y, x.sin())\n>>>\n>>> # Example of a custom op that only works for one device type.\n>>> @custom_op(\"mylib::numpy_sin_cpu\", mutates_args=(), device_types=\"cpu\")\n>>> def numpy_sin_cpu(x: Tensor) -> Tensor:\n>>>     x_np = x.numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np)\n>>>\n>>> x = torch.randn(3)\n>>> y = numpy_sin_cpu(x)\n>>> assert torch.allclose(y, x.sin())\n>>>\n>>> # Example of a custom op that mutates an input\n>>> @custom_op(\"mylib::numpy_sin_inplace\", mutates_args={\"x\"}, device_types=\"cpu\")\n>>> def numpy_sin_inplace(x: Tensor) -> None:\n>>>     x_np = x.numpy()\n>>>     np.sin(x_np, out=x_np)\n>>>\n>>> x = torch.randn(3)\n>>> expected = x.sin()\n>>> numpy_sin_inplace(x)\n>>> assert torch.allclose(x, expected)\n>>>\n>>> # Example of a factory function\n>>> @torch.library.custom_op(\"mylib::bar\", mutates_args={}, device_types=\"cpu\")\n>>> def bar(device: torch.device) -> Tensor:\n>>>     return torch.ones(3)\n>>>\n>>> bar(\"cpu\")\n\n```\n\nCreate a custom operator whose implementation is backed by 1+ triton kernels.\nThis is a more structured way of using triton kernels with PyTorch.\nPrefer using triton kernels with notorch.librarycustom operator wrappers\n(liketorch.library.custom_op(),torch.library.triton_op()) because\nthat is simpler;\nonly usetorch.library.custom_op()/torch.library.triton_op()if you\nwant to create an operator that behaves like PyTorch built-in operators.\nFor example, you may use atorch.librarywrapper API to define the\nbehavior of the triton kernel when passed a tensor subclass or under\na TorchDispatchMode.\ntorch.library\ntorch.library.custom_op()\ntorch.library.triton_op()\ntorch.library.custom_op()\ntorch.library.triton_op()\ntorch.library\nUsetorch.library.triton_op()instead oftorch.library.custom_op()when the implementation\nconsists of 1+ triton kernels.torch.library.custom_op()treats\ncustom operators as opaque (torch.compile()andtorch.export.export()will never trace into them), buttriton_opmakes the implementation visible to these subsystems, allowing them\nto optimize the triton kernel(s).\ntorch.library.triton_op()\ntorch.library.custom_op()\ntorch.library.custom_op()\ntorch.compile()\ntorch.export.export()\ntriton_op\nNote thatfnmust only consist of calls to PyTorch-understood\noperators and triton kernels. Any triton kernels called insidefnmust be wrapped in a call totorch.library.wrap_triton().\nfn\nfn\ntorch.library.wrap_triton()\nname(str) \u2013 A name for the custom op that looks like \u201c{namespace}::{name}\u201d,\ne.g. \u201cmylib::my_linear\u201d. The name is used as the op\u2019s stable identifier\nin PyTorch subsystems (e.g. torch.export, FX graphs).\nTo avoid name collisions, please use your project name as the namespace;\ne.g. all custom ops in pytorch/fbgemm use \u201cfbgemm\u201d as the namespace.\nmutates_args(Iterable[str] or\"unknown\") \u2013 The names of args that the function mutates.\nThis MUST be accurate, otherwise, the behavior is undefined. If \u201cunknown\u201d,\nit pessimistically assumes that all inputs to the operator are being mutated.\nschema(None|str) \u2013 A schema string for the operator. If None\n(recommended) we\u2019ll infer a schema for the operator from its type\nannotations. We recommend letting us infer a schema unless you\nhave a specific reason not to.\nExample: \u201c(Tensor x, int y) -> (Tensor, Tensor)\u201d.\nCallable\nExample:\n\n```python\n>>> import torch\n>>> from torch.library import triton_op, wrap_triton\n>>>\n>>> import triton\n>>> from triton import language as tl\n>>>\n>>> @triton.jit\n>>> def add_kernel(\n>>>     in_ptr0,\n>>>     in_ptr1,\n>>>     out_ptr,\n>>>     n_elements,\n>>>     BLOCK_SIZE: \"tl.constexpr\",\n>>> ):\n>>>     pid = tl.program_id(axis=0)\n>>>     block_start = pid * BLOCK_SIZE\n>>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n>>>     mask = offsets < n_elements\n>>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n>>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n>>>     output = x + y\n>>>     tl.store(out_ptr + offsets, output, mask=mask)\n>>>\n>>> @triton_op(\"mylib::add\", mutates_args={})\n>>> def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n>>>     output = torch.empty_like(x)\n>>>     n_elements = output.numel()\n>>>\n>>>     def grid(meta):\n>>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n>>>\n>>>     # NB: we need to wrap the triton kernel in a call to wrap_triton\n>>>     wrap_triton(add_kernel)[grid](x, y, output, n_elements, 16)\n>>>     return output\n>>>\n>>> @torch.compile\n>>> def f(x, y):\n>>>     return add(x, y)\n>>>\n>>> x = torch.randn(3, device=\"cuda\")\n>>> y = torch.randn(3, device=\"cuda\")\n>>>\n>>> z = f(x, y)\n>>> assert torch.allclose(z, x + y)\n\n```\n\nAllows capture of a triton kernel into a graph via make_fx or\nnon-stricttorch.export.\ntorch.export\nThese technologies perform Dispatcher-based tracing (via__torch_dispatch__) and cannot see calls to raw triton kernels.\nThewrap_tritonAPI wraps a triton kernel into a callable that\ncan actually be traced into a graph.\n__torch_dispatch__\nwrap_triton\nPlease use this API together withtorch.library.triton_op().\ntorch.library.triton_op()\nExamples\n\n```python\n>>> import torch\n>>> import triton\n>>> from triton import language as tl\n>>> from torch.fx.experimental.proxy_tensor import make_fx\n>>> from torch.library import wrap_triton\n>>>\n>>> @triton.jit\n>>> def add_kernel(\n>>>     in_ptr0,\n>>>     in_ptr1,\n>>>     out_ptr,\n>>>     n_elements,\n>>>     BLOCK_SIZE: \"tl.constexpr\",\n>>> ):\n>>>     pid = tl.program_id(axis=0)\n>>>     block_start = pid * BLOCK_SIZE\n>>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n>>>     mask = offsets < n_elements\n>>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n>>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n>>>     output = x + y\n>>>     tl.store(out_ptr + offsets, output, mask=mask)\n>>>\n>>> def add(x, y):\n>>>     output = torch.empty_like(x)\n>>>     n_elements = output.numel()\n>>>\n>>>     def grid_fn(meta):\n>>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n>>>\n>>>     wrap_triton(add_kernel)[grid_fn](x, y, output, n_elements, 16)\n>>>     return output\n>>>\n>>> x = torch.randn(3, device=\"cuda\")\n>>> y = torch.randn(3, device=\"cuda\")\n>>> gm = make_fx(add)(x, y)\n>>> print(gm.code)\n>>> # def forward(self, x_1, y_1):\n>>> #     empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False)\n>>> #     triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation(\n>>> #         kernel_idx = 0, constant_args_idx = 0,\n>>> #         grid = [(1, 1, 1)], kwargs = {\n>>> #             'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like,\n>>> #             'n_elements': 3, 'BLOCK_SIZE': 16\n>>> #         })\n>>> #     return empty_like\n\n```\n\nAny\n\n## Extending custom ops (created from Python or C++)#\n\nUse theregister.*methods, such astorch.library.register_kernel()andtorch.library.register_fake(), to add implementations\nfor any operators (they may have been created usingtorch.library.custom_op()or\nvia PyTorch\u2019s C++ operator registration APIs).\nregister.*\ntorch.library.register_kernel()\ntorch.library.register_fake()\ntorch.library.custom_op()\nRegister an implementation for a device type for this operator.\nSome valid device_types are: \u201ccpu\u201d, \u201ccuda\u201d, \u201cxla\u201d, \u201cmps\u201d, \u201cipu\u201d, \u201cxpu\u201d.\nThis API may be used as a decorator.\nop(str|OpOverload) \u2013 The operator to register an impl to.\ndevice_types(None|str|Sequence[str]) \u2013 The device_types to register an impl to.\nIf None, we will register to all device types \u2013 please only use\nthis option if your implementation is truly device-type-agnostic.\nfunc(Callable) \u2013 The function to register as the implementation for\nthe given device types.\nlib(Optional[Library]) \u2013 If provided, the lifetime of this registration\n\n```python\n>>> import torch\n>>> from torch import Tensor\n>>> from torch.library import custom_op\n>>> import numpy as np\n>>>\n>>> # Create a custom op that works on cpu\n>>> @custom_op(\"mylib::numpy_sin\", mutates_args=(), device_types=\"cpu\")\n>>> def numpy_sin(x: Tensor) -> Tensor:\n>>>     x_np = x.numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np)\n>>>\n>>> # Add implementations for the cuda device\n>>> @torch.library.register_kernel(\"mylib::numpy_sin\", \"cuda\")\n>>> def _(x):\n>>>     x_np = x.cpu().numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np).to(device=x.device)\n>>>\n>>> x_cpu = torch.randn(3)\n>>> x_cuda = x_cpu.cuda()\n>>> assert torch.allclose(numpy_sin(x_cpu), x_cpu.sin())\n>>> assert torch.allclose(numpy_sin(x_cuda), x_cuda.sin())\n\n```\n\nRegister an autocast dispatch rule for this custom op.\nValiddevice_typeinclude: \u201ccpu\u201d and \u201ccuda\u201d.\nop(str|OpOverload) \u2013 The operator to register an autocast dispatch rule to.\ndevice_type(str) \u2013 Device type to use. \u2018cuda\u2019 or \u2018cpu\u2019.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\ncast_inputs(torch.dtype) \u2013 When custom op runs in an autocast-enabled region,\ncasts incoming floating-point Tensors to the target dtype (non-floating-point Tensors\nare not affected), then executes custom op with autocast disabled.\ntorch.dtype\nlib(Optional[Library]) \u2013 If provided, the lifetime of this registration\n\n```python\n>>> import torch\n>>> from torch import Tensor\n>>> from torch.library import custom_op\n>>>\n>>> # Create a custom op that works on cuda\n>>> @torch.library.custom_op(\"mylib::my_sin\", mutates_args=())\n>>> def my_sin(x: Tensor) -> Tensor:\n>>>     return torch.sin(x)\n>>>\n>>> # Register autocast dispatch rule for the cuda device\n>>> torch.library.register_autocast(\"mylib::my_sin\", \"cuda\", torch.float16)\n>>>\n>>> x = torch.randn(3, dtype=torch.float32, device=\"cuda\")\n>>> with torch.autocast(\"cuda\", dtype=torch.float16):\n>>>     y = torch.ops.mylib.my_sin(x)\n>>> assert y.dtype == torch.float16\n\n```\n\nRegister a backward formula for this custom op.\nIn order for an operator to work with autograd, you need to register\na backward formula:\n1. You must tell us how to compute gradients during the backward pass\nby providing us a \u201cbackward\u201d function.\n2. If you need any values from the forward to compute gradients, you can\nusesetup_contextto save values for backward.\nbackwardruns during the backward pass. It accepts(ctx,*grads):\n-gradsis one or more gradients. The number of gradients matches\nthe number of outputs of the operator.\nThectxobject isthe same ctx objectused bytorch.autograd.Function. The semantics ofbackward_fnare the\nsame astorch.autograd.Function.backward().\nbackward\n(ctx,*grads)\ngrads\nctx\ntorch.autograd.Function\nbackward_fn\ntorch.autograd.Function.backward()\nsetup_context(ctx,inputs,output)runs during the forward pass.\nPlease save quantities needed for backward onto thectxobject via\neithertorch.autograd.function.FunctionCtx.save_for_backward()or assigning them as attributes ofctx. If your custom op has\nkwarg-only arguments, we expect the signature ofsetup_contextto besetup_context(ctx,inputs,keyword_only_inputs,output).\nsetup_context(ctx,inputs,output)\nctx\ntorch.autograd.function.FunctionCtx.save_for_backward()\nctx\nsetup_context\nsetup_context(ctx,inputs,keyword_only_inputs,output)\nBothsetup_context_fnandbackward_fnmust be traceable. That is,\nthey may not directly accesstorch.Tensor.data_ptr()and they must\nnot depend on or mutate global state. If you need a non-traceable backward,\nyou can make it a separate custom_op that you call insidebackward_fn.\nsetup_context_fn\nbackward_fn\ntorch.Tensor.data_ptr()\nbackward_fn\nIf you need different autograd behavior on different devices, then we\nrecommend creating two different custom operators, one for each device\nthat needs different behavior, and switching between them at runtime.\nExamples\n\n```python\n>>> import torch\n>>> import numpy as np\n>>> from torch import Tensor\n>>>\n>>> @torch.library.custom_op(\"mylib::numpy_sin\", mutates_args=())\n>>> def numpy_sin(x: Tensor) -> Tensor:\n>>>     x_np = x.cpu().numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np).to(device=x.device)\n>>>\n>>> def setup_context(ctx, inputs, output) -> Tensor:\n>>>     x, = inputs\n>>>     ctx.save_for_backward(x)\n>>>\n>>> def backward(ctx, grad):\n>>>     x, = ctx.saved_tensors\n>>>     return grad * x.cos()\n>>>\n>>> torch.library.register_autograd(\n...     \"mylib::numpy_sin\", backward, setup_context=setup_context\n... )\n>>>\n>>> x = torch.randn(3, requires_grad=True)\n>>> y = numpy_sin(x)\n>>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))\n>>> assert torch.allclose(grad_x, x.cos())\n>>>\n>>> # Example with a keyword-only arg\n>>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\n>>> def numpy_mul(x: Tensor, *, val: float) -> Tensor:\n>>>     x_np = x.cpu().numpy()\n>>>     y_np = x_np * val\n>>>     return torch.from_numpy(y_np).to(device=x.device)\n>>>\n>>> def setup_context(ctx, inputs, keyword_only_inputs, output) -> Tensor:\n>>>     ctx.val = keyword_only_inputs[\"val\"]\n>>>\n>>> def backward(ctx, grad):\n>>>     return grad * ctx.val\n>>>\n>>> torch.library.register_autograd(\n...     \"mylib::numpy_mul\", backward, setup_context=setup_context\n... )\n>>>\n>>> x = torch.randn(3, requires_grad=True)\n>>> y = numpy_mul(x, val=3.14)\n>>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))\n>>> assert torch.allclose(grad_x, torch.full_like(x, 3.14))\n\n```\n\nRegister a FakeTensor implementation (\u201cfake impl\u201d) for this operator.\nAlso sometimes known as a \u201cmeta kernel\u201d, \u201cabstract impl\u201d.\nAn \u201cFakeTensor implementation\u201d specifies the behavior of this operator on\nTensors that carry no data (\u201cFakeTensor\u201d). Given some input Tensors with\ncertain properties (sizes/strides/storage_offset/device), it specifies\nwhat the properties of the output Tensors are.\nThe FakeTensor implementation has the same signature as the operator.\nIt is run for both FakeTensors and meta tensors. To write a FakeTensor\nimplementation, assume that all Tensor inputs to the operator are\nregular CPU/CUDA/Meta tensors, but they do not have storage, and\nyou are trying to return regular CPU/CUDA/Meta tensor(s) as output.\nThe FakeTensor implementation must consist of only PyTorch operations\n(and may not directly access the storage or data of any input or\nintermediate Tensors).\nThis API may be used as a decorator (see examples).\nFor a detailed guide on custom ops, please seehttps://pytorch.org/tutorials/advanced/custom_ops_landing_page.html\nop_name\u2013 Operator name (along with the overload) or OpOverload object.\nfunc(Optional[Callable]) \u2013 Fake tensor implementation.\nlib(Optional[Library]) \u2013 Library to register the fake tensor to.\nallow_override(bool) \u2013 Flag controlling if we want to override an\nexisting registered fake impl. This is by default off,\nand will error you\u2019re trying to register a fake impl to\nan operator that already has a fake impl. This also only\napplies if the custom operator was not created via\ntorch.library.custom_op, as overriding and existing fake\nimpl is already allowed.\nExamples\n\n```python\n>>> import torch\n>>> import numpy as np\n>>> from torch import Tensor\n>>>\n>>> # Example 1: an operator without data-dependent output shape\n>>> @torch.library.custom_op(\"mylib::custom_linear\", mutates_args=())\n>>> def custom_linear(x: Tensor, weight: Tensor, bias: Tensor) -> Tensor:\n>>>     raise NotImplementedError(\"Implementation goes here\")\n>>>\n>>> @torch.library.register_fake(\"mylib::custom_linear\")\n>>> def _(x, weight, bias):\n>>>     assert x.dim() == 2\n>>>     assert weight.dim() == 2\n>>>     assert bias.dim() == 1\n>>>     assert x.shape[1] == weight.shape[1]\n>>>     assert weight.shape[0] == bias.shape[0]\n>>>     assert x.device == weight.device\n>>>\n>>>     return (x @ weight.t()) + bias\n>>>\n>>> with torch._subclasses.fake_tensor.FakeTensorMode():\n>>>     x = torch.randn(2, 3)\n>>>     w = torch.randn(3, 3)\n>>>     b = torch.randn(3)\n>>>     y = torch.ops.mylib.custom_linear(x, w, b)\n>>>\n>>> assert y.shape == (2, 3)\n>>>\n>>> # Example 2: an operator with data-dependent output shape\n>>> @torch.library.custom_op(\"mylib::custom_nonzero\", mutates_args=())\n>>> def custom_nonzero(x: Tensor) -> Tensor:\n>>>     x_np = x.numpy(force=True)\n>>>     res = np.stack(np.nonzero(x_np), axis=1)\n>>>     return torch.tensor(res, device=x.device)\n>>>\n>>> @torch.library.register_fake(\"mylib::custom_nonzero\")\n>>> def _(x):\n>>> # Number of nonzero-elements is data-dependent.\n>>> # Since we cannot peek at the data in an fake impl,\n>>> # we use the ctx object to construct a new symint that\n>>> # represents the data-dependent size.\n>>>     ctx = torch.library.get_ctx()\n>>>     nnz = ctx.new_dynamic_size()\n>>>     shape = [nnz, x.dim()]\n>>>     result = x.new_empty(shape, dtype=torch.int64)\n>>>     return result\n>>>\n>>> from torch.fx.experimental.proxy_tensor import make_fx\n>>>\n>>> x = torch.tensor([0, 1, 2, 3, 4, 0])\n>>> trace = make_fx(torch.ops.mylib.custom_nonzero, tracing_mode=\"symbolic\")(x)\n>>> trace.print_readable()\n>>>\n>>> assert torch.allclose(trace(x), torch.ops.mylib.custom_nonzero(x))\n\n```\n\nRegister a vmap implementation to supporttorch.vmap()for this custom op.\ntorch.vmap()\nThis API may be used as a decorator (see examples).\nIn order for an operator to work withtorch.vmap(), you may need to register a\nvmap implementation in the following signature:\ntorch.vmap()\nvmap_func(info,in_dims:Tuple[Optional[int]],*args,**kwargs),\nvmap_func(info,in_dims:Tuple[Optional[int]],*args,**kwargs)\nwhere*argsand**kwargsare the arguments and kwargs forop.\nWe do not support kwarg-only Tensor args.\n*args\n**kwargs\nop\nIt specifies how do we compute the batched version ofopgiven inputs with an additional\ndimension (specified byin_dims).\nop\nin_dims\nFor each arg inargs,in_dimshas a correspondingOptional[int]. It isNoneif the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer\nspecifying what dimension of the Tensor is being vmapped over.\nargs\nin_dims\nOptional[int]\nNone\ninfois a collection of additional metadata that may be helpful:info.batch_sizespecifies the size of the dimension being vmapped over, whileinfo.randomnessis therandomnessoption that was passed totorch.vmap().\ninfo\ninfo.batch_size\ninfo.randomness\nrandomness\ntorch.vmap()\nThe return of the functionfuncis a tuple of(output,out_dims). Similar toin_dims,out_dimsshould be of the same structure asoutputand contain oneout_dimper output that specifies if the output has the vmapped dimension and what index it is in.\nfunc\n(output,out_dims)\nin_dims\nout_dims\noutput\nout_dim\nExamples\n\n```python\n>>> import torch\n>>> import numpy as np\n>>> from torch import Tensor\n>>> from typing import Tuple\n>>>\n>>> def to_numpy(tensor):\n>>>     return tensor.cpu().numpy()\n>>>\n>>> lib = torch.library.Library(\"mylib\", \"FRAGMENT\")\n>>> @torch.library.custom_op(\"mylib::numpy_cube\", mutates_args=())\n>>> def numpy_cube(x: Tensor) -> Tuple[Tensor, Tensor]:\n>>>     x_np = to_numpy(x)\n>>>     dx = torch.tensor(3 * x_np ** 2, device=x.device)\n>>>     return torch.tensor(x_np ** 3, device=x.device), dx\n>>>\n>>> def numpy_cube_vmap(info, in_dims, x):\n>>>     result = numpy_cube(x)\n>>>     return result, (in_dims[0], in_dims[0])\n>>>\n>>> torch.library.register_vmap(numpy_cube, numpy_cube_vmap)\n>>>\n>>> x = torch.randn(3)\n>>> torch.vmap(numpy_cube)(x)\n>>>\n>>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\n>>> def numpy_mul(x: Tensor, y: Tensor) -> Tensor:\n>>>     return torch.tensor(to_numpy(x) * to_numpy(y), device=x.device)\n>>>\n>>> @torch.library.register_vmap(\"mylib::numpy_mul\")\n>>> def numpy_mul_vmap(info, in_dims, x, y):\n>>>     x_bdim, y_bdim = in_dims\n>>>     x = x.movedim(x_bdim, -1) if x_bdim is not None else x.unsqueeze(-1)\n>>>     y = y.movedim(y_bdim, -1) if y_bdim is not None else y.unsqueeze(-1)\n>>>     result = x * y\n>>>     result = result.movedim(-1, 0)\n>>>     return result, 0\n>>>\n>>>\n>>> x = torch.randn(3)\n>>> y = torch.randn(3)\n>>> torch.vmap(numpy_mul)(x, y)\n\n```\n\nNote\nThe vmap function should aim to preserve the semantics of the entire custom operator.\nThat is,grad(vmap(op))should be replaceable with agrad(map(op)).\ngrad(vmap(op))\ngrad(map(op))\nIf your custom operator has any custom behavior in the backward pass, please\nkeep this in mind.\nThis API was renamed totorch.library.register_fake()in PyTorch 2.4.\nPlease use that instead.\ntorch.library.register_fake()\nget_ctx() returns the current AbstractImplCtx object.\nCallingget_ctx()is only valid inside of an fake impl\n(seetorch.library.register_fake()for more usage details.\nget_ctx()\ntorch.library.register_fake()\nFakeImplCtx\nRegisters a torch_dispatch rule for the given operator andtorch_dispatch_class.\ntorch_dispatch_class\nThis allows for open registration to specify the behavior between the operator\nand thetorch_dispatch_classwithout needing to modify thetorch_dispatch_classor the operator directly.\ntorch_dispatch_class\ntorch_dispatch_class\nThetorch_dispatch_classis either a Tensor subclass with__torch_dispatch__or a\nTorchDispatchMode.\ntorch_dispatch_class\n__torch_dispatch__\nIf it is a Tensor subclass, we expectfuncto have the following signature:(cls,func:OpOverload,types:Tuple[type,...],args,kwargs)->Any\nfunc\n(cls,func:OpOverload,types:Tuple[type,...],args,kwargs)->Any\nIf it is a TorchDispatchMode, we expectfuncto have the following signature:(mode,func:OpOverload,types:Tuple[type,...],args,kwargs)->Any\nfunc\n(mode,func:OpOverload,types:Tuple[type,...],args,kwargs)->Any\nargsandkwargswill have been normalized the same way they are\nin__torch_dispatch__(see__torch_dispatch__ calling convention).\nargs\nkwargs\n__torch_dispatch__\nExamples\n\n```python\n>>> import torch\n>>>\n>>> @torch.library.custom_op(\"mylib::foo\", mutates_args={})\n>>> def foo(x: torch.Tensor) -> torch.Tensor:\n>>>     return x.clone()\n>>>\n>>> class MyMode(torch.utils._python_dispatch.TorchDispatchMode):\n>>>     def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n>>>         return func(*args, **kwargs)\n>>>\n>>> @torch.library.register_torch_dispatch(\"mylib::foo\", MyMode)\n>>> def _(mode, func, types, args, kwargs):\n>>>     x, = args\n>>>     return x + 1\n>>>\n>>> x = torch.randn(3)\n>>> y = foo(x)\n>>> assert torch.allclose(y, x)\n>>>\n>>> with MyMode():\n>>>     y = foo(x)\n>>> assert torch.allclose(y, x + 1)\n\n```\n\nParses the schema of a given function with type hints. The schema is inferred from the\nfunction\u2019s type hints, and can be used to define a new operator.\nWe make the following assumptions:\nNone of the outputs alias any of the inputs or each other.\nmutates_args\nmutates_args\nCallers (e.g. the custom ops API) are responsible for checking these assumptions.\nprototype_function(Callable) \u2013 The function from which to infer a schema for from its type annotations.\nop_name(Optional[str]) \u2013 The name of the operator in the schema. Ifnameis None, then the\nname is not included in the inferred schema. Note that the input schema totorch.library.Library.definerequires a operator name.\nname\ntorch.library.Library.define\nmutates_args(\"unknown\"|Iterable[str]) \u2013 The arguments that are mutated in the function.\nThe inferred schema.\nstr\nExample\n\n```python\n>>> def foo_impl(x: torch.Tensor) -> torch.Tensor:\n>>>     return x.sin()\n>>>\n>>> infer_schema(foo_impl, op_name=\"foo\", mutates_args={})\nfoo(Tensor x) -> Tensor\n>>>\n>>> infer_schema(foo_impl, mutates_args={})\n(Tensor x) -> Tensor\n\n```\n\nCustomOpDef is a wrapper around a function that turns it into a custom op.\nIt has various methods for registering additional behavior for this\ncustom op.\nYou should not instantiate CustomOpDef directly; instead, use thetorch.library.custom_op()API.\ntorch.library.custom_op()\nDisable or re-enable an already registered kernel for this custom operator.\nIf the kernel is already disabled/enabled, this is a no-op.\nNote\nIf a kernel is first disabled and then registered, it is disabled until enabled again.\ndevice_type(str) \u2013 The device type to disable/enable the kernel for.\ndisable(bool) \u2013 Whether to disable or enable the kernel.\nExample\n\n```python\n>>> inp = torch.randn(1)\n>>>\n>>> # define custom op `f`.\n>>> @custom_op(\"mylib::f\", mutates_args=())\n>>> def f(x: Tensor) -> Tensor:\n>>>     return torch.zeros(1)\n>>>\n>>> print(f(inp))  # tensor([0.]), default kernel\n>>>\n>>> @f.register_kernel(\"cpu\")\n>>> def _(x):\n>>>     return torch.ones(1)\n>>>\n>>> print(f(inp))  # tensor([1.]), CPU kernel\n>>>\n>>> # temporarily disable the CPU kernel\n>>> with f.set_kernel_enabled(\"cpu\", enabled = False):\n>>>     print(f(inp))  # tensor([0.]) with CPU kernel disabled\n\n```\n\nReturns the computed kernel for a given operator and dispatch key.\nThis function retrieves the kernel that would be executed for a given\noperator and dispatch key combination. The returned SafeKernelFunction\ncan be used to call the kernel in a boxed fashion. The intended use\ncase for this function is to retrieve the original kernel for a given\ndispatch key and then register another kernel to the same dispatch key\nthat calls into the original kernel for certain cases.\nop(Union[str,OpOverload,CustomOpDef]) \u2013 Operator name (along with the overload) or OpOverload object\nCan be a string (e.g., \u201caten::add.Tensor\u201d), an OpOverload, or a CustomOpDef.\ndispatch_key(str|torch.DispatchKey) \u2013 The dispatch key to get the kernel for.\nCan be a string (e.g., \u201cCPU\u201d, \u201cCUDA\u201d) or a DispatchKey enum value.\nA safe kernel function that can be used tocall the kernel.\ncall the kernel.\ntorch._C._SafeKernelFunction\nRuntimeError\u2013 If the operator does not exist.\nExample\n\n```python\n>>> # Get the CPU kernel for torch.add\n>>> kernel = torch.library.get_kernel(\"aten::add.Tensor\", \"CPU\")\n>>>\n>>> # You can also use DispatchKey enum\n>>> kernel = torch.library.get_kernel(\"aten::add.Tensor\", torch.DispatchKey.CPU)\n>>>\n>>> # Or use an OpOverload directly\n>>> kernel = torch.library.get_kernel(torch.ops.aten.add.Tensor, \"CPU\")\n>>>\n>>> # Example: Using get_kernel in a custom op with conditional dispatch\n>>> # Get the original kernel for torch.sin\n>>> original_sin_kernel = torch.library.get_kernel(\"aten::sin\", \"CPU\")\n>>>\n>>> # If input has negative values, use original sin, otherwise return zeros\n>>> def conditional_sin_impl(dispatch_keys, x):\n>>>     if (x < 0).any():\n>>>         return original_sin_kernel.call_boxed(dispatch_keys, x)\n>>>     else:\n>>>         return torch.zeros_like(x)\n>>>\n>>> lib = torch.library.Library(\"aten\", \"IMPL\")\n>>> # with_keyset=True so the first argument to the impl is the current DispatchKeySet\n>>> which needs to be the first argument to ``kernel.call_boxed``\n>>> lib.impl(\"sin\", conditional_sin_impl, \"CPU\", with_keyset=True)\n>>>\n>>> # Test the conditional behavior\n>>> x_positive = torch.tensor([1.0, 2.0])\n>>> x_mixed = torch.tensor([-1.0, 2.0])\n>>> torch.sin(x_positive)\ntensor([0., 0.])\n>>> torch.sin(x_mixed)\ntensor([-0.8415, 0.9093])\n\n```\n\n\n## Low-level APIs#\n\nThe following APIs are direct bindings to PyTorch\u2019s C++ low-level\noperator registration APIs.\nWarning\nThe low-level operator registration APIs and the PyTorch Dispatcher are a complicated PyTorch concept. We recommend you use the higher level APIs above (that do not require a torch.library.Library object) when possible.This blog postis a good starting point to learn about the PyTorch Dispatcher.\nA tutorial that walks you through some examples on how to use this API is available onGoogle Colab.\nA class to create libraries that can be used to register new operators or\noverride operators in existing libraries from Python.\nA user can optionally pass in a dispatch keyname if they only want to register\nkernels corresponding to only one specific dispatch key.\nTo create a library to override operators in an existing library (with name ns), set the kind to \u201cIMPL\u201d.\nTo create a new library (with name ns) to register new operators, set the kind to \u201cDEF\u201d.\nTo create a fragment of a possibly existing library to register operators (and bypass\nthe limitation that there is only one library for a given namespace), set the kind to\n\u201cFRAGMENT\u201d.\nns\u2013 library name\nkind\u2013 \u201cDEF\u201d, \u201cIMPL\u201d, \u201cFRAGMENT\u201d\ndispatch_key\u2013 PyTorch dispatch key (default: \u201c\u201d)\nDefines a new operator and its semantics in the ns namespace.\nschema\u2013 function schema to define a new operator.\nalias_analysis(optional) \u2013 Indicates if the aliasing properties of the operator arguments can be\ninferred from the schema (default behavior) or not (\u201cCONSERVATIVE\u201d).\ntags(Tag|Sequence[Tag]) \u2013 one or more torch.Tag to apply to this\noperator. Tagging an operator changes the operator\u2019s behavior\nunder various PyTorch subsystems; please read the docs for the\ntorch.Tag carefully before applying it.\nname of the operator as inferred from the schema.\nExample:\n\n```python\n>>> my_lib = Library(\"mylib\", \"DEF\")\n>>> my_lib.define(\"sum(Tensor self) -> Tensor\")\n\n```\n\nRegisters the function implementation as the fallback for the given key.\nThis function only works for a library with global namespace (\u201c_\u201d).\nfn\u2013 function used as fallback for the given dispatch key orfallthrough_kernel()to register a fallthrough.\nfallthrough_kernel()\ndispatch_key\u2013 dispatch key that the input function should be registered for. By default, it uses\nthe dispatch key that the library was created with.\nwith_keyset\u2013 flag controlling if the current dispatcher call keyset should be passed as the first argument\ntofnwhen calling. This should be used to create the appropriate keyset for redispatch calls.\nfn\nExample:\n\n```python\n>>> my_lib = Library(\"_\", \"IMPL\")\n>>> def fallback_kernel(op, *args, **kwargs):\n>>>     # Handle all autocast ops generically\n>>>     # ...\n>>> my_lib.fallback(fallback_kernel, \"Autocast\")\n\n```\n\nRegisters the function implementation for an operator defined in the library.\nop_name\u2013 operator name (along with the overload) or OpOverload object.\nfn\u2013 function that\u2019s the operator implementation for the input dispatch key orfallthrough_kernel()to register a fallthrough.\nfallthrough_kernel()\ndispatch_key\u2013 dispatch key that the input function should be registered for. By default, it uses\nthe dispatch key that the library was created with.\nwith_keyset\u2013 flag controlling if the current dispatcher call keyset should be passed as the first argument\ntofnwhen calling. This should be used to create the appropriate keyset for redispatch calls.\nfn\nallow_override\u2013 Flag controlling if we want to override an\nexisting registered kernel implementation. This is by\ndefault off, and will error you\u2019re trying to register a\nkernel to a dispatch key with a kernel already\nregistered.\nExample:\n\n```python\n>>> my_lib = Library(\"aten\", \"IMPL\")\n>>> def div_cpu(self, other):\n>>>     return self * (1 / other)\n>>> my_lib.impl(\"div.Tensor\", div_cpu, \"CPU\")\n\n```\n\nA dummy function to pass toLibrary.implin order to register a fallthrough.\nLibrary.impl\nDefines a new operator.\nIn PyTorch, defining an op (short for \u201coperator\u201d) is a two step-process:\n- we need to define the op (by providing an operator name and schema)\n- we need to implement behavior for how the operator interacts with\nvarious PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc.\nThis entrypoint defines the custom operator (the first step)\nyou must then perform the second step by calling variousimpl_*APIs, liketorch.library.impl()ortorch.library.register_fake().\nimpl_*\ntorch.library.impl()\ntorch.library.register_fake()\nqualname(str) \u2013 The qualified name for the operator. Should be\na string that looks like \u201cnamespace::name\u201d, e.g. \u201caten::sin\u201d.\nOperators in PyTorch need a namespace to\navoid name collisions; a given operator may only be created once.\nIf you are writing a Python library, we recommend the namespace to\nbe the name of your top-level module.\nschema(str) \u2013 The schema of the operator. E.g. \u201c(Tensor x) -> Tensor\u201d\nfor an op that accepts one Tensor and returns one Tensor. It does\nnot contain the operator name (that is passed inqualname).\nqualname\nlib(Optional[Library]) \u2013 If provided, the lifetime of this operator\nwill be tied to the lifetime of the Library object.\ntags(Tag|Sequence[Tag]) \u2013 one or more torch.Tag to apply to this\noperator. Tagging an operator changes the operator\u2019s behavior\nunder various PyTorch subsystems; please read the docs for the\ntorch.Tag carefully before applying it.\n\n```python\n>>> import torch\n>>> import numpy as np\n>>>\n>>> # Define the operator\n>>> torch.library.define(\"mylib::sin\", \"(Tensor x) -> Tensor\")\n>>>\n>>> # Add implementations for the operator\n>>> @torch.library.impl(\"mylib::sin\", \"cpu\")\n>>> def f(x):\n>>>     return torch.from_numpy(np.sin(x.numpy()))\n>>>\n>>> # Call the new operator from torch.ops.\n>>> x = torch.randn(3)\n>>> y = torch.ops.mylib.sin(x)\n>>> assert torch.allclose(y, x.sin())\n\n```\n\nRegister an implementation for a device type for this operator.\nYou may pass \u201cdefault\u201d fortypesto register this implementation as the\ndefault implementation for ALL device types.\nPlease only use this if the implementation truly supports all device types;\nfor example, this is true if it is a composition of built-in PyTorch operators.\ntypes\nThis API may be used as a decorator. You can use nested decorators\nwith this API provided they return a function and are placed inside\nthis API (see Example 2).\nSome valid types are: \u201ccpu\u201d, \u201ccuda\u201d, \u201cxla\u201d, \u201cmps\u201d, \u201cipu\u201d, \u201cxpu\u201d.\nqualname(str) \u2013 Should be a string that looks like \u201cnamespace::operator_name\u201d.\ntypes(str|Sequence[str]) \u2013 The device types to register an impl to.\nlib(Optional[Library]) \u2013 If provided, the lifetime of this registration\nwill be tied to the lifetime of the Library object.\nExamples\n\n```python\n>>> import torch\n>>> import numpy as np\n>>> # Example 1: Register function.\n>>> # Define the operator\n>>> torch.library.define(\"mylib::mysin\", \"(Tensor x) -> Tensor\")\n>>>\n>>> # Add implementations for the cpu device\n>>> @torch.library.impl(\"mylib::mysin\", \"cpu\")\n>>> def f(x):\n>>>     return torch.from_numpy(np.sin(x.numpy()))\n>>>\n>>> x = torch.randn(3)\n>>> y = torch.ops.mylib.mysin(x)\n>>> assert torch.allclose(y, x.sin())\n>>>\n>>> # Example 2: Register function with decorator.\n>>> def custom_decorator(func):\n>>>     def wrapper(*args, **kwargs):\n>>>         return func(*args, **kwargs) + 1\n>>>     return wrapper\n>>>\n>>> # Define the operator\n>>> torch.library.define(\"mylib::sin_plus_one\", \"(Tensor x) -> Tensor\")\n>>>\n>>> # Add implementations for the operator\n>>> @torch.library.impl(\"mylib::sin_plus_one\", \"cpu\")\n>>> @custom_decorator\n>>> def f(x):\n>>>     return torch.from_numpy(np.sin(x.numpy()))\n>>>\n>>> # Call the new operator from torch.ops.\n>>> x = torch.randn(3)\n>>>\n>>> y1 = torch.ops.mylib.sin_plus_one(x)\n>>> y2 = torch.sin(x) + 1\n>>> assert torch.allclose(y1, y2)\n\n```\n",
  "url": "https://pytorch.org/docs/stable/library.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
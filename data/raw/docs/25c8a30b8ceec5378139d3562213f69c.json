{
  "doc_id": "25c8a30b8ceec5378139d3562213f69c",
  "source": "pytorch_docs",
  "title": "Complex Numbers \u2014 PyTorch 2.9 documentation",
  "text": "\n## Complex Numbers#\n\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\nComplex numbers are numbers that can be expressed in the forma+bja + bja+bj, where a and b are real numbers,\nandjis called the imaginary unit, which satisfies the equationj2=\u22121j^2 = -1j2=\u22121. Complex numbers frequently occur in mathematics and\nengineering, especially in topics like signal processing. Traditionally many users and libraries (e.g., TorchAudio) have\nhandled complex numbers by representing the data in float tensors with shape(...,2)(..., 2)(...,2)where the last\ndimension contains the real and imaginary values.\nTensors of complex dtypes provide a more natural user experience while working with complex numbers. Operations on\ncomplex tensors (e.g.,torch.mv(),torch.matmul()) are likely to be faster and more memory efficient\nthan operations on float tensors mimicking them. Operations involving complex numbers in PyTorch are optimized\nto use vectorized assembly instructions and specialized kernels (e.g. LAPACK, cuBlas).\ntorch.mv()\ntorch.matmul()\nNote\nSpectral operations in thetorch.fft modulesupport\nnative complex tensors.\nWarning\nComplex tensors is a beta feature and subject to change.\n\n## Creating Complex Tensors#\n\nWe support two complex dtypes:torch.cfloatandtorch.cdouble\ntorch.cfloat\ntorch.cdouble\n\n```python\n>>> x = torch.randn(2,2, dtype=torch.cfloat)\n>>> x\ntensor([[-0.4621-0.0303j, -0.2438-0.5874j],\n     [ 0.7706+0.1421j,  1.2110+0.1918j]])\n\n```\n\nNote\nThe default dtype for complex tensors is determined by the default floating point dtype.\nIf the default floating point dtype istorch.float64then complex numbers are inferred to\nhave a dtype oftorch.complex128, otherwise they are assumed to have a dtype oftorch.complex64.\ntorch.float64\ntorch.complex128\ntorch.complex64\nAll factory functions apart fromtorch.linspace(),torch.logspace(), andtorch.arange()are\nsupported for complex tensors.\ntorch.linspace()\ntorch.logspace()\ntorch.arange()\n\n## Transition from the old representation#\n\nUsers who currently worked around the lack of complex tensors with real tensors of shape(...,2)(..., 2)(...,2)can easily to switch using the complex tensors in their code usingtorch.view_as_complex()andtorch.view_as_real(). Note that these functions don\u2019t perform any copy and return a\nview of the input tensor.\ntorch.view_as_complex()\ntorch.view_as_real()\n\n```python\n>>> x = torch.randn(3, 2)\n>>> x\ntensor([[ 0.6125, -0.1681],\n     [-0.3773,  1.3487],\n     [-0.0861, -0.7981]])\n>>> y = torch.view_as_complex(x)\n>>> y\ntensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])\n>>> torch.view_as_real(y)\ntensor([[ 0.6125, -0.1681],\n     [-0.3773,  1.3487],\n     [-0.0861, -0.7981]])\n\n```\n\n\n## Accessing real and imag#\n\nThe real and imaginary values of a complex tensor can be accessed using therealandimag.\nreal\nimag\nNote\nAccessingrealandimagattributes doesn\u2019t allocate any memory, and in-place updates on therealandimagtensors will update the original complex tensor. Also, the\nreturnedrealandimagtensors are not contiguous.\nreal\nimag\nreal\nimag\nreal\nimag\n\n```python\n>>> y.real\ntensor([ 0.6125, -0.3773, -0.0861])\n>>> y.imag\ntensor([-0.1681,  1.3487, -0.7981])\n\n>>> y.real.mul_(2)\ntensor([ 1.2250, -0.7546, -0.1722])\n>>> y\ntensor([ 1.2250-0.1681j, -0.7546+1.3487j, -0.1722-0.7981j])\n>>> y.real.stride()\n(2,)\n\n```\n\n\n## Angle and abs#\n\nThe angle and absolute values of a complex tensor can be computed usingtorch.angle()andtorch.abs().\ntorch.angle()\ntorch.abs()\n\n```python\n>>> x1=torch.tensor([3j, 4+4j])\n>>> x1.abs()\ntensor([3.0000, 5.6569])\n>>> x1.angle()\ntensor([1.5708, 0.7854])\n\n```\n\n\n## Linear Algebra#\n\nMany linear algebra operations, liketorch.matmul(),torch.linalg.svd(),torch.linalg.solve()etc., support complex numbers.\nIf you\u2019d like to request an operation we don\u2019t currently support, pleasesearchif an issue has already been filed and if not,file one.\ntorch.matmul()\ntorch.linalg.svd()\ntorch.linalg.solve()\n\n## Serialization#\n\nComplex tensors can be serialized, allowing data to be saved as complex values.\n\n```python\n>>> torch.save(y, 'complex_tensor.pt')\n>>> torch.load('complex_tensor.pt')\ntensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])\n\n```\n\n\n## Autograd#\n\nPyTorch supports autograd for complex tensors. The gradient computed is the Conjugate Wirtinger derivative,\nthe negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus,\nall the existing optimizers can be implemented to work out of the box with complex parameters. For more details,\ncheck out the noteAutograd for Complex Numbers.\n\n## Optimizers#\n\nSemantically, we define stepping through a PyTorch optimizer with complex parameters as being equivalent to stepping\nthrough the same optimizer on thetorch.view_as_real()equivalent of the complex params. More concretely:\ntorch.view_as_real()\n\n```python\n>>> params = [torch.rand(2, 3, dtype=torch.complex64) for _ in range(5)]\n>>> real_params = [torch.view_as_real(p) for p in params]\n\n>>> complex_optim = torch.optim.AdamW(params)\n>>> real_optim = torch.optim.AdamW(real_params)\n\n```\n\nreal_optimandcomplex_optimwill compute the same updates on the parameters, though there may be slight numerical\ndiscrepancies between the two optimizers, similar to numerical discrepancies between foreach vs forloop optimizers\nand capturable vs default optimizers. For more details, seenumbercial accuracy.\nreal_optim\ncomplex_optim\nSpecifically, while you can think of our optimizer\u2019s handling of complex tensors as the same as optimizing over theirp.realandp.imagpieces separately, the implementation details are not precisely that. Note that thetorch.view_as_real()equivalent will convert a complex tensor to a real tensor with shape(...,2)(..., 2)(...,2),\nwhereas splitting a complex tensor into two tensors is 2 tensors of size(...)(...)(...). This distinction has no impact on\npointwise optimizers (like AdamW) but will cause slight discrepancy in optimizers that do global reductions (like LBFGS).\nWe currently do not have optimizers that do per-Tensor reductions and thus do not yet define this behavior. Open an issue\nif you have a use case that requires precisely defining this behavior.\np.real\np.imag\ntorch.view_as_real()\nWe do not fully support the following subsystems:\nQuantization\nJIT\nSparse Tensors\nDistributed\nIf any of these would help your use case, pleasesearchif an issue has already been filed and if not,file one.",
  "url": "https://pytorch.org/docs/stable/complex_numbers.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
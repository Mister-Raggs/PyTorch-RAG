{
  "doc_id": "f26b51b4ad1996174dddd0ceb246183a",
  "source": "pytorch_docs",
  "title": "torch.nn \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.nn#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jul 25, 2025\nThese are the basic building blocks for graphs:\ntorch.nn\nContainers\nConvolution Layers\nPooling layers\nPadding Layers\nNon-linear Activations (weighted sum, nonlinearity)\nNon-linear Activations (other)\nNormalization Layers\nRecurrent Layers\nTransformer Layers\nLinear Layers\nDropout Layers\nSparse Layers\nDistance Functions\nLoss Functions\nVision Layers\nShuffle Layers\nDataParallel Layers (multi-GPU, distributed)\nUtilities\nQuantized Functions\nLazy Modules Initialization\nBuffer\n\nBuffer\nA kind of Tensor that should not be considered a model parameter.\nParameter\n\nParameter\nA kind of Tensor that is to be considered a module parameter.\nUninitializedParameter\n\nUninitializedParameter\nA parameter that is not initialized.\nUninitializedBuffer\n\nUninitializedBuffer\nA buffer that is not initialized.\n\n## Containers#\n\nModule\n\nModule\nBase class for all neural network modules.\nSequential\n\nSequential\nA sequential container.\nModuleList\n\nModuleList\nHolds submodules in a list.\nModuleDict\n\nModuleDict\nHolds submodules in a dictionary.\nParameterList\n\nParameterList\nHolds parameters in a list.\nParameterDict\n\nParameterDict\nHolds parameters in a dictionary.\nGlobal Hooks For Module\nregister_module_forward_pre_hook\n\nregister_module_forward_pre_hook\nRegister a forward pre-hook common to all modules.\nregister_module_forward_hook\n\nregister_module_forward_hook\nRegister a global forward hook for all the modules.\nregister_module_backward_hook\n\nregister_module_backward_hook\nRegister a backward hook common to all the modules.\nregister_module_full_backward_pre_hook\n\nregister_module_full_backward_pre_hook\nRegister a backward pre-hook common to all the modules.\nregister_module_full_backward_hook\n\nregister_module_full_backward_hook\nRegister a backward hook common to all the modules.\nregister_module_buffer_registration_hook\n\nregister_module_buffer_registration_hook\nRegister a buffer registration hook common to all modules.\nregister_module_module_registration_hook\n\nregister_module_module_registration_hook\nRegister a module registration hook common to all modules.\nregister_module_parameter_registration_hook\n\nregister_module_parameter_registration_hook\nRegister a parameter registration hook common to all modules.\n\n## Convolution Layers#\n\nnn.Conv1d\nnn.Conv1d\nApplies a 1D convolution over an input signal composed of several input planes.\nnn.Conv2d\nnn.Conv2d\nApplies a 2D convolution over an input signal composed of several input planes.\nnn.Conv3d\nnn.Conv3d\nApplies a 3D convolution over an input signal composed of several input planes.\nnn.ConvTranspose1d\nnn.ConvTranspose1d\nApplies a 1D transposed convolution operator over an input image composed of several input planes.\nnn.ConvTranspose2d\nnn.ConvTranspose2d\nApplies a 2D transposed convolution operator over an input image composed of several input planes.\nnn.ConvTranspose3d\nnn.ConvTranspose3d\nApplies a 3D transposed convolution operator over an input image composed of several input planes.\nnn.LazyConv1d\nnn.LazyConv1d\nAtorch.nn.Conv1dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.Conv1d\nin_channels\nnn.LazyConv2d\nnn.LazyConv2d\nAtorch.nn.Conv2dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.Conv2d\nin_channels\nnn.LazyConv3d\nnn.LazyConv3d\nAtorch.nn.Conv3dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.Conv3d\nin_channels\nnn.LazyConvTranspose1d\nnn.LazyConvTranspose1d\nAtorch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.ConvTranspose1d\nin_channels\nnn.LazyConvTranspose2d\nnn.LazyConvTranspose2d\nAtorch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.ConvTranspose2d\nin_channels\nnn.LazyConvTranspose3d\nnn.LazyConvTranspose3d\nAtorch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.ConvTranspose3d\nin_channels\nnn.Unfold\nnn.Unfold\nExtracts sliding local blocks from a batched input tensor.\nnn.Fold\nnn.Fold\nCombines an array of sliding local blocks into a large containing tensor.\n\n## Pooling layers#\n\nnn.MaxPool1d\nnn.MaxPool1d\nApplies a 1D max pooling over an input signal composed of several input planes.\nnn.MaxPool2d\nnn.MaxPool2d\nApplies a 2D max pooling over an input signal composed of several input planes.\nnn.MaxPool3d\nnn.MaxPool3d\nApplies a 3D max pooling over an input signal composed of several input planes.\nnn.MaxUnpool1d\nnn.MaxUnpool1d\nComputes a partial inverse ofMaxPool1d.\nMaxPool1d\nnn.MaxUnpool2d\nnn.MaxUnpool2d\nComputes a partial inverse ofMaxPool2d.\nMaxPool2d\nnn.MaxUnpool3d\nnn.MaxUnpool3d\nComputes a partial inverse ofMaxPool3d.\nMaxPool3d\nnn.AvgPool1d\nnn.AvgPool1d\nApplies a 1D average pooling over an input signal composed of several input planes.\nnn.AvgPool2d\nnn.AvgPool2d\nApplies a 2D average pooling over an input signal composed of several input planes.\nnn.AvgPool3d\nnn.AvgPool3d\nApplies a 3D average pooling over an input signal composed of several input planes.\nnn.FractionalMaxPool2d\nnn.FractionalMaxPool2d\nApplies a 2D fractional max pooling over an input signal composed of several input planes.\nnn.FractionalMaxPool3d\nnn.FractionalMaxPool3d\nApplies a 3D fractional max pooling over an input signal composed of several input planes.\nnn.LPPool1d\nnn.LPPool1d\nApplies a 1D power-average pooling over an input signal composed of several input planes.\nnn.LPPool2d\nnn.LPPool2d\nApplies a 2D power-average pooling over an input signal composed of several input planes.\nnn.LPPool3d\nnn.LPPool3d\nApplies a 3D power-average pooling over an input signal composed of several input planes.\nnn.AdaptiveMaxPool1d\nnn.AdaptiveMaxPool1d\nApplies a 1D adaptive max pooling over an input signal composed of several input planes.\nnn.AdaptiveMaxPool2d\nnn.AdaptiveMaxPool2d\nApplies a 2D adaptive max pooling over an input signal composed of several input planes.\nnn.AdaptiveMaxPool3d\nnn.AdaptiveMaxPool3d\nApplies a 3D adaptive max pooling over an input signal composed of several input planes.\nnn.AdaptiveAvgPool1d\nnn.AdaptiveAvgPool1d\nApplies a 1D adaptive average pooling over an input signal composed of several input planes.\nnn.AdaptiveAvgPool2d\nnn.AdaptiveAvgPool2d\nApplies a 2D adaptive average pooling over an input signal composed of several input planes.\nnn.AdaptiveAvgPool3d\nnn.AdaptiveAvgPool3d\nApplies a 3D adaptive average pooling over an input signal composed of several input planes.\n\n## Padding Layers#\n\nnn.ReflectionPad1d\nnn.ReflectionPad1d\nPads the input tensor using the reflection of the input boundary.\nnn.ReflectionPad2d\nnn.ReflectionPad2d\nPads the input tensor using the reflection of the input boundary.\nnn.ReflectionPad3d\nnn.ReflectionPad3d\nPads the input tensor using the reflection of the input boundary.\nnn.ReplicationPad1d\nnn.ReplicationPad1d\nPads the input tensor using replication of the input boundary.\nnn.ReplicationPad2d\nnn.ReplicationPad2d\nPads the input tensor using replication of the input boundary.\nnn.ReplicationPad3d\nnn.ReplicationPad3d\nPads the input tensor using replication of the input boundary.\nnn.ZeroPad1d\nnn.ZeroPad1d\nPads the input tensor boundaries with zero.\nnn.ZeroPad2d\nnn.ZeroPad2d\nPads the input tensor boundaries with zero.\nnn.ZeroPad3d\nnn.ZeroPad3d\nPads the input tensor boundaries with zero.\nnn.ConstantPad1d\nnn.ConstantPad1d\nPads the input tensor boundaries with a constant value.\nnn.ConstantPad2d\nnn.ConstantPad2d\nPads the input tensor boundaries with a constant value.\nnn.ConstantPad3d\nnn.ConstantPad3d\nPads the input tensor boundaries with a constant value.\nnn.CircularPad1d\nnn.CircularPad1d\nPads the input tensor using circular padding of the input boundary.\nnn.CircularPad2d\nnn.CircularPad2d\nPads the input tensor using circular padding of the input boundary.\nnn.CircularPad3d\nnn.CircularPad3d\nPads the input tensor using circular padding of the input boundary.\n\n## Non-linear Activations (weighted sum, nonlinearity)#\n\nnn.ELU\nnn.ELU\nApplies the Exponential Linear Unit (ELU) function, element-wise.\nnn.Hardshrink\nnn.Hardshrink\nApplies the Hard Shrinkage (Hardshrink) function element-wise.\nnn.Hardsigmoid\nnn.Hardsigmoid\nApplies the Hardsigmoid function element-wise.\nnn.Hardtanh\nnn.Hardtanh\nApplies the HardTanh function element-wise.\nnn.Hardswish\nnn.Hardswish\nApplies the Hardswish function, element-wise.\nnn.LeakyReLU\nnn.LeakyReLU\nApplies the LeakyReLU function element-wise.\nnn.LogSigmoid\nnn.LogSigmoid\nApplies the Logsigmoid function element-wise.\nnn.MultiheadAttention\nnn.MultiheadAttention\nAllows the model to jointly attend to information from different representation subspaces.\nnn.PReLU\nnn.PReLU\nApplies the element-wise PReLU function.\nnn.ReLU\nnn.ReLU\nApplies the rectified linear unit function element-wise.\nnn.ReLU6\nnn.ReLU6\nApplies the ReLU6 function element-wise.\nnn.RReLU\nnn.RReLU\nApplies the randomized leaky rectified linear unit function, element-wise.\nnn.SELU\nnn.SELU\nApplies the SELU function element-wise.\nnn.CELU\nnn.CELU\nApplies the CELU function element-wise.\nnn.GELU\nnn.GELU\nApplies the Gaussian Error Linear Units function.\nnn.Sigmoid\nnn.Sigmoid\nApplies the Sigmoid function element-wise.\nnn.SiLU\nnn.SiLU\nApplies the Sigmoid Linear Unit (SiLU) function, element-wise.\nnn.Mish\nnn.Mish\nApplies the Mish function, element-wise.\nnn.Softplus\nnn.Softplus\nApplies the Softplus function element-wise.\nnn.Softshrink\nnn.Softshrink\nApplies the soft shrinkage function element-wise.\nnn.Softsign\nnn.Softsign\nApplies the element-wise Softsign function.\nnn.Tanh\nnn.Tanh\nApplies the Hyperbolic Tangent (Tanh) function element-wise.\nnn.Tanhshrink\nnn.Tanhshrink\nApplies the element-wise Tanhshrink function.\nnn.Threshold\nnn.Threshold\nThresholds each element of the input Tensor.\nnn.GLU\nnn.GLU\nApplies the gated linear unit function.\n\n## Non-linear Activations (other)#\n\nnn.Softmin\nnn.Softmin\nApplies the Softmin function to an n-dimensional input Tensor.\nnn.Softmax\nnn.Softmax\nApplies the Softmax function to an n-dimensional input Tensor.\nnn.Softmax2d\nnn.Softmax2d\nApplies SoftMax over features to each spatial location.\nnn.LogSoftmax\nnn.LogSoftmax\nApplies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor.\nnn.AdaptiveLogSoftmaxWithLoss\nnn.AdaptiveLogSoftmaxWithLoss\nEfficient softmax approximation.\n\n## Normalization Layers#\n\nnn.BatchNorm1d\nnn.BatchNorm1d\nApplies Batch Normalization over a 2D or 3D input.\nnn.BatchNorm2d\nnn.BatchNorm2d\nApplies Batch Normalization over a 4D input.\nnn.BatchNorm3d\nnn.BatchNorm3d\nApplies Batch Normalization over a 5D input.\nnn.LazyBatchNorm1d\nnn.LazyBatchNorm1d\nAtorch.nn.BatchNorm1dmodule with lazy initialization.\ntorch.nn.BatchNorm1d\nnn.LazyBatchNorm2d\nnn.LazyBatchNorm2d\nAtorch.nn.BatchNorm2dmodule with lazy initialization.\ntorch.nn.BatchNorm2d\nnn.LazyBatchNorm3d\nnn.LazyBatchNorm3d\nAtorch.nn.BatchNorm3dmodule with lazy initialization.\ntorch.nn.BatchNorm3d\nnn.GroupNorm\nnn.GroupNorm\nApplies Group Normalization over a mini-batch of inputs.\nnn.SyncBatchNorm\nnn.SyncBatchNorm\nApplies Batch Normalization over a N-Dimensional input.\nnn.InstanceNorm1d\nnn.InstanceNorm1d\nApplies Instance Normalization.\nnn.InstanceNorm2d\nnn.InstanceNorm2d\nApplies Instance Normalization.\nnn.InstanceNorm3d\nnn.InstanceNorm3d\nApplies Instance Normalization.\nnn.LazyInstanceNorm1d\nnn.LazyInstanceNorm1d\nAtorch.nn.InstanceNorm1dmodule with lazy initialization of thenum_featuresargument.\ntorch.nn.InstanceNorm1d\nnum_features\nnn.LazyInstanceNorm2d\nnn.LazyInstanceNorm2d\nAtorch.nn.InstanceNorm2dmodule with lazy initialization of thenum_featuresargument.\ntorch.nn.InstanceNorm2d\nnum_features\nnn.LazyInstanceNorm3d\nnn.LazyInstanceNorm3d\nAtorch.nn.InstanceNorm3dmodule with lazy initialization of thenum_featuresargument.\ntorch.nn.InstanceNorm3d\nnum_features\nnn.LayerNorm\nnn.LayerNorm\nApplies Layer Normalization over a mini-batch of inputs.\nnn.LocalResponseNorm\nnn.LocalResponseNorm\nApplies local response normalization over an input signal.\nnn.RMSNorm\nnn.RMSNorm\nApplies Root Mean Square Layer Normalization over a mini-batch of inputs.\n\n## Recurrent Layers#\n\nnn.RNNBase\nnn.RNNBase\nBase class for RNN modules (RNN, LSTM, GRU).\nnn.RNN\nnn.RNN\nApply a multi-layer Elman RNN withtanh\u2061\\tanhtanhorReLU\\text{ReLU}ReLUnon-linearity to an input sequence.\nnn.LSTM\nnn.LSTM\nApply a multi-layer long short-term memory (LSTM) RNN to an input sequence.\nnn.GRU\nnn.GRU\nApply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\nnn.RNNCell\nnn.RNNCell\nAn Elman RNN cell with tanh or ReLU non-linearity.\nnn.LSTMCell\nnn.LSTMCell\nA long short-term memory (LSTM) cell.\nnn.GRUCell\nnn.GRUCell\nA gated recurrent unit (GRU) cell.\n\n## Transformer Layers#\n\nnn.Transformer\nnn.Transformer\nA basic transformer layer.\nnn.TransformerEncoder\nnn.TransformerEncoder\nTransformerEncoder is a stack of N encoder layers.\nnn.TransformerDecoder\nnn.TransformerDecoder\nTransformerDecoder is a stack of N decoder layers.\nnn.TransformerEncoderLayer\nnn.TransformerEncoderLayer\nTransformerEncoderLayer is made up of self-attn and feedforward network.\nnn.TransformerDecoderLayer\nnn.TransformerDecoderLayer\nTransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n\n## Linear Layers#\n\nnn.Identity\nnn.Identity\nA placeholder identity operator that is argument-insensitive.\nnn.Linear\nnn.Linear\nApplies an affine linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b.\nnn.Bilinear\nnn.Bilinear\nApplies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b.\nnn.LazyLinear\nnn.LazyLinear\nAtorch.nn.Linearmodule wherein_featuresis inferred.\ntorch.nn.Linear\n\n## Dropout Layers#\n\nnn.Dropout\nnn.Dropout\nDuring training, randomly zeroes some of the elements of the input tensor with probabilityp.\np\nnn.Dropout1d\nnn.Dropout1d\nRandomly zero out entire channels.\nnn.Dropout2d\nnn.Dropout2d\nRandomly zero out entire channels.\nnn.Dropout3d\nnn.Dropout3d\nRandomly zero out entire channels.\nnn.AlphaDropout\nnn.AlphaDropout\nApplies Alpha Dropout over the input.\nnn.FeatureAlphaDropout\nnn.FeatureAlphaDropout\nRandomly masks out entire channels.\n\n## Sparse Layers#\n\nnn.Embedding\nnn.Embedding\nA simple lookup table that stores embeddings of a fixed dictionary and size.\nnn.EmbeddingBag\nnn.EmbeddingBag\nCompute sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.\n\n## Distance Functions#\n\nnn.CosineSimilarity\nnn.CosineSimilarity\nReturns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed alongdim.\nnn.PairwiseDistance\nnn.PairwiseDistance\nComputes the pairwise distance between input vectors, or between columns of input matrices.\n\n## Loss Functions#\n\nnn.L1Loss\nnn.L1Loss\nCreates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy.\nnn.MSELoss\nnn.MSELoss\nCreates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy.\nnn.CrossEntropyLoss\nnn.CrossEntropyLoss\nThis criterion computes the cross entropy loss between input logits and target.\nnn.CTCLoss\nnn.CTCLoss\nThe Connectionist Temporal Classification loss.\nnn.NLLLoss\nnn.NLLLoss\nThe negative log likelihood loss.\nnn.PoissonNLLLoss\nnn.PoissonNLLLoss\nNegative log likelihood loss with Poisson distribution of target.\nnn.GaussianNLLLoss\nnn.GaussianNLLLoss\nGaussian negative log likelihood loss.\nnn.KLDivLoss\nnn.KLDivLoss\nThe Kullback-Leibler divergence loss.\nnn.BCELoss\nnn.BCELoss\nCreates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:\nnn.BCEWithLogitsLoss\nnn.BCEWithLogitsLoss\nThis loss combines aSigmoidlayer and theBCELossin one single class.\nnn.MarginRankingLoss\nnn.MarginRankingLoss\nCreates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batch or 0DTensors, and a label 1D mini-batch or 0DTensoryyy(containing 1 or -1).\nnn.HingeEmbeddingLoss\nnn.HingeEmbeddingLoss\nMeasures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1).\nnn.MultiLabelMarginLoss\nnn.MultiLabelMarginLoss\nCreates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices).\nnn.HuberLoss\nnn.HuberLoss\nCreates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.\nnn.SmoothL1Loss\nnn.SmoothL1Loss\nCreates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.\nnn.SoftMarginLoss\nnn.SoftMarginLoss\nCreates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1).\nnn.MultiLabelSoftMarginLoss\nnn.MultiLabelSoftMarginLoss\nCreates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C).\nnn.CosineEmbeddingLoss\nnn.CosineEmbeddingLoss\nCreates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1.\nnn.MultiMarginLoss\nnn.MultiMarginLoss\nCreates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):\nnn.TripletMarginLoss\nnn.TripletMarginLoss\nCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000.\nnn.TripletMarginWithDistanceLoss\nnn.TripletMarginWithDistanceLoss\nCreates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n\n## Vision Layers#\n\nnn.PixelShuffle\nnn.PixelShuffle\nRearrange elements in a tensor according to an upscaling factor.\nnn.PixelUnshuffle\nnn.PixelUnshuffle\nReverse the PixelShuffle operation.\nnn.Upsample\nnn.Upsample\nUpsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.\nnn.UpsamplingNearest2d\nnn.UpsamplingNearest2d\nApplies a 2D nearest neighbor upsampling to an input signal composed of several input channels.\nnn.UpsamplingBilinear2d\nnn.UpsamplingBilinear2d\nApplies a 2D bilinear upsampling to an input signal composed of several input channels.\n\n## Shuffle Layers#\n\nnn.ChannelShuffle\nnn.ChannelShuffle\nDivides and rearranges the channels in a tensor.\n\n## DataParallel Layers (multi-GPU, distributed)#\n\nnn.DataParallel\nnn.DataParallel\nImplements data parallelism at the module level.\nnn.parallel.DistributedDataParallel\nnn.parallel.DistributedDataParallel\nImplement distributed data parallelism based ontorch.distributedat module level.\ntorch.distributed\n\n## Utilities#\n\nFrom thetorch.nn.utilsmodule:\ntorch.nn.utils\nUtility functions to clip parameter gradients.\nclip_grad_norm_\n\nclip_grad_norm_\nClip the gradient norm of an iterable of parameters.\nclip_grad_norm\n\nclip_grad_norm\nClip the gradient norm of an iterable of parameters.\nclip_grad_value_\n\nclip_grad_value_\nClip the gradients of an iterable of parameters at specified value.\nget_total_norm\n\nget_total_norm\nCompute the norm of an iterable of tensors.\nclip_grads_with_norm_\n\nclip_grads_with_norm_\nScale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm.\nUtility functions to flatten and unflatten Module parameters to and from a single vector.\nparameters_to_vector\n\nparameters_to_vector\nFlatten an iterable of parameters into a single vector.\nvector_to_parameters\n\nvector_to_parameters\nCopy slices of a vector into an iterable of parameters.\nUtility functions to fuse Modules with BatchNorm modules.\nfuse_conv_bn_eval\n\nfuse_conv_bn_eval\nFuse a convolutional module and a BatchNorm module into a single, new convolutional module.\nfuse_conv_bn_weights\n\nfuse_conv_bn_weights\nFuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters.\nfuse_linear_bn_eval\n\nfuse_linear_bn_eval\nFuse a linear module and a BatchNorm module into a single, new linear module.\nfuse_linear_bn_weights\n\nfuse_linear_bn_weights\nFuse linear module parameters and BatchNorm module parameters into new linear module parameters.\nUtility functions to convert Module parameter memory formats.\nconvert_conv2d_weight_memory_format\n\nconvert_conv2d_weight_memory_format\nConvertmemory_formatofnn.Conv2d.weighttomemory_format.\nmemory_format\nnn.Conv2d.weight\nmemory_format\nconvert_conv3d_weight_memory_format\n\nconvert_conv3d_weight_memory_format\nConvertmemory_formatofnn.Conv3d.weighttomemory_formatThe conversion recursively applies to nestednn.Module, includingmodule.\nmemory_format\nnn.Conv3d.weight\nmemory_format\nnn.Module\nmodule\nUtility functions to apply and remove weight normalization from Module parameters.\nweight_norm\n\nweight_norm\nApply weight normalization to a parameter in the given module.\nremove_weight_norm\n\nremove_weight_norm\nRemove the weight normalization reparameterization from a module.\nspectral_norm\n\nspectral_norm\nApply spectral normalization to a parameter in the given module.\nremove_spectral_norm\n\nremove_spectral_norm\nRemove the spectral normalization reparameterization from a module.\nUtility functions for initializing Module parameters.\nskip_init\n\nskip_init\nGiven a module class object and args / kwargs, instantiate the module without initializing parameters / buffers.\nUtility classes and functions for pruning Module parameters.\nprune.BasePruningMethod\nprune.BasePruningMethod\nAbstract base class for creation of new pruning techniques.\nprune.PruningContainer\nprune.PruningContainer\nContainer holding a sequence of pruning methods for iterative pruning.\nprune.Identity\nprune.Identity\nUtility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.\nprune.RandomUnstructured\nprune.RandomUnstructured\nPrune (currently unpruned) units in a tensor at random.\nprune.L1Unstructured\nprune.L1Unstructured\nPrune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.\nprune.RandomStructured\nprune.RandomStructured\nPrune entire (currently unpruned) channels in a tensor at random.\nprune.LnStructured\nprune.LnStructured\nPrune entire (currently unpruned) channels in a tensor based on their Ln-norm.\nn\nprune.CustomFromMask\nprune.CustomFromMask\n\nprune.identity\nprune.identity\nApply pruning reparametrization without pruning any units.\nprune.random_unstructured\nprune.random_unstructured\nPrune tensor by removing random (currently unpruned) units.\nprune.l1_unstructured\nprune.l1_unstructured\nPrune tensor by removing units with the lowest L1-norm.\nprune.random_structured\nprune.random_structured\nPrune tensor by removing random channels along the specified dimension.\nprune.ln_structured\nprune.ln_structured\nPrune tensor by removing channels with the lowest Ln-norm along the specified dimension.\nn\nprune.global_unstructured\nprune.global_unstructured\nGlobally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method.\nparameters\npruning_method\nprune.custom_from_mask\nprune.custom_from_mask\nPrune tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask.\nname\nmodule\nmask\nprune.remove\nprune.remove\nRemove the pruning reparameterization from a module and the pruning method from the forward hook.\nprune.is_pruned\nprune.is_pruned\nCheck if a module is pruned by looking for pruning pre-hooks.\nParametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization().\ntorch.nn.utils.parameterize.register_parametrization()\nparametrizations.orthogonal\nparametrizations.orthogonal\nApply an orthogonal or unitary parametrization to a matrix or a batch of matrices.\nparametrizations.weight_norm\nparametrizations.weight_norm\nApply weight normalization to a parameter in the given module.\nparametrizations.spectral_norm\nparametrizations.spectral_norm\nApply spectral normalization to a parameter in the given module.\nUtility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizations tutorialfor more information on how to implement your own parametrizations.\nparametrize.register_parametrization\nparametrize.register_parametrization\nRegister a parametrization to a tensor in a module.\nparametrize.remove_parametrizations\nparametrize.remove_parametrizations\nRemove the parametrizations on a tensor in a module.\nparametrize.cached\nparametrize.cached\nContext manager that enables the caching system within parametrizations registered withregister_parametrization().\nregister_parametrization()\nparametrize.is_parametrized\nparametrize.is_parametrized\nDetermine if a module has a parametrization.\nparametrize.transfer_parametrizations_and_params\nparametrize.transfer_parametrizations_and_params\nTransfer parametrizations and the parameters they parametrize fromfrom_moduletoto_module.\nfrom_module\nto_module\nparametrize.type_before_parametrizations\nparametrize.type_before_parametrizations\nReturn the module type before parametrizations were applied and if not, then it returns the module type.\nparametrize.ParametrizationList\nparametrize.ParametrizationList\nA sequential container that holds and manages the original parameters or buffers of a parametrizedtorch.nn.Module.\ntorch.nn.Module\nUtility functions to call a given Module in a stateless manner.\nstateless.functional_call\nstateless.functional_call\nPerform a functional call on the module by replacing the module parameters and buffers with the provided ones.\nUtility functions in other modules\nnn.utils.rnn.PackedSequence\nnn.utils.rnn.PackedSequence\nHolds the data and list ofbatch_sizesof a packed sequence.\nbatch_sizes\nnn.utils.rnn.pack_padded_sequence\nnn.utils.rnn.pack_padded_sequence\nPacks a Tensor containing padded sequences of variable length.\nnn.utils.rnn.pad_packed_sequence\nnn.utils.rnn.pad_packed_sequence\nPad a packed batch of variable length sequences.\nnn.utils.rnn.pad_sequence\nnn.utils.rnn.pad_sequence\nPad a list of variable length Tensors withpadding_value.\npadding_value\nnn.utils.rnn.pack_sequence\nnn.utils.rnn.pack_sequence\nPacks a list of variable length Tensors.\nnn.utils.rnn.unpack_sequence\nnn.utils.rnn.unpack_sequence\nUnpack PackedSequence into a list of variable length Tensors.\nnn.utils.rnn.unpad_sequence\nnn.utils.rnn.unpad_sequence\nUnpad padded Tensor into a list of variable length Tensors.\nnn.utils.rnn.invert_permutation\nnn.utils.rnn.invert_permutation\nReturns the inverse ofpermutation.\npermutation\nnn.parameter.is_lazy\nnn.parameter.is_lazy\nReturns whetherparamis anUninitializedParameterorUninitializedBuffer.\nparam\nUninitializedParameter\nUninitializedBuffer\nnn.factory_kwargs\nnn.factory_kwargs\nReturn a canonicalized dict of factory kwargs.\nnn.modules.flatten.Flatten\nnn.modules.flatten.Flatten\nFlattens a contiguous range of dims into a tensor.\nnn.modules.flatten.Unflatten\nnn.modules.flatten.Unflatten\nUnflattens a tensor dim expanding it to a desired shape.\n\n## Quantized Functions#\n\nQuantization refers to techniques for performing computations and storing tensors at lower bitwidths than\nfloating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to theQuantizationdocumentation.\n\n## Lazy Modules Initialization#\n\nnn.modules.lazy.LazyModuleMixin\nnn.modules.lazy.LazyModuleMixin\nA mixin for modules that lazily initialize parameters, also known as \"lazy modules\".",
  "url": "https://pytorch.org/docs/stable/nn.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
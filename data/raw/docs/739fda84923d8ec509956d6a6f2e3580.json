{
  "doc_id": "739fda84923d8ec509956d6a6f2e3580",
  "source": "pytorch_docs",
  "title": "PyTorch Symmetric Memory \u2014 PyTorch 2.9 documentation",
  "text": "\n## PyTorch Symmetric Memory#\n\nCreated On: Nov 10, 2025 | Last Updated On: Nov 10, 2025\nNote\ntorch.distributed._symmetric_memoryis currently in alpha state and under\ndevelopment. API changes may be possible.\ntorch.distributed._symmetric_memory\n\n## Why Symmetric Memory?#\n\nWith rapidly evolving parallelization techniques, existing frameworks and\nlibraries often struggle to keep up, and developers increasingly rely on custom\nimplementations directly scheduling communications and computations. In recent\nyears we\u2019ve witnessed a shift from primarily relying on one-dimensional\ndata-parallelism techniques to multi-dimensional parallelism ones. The latter\nhave different latency requirements for different types of communications and\nthus require fine-grained overlapping of compute and communications.\nTo minimize compute interference, they also require the use of copy engines and\nnetwork interface cards (NICs) to drive communication. Network transport\nprotocols such as remote direct memory access (RDMA) enhance the performance by\nenabling direct, high-speed, and low-latency communication between processors\nand memory. This increase in variety indicates the need for finer-grained\ncommunication primitives than are offered today by high-level collective APIs,\nones that would enable developers to implement specific algorithms tailored for\ntheir use cases, such as low-latency collectives, fine-grained\ncompute-communications overlap, or custom fusions.\nFurthermore, today\u2019s advanced AI systems connect GPUs with high-bandwidth links\n(such as NVLinks, InfiniBand or RoCE), making GPU global memory directly\naccessible to peers. Such connections present a great opportunity for\nprogrammers to program the system as a single, gigantic GPU with vast accessible\nmemory, instead of programming singular \u201cGPU islands.\u201d\nIn this document, we will show how you can use PyTorch Symmetric Memory to\nprogram modern GPU systems as a \u201csingle GPU\u201d and achieve fine-grained remote\naccess.\n\n## What PyTorch Symmetric Memory unlocks?#\n\nPyTorch Symmetric Memory unlocks three new capabilities:\nCustomized communication patterns: Increased flexibility in kernel writing\nallows developers to write custom kernels that implement their custom\ncomputations and communications, directly tailored to the need of the\napplication. It will also be straightforward to add support for new data types\nalong with the special compute that those data types might require, even if it\u2019s\nnot present yet in the standard libraries.\nIn-kernel compute-comm fusion: Device-initiated communication capability\nallows developers to write kernels with both computation and communication\ninstructions, allowing for the fusion of computation and data movement in the\nsmallest possible granularity.\nLow-latency remote access: Network transport protocols like RDMA enhance the\nperformance of symmetric memory in networked environments by enabling direct,\nhigh-speed, and low-latency communication between processors and memory. RDMA\neliminates the overhead associated with the traditional network stack and CPU\ninvolvement. It also offloads data transfer from the compute to the NICs,\nfreeing up compute resources for computational tasks.\nNext, we will show you how PyTorch Symmetric Memory (SymmMem) enables new\napplications with the above capabilities.\n\n## A \u201cHello World\u201d example#\n\nThe PyTorch SymmMem programming model involves two key elements:\ncreating symmetric tensors\ncreating SymmMem kernels\nTo create symmetric tensors, one can use thetorch.distributed._symmetric_memorypackage:\ntorch.distributed._symmetric_memory\n\n```python\nimport torch.distributed._symmetric_memory as symm_mem\n\nt = symm_mem.empty(128, device=torch.device(\"cuda\", rank))\nhdl = symm_mem.rendezvous(t, group)\n\n```\n\nThesymm_mem.emptyfunction creates a tensor that is backed by a symmetric\nmemory allocation. Therendezvousfunction establishes a rendezvous with peers\nin the group, and returns a handle to the symmetric memory allocation. The\nhandle provides method to access information related to the symmetric memory\nallocation, such as pointers to symmetric buffer on peer ranks, multicast\npointer (if supported), and signal pads.\nsymm_mem.empty\nrendezvous\nTheemptyandrendezvousfunctions must be called in the same order on all\nranks in the group.\nempty\nrendezvous\nThen, collectives can be called on these tensors. For example, to perform a\none-shot all-reduce:\n\n```python\n# Most SymmMem ops are under the torch.ops.symm_mem namespace\ntorch.ops.symm_mem.one_shot_all_reduce(t, \"sum\", group)\n\n```\n\nPlease note thattorch.ops.symm_memis an \u201cop namespace\u201d instead of a python\nmodule. Therefore, you can\u2019t import it byimporttorch.ops.symm_mem, neither\ncan you import an op byfromtorch.ops.symm_memimportone_shot_all_reduce.\nYou can call the op directly as in the example above.\ntorch.ops.symm_mem\nimporttorch.ops.symm_mem\nfromtorch.ops.symm_memimportone_shot_all_reduce\n\n## Write your own kernel#\n\nTo write your own kernel doing communications with symmetric memory, you\u2019ll need\naccess to the addresses of mapped peer buffers and access to signal pads that\nare required for synchronization. In the kernel you\u2019ll also need to perform\ncorrect synchronizations to make sure that peers are ready for communication,\nand signal to them that this GPU is ready.\nPyTorch Symmetric Memory provides CUDA Graph-compatible synchronization\nprimitives that operate on the signal pad accompanying each symmetric memory\nallocation. Kernels using symmetric memory can be written both in CUDA and in\nTriton. Here\u2019s an example allocating symmetric tensor and exchanging handles:\n\n```python\nimport torch.distributed._symmetric_memory as symm_mem\n\ndist.init_process_group()\nrank = dist.get_rank()\n\n# Allocate a tensor\nt = symm_mem.empty(4096, device=f\"cuda:{rank}\")\n# Establish symmetric memory and obtain the handle\nhdl = symm_mem.rendezvous(t, dist.group.WORLD)\n\n```\n\nAccess to buffer pointers, multimem pointer, and signal pads is provided via:\n\n```python\nhdl.buffer_ptrs\nhdl.multicast_ptr\nhdl.signal_pad_ptrs\n\n```\n\nData pointed to bybuffer_ptrscan be accessed just like regular local data,\nand any necessary compute can also be performed in the usual ways. As with local\ndata, you can and should use vectorized accesses to improve efficiency.\nbuffer_ptrs\nSymmetric memory is especially convenient for writing kernels in Triton. While\npreviously Triton removed the barriers to writing efficient CUDA code, now\ncommunications can be added easily to Triton kernels. The kernel below\ndemonstrates a low-latency, all-reduce kernel written in Triton.\n\n```python\n@triton.jit\ndef one_shot_all_reduce_kernel(\n    buf_tuple,\n    signal_pad_ptrs,\n    output_ptr,\n    numel: tl.constexpr,\n    rank: tl.constexpr,\n    world_size: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    ptx_utils.symm_mem_sync(\n        signal_pad_ptrs, None, rank, world_size, hasSubsequenceMemAccess=True\n    )\n\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    while block_start < numel:\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < numel\n        acc = tl.zeros((BLOCK_SIZE,), dtype=tl.bfloat16)\n\n        for i in tl.static_range(world_size):\n            buffer_rank = buf_tuple[i]\n            x = tl.load(buffer_rank + offsets, mask=mask)\n            acc += x\n\n        tl.store(output_ptr + offsets, acc, mask=mask)\n        block_start += tl.num_programs(axis=0) * BLOCK_SIZE\n\n    ptx_utils.symm_mem_sync(\n        signal_pad_ptrs, None, rank, world_size, hasPreviousMemAccess=True\n    )\n\n```\n\nSynchronizations at the beginning and the end of the kernel above guarantee that\nall the processes see consistent data. The bulk of the kernel is recognizable\nTriton code, and Triton will optimize it behind the scene, making sure memory\naccesses are performed in an efficient way with vectorization and unrolling. As\nwith all Triton kernels, it is easily modifiable to add extra computations or\nchange the communication algorithm. Visit\nhttps://github.com/meta-pytorch/kraken/blob/main/kraken to see additional\nutilities and examples of using symmetric memory to implement common patterns in\nTriton.\n\n## Scale out#\n\nLarge language models distribute experts onto more than 8 GPUs, hence requiring\nmulti-node access capability. NICs capable of RDMA come to help. In addition,\nsoftware libraries such as NVSHMEM or rocSHMEM abstract away the programming\ndifference between intra-node access and inter-node access with primitives that\nare slightly higher level than pointer access, such as put and get.\nPyTorch provides NVSHMEM plugins to augment Triton kernels\u2019 cross-node\ncapabilities. As shown in the code snippet below, one can initiate a cross-node\nput command within the kernel.\n\n```python\nimport torch.distributed._symmetric_memory._nvshmem_triton as nvshmem\nfrom torch.distributed._symmetric_memory._nvshmem_triton import requires_nvshmem\n\n@requires_nvshmem\n@triton.jit\ndef my_put_kernel(\n    dest,\n    src,\n    nelems,\n    pe,\n):\n    nvshmem.put(dest, src, nelems, pe)\n\n```\n\nTherequires_nvshmemdecorator is used to indicate that the kernel requires\nthe NVSHMEM device library as an external dependency. When Triton compiles the\nkernel, the decorator will search your system paths for the NVSHMEM device\nlibrary. If it is available, Triton will include the necessary device assembly\nto use the NVSHMEM functions.\nrequires_nvshmem\n\n## API Reference#\n\nSimilar totorch.empty(). The returned tensor can be used bytorch._distributed._symmetric_memory.rendezvous()to establish a\nsymmetric memory tensor among participating processes.\ntorch.empty()\ntorch._distributed._symmetric_memory.rendezvous()\nsize(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple.\ndtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nNone\ntorch.set_default_dtype()\ndevice(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_device()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\ntorch.device\nNone\ntorch.set_default_device()\ndevice\nEstablish a symmetric memory tensor among participating processes. This is\na collective operation.\ntensor(torch.Tensor) \u2013 the local tensor used to establish the symmetric memory tensor.\nIt must be allocated viatorch._distributed._symmetric_memory.empty(). The shape,\ndtype, and device type must be identical across all participating processes.\ntorch.Tensor\ntorch._distributed._symmetric_memory.empty()\ngroup(Union[str,torch.distributed.ProcessGroup]) \u2013 The group identifying the\nparticipating processes. This can be either a group name or a process group object.\ntorch.distributed.ProcessGroup\n_SymmetricMemory\nCheck if NVSHMEM is available in current build and on current system.\nbool\nSet the backend for symmetric memory allocation. This is a global setting\nand affects all subsequent calls totorch._distributed._symmetric_memory.empty().  Note that the backend\ncannot be changed once a symmetric memory tensor has been allocated.\ntorch._distributed._symmetric_memory.empty()\nbackend(str) \u2013 the backend for symmetric memory allocation. Currently,\nonly\u201cNVSHMEM\u201d,\u201cCUDA\u201d,\u201cNCCL\u201dare supported.\nGet the backend for symmetric memory allocation for a given device. If not\nfound, return None.\ndevice(torch.deviceor str) \u2013 the device for which to get the backend.\nstr| None\n\n## Op Reference#\n\nNote\nThe following ops are hosted in thetorch.ops.symm_memnamespace. You can call\nthem directly viatorch.ops.symm_mem.<op_name>.\ntorch.ops.symm_mem\ntorch.ops.symm_mem.<op_name>\nPerforms a multimem all-reduce operation on the input tensor. This operation\nrequires hardware support for multimem operations. On NVIDIA GPUs, NVLink\nSHARP is required.\ninput(Tensor) \u2013 Input tensor to perform all-reduce on. Must be symmetric.\nreduce_op(str) \u2013 Reduction operation to perform. Currently only \u201csum\u201d is supported.\ngroup_name(str) \u2013 Name of the group to perform all-reduce on.\nPerforms a multimem all-gather operation on the input tensor. This operation requires hardware support for multimem operations. On NVIDIA GPUs, NVLink SHARP is required.\ninput(Tensor) \u2013 Input tensor to perform all-gather on.\ngroup_name(str) \u2013 Name of the group to perform all-gather on.\nout(Tensor) \u2013 Output tensor to store the result of the all-gather operation. Must be symmetric.\nPerforms a one-shot all-reduce operation on the input tensor.\ninput(Tensor) \u2013 Input tensor to perform all-reduce on. Must be symmetric.\nreduce_op(str) \u2013 Reduction operation to perform. Currently only \u201csum\u201d is supported.\ngroup_name(str) \u2013 Name of the group to perform all-reduce on.\nPerforms a one-shot all-reduce operation based on the input tensor and writes the result to the output tensor.\ninput(Tensor) \u2013 Input tensor to perform all-reduce on. Must be symmetric.\nreduce_op(str) \u2013 Reduction operation to perform. Currently only \u201csum\u201d is supported.\ngroup_name(str) \u2013 Name of the group to perform all-reduce on.\nout(Tensor) \u2013 Output tensor to store the result of the all-reduce operation. Can be a regular tensor.\nPerforms a two-shot all-reduce operation on the input tensor.\ninput(Tensor) \u2013 Input tensor to perform all-reduce on. Must be symmetric.\nreduce_op(str) \u2013 Reduction operation to perform. Currently only \u201csum\u201d is supported.\ngroup_name(str) \u2013 Name of the group to perform all-reduce on.\nPerforms an all-to-all-v operation using NVSHMEM, with split information provided on device.\ninput(Tensor) \u2013 Input tensor to perform all-to-all on. Must be symmetric.\nout(Tensor) \u2013 Output tensor to store the result of the all-to-all operation. Must be symmetric.\nin_splits(Tensor) \u2013 Tensor containing splits of data to send to each peer. Must be symmetric. Must be of size (group_size,). The splits are in the unit of elements in the 1st dimension.\nout_splits_offsets(Tensor) \u2013 Tensor containing the splits and offsets of data received from each peer. Must be symmetric. Must be of size (2, group_size). The rows are (in order): output splits and output offsets.\ngroup_name(str) \u2013 Name of the group to perform all-to-all on.\nPerform a 2D all-to-all-v operation using NVSHMEM, with split information provided on device. In Mixture of Experts models, this operation can be used to dispatch tokens.\ninput(Tensor) \u2013 Input tensor to perform all-to-all on. Must be symmetric.\nout(Tensor) \u2013 Output tensor to store the result of the all-to-all operation. Must be symmetric.\nin_splits(Tensor) \u2013 Tensor containing the splits of data to send to each expert. Must be symmetric. Must be of size (group_size * ne,), where ne is the number of experts per rank. The splits are in the unit of elements in the 1st dimension.\nout_splits_offsets(Tensor) \u2013 Tensor containing the splits and offsets of data received from each peer. Must be symmetric. Must be of size (2, group_size * ne). The rows are (in order): output splits and output offsets.\ngroup_name(str) \u2013 Name of the group to perform all-to-all on.\nmajor_align(int) \u2013 Optional alignment for the major dimension of the output chunk for each expert. If not provided, the alignment is assumed to be 1. Any alignment adjustment will be reflected in the output offsets.\nA 2D AllToAllv shuffle is illustrated below:\n(world_size = 2, ne = 2, total number of experts = 4):\n\n```python\nSource: |       Rank 0      |       Rank 1      |\n        | c0 | c1 | c2 | c3 | d0 | d1 | d2 | d3 |\n\nDest  : |       Rank 0      |       Rank 1      |\n        | c0 | d0 | c1 | d1 | c2 | d2 | c3 | d3 |\n\n```\n\nwhere eachc_i/d_iare slices of theinputtensor, targeting experti, with length indicated by input splits.  That is, the 2D AllToAllv\nshuffle achieves a transpose from rank-major order at input to expert-major\norder at output.\nIfmajor_alignis not 1, the output offsets of c1, c2, c3 will be\nup-aligned to this value. For example, if c0 has length 5 and d0 has\nlength 7 (making a total of 12), and if themajor_alignis set to 16,\nthe output offset of c1 will be 16. Similar for c2 and c3. This value has\nno effect on the offset of the minor dimension, i.e.  d0, d1, d2 and d3.\nNote: since cutlass does not support empty bins, we set the aligned length\ntomajor_alignif it is 0. Seepytorch/pytorch#152668.\nPerform a 2D AllToAllv shuffle operation, with input split and offset\ninformation provided on device. The input offsets are not required to be\nexact prefix sum of the input splits, i.e. paddings are allowed between the\nsplit chunks. The paddings, however, will not be transferred to peer\nranks.\nIn Mixture of Experts models, this operation can be used to combine tokens\nprocessed by experts on parallel ranks. This operation can be viewed as an\n\u201creverse\u201d operation to theall_to_all_vdev_2doperation (which shuffles\ntokens to experts).\ninput(Tensor) \u2013 Input tensor to perform all-to-all on. Must be symmetric.\nout(Tensor) \u2013 Output tensor to store the result of the all-to-all operation. Must be symmetric.\nin_splits_offsets(Tensor) \u2013 Tensor containing the splits and offsets of data to send to each expert. Must be symmetric. Must be of size (2, group_size * ne), whereneis the number of experts. The rows are (in order): input splits and input offsets. The splits are in the unit of elements in the 1st dimension.\nout_splits_offsets(Tensor) \u2013 Tensor containing the splits and offsets of data received from each peer. Must be symmetric. Must be of size (2, group_size * ne). The rows are (in order): output splits and output offsets.\ngroup_name(str) \u2013 Name of the group to perform all-to-all on.",
  "url": "https://pytorch.org/docs/stable/symmetric_memory.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "320973fee640375481da64f85445d720",
  "source": "pytorch_docs",
  "title": "torch.func API Reference \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.func API Reference#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\n\n## Function Transforms#\n\nvmap\n\nvmap\nvmap is the vectorizing map;vmap(func)returns a new function that mapsfuncover some dimension of the inputs.\nvmap(func)\nfunc\ngrad\n\ngrad\ngradoperator helps computing gradients offuncwith respect to the input(s) specified byargnums.\ngrad\nfunc\nargnums\ngrad_and_value\n\ngrad_and_value\nReturns a function to compute a tuple of the gradient and primal, or forward, computation.\nvjp\n\nvjp\nStanding for the vector-Jacobian product, returns a tuple containing the results offuncapplied toprimalsand a function that, when givencotangents, computes the reverse-mode Jacobian offuncwith respect toprimalstimescotangents.\nfunc\nprimals\ncotangents\nfunc\nprimals\ncotangents\njvp\n\njvp\nStanding for the Jacobian-vector product, returns a tuple containing the output offunc(*primals)and the \"Jacobian offuncevaluated atprimals\" timestangents.\nfunc\nprimals\ntangents\nlinearize\n\nlinearize\nReturns the value offuncatprimalsand linear approximation atprimals.\nfunc\nprimals\nprimals\njacrev\n\njacrev\nComputes the Jacobian offuncwith respect to the arg(s) at indexargnumusing reverse mode autodiff\nfunc\nargnum\njacfwd\n\njacfwd\nComputes the Jacobian offuncwith respect to the arg(s) at indexargnumusing forward-mode autodiff\nfunc\nargnum\nhessian\n\nhessian\nComputes the Hessian offuncwith respect to the arg(s) at indexargnumvia a forward-over-reverse strategy.\nfunc\nargnum\nfunctionalize\n\nfunctionalize\nfunctionalize is a transform that can be used to remove (intermediate) mutations and aliasing from a function, while preserving the function's semantics.\n\n## Utilities for working with torch.nn.Modules#\n\nIn general, you can transform over a function that calls atorch.nn.Module.\nFor example, the following is an example of computing a jacobian of a function\nthat takes three values and returns three values:\ntorch.nn.Module\n\n```python\nmodel = torch.nn.Linear(3, 3)\n\ndef f(x):\n    return model(x)\n\nx = torch.randn(3)\njacobian = jacrev(f)(x)\nassert jacobian.shape == (3, 3)\n\n```\n\nHowever, if you want to do something like compute a jacobian over the parameters of the model, then there needs to be a way to construct a function where the parameters are the inputs to the function. That\u2019s whatfunctional_call()is for: it accepts an nn.Module, the transformedparameters, and the inputs to the Module\u2019s forward pass. It returns the value of running the Module\u2019s forward pass with the replaced parameters.\nfunctional_call()\nparameters\nHere\u2019s how we would compute the Jacobian over the parameters\n\n```python\nmodel = torch.nn.Linear(3, 3)\n\ndef f(params, x):\n    return torch.func.functional_call(model, params, x)\n\nx = torch.randn(3)\njacobian = jacrev(f)(dict(model.named_parameters()), x)\n\n```\n\nfunctional_call\n\nfunctional_call\nPerforms a functional call on the module by replacing the module parameters and buffers with the provided ones.\nstack_module_state\n\nstack_module_state\nPrepares a list of torch.nn.Modules for ensembling withvmap().\nvmap()\nreplace_all_batch_norm_modules_\n\nreplace_all_batch_norm_modules_\nIn place updatesrootby setting therunning_meanandrunning_varto be None and setting track_running_stats to be False for any nn.BatchNorm module inroot\nroot\nrunning_mean\nrunning_var\nroot\nIf you\u2019re looking for information on fixing Batch Norm modules, please follow the\nguidance here\n\n## Debug utilities#\n\ndebug_unwrap\n\ndebug_unwrap\nUnwraps a functorch tensor (e.g.",
  "url": "https://pytorch.org/docs/stable/func.api.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
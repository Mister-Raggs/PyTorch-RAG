{
  "doc_id": "6d7aed1767c33ffe3142b8306401b2f9",
  "source": "pytorch_docs",
  "title": "Understanding CUDA Memory Usage \u2014 PyTorch 2.9 documentation",
  "text": "\n## Understanding CUDA Memory Usage#\n\nCreated On: Aug 23, 2023 | Last Updated On: Sep 02, 2025\nTo debug CUDA memory use, PyTorch provides a way to generate memory snapshots that record the state of allocated CUDA memory\nat any point in time, and optionally record the history of allocation events that led up to that snapshot.\nThe generated snapshots can then be drag and dropped onto the interactiver viewer hosted atpytorch.org/memory_vizwhich\ncan be used to explore the snapshot.\nNote\nThe memory profiler and visualizer described in this document only have visibility into the CUDA memory that is\nallocated and managed through the PyTorch allocator.  Any memory allocated directly from CUDA APIs will not be\nvisible in the PyTorch memory profiler.\nNCCL (used for distributed communication on CUDA devices) is a common example of a library that allocates some\nGPU memory that is invisible to the PyTorch memory profiler.  SeeIdentifying Non-PyTorch allocationsfor more info.\n\n## Generating a Snapshot#\n\nThe common pattern for recording a snapshot is to enable memory history, run the code to be observed, and then save a file with a pickled snapshot:\n\n```python\n# enable memory history, which will\n# add tracebacks and event history to snapshots\ntorch.cuda.memory._record_memory_history()\n\nrun_your_code()\ntorch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n\n```\n\n\n## Using the visualizer#\n\nOpenhttps://pytorch.org/memory_vizand drag/drop the pickled snapshot file into the visualizer.\nThe visualizer is a javascript application that runs locally on your computer. It does not upload any snapshot data.\n\n## Active Memory Timeline#\n\nThe Active Memory Timeline shows all the live tensors over time in the snapshot on a particular GPU. Pan/Zoom over the plot to look at smaller allocations.\nMouse over allocated blocks to see a stack trace for when that block was allocated, and details like its address. The detail slider can be adjusted to\nrender fewer allocations and improve performance when there is a lot of data.\n\n## Allocator State History#\n\nThe Allocator State History shows individual allocator events in a timeline on the left. Select an event in the timeline to see a visual summary of the\nallocator state at that event. This summary shows each individual segment returned from cudaMalloc and how it is split up into blocks of individual allocations\nor free space. Mouse over segments and blocks to see the stack trace when the memory was allocated. Mouse over events to see the stack trace when the event occurred,\nsuch as when a tensor was freed. Out of memory errors are reported as OOM events. Looking at the state of memory during an OOM may provide insight into why\nan allocation failed even though reserved memory still exists.\nThe stack trace information also reports the address at which an allocation occurred.\nThe address b7f064c000000_0 refers to the (b)lock at address 7f064c000000 which is the \u201c_0\u201dth time this address was allocated.\nThis unique string can be looked up in the Active Memory Timeline and searched\nin the Active State History to examine the memory state when a tensor was allocated or freed.\n\n## Identifying Non-PyTorch allocations#\n\nIf you suspect CUDA memory is being allocated outside of PyTorch, you can collect the raw CUDA allocation info using\nthe pynvml package, and compare that to the allocation reported by pytorch.\nTo collect raw memory usage outside pytorch, usedevice_memory_used()\ndevice_memory_used()\n\n```python\nimport torch\ndevice_idx = ...\nprint(torch.cuda.device_memory_used(device_idx))\n\n```\n\n\n## Snapshot API Reference#\n\nEnable recording of stack traces associated with memory\nallocations, so you can tell what allocated any piece of memory intorch.cuda.memory._snapshot().\ntorch.cuda.memory._snapshot()\nIn addition to keeping stack traces with each current allocation and free,\nthis will also enable recording of a history of all alloc/free events.\nUsetorch.cuda.memory._snapshot()to retrieve this information,\nand the tools in_memory_viz.pyto visualize snapshots.\ntorch.cuda.memory._snapshot()\n\n## Buffer behavior#\n\nThis will store up tomax_entriesinstances ofTraceEntrywhen enabled.\nPython trace collection defaults tosys.maxsize, meaning long-running\nor indefinitely running jobs should set a reasonable limit to avoid excessive\nmemory use. Expect each entry to be several KB.\nLonger running workflows or those with smallermax_entriesvalues will only\nstore the last accumulatedmax_entriesentries, meaning new entries overwrite\nolder entries.\nC++ implementation for reference to ring buffer implementation:\n\n```python\nif (record_history) {\n  if (alloc_trace->size() < alloc_trace_max_entries_) {\n    alloc_trace->emplace_back(te);\n  } else {\n    (*alloc_trace)[alloc_trace_next++] = te;\n    if (alloc_trace_next == alloc_trace_max_entries_) {\n      alloc_trace_next = 0;\n    }\n  }\n}\n\n```\n\n\n## Latency impact#\n\nThe Python trace collection is fast (2us per trace), so you may consider\nenabling this on production jobs if you anticipate ever having to debug\nmemory issues.\nC++ trace collection is also fast (~50ns/frame), which for many typical programs\nworks out to ~2us per trace, but can vary depending on stack depth.\nNone, disable recording memory history.\u201cstate\u201d, keep information for currently allocated memory.\u201call\u201d, additionally keep a history of all alloc/free calls.\nDefaults to \u201call\u201d.\nLiteral[None, \u201cstate\u201d, \u201call\u201d], optional\nNone, Do not record any tracebacks.\u201cstate\u201d, Record tracebacks for currently allocated memory.\u201calloc\u201d, additionally keep tracebacks for alloc calls.\u201call\u201d, additionally keep tracebacks for free calls.\nDefaults to \u201call\u201d.\nLiteral[None, \u201cstate\u201d, \u201calloc\u201d, \u201call\u201d], optional\n\u201cpython\u201d, include Python, TorchScript, and inductor frames in tracebacks\u201call\u201d, additionally include C++ frames\nDefaults to \u201call\u201d.\nLiteral[\u201cpython\u201d, \u201call\u201d], optional\nKeep a maximum ofmax_entriesalloc/free events in the recorded history recorded.\nint, optional\nSave a snapshot of CUDA memory state at the time it was called.\nThe state is represented as a dictionary with the following structure.\n\n```python\nclass Snapshot(TypedDict):\n    segments: List[Segment]\n    device_traces: List[List[TraceEntry]]\n\n\nclass Segment(TypedDict):\n    # Segments are memory returned from a cudaMalloc call.\n    # The size of reserved memory is the sum of all Segments.\n    # Segments are cached and reused for future allocations.\n    # If the reuse is smaller than the segment, the segment\n    # is split into more then one Block.\n    # empty_cache() frees Segments that are entirely inactive.\n    address: int\n    total_size: int  #  cudaMalloc'd size of segment\n    stream: int\n    segment_type: Literal[\"small\", \"large\"]  # 'large' (>1MB)\n    allocated_size: int  # size of memory in use\n    active_size: int  # size of memory in use or in active_awaiting_free state\n    blocks: List[Block]\n\n\nclass Block(TypedDict):\n    # A piece of memory returned from the allocator, or\n    # current cached but inactive.\n    size: int\n    requested_size: int  # size requested during malloc, may be smaller than\n    # size due to rounding\n    address: int\n    state: Literal[\n        \"active_allocated\",  # used by a tensor\n        \"active_awaiting_free\",  # waiting for another stream to finish using\n        # this, then it will become free\n        \"inactive\",\n    ]  # free for reuse\n    frames: List[Frame]  # stack trace from where the allocation occurred\n\n\nclass Frame(TypedDict):\n    filename: str\n    line: int\n    name: str\n\n\nclass TraceEntry(TypedDict):\n    # When `torch.cuda.memory._record_memory_history()` is enabled,\n    # the snapshot will contain TraceEntry objects that record each\n    # action the allocator took.\n    action: Literal[\n        \"alloc\"  # memory allocated\n        \"free_requested\",  # the allocated received a call to free memory\n        \"free_completed\",  # the memory that was requested to be freed is now\n        # able to be used in future allocation calls\n        \"segment_alloc\",  # the caching allocator ask cudaMalloc for more memory\n        # and added it as a segment in its cache\n        \"segment_free\",  # the caching allocator called cudaFree to return memory\n        # to cuda possibly trying free up memory to\n        # allocate more segments or because empty_caches was called\n        \"oom\",  # the allocator threw an OOM exception. 'size' is\n        # the requested number of bytes that did not succeed\n        \"snapshot\",  # the allocator generated a memory snapshot\n        # useful to coorelate a previously taken\n        # snapshot with this trace\n    ]\n    addr: int  # not present for OOM\n    frames: List[Frame]\n    size: int\n    stream: int\n    device_free: int  # only present for OOM, the amount of\n    # memory cuda still reports to be free\n\n```\n\nThe Snapshot dictionary object\nSave a pickled version of thetorch.memory._snapshot()dictionary to a file.\nThis file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz\nSnapshot file sizes scale withmax_entriesand stack trace depth per entry,\nwith several KB per entry. These can easily be in the GB range for longer running\nworkflows with largemax_entries.\nfilename(str,optional) \u2013 Name of the file to create. Defaults to \u201cdump_snapshot.pickle\u201d.",
  "url": "https://pytorch.org/docs/stable/torch_cuda_memory.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
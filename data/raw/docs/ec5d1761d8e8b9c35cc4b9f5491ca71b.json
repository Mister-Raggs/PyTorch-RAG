{
  "doc_id": "ec5d1761d8e8b9c35cc4b9f5491ca71b",
  "source": "pytorch_docs",
  "title": "Custom Backends \u2014 PyTorch 2.9 documentation",
  "text": "\n## Custom Backends#\n\nCreated On: Jun 10, 2025 | Last Updated On: Jun 10, 2025\n\n## Overview#\n\ntorch.compileprovides a straightforward method to enable users\nto define custom backends.\ntorch.compile\nA backend function has the contract(gm:torch.fx.GraphModule,example_inputs:List[torch.Tensor])->Callable.\n(gm:torch.fx.GraphModule,example_inputs:List[torch.Tensor])->Callable\nBackend functions can be called by TorchDynamo, the graph tracing component oftorch.compile,\nafter tracing an FX graph and are\nexpected to return a compiled function that is equivalent to the traced FX graph.\nThe returned callable should have the same contract as theforwardfunction of the originaltorch.fx.GraphModulepassed into the backend:(*args:torch.Tensor)->List[torch.Tensor].\ntorch.compile\nforward\ntorch.fx.GraphModule\n(*args:torch.Tensor)->List[torch.Tensor]\nIn order for TorchDynamo to call your backend, pass your backend function as thebackendkwarg intorch.compile. For example,\nbackend\ntorch.compile\n\n```python\nimport torch\n\ndef my_custom_backend(gm, example_inputs):\n    return gm.forward\n\ndef f(...):\n    ...\n\nf_opt = torch.compile(f, backend=my_custom_backend)\n\n@torch.compile(backend=my_custom_backend)\ndef g(...):\n    ...\n\n```\n\nSee below for more examples.\n\n## Registering Custom Backends#\n\nYou can register your backend using theregister_backenddecorator, for example,\nregister_backend\n\n```python\nfrom torch._dynamo import register_backend\n\n@register_backend\ndef my_compiler(gm, example_inputs):\n    ...\n\n```\n\nBesides theregister_backenddecorator, if your backend is in another python package, you could also register your\nbackend through entry points of python package, which provides a way for a package to register a plugin for another one.\nregister_backend\nHint\nYou can learn more aboutentry_pointsin thepython packaging documentation.\nentry_points\nTo register your backend throughentry_points, you could add your backend function to thetorch_dynamo_backendsentry point group in thesetup.pyfile of your package like:\nentry_points\ntorch_dynamo_backends\nsetup.py\n\n```python\n...\nsetup(\n    ...\n    'torch_dynamo_backends': [\n        'my_compiler = your_module.submodule:my_compiler',\n    ]\n    ...\n)\n\n```\n\nPlease replace themy_compilerbefore=to the name of your backend\u2019s name and replace the part after=to\nthe module and function name of your backend function.\nThe entry point will be added to your python environment after the installation of the package.\nWhen you calltorch.compile(model,backend=\"my_compiler\"), PyTorch would first search the backend namedmy_compilerthat has been registered withregister_backend. If not found, it will continue to search in all backends registered\nviaentry_points.\nmy_compiler\n=\n=\ntorch.compile(model,backend=\"my_compiler\")\nmy_compiler\nregister_backend\nentry_points\nRegistration serves two purposes:\nYou can pass a string containing your backend function\u2019s name totorch.compileinstead of the function itself,\nfor example,torch.compile(model,backend=\"my_compiler\").\ntorch.compile\ntorch.compile(model,backend=\"my_compiler\")\nIt is required for use with theminifier. Any generated\ncode from the minifier must call your code that registers your backend function, typically through animportstatement.\nimport\n\n## Custom Backends after AOTAutograd#\n\nIt is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo.\nThis is useful for 2 main reasons:\nUsers can define backends that support model training, as AOTAutograd can generate the backward graph for compilation.\nAOTAutograd produces FX graphs consisting ofcore Aten ops. As a result,\ncustom backends only need to support the core Aten opset, which is a significantly smaller opset than the entire torch/Aten opset.\nWrap your backend withtorch._dynamo.backends.common.aot_autogradand usetorch.compilewith thebackendkwarg as before.\nBackend functions wrapped byaot_autogradshould have the same contract as before.\ntorch._dynamo.backends.common.aot_autograd\ntorch.compile\nbackend\naot_autograd\nBackend functions are passed toaot_autogradthrough thefw_compiler(forward compiler)\norbw_compiler(backward compiler) kwargs. Ifbw_compileris not specified, the backward compile function\ndefaults to the forward compile function.\naot_autograd\nfw_compiler\nbw_compiler\nbw_compiler\nOne caveat is that AOTAutograd requires compiled functions returned by backends to be \u201cboxed\u201d. This can be done by wrapping\nthe compiled function withfunctorch.compile.make_boxed_func.\nfunctorch.compile.make_boxed_func\nFor example,\n\n```python\nfrom torch._dynamo.backends.common import aot_autograd\nfrom functorch.compile import make_boxed_func\n\ndef my_compiler(gm, example_inputs):\n    return make_boxed_func(gm.forward)\n\nmy_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n\nmodel_opt = torch.compile(model, backend=my_backend)\n\n```\n\n\n## Examples#\n\n\n## Debugging Backend#\n\nIf you want to better understand what is going on during a\ncompilation, you can create a custom compiler, which is referred to as\nbackend in this section, that will print pretty print the fxGraphModuleextracted from Dynamo\u2019s bytecode analysis\nand return aforward()callable.\nGraphModule\nforward()\nFor example:\n\n```python\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef fn(x, y):\n    a = torch.cos(x)\n    b = torch.sin(y)\n    return a + b\nfn(torch.randn(10), torch.randn(10))\n\n```\n\nRunning the above example produces the following output:\n\n```python\nmy_compiler() called with FX graph:\nopcode         name    target                                                  args        kwargs\n-------------  ------  ------------------------------------------------------  ----------  --------\nplaceholder    x       x                                                       ()          {}\nplaceholder    y       y                                                       ()          {}\ncall_function  cos     <built-in method cos of type object at 0x7f1a894649a8>  (x,)        {}\ncall_function  sin     <built-in method sin of type object at 0x7f1a894649a8>  (y,)        {}\ncall_function  add     <built-in function add>                                 (cos, sin)  {}\noutput         output  output                                                  ((add,),)   {}\n\n```\n\nThis works fortorch.nn.Moduleas well as shown below:\ntorch.nn.Module\n\n```python\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\nclass MockModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n    def forward(self, x):\n        return self.relu(torch.cos(x))\nmod = MockModule()\noptimized_mod = torch.compile(mod, backend=my_compiler)\noptimized_mod(torch.randn(10))\n\n```\n\nLet\u2019s take a look at one more example with control flow:\n\n```python\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n\n```\n\nRunning this example produces the following output:\n\n```python\nmy_compiler() called with FX graph:\nopcode         name     target                                                  args              kwargs\n-------------  -------  ------------------------------------------------------  ----------------  --------\nplaceholder    a        a                                                       ()                {}\nplaceholder    b        b                                                       ()                {}\ncall_function  abs_1    <built-in method abs of type object at 0x7f8d259298a0>  (a,)              {}\ncall_function  add      <built-in function add>                                 (abs_1, 1)        {}\ncall_function  truediv  <built-in function truediv>                             (a, add)          {}\ncall_method    sum_1    sum                                                     (b,)              {}\ncall_function  lt       <built-in function lt>                                  (sum_1, 0)        {}\noutput         output   output                                                  ((truediv, lt),)  {}\n\nmy_compiler() called with FX graph:\nopcode         name    target                   args         kwargs\n-------------  ------  -----------------------  -----------  --------\nplaceholder    b       b                        ()           {}\nplaceholder    x       x                        ()           {}\ncall_function  mul     <built-in function mul>  (b, -1)      {}\ncall_function  mul_1   <built-in function mul>  (x, mul)     {}\noutput         output  output                   ((mul_1,),)  {}\n\nmy_compiler() called with FX graph:\nopcode         name    target                   args       kwargs\n-------------  ------  -----------------------  ---------  --------\nplaceholder    b       b                        ()         {}\nplaceholder    x       x                        ()         {}\ncall_function  mul     <built-in function mul>  (x, b)     {}\noutput         output  output                   ((mul,),)  {}\n\nThe order of the last two graphs is nondeterministic depending\non which one is encountered first by the just-in-time compiler.\n\n```\n\n\n## Speedy Backend#\n\nIntegrating a custom backend that offers superior performance is also\neasy and we\u2019ll integrate a real one\nwithoptimize_for_inference:\n\n```python\ndef optimize_for_inference_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    scripted = torch.jit.script(gm)\n    return torch.jit.optimize_for_inference(scripted)\n\n```\n\nAnd then you should be able to optimize any existing code with:\n\n```python\n@torch.compile(backend=optimize_for_inference_compiler)\ndef code_to_accelerate():\n    ...\n\n```\n\n\n## Composable Backends#\n\nTorchDynamo includes many backends, which can be listed withtorch._dynamo.list_backends(). You can combine these backends\ntogether with the following code:\ntorch._dynamo.list_backends()\n\n```python\nfrom torch._dynamo import lookup_backend\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    try:\n        trt_compiled = lookup_backend(\"tensorrt\")(gm, example_inputs)\n        if trt_compiled is not None:\n            return trt_compiled\n    except Exception:\n        pass\n    # first backend failed, try something else...\n    try:\n        inductor_compiled = lookup_backend(\"inductor\")(gm, example_inputs)\n        if inductor_compiled is not None:\n            return inductor_compiled\n    except Exception:\n        pass\n    return gm.forward\n\n```\n",
  "url": "https://pytorch.org/docs/stable/torch.compiler_custom_backends.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
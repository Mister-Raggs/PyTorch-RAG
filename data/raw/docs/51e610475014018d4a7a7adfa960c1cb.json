{
  "doc_id": "51e610475014018d4a7a7adfa960c1cb",
  "source": "pytorch_docs",
  "title": "PyTorch Design Philosophy \u2014 PyTorch 2.9 documentation",
  "text": "\n## PyTorch Design Philosophy#\n\nCreated On: Jun 10, 2022 | Last Updated On: Apr 16, 2025\nThis document is designed to help contributors and module maintainers\nunderstand the high-level design principles that have developed over\ntime in PyTorch. These are not meant to be hard-and-fast rules, but to\nserve as a guide to help trade off different concerns and to resolve\ndisagreements that may come up while developing PyTorch. For more\ninformation on contributing, module maintainership, and how to escalate a\ndisagreement to the Core Maintainers, please seePyTorch\nGovernance.\n\n## Design Principles#\n\n\n## Principle 1: Usability over Performance#\n\nThis principle may be surprising! As one Hacker News poster wrote:PyTorch is amazing! [\u2026] Although I\u2019m confused. How can a ML framework be\nnot obsessed with speed/performance?SeeHacker News discussion on\nPyTorch.\nSoumith\u2019s blog post onGrowing the PyTorch\nCommunitygoes into this in some depth, but at a high-level:\nPyTorch\u2019s primary goal is usability\nA secondary goal is to havereasonableperformance\nWe believe the ability to maintain our flexibility to support\nresearchers who are building on top of our abstractions remains\ncritical. We can\u2019t see what the future of what workloads will be, but we\nknow we want them to be built first on PyTorch and that requires\nflexibility.\nIn more concrete terms, we operate in ausability-firstmanner and try\nto avoid jumping torestriction-firstregimes (for example, static shapes,\ngraph-mode only) without a clear-eyed view of the tradeoffs. Often there\nis a temptation to impose strict user restrictions upfront because it\ncan simplify implementation, but this comes with risks:\nThe performance may not be worth the user friction, either because\nthe performance benefit is not compelling enough or it only applies to\na relatively narrow set of subproblems.\nEven if the performance benefit is compelling, the restrictions can\nfragment the ecosystem into different sets of limitations that can\nquickly become incomprehensible to users.\nWe want users to be able to seamlessly move their PyTorch code to\ndifferent hardware and software platforms, to interoperate with\ndifferent libraries and frameworks, and to experience the full richness\nof the PyTorch user experience, not a least common denominator subset.\n\n## Principle 2: Simple Over Easy#\n\nHere, we borrow fromThe Zen of\nPython:\nExplicit is better than implicit\nSimple is better than complex\nA more concise way of describing these two goals isSimple Over\nEasy. Let\u2019s start with an example becausesimpleandeasyare\noften used interchangeably in everyday English. Consider how one may\nmodeldevicesin PyTorch:\nSimple / Explicit (to understand, debug):every tensor is associated\nwith a device. The user explicitly specifies tensor device movement.\nOperations that require cross-device movement result in an error.\nEasy / Implicit (to use):the user does not have to worry about\ndevices; the system figures out the globally optimal device\nplacement.\nIn this specific case, and as a general design philosophy, PyTorch\nfavors exposing simple and explicit building blocks rather than APIs\nthat are easy-to-use by practitioners. The simple version is immediately\nunderstandable and debuggable by a new PyTorch user: you get a clear\nerror if you call an operator requiring cross-device movement at the\npoint in the program where the operator is actually invoked. The easy\nsolution may let a new user move faster initially, but debugging such a\nsystem can be complex: How did the system make its determination? What\nis the API for plugging into such a system and how are objects\nrepresented in its IR?\nSome classic arguments in favor of this sort of design come fromA\nNote on Distributed\nComputation(TLDR: Do not\nmodel resources with very different performance characteristics\nuniformly, the details will leak) and theEnd-to-End\nPrinciple(TLDR: building smarts into the lower-layers of the stack can prevent\nbuilding performant features at higher layers in the stack, and often\ndoesn\u2019t work anyway). For example, we could build operator-level or\nglobal device movement rules, but the precise choices aren\u2019t obvious and\nbuilding an extensible mechanism has unavoidable complexity and latency\ncosts.\nA caveat here is that this does not mean that higher-level \u201ceasy\u201d APIs\nare not valuable; certainly there is a value in, for example,\nhigher-levels in the stack to support efficient tensor computations\nacross heterogeneous compute in a large cluster. Instead, what we mean\nis that focusing on simple lower-level building blocks helps inform the\neasy API while still maintaining a good experience when users need to\nleave the beaten path. It also allows space for innovation and the\ngrowth of more opinionated tools at a rate we cannot support in the\nPyTorch core library, but ultimately benefit from, as evidenced by\nourrich ecosystem. In other\nwords, not automating at the start allows us to potentially reach levels\nof good automation faster.\n\n## Principle 3: Python First with Best In Class Language Interoperability#\n\nThis principle began asPython First:\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python. You can use it\nnaturally like you would useNumPy,SciPy,scikit-learn,\nor other Python libraries. You can write your new neural network\nlayers in Python itself, using your favorite libraries and use\npackages such asCythonandNumba. Our goal is to not reinvent\nthe wheel where appropriate.\nOne thing PyTorch has needed to deal with over the years is Python\noverhead: we first rewrote theautogradengine in C++, then the majority\nof operator definitions, then developed TorchScript and the C++\nfrontend.\nStill, working in Python provides easily the best experience for our\nusers: it is flexible, familiar, and perhaps most importantly, has a\nhuge ecosystem of scientific computing libraries and extensions\navailable for use. This fact motivates a few of our most recent\ncontributions, which attempt to hit a Pareto optimal point close to the\nPython usability end of the curve:\nTorchDynamo,\na Python frame evaluation tool capable of speeding up existing\neager-mode PyTorch programs with minimal user intervention.\ntorch_functionandtorch_dispatchextension points, which have enabled Python-first functionality to be\nbuilt on-top of C++ internals, such as thetorch.fx\ntracerandfunctorchrespectively.\nThese design principles are not hard-and-fast rules, but hard won\nchoices and anchor how we built PyTorch to be the debuggable, hackable\nand flexible framework it is today. As we have more contributors and\nmaintainers, we look forward to applying these core principles with you\nacross our libraries and ecosystem. We are also open to evolving them as\nwe learn new things and the AI space evolves, as we know it will.",
  "url": "https://pytorch.org/docs/stable/community/design.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
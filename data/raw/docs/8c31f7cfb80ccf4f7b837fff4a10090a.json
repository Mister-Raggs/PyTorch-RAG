{
  "doc_id": "8c31f7cfb80ccf4f7b837fff4a10090a",
  "source": "pytorch_docs",
  "title": "torch.export IR Specification \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.export IR Specification#\n\nCreated On: Oct 05, 2023 | Last Updated On: Jul 16, 2025\nExport IR is an intermediate representation (IR) for compilers, which bears\nsimilarities toMLIRand TorchScript. It is specifically designed to express the\nsemantics of PyTorch programs. Export IR primarily represents computation in a\nstreamlined list of operations, with limited support for dynamism such as\ncontrol flows.\nTo create an Export IR graph, a frontend can be used that soundly captures a\nPyTorch program via a trace-specializing mechanism. The resulting Export IR can\nthen be optimized and executed by a backend. This can be done today throughtorch.export.export().\ntorch.export.export()\nThe key concepts that will be covered in this document include:\nExportedProgram: the data structure containing the Export IR program\nGraph: which consists of a list of nodes.\nNodes: which represents operations, control flow, and metadata stored on this node.\nValues are produced and consumed by nodes.\nTypes are associated with values and nodes.\nThe size and memory layout of values are also defined.\n\n## Assumptions#\n\nThis doc assumes that the audience is sufficiently familiar with PyTorch,\nspecifically withtorch.fxand its related toolings. Thus it will stop\ndescribing contents present intorch.fxdocumentation and paper.\ntorch.fx\ntorch.fx\n\n## What is Export IR#\n\nExport IR is a graph-based intermediate representation IR of PyTorch programs.\nExport IR is realized on top oftorch.fx.Graph. In other words,all\nExport IR graphs are also valid FX graphs, and if interpreted using standard\nFX semantics, Export IR can be interpreted soundly. One implication is that an\nexported graph can be converted to a valid Python program via standard FX\ncodegen.\ntorch.fx.Graph\nThis documentation will primarily focus on highlighting areas where Export IR\ndiffers from FX in terms of its strictness, while skipping parts where it shares\nsimilarities with FX.\n\n## ExportedProgram#\n\nThe top-level Export IR construct is antorch.export.ExportedProgramclass. It bundles the computational graph of a PyTorch model (which is usually atorch.nn.Module) with the parameters or weights that this model\nconsumes.\ntorch.export.ExportedProgram\ntorch.nn.Module\nSome notable attributes of thetorch.export.ExportedProgramclass are:\ntorch.export.ExportedProgram\ngraph_module(torch.fx.GraphModule): Data structure containing\nthe flattened computational graph of the PyTorch model. The graph can be\ndirectly accessed throughExportedProgram.graph.\ngraph_module\ntorch.fx.GraphModule\nExportedProgram.graph\ngraph_signature(torch.export.ExportGraphSignature): The graph\nsignature, which specifies the parameters and buffer names used and mutated\nwithin the graph. Instead of storing parameters and buffers as attributes of\nthe graph, they are lifted as inputs to the graph. The graph_signature is\nutilized to keep track of additional information on these parameters and\nbuffers.\ngraph_signature\ntorch.export.ExportGraphSignature\nstate_dict(Dict[str,Union[torch.Tensor,torch.nn.Parameter]]): Data\nstructure containing the parameters and buffers.\nstate_dict\nDict[str,Union[torch.Tensor,torch.nn.Parameter]]\nrange_constraints(Dict[sympy.Symbol,RangeConstraint]): For programs\nthat are exported with data dependent behavior, the metadata on each node will\ncontain symbolic shapes (which look likes0,i0). This attribute maps\nthe symbolic shapes to their lower/upper ranges.\nrange_constraints\nDict[sympy.Symbol,RangeConstraint]\ns0\ni0\n\n## Graph#\n\nAn Export IR Graph is a PyTorch program represented in the form of a DAG\n(directed acyclic graph). Each node in this graph represents a particular\ncomputation or operation, and edges of this graph consist of references between\nnodes.\nWe can view Graph having this schema:\n\n```python\nclass Graph:\n  nodes: List[Node]\n\n```\n\nIn practice, Export IR\u2019s graph is realized astorch.fx.GraphPython class.\ntorch.fx.Graph\nAn Export IR graph contains the following nodes (Nodes will be described in more\ndetails in the next section):\n0 or more nodes of op typeplaceholder\nplaceholder\n0 or more nodes of op typecall_function\ncall_function\nexactly 1 node of op typeoutput\noutput\nCollorary:The smallest valid Graph will be of one node. i.e. nodes is never empty.\nDefinition:The set ofplaceholdernodes of a Graph represents theinputsof the\nGraph of GraphModule. Theoutputnode of a Graph represents theoutputsof the Graph of GraphModule.\nplaceholder\noutput\nExample:\n\n```python\nimport torch\nfrom torch import nn\n\nclass MyModule(nn.Module):\n\n    def forward(self, x, y):\n      return x + y\n\nexample_args = (torch.randn(1), torch.randn(1))\nmod = torch.export.export(MyModule(), example_args)\nprint(mod.graph)\n\n```\n\n\n```python\ngraph():\n  %x : [num_users=1] = placeholder[target=x]\n  %y : [num_users=1] = placeholder[target=y]\n  %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x, %y), kwargs = {})\n  return (add,)\n\n```\n\nThe above is the textual representation of a Graph, with each line being a node.\n\n## Node#\n\nA Node represents a particular computation or operation and is represented in\nPython using thetorch.fx.Nodeclass. Edges between nodes are\nrepresented as direct references to other nodes via theargsproperty of the\nNode class. Using the same FX machinery, we can represent the following\noperations that a computational graph typically needs, such as operator calls,\nplaceholders (aka inputs), conditionals, and loops.\ntorch.fx.Node\nargs\nThe Node has the following schema:\n\n```python\nclass Node:\n  name: str # name of node\n  op_name: str  # type of operation\n\n  # interpretation of the fields below depends on op_name\n  target: [str|Callable]\n  args: List[object]\n  kwargs: Dict[str, object]\n  meta: Dict[str, object]\n\n```\n\nFX Text Format\nAs in the example above, notice that each line has this format:\n\n```python\n%<name>:[...] = <op_name>[target=<target>](args = (%arg1, %arg2, arg3, arg4, \u2026)), kwargs = {\"keyword\": arg5})\n\n```\n\nThis format captures everything present in the Node class, with the exception ofmeta, in a compact format.\nmeta\nConcretely:\nis the name of the node as it would appear innode.name.\nnode.name\n<op_name>is thenode.opfield, which must be one of these:<call_function>,<placeholder>,<get_attr>, or<output>.\nnode.op\n<call_function>\n<placeholder>\n<get_attr>\n<output>\nis the target of the node asnode.target. The meaning of this\nfield depends onop_name.\nnode.target\nop_name\nargs1, \u2026 args 4\u2026are what is listed in thenode.argstuple. If a\nvalue in the list is antorch.fx.Node, then it will be especially\nindicated with a leading%.\nnode.args\ntorch.fx.Node\nFor example, a call to the add operator would appear as:\n\n```python\n%add1 = call_function[target = torch.op.aten.add.Tensor](args = (%x, %y), kwargs = {})\n\n```\n\nWhere%x,%yare two other Nodes that have names x and y. Worth noting\nthat the stringtorch.op.aten.add.Tensorrepresents the callable object that\nis actually stored in the target field, not merely its string name.\n%x\n%y\ntorch.op.aten.add.Tensor\nThe final line of this text format is:\n\n```python\nreturn [add]\n\n```\n\nwhich is a Node withop_name=output, indicating that we are returning this\none element.\nop_name=output\n\n## call_function#\n\nAcall_functionnode represents a call to an operator.\ncall_function\nDefinitions\nFunctional:We say a callable is \u201cfunctional\u201d if it satisfies all the\nfollowing requirements:\nNon-mutating: The operator does not mutate the value of its input (for\ntensors, this includes both metadata and data).\nNo side effects: The operator does not mutate states that are visible\nfrom outside, like changing values of module parameters.\nOperator:is a functional callable with a predefined schema. Examples of\nsuch operators include functional ATen operators.\nRepresentation in FX\n\n```python\n%name = call_function[target = operator](args = (%x, %y, \u2026), kwargs = {})\n\n```\n\nDifferences from vanilla FX call_function\nIn FX graph, a call_function can refer to any callable, in Export IR, we\nrestrict it to only a select subset of ATen operators, custom operators, and\ncontrol flow operators.\nIn Export IR, constant arguments will be embedded within the graph.\nIn FX graph, a get_attr node can represent reading any attribute stored in\nthe graph module. However, in Export IR this is restricted to reading only\nsubmodules as all parameters/buffers will be passed in as inputs to the graph\nmodule.\nNode.metais a dict attached to every FX node. However, the FX spec does not\nspecify what metadata can or will be there. Export IR provides a stronger\ncontract, specifically allcall_functionnodes will guarantee having and\nonly having the following metadata fields:\nNode.meta\ncall_function\nnode.meta[\"stack_trace\"]is a string containing the Python stack trace\nreferencing the original Python source code. An example stack trace looks\nlike:\nnode.meta[\"stack_trace\"]\n\n```python\nFile \"my_module.py\", line 19, in forward\nreturn x + dummy_helper(y)\nFile \"helper_utility.py\", line 89, in dummy_helper\nreturn y + 1\n\n```\n\nnode.meta[\"val\"]describes the output of running the operation. It can be\nof type<symint>,<FakeTensor>, aList[Union[FakeTensor,SymInt]], orNone.\nnode.meta[\"val\"]\n<symint>\n<FakeTensor>\nList[Union[FakeTensor,SymInt]]\nNone\nnode.meta[\"nn_module_stack\"]describes the \u201cstacktrace\u201d of thetorch.nn.Modulefrom which the node came, if it was from atorch.nn.Modulecall. For example, if a node containing theaddmmop called from atorch.nn.Linearmodule inside of atorch.nn.Sequentialmodule, thenn_module_stackwould look\nsomething like:\nnode.meta[\"nn_module_stack\"]\ntorch.nn.Module\ntorch.nn.Module\naddmm\ntorch.nn.Linear\ntorch.nn.Sequential\nnn_module_stack\n\n```python\n{'self_linear': ('self.linear', <class 'torch.nn.Linear'>), 'self_sequential': ('self.sequential', <class 'torch.nn.Sequential'>)}\n\n```\n\nnode.meta[\"source_fn_stack\"]contains the torch function or the leaftorch.nn.Moduleclass this node was called from before decomposition.\nFor example, a node containing theaddmmop from atorch.nn.Linearmodule call would containtorch.nn.Linearin\ntheirsource_fn, and a node containing theaddmmop from atorch.nn.functional.Linearmodule call would containtorch.nn.functional.Linearin theirsource_fn.\nnode.meta[\"source_fn_stack\"]\ntorch.nn.Module\naddmm\ntorch.nn.Linear\ntorch.nn.Linear\nsource_fn\naddmm\ntorch.nn.functional.Linear\ntorch.nn.functional.Linear\nsource_fn\n\n## placeholder#\n\nPlaceholder represents an input to a graph. Its semantics are exactly the same as in FX.\nPlaceholder nodes must be the first N nodes in the nodes list of a graph. N can be zero.\nRepresentation in FX\n\n```python\n%name = placeholder[target = name](args = ())\n\n```\n\nThe target field is a string which is the name of input.\nargs, if non-empty, should be of size 1 representing the default value of this input.\nargs\nMetadata\nPlaceholder nodes also havemeta[\u2018val\u2019], likecall_functionnodes. Thevalfield in this case represents the input shape/dtype that the graph is\nexpected to receive for this input parameter.\nmeta[\u2018val\u2019]\ncall_function\nval\n\n## output#\n\nAn output call represents a return statement in a function; it thus terminates the\ncurrent graph. There is one and only one output node, and it will always be the\nlast node of the graph.\nRepresentation in FX\n\n```python\noutput[](args = (%something, \u2026))\n\n```\n\nThis has the exact semantics as intorch.fx.argsrepresents the node\nto be returned.\ntorch.fx\nargs\nMetadata\nOutput node has the same metadata ascall_functionnodes.\ncall_function\n\n## get_attr#\n\nget_attrnodes represent reading a submodule from the encapsulatingtorch.fx.GraphModule. Unlike a vanilla FX graph fromtorch.fx.symbolic_trace()in whichget_attrnodes are used to read\nattributes such as parameters and buffers from the top-leveltorch.fx.GraphModule, parameters and buffers are passed in as\ninputs to the graph module, and stored in the top-leveltorch.export.ExportedProgram.\nget_attr\ntorch.fx.GraphModule\ntorch.fx.symbolic_trace()\nget_attr\ntorch.fx.GraphModule\ntorch.export.ExportedProgram\nRepresentation in FX\n\n```python\n%name = get_attr[target = name](args = ())\n\n```\n\nExample\nConsider the following model:\n\n```python\nfrom functorch.experimental.control_flow import cond\n\ndef true_fn(x):\n    return x.sin()\n\ndef false_fn(x):\n    return x.cos()\n\ndef f(x, y):\n    return cond(y, true_fn, false_fn, [x])\n\n```\n\nGraph:\n\n```python\ngraph():\n    %x_1 : [num_users=1] = placeholder[target=x_1]\n    %y_1 : [num_users=1] = placeholder[target=y_1]\n    %true_graph_0 : [num_users=1] = get_attr[target=true_graph_0]\n    %false_graph_0 : [num_users=1] = get_attr[target=false_graph_0]\n    %conditional : [num_users=1] = call_function[target=torch.ops.higher_order.cond](args = (%y_1, %true_graph_0, %false_graph_0, [%x_1]), kwargs = {})\n    return conditional\n\n```\n\nThe line,%true_graph_0:[num_users=1]=get_attr[target=true_graph_0],\nreads the submoduletrue_graph_0which contains thesinoperator.\n%true_graph_0:[num_users=1]=get_attr[target=true_graph_0]\ntrue_graph_0\nsin\n\n## References#\n\n\n## SymInt#\n\nA SymInt is an object that can either be a literal integer or a symbol that represents\nan Integer (represented in Python bysympy.Symbolclass). When SymInt is a\nsymbol, it describes a variable of type integer that is unknown to the graph at\ncompile time, that is, its value is only known at runtime.\nsympy.Symbol\n\n## FakeTensor#\n\nA FakeTensor is an object that contains the metadata of a tensor. It can be\nviewed as having the following metadata.\n\n```python\nclass FakeTensor:\n  size: List[SymInt]\n  dtype: torch.dtype\n  device: torch.device\n  dim_order: List[int]  # This doesn't exist yet\n\n```\n\nThe size field of FakeTensor is a list of integers or SymInts. If SymInts are\npresent, this means this tensor has a dynamic shape. If integers are present, it\nis assumed that the tensor will have that exact static shape. The rank of the\nTensorMeta is never dynamic. The dtype field represents the dtype of the\noutput of that node. There are no implicit type promotions in Edge IR. There\nare no strides in FakeTensor.\nIn other words:\nIf the operator in node.target returns a Tensor, thennode.meta['val']is a\nFakeTensor describing that tensor.\nnode.meta['val']\nIf the operator in node.target returns an n-tuple of Tensors, thennode.meta['val']is an n-tuple of FakeTensors describing each tensor.\nnode.meta['val']\nIf the operator in node.target returns an int/float/scalar that is known at\ncompile time, thennode.meta['val']is None.\nnode.meta['val']\nIf the operator in node.target returns an int/float/scalar that is not known\nat compile time, thennode.meta['val']is of type SymInt.\nnode.meta['val']\nFor example:\naten::addreturns a Tensor; so its spec will be a FakeTensor with dtype\nand size of the tensor returned by this operator.\naten::add\naten::sym_sizereturns an integer; so its val will be a SymInt because its\nvalue is only available at runtime.\naten::sym_size\nmax_pool2d_with_indexesreturns a tuple of (Tensor, Tensor); so the spec\nwill also be a 2-tuple of FakeTensor objects, the first TensorMeta describes\nthe first element of the return value etc.\nmax_pool2d_with_indexes\nPython code:\n\n```python\ndef add_one(x):\n  return torch.ops.aten(x, 1)\n\n```\n\nGraph:\n\n```python\ngraph():\n  %ph_0 : [#users=1] = placeholder[target=ph_0]\n  %add_tensor : [#users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%ph_0, 1), kwargs = {})\n  return [add_tensor]\n\n```\n\nFakeTensor:\n\n```python\nFakeTensor(dtype=torch.int, size=[2,], device=CPU)\n\n```\n\n\n## Pytree-able Types#\n\nWe define a type \u201cPytree-able\u201d, if it is either a leaf type or a container type\nthat contains other Pytree-able types.\nNote:\nThe concept of pytree is the same as the one documentedherefor JAX:\nThe following types are defined asleaf type:\nType\nDefinition\nTensor\ntorch.Tensor\ntorch.Tensor\nScalar\nAny numerical types from Python, including integral types, floating point types, and zero dimensional tensors.\nint\nPython int (bound as int64_t in C++)\nfloat\nPython float (bound as double in C++)\nbool\nPython bool\nstr\nPython string\nScalarType\ntorch.dtype\ntorch.dtype\nLayout\ntorch.layout\ntorch.layout\nMemoryFormat\ntorch.memory_format\ntorch.memory_format\nDevice\ntorch.device\ntorch.device\nThe following types are defined ascontainer type:\nType\nDefinition\nTuple\nPython tuple\nList\nPython list\nDict\nPython dict with Scalar keys\nNamedTuple\nPython namedtuple\nDataclass\nMust be registered throughregister_dataclass\nCustom class\nAny custom class defined with_register_pytree_node",
  "url": "https://pytorch.org/docs/stable/export/ir_spec.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
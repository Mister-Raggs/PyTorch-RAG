{
  "doc_id": "b6f56963e0cb14af63d86ce55be69bab",
  "source": "pytorch_docs",
  "title": "Benchmark Utils - torch.utils.benchmark \u2014 PyTorch 2.9 documentation",
  "text": "\n## Benchmark Utils - torch.utils.benchmark#\n\nCreated On: Nov 02, 2020 | Last Updated On: Jun 12, 2025\nHelper class for measuring execution time of PyTorch statements.\nFor a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html\nThe PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences:\nTimer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous accelerator functions when\nnecessary.\nWhen measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) Thetimeitmethod is replicated for cases where an adaptive strategy is not\ndesired.\nWhen defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison.\nIn addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed.\nDirectly analogous totimeit.Timerconstructor arguments:\nstmt,setup,timer,globals\nPyTorch Timer specific constructor arguments:\nlabel,sub_label,description,env,num_threads\nstmt(str) \u2013 Code snippet to be run in a loop and timed.\nsetup(str) \u2013 Optional setup code. Used to define variables used instmt\nglobal_setup(str) \u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.\ntimer(Callable[[],float]) \u2013 Callable which returns the current time. If PyTorch was built\nwithout accelerators or there is no accelerator present, this defaults totimeit.default_timer; otherwise it will synchronize accelerators before\nmeasuring the time.\nglobals(Optional[dict[str,Any]]) \u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds.\nlabel(Optional[str]) \u2013 String which summarizesstmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability.\nsub_label(Optional[str]) \u2013Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d\u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare.\nProvide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d\n\u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare.\ndescription(Optional[str]) \u2013String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form:|n=1|n=4|...-------------...ReLU(x+1):(float)|...|...|...ReLU(x+1):(int)|...|...|...usingCompare. It is also included when printing a Measurement.\nString to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form:\n\n```python\n                        | n=1 | n=4 | ...\n                        ------------- ...\nReLU(x + 1): (float)    | ... | ... | ...\nReLU(x + 1): (int)      | ... | ... | ...\n\n```\n\nusingCompare. It is also included when printing a Measurement.\nenv(Optional[str]) \u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivalent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.\nnum_threads(int) \u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performance is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores.\nSimilar toblocked_autorangebut also checks for variablility in measurements\nand repeats until iqr/median is smaller thanthresholdormax_run_timeis reached.\nAt a high level, adaptive_autorange executes the following pseudo-code:\n\n```python\n`setup`\n\ntimes = []\nwhile times.sum < max_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    times.append(timer() - start)\n\n    enough_data = len(times)>3 and times.sum > min_run_time\n    small_iqr=times.iqr/times.mean<threshold\n\n    if enough_data and small_iqr:\n        break\n\n```\n\nthreshold(float) \u2013 value of iqr/median threshold for stopping\nmin_run_time(float) \u2013 total runtime needed before checkingthreshold\nmax_run_time(float) \u2013 total runtime  for all measurements regardless ofthreshold\nAMeasurementobject that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.)\nMeasurement\nMeasure many replicates while keeping timer overhead to a minimum.\nAt a high level, blocked_autorange executes the following pseudo-code:\n\n```python\n`setup`\n\ntotal_time = 0\nwhile total_time < min_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    total_time += (timer() - start)\n\n```\n\nNote the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives:\nA small block size results in more replicates and generally\nbetter statistics.\nA large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because accelerator synchronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement.\nblocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop.\nAMeasurementobject that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.)\nMeasurement\nCollect instruction counts using Callgrind.\nUnlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, however this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements.\nIn order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.\nBecause there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and TorchScripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly.\nBy default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt.\nACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results.\nMirrors the semantics of timeit.Timer.timeit().\nExecute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\nMeasurement\nThe result of a Timer measurement.\nThis class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers.\nConvenience method for merging replicates.\nMerge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates)\nlist[\u2018Measurement\u2019]\nApproximate significant figure estimate.\nThis property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.\nThe significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare.\nTop level container for Callgrind results collected by Timer.\nManipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized().\nStrip library names and some prefixes from function strings.\nWhen comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:\n\n```python\n23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)\n\n```\n\nStripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivalent call sites\nwhen diffing.\nCallgrindStats\nReturns the total number of instructions executed.\nSeeFunctionCounts.denoise()for an explanation of thedenoisearg.\nint\nDiff two sets of counts.\nOne common reason to collect instruction counts is to determine the\nthe effect that a particular change will have on the number of instructions\nneeded to perform some unit of work. If a change increases that number, the\nnext logical question is \u201cwhy\u201d. This generally involves looking at what part\nif the code increased in instruction count. This function automates that\nprocess so that one can easily diff counts on both an inclusive and\nexclusive basis.\nFunctionCounts\nReturns detailed function counts.\nConceptually, the FunctionCounts returned can be thought of as a tuple\nof (count, path_and_function_name) tuples.\ninclusivematches the semantics of callgrind. If True, the counts\ninclude instructions executed by children.inclusive=Trueis useful\nfor identifying hot spots in code;inclusive=Falseis useful for\nreducing noise when diffing counts from two different runs. (See\nCallgrindStats.delta(\u2026) for more details)\nFunctionCounts\nContainer for manipulating Callgrind results.\nAddition and subtraction to combine or diff results.\nTuple-like indexing.\nAdenoisefunction which strips CPython calls which are known to\nbe non-deterministic and quite noisy.\nTwo higher order methods (filterandtransform) for custom\nmanipulation.\nRemove known noisy instructions.\nSeveral instructions in the CPython interpreter are rather noisy. These\ninstructions involve unicode to dictionary lookups which Python uses to\nmap variable names. FunctionCounts is generally a content agnostic\ncontainer, however this is sufficiently important for obtaining\nreliable results to warrant an exception.\nFunctionCounts\nKeep only the elements wherefilter_fnapplied to function name returns True.\nFunctionCounts\nApplymap_fnto all of the function names.\nThis can be used to regularize function names (e.g. stripping irrelevant\nparts of the file path), coalesce entries by mapping multiple functions\nto the same name (in which case the counts are added together), etc.\nFunctionCounts\nHelper class for displaying the results of many measurements in a\nformatted table.\nThe table format is based on the information fields provided intorch.utils.benchmark.Timer(description,label,sub_label,num_threads, etc).\ntorch.utils.benchmark.Timer\nThe table can be directly printed usingprint()or casted as astr.\nprint()\nFor a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html\nresults(list[torch.utils.benchmark.utils.common.Measurement]) \u2013 List of Measurement to display.\nColorize formatted table.\nColorize columnwise by default.\nAppend results to already stored ones.\nAll added results must be instances ofMeasurement.\nMeasurement\nEnables warning highlighting when building formatted table.\nPrint formatted table\nEnables trimming of significant figures when building the formatted table.",
  "url": "https://pytorch.org/docs/stable/benchmark_utils.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
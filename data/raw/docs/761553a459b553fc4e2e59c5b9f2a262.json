{
  "doc_id": "761553a459b553fc4e2e59c5b9f2a262",
  "source": "pytorch_docs",
  "title": "torch.nn.functional.scaled_dot_product_attention \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.nn.functional.scaled_dot_product_attention#\n\nis_causal=False, scale=None, enable_gqa=False) -> Tensor:\nComputes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed,\nand applying dropout if a probability greater than 0.0 is specified. The optional scale argument can only be\nspecified as a keyword argument.\n\n```python\n# Efficient implementation equivalent to the following:\ndef scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n    L, S = query.size(-2), key.size(-2)\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n    if is_causal:\n        assert attn_mask is None\n        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n        else:\n            attn_bias = attn_mask + attn_bias\n\n    if enable_gqa:\n        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n    attn_weight += attn_bias\n    attn_weight = torch.softmax(attn_weight, dim=-1)\n    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n    return attn_weight @ value\n\n```\n\nWarning\nThis function is beta and subject to change.\nWarning\nThis function always applies dropout according to the specifieddropout_pargument.\nTo disable dropout during evaluation, be sure to pass a value of0.0when the module\nthat makes the function call is not in training mode.\ndropout_p\n0.0\nFor example:\n\n```python\nclass MyModel(nn.Module):\n    def __init__(self, p=0.5):\n        super().__init__()\n        self.p = p\n\n    def forward(self, ...):\n        return F.scaled_dot_product_attention(...,\n            dropout_p=(self.p if self.training else 0.0))\n\n```\n\nNote\nThere are currently three supported implementations of scaled dot product attention:\nFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\nMemory-Efficient Attention\nA PyTorch implementation defined in C++ matching the above formulation\nThe function may call optimized kernels for improved performance when using the CUDA backend.\nFor all other backends, the PyTorch implementation will be used.\nAll implementations are enabled by default. Scaled dot product attention attempts to automatically select the\nmost optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation\nis used, the following functions are provided for enabling and disabling implementations.\nThe context manager is the preferred mechanism:\ntorch.nn.attention.sdpa_kernel(): A context manager used to enable or disable any of the implementations.\ntorch.nn.attention.sdpa_kernel()\ntorch.backends.cuda.enable_flash_sdp(): Globally enables or disables FlashAttention.\ntorch.backends.cuda.enable_flash_sdp()\ntorch.backends.cuda.enable_mem_efficient_sdp(): Globally enables or disables  Memory-Efficient Attention.\ntorch.backends.cuda.enable_mem_efficient_sdp()\ntorch.backends.cuda.enable_math_sdp(): Globally enables or disables  the PyTorch C++ implementation.\ntorch.backends.cuda.enable_math_sdp()\nEach of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation,\ndisable the PyTorch C++ implementation usingtorch.nn.attention.sdpa_kernel().\nIn the event that a fused implementation is not available, a warning will be raised with the\nreasons why the fused implementation cannot run.\ntorch.nn.attention.sdpa_kernel()\nDue to the nature of fusing floating point operations, the output of this function may be different\ndepending on what backend kernel is chosen.\nThe c++ implementation supports torch.float64 and can be used when higher precision is required.\nFor math backend, all intermediates are kept in torch.float if inputs are in torch.half or torch.bfloat16.\nFor more information please seeNumerical accuracy\nGrouped Query Attention (GQA) is an experimental feature. It currently works only for Flash_attention\nand math kernel on CUDA tensor, and does not support Nested tensor.\nConstraints for GQA:\nnumber_of_heads_query % number_of_heads_key_value == 0 and,\nnumber_of_heads_key == number_of_heads_value\nNote\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by settingtorch.backends.cudnn.deterministic=True. SeeReproducibilityfor more information.\ntorch.backends.cudnn.deterministic=True\nquery(Tensor) \u2013 Query tensor; shape(N,...,Hq,L,E)(N, ..., Hq, L, E)(N,...,Hq,L,E).\nkey(Tensor) \u2013 Key tensor; shape(N,...,H,S,E)(N, ..., H, S, E)(N,...,H,S,E).\nvalue(Tensor) \u2013 Value tensor; shape(N,...,H,S,Ev)(N, ..., H, S, Ev)(N,...,H,S,Ev).\nattn_mask(optional Tensor) \u2013 Attention mask; shape must be broadcastable to the shape of attention weights,\nwhich is(N,...,L,S)(N,..., L, S)(N,...,L,S). Two types of masks are supported.\nA boolean mask where a value of True indicates that the elementshouldtake part in attention.\nA float mask of the same type as query, key, value that is added to the attention score.\ndropout_p(float) \u2013 Dropout probability; if greater than 0.0, dropout is applied\nis_causal(bool) \u2013 If set to true, the attention masking is a lower triangular matrix when the mask is a\nsquare matrix. The attention masking has the form of the upper left causal bias due to the alignment\n(seetorch.nn.attention.bias.CausalBias) when the mask is a non-square matrix.\nAn error is thrown if both attn_mask and is_causal are set.\ntorch.nn.attention.bias.CausalBias\nscale(optional python:float,keyword-only) \u2013 Scaling factor applied prior to softmax. If None, the default value is set\nto1E\\frac{1}{\\sqrt{E}}E\u200b1\u200b.\nenable_gqa(bool) \u2013 If set to True, Grouped Query Attention (GQA) is enabled, by default it is set to False.\nAttention output; shape(N,...,Hq,L,Ev)(N, ..., Hq, L, Ev)(N,...,Hq,L,Ev).\noutput (Tensor)\nN:Batch\u00a0size...:Any\u00a0number\u00a0of\u00a0other\u00a0batch\u00a0dimensions\u00a0(optional)N: \\text{Batch size} ... : \\text{Any number of other batch dimensions (optional)}N:Batch\u00a0size...:Any\u00a0number\u00a0of\u00a0other\u00a0batch\u00a0dimensions\u00a0(optional)\nS:Source\u00a0sequence\u00a0lengthS: \\text{Source sequence length}S:Source\u00a0sequence\u00a0length\nL:Target\u00a0sequence\u00a0lengthL: \\text{Target sequence length}L:Target\u00a0sequence\u00a0length\nE:Embedding\u00a0dimension\u00a0of\u00a0the\u00a0query\u00a0and\u00a0keyE: \\text{Embedding dimension of the query and key}E:Embedding\u00a0dimension\u00a0of\u00a0the\u00a0query\u00a0and\u00a0key\nEv:Embedding\u00a0dimension\u00a0of\u00a0the\u00a0valueEv: \\text{Embedding dimension of the value}Ev:Embedding\u00a0dimension\u00a0of\u00a0the\u00a0value\nHq:Number\u00a0of\u00a0heads\u00a0of\u00a0queryHq: \\text{Number of heads of query}Hq:Number\u00a0of\u00a0heads\u00a0of\u00a0query\nH:Number\u00a0of\u00a0heads\u00a0of\u00a0key\u00a0and\u00a0valueH: \\text{Number of heads of key and value}H:Number\u00a0of\u00a0heads\u00a0of\u00a0key\u00a0and\u00a0value\nExamples\n\n```python\n>>> # Optionally use the context manager to ensure one of the fused kernels is run\n>>> query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> with sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n>>>     F.scaled_dot_product_attention(query,key,value)\n\n```\n\n\n```python\n>>> # Sample for GQA for llama3\n>>> query = torch.rand(32, 32, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> with sdpa_kernel(backends=[SDPBackend.MATH]):\n>>>     F.scaled_dot_product_attention(query,key,value,enable_gqa=True)\n\n```\n",
  "url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
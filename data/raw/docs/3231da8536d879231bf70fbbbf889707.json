{
  "doc_id": "3231da8536d879231bf70fbbbf889707",
  "source": "pytorch_docs",
  "title": "Tensor Parallelism - torch.distributed.tensor.parallel \u2014 PyTorch 2.9 documentation",
  "text": "\n## Tensor Parallelism - torch.distributed.tensor.parallel#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jun 13, 2025\nTensor Parallelism(TP) is built on top of the PyTorch DistributedTensor\n(DTensor)[https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md]\nand provides different parallelism styles: Colwise, Rowwise, and Sequence Parallelism.\nWarning\nTensor Parallelism APIs are experimental and subject to change.\nThe entrypoint to parallelize yournn.Moduleusing Tensor Parallelism is:\nnn.Module\nApply Tensor Parallelism in PyTorch by parallelizing modules or sub-modules based on a user-specified plan.\nWe parallelize module or sub_modules based on a parallelize_plan. The parallelize_plan containsParallelStyle, which indicates how user wants the module or sub_module\nto be parallelized.\nParallelStyle\nUser can also specify different parallel style per module fully qualified name (FQN).\nNote thatparallelize_moduleonly accepts a 1-DDeviceMesh, if you have a 2-D or N-DDeviceMesh,\nslice the DeviceMesh to a 1-D sub DeviceMesh first then pass to this API(i.e.device_mesh[\"tp\"])\nparallelize_module\nDeviceMesh\nDeviceMesh\ndevice_mesh[\"tp\"]\nmodule(nn.Module) \u2013 Module to be parallelized.\nnn.Module\ndevice_mesh(DeviceMesh, optional) \u2013 Object which describes the mesh topology of devices for the DTensor.\nIf not specified, the call must be under a DeviceMesh context.\nDeviceMesh\nparallelize_plan(Union[ParallelStyle, Dict[str,ParallelStyle]], optional) \u2013 The plan used to parallelize the module. It can be either aParallelStyleobject which contains how we prepare\ninput/output for Tensor Parallelism or it can be a dict of module\nFQN and its correspondingParallelStyleobject. If not\nspecified, the call will do nothing at the moment.\nParallelStyle\nParallelStyle\nParallelStyle\nParallelStyle\nsrc_data_rank(int,optional) \u2013 the rank of the source data for the logical/global tensor, it is used bydistribute_tensor()to scatter/broadcast the shards/replicas to other ranks. By default,\nwe usegroup_rank=0on each DeviceMesh dimension as the source data to preserve the single-device\nsemantic. If passingNoneexplicitly,parallelize_module()simply uses its local data instead\nof trying to preserve the single-device semantic via scatter/broadcast. Default: 0\ndistribute_tensor()\ngroup_rank=0\nNone\nparallelize_module()\nAnn.Moduleobject parallelized.\nnn.Module\nModule\n\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>>\n>>> # Define the module.\n>>> m = Model(...)\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>> m = parallelize_module(m, tp_mesh, {\"w1\": ColwiseParallel(), \"w2\": RowwiseParallel()})\n>>>\n\n```\n\nNote\nFor complex module architecture like Attention, MLP layers, we recommend composing\ndifferent ParallelStyles together (i.e.ColwiseParallelandRowwiseParallel) and pass\nas a parallelize_plan, to achieves the desired sharding computation.\nColwiseParallel\nRowwiseParallel\nTensor Parallelism supports the following parallel styles:\nPartition a compatible nn.Module in a column-wise fashion. Currently supports nn.Linear and nn.Embedding.\nUsers can compose it together with RowwiseParallel to achieve the sharding of more complicated modules.\n(i.e. MLP, Attention)\ninput_layouts(Placement,optional) \u2013 The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to\nbecome a DTensor. If not specified, we assume the input tensor to be replicated.\noutput_layouts(Placement,optional) \u2013 The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module\nwith the user desired layout. If not specified, the output tensor is sharded on the last dimension.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module output, default: True.\ntorch.Tensor\nDTensor\nAParallelStyleobject that represents Colwise sharding of the nn.Module.\nParallelStyle\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> m = Model(...)  # m is a nn.Module that contains a \"w1\" nn.Linear submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # By default, the input of the \"w1\" Linear will be converted to Replicated DTensor\n>>> # and the output of \"w1\" will return :class:`torch.Tensor` that shards on the last dim.\n>>>\n>>> sharded_mod = parallelize_module(m, tp_mesh, {\"w1\": ColwiseParallel()})\n>>> ...\n\n```\n\nNote\nBy defaultColwiseParalleloutput is sharded on the last dimension if theoutput_layoutsnot\nspecified, if there\u2019re operators that require specific tensor shape (i.e. before the pairedRowwiseParallel),\nkeep in mind that if the output is sharded the operator might need to be adjusted to the sharded size.\nColwiseParallel\noutput_layouts\nRowwiseParallel\nPartition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding.\nUsers can compose it with ColwiseParallel to achieve the sharding of more complicated modules.\n(i.e. MLP, Attention)\ninput_layouts(Placement,optional) \u2013 The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to\nbecome a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.\noutput_layouts(Placement,optional) \u2013 The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module\nwith the user desired layout. If not specified, the output tensor is replicated.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module output, default: True.\ntorch.Tensor\nDTensor\nAParallelStyleobject that represents Rowwise sharding of the nn.Module.\nParallelStyle\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, RowwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> m = Model(...)  # m is a nn.Module that contains a \"w2\" nn.Linear submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # By default, the input of the \"w2\" Linear will be converted to DTensor that shards on the last dim\n>>> # and the output of \"w2\" will return a replicated :class:`torch.Tensor`.\n>>>\n>>> sharded_mod = parallelize_module(m, tp_mesh, {\"w2\": RowwiseParallel()}),\n>>> ...\n\n```\n\nSequenceParallel replicates a compatiblenn.Moduleparameters and runs the sharded computation with\ninput sharded on the sequence dimension. This currently supportsnn.LayerNorm,nn.Dropout, and theRMSNorm python implementation\nnn.Module\nnn.LayerNorm\nnn.Dropout\nThis style implements the operation that is described in the paperReducing Activation Recomputation in Large Transformer Models\nIf the input passed in to thisnn.Moduleis atorch.Tensor, it assumes that the input is already sharded\non the sequence dimension and converts the input to aDTensorsharded on the sequence dimension. If the input\npassed in to thisnn.Moduleis already aDTensorbut is not sharded on the sequence dimension, it would\nredistribute the input to be sharded on the sequence dimension.\nnn.Module\ntorch.Tensor\nDTensor\nnn.Module\nDTensor\nThe output of thenn.Modulewill be sharded on the sequence dimension.\nnn.Module\nsequence_dim(int,optional) \u2013 The sequence dimension of the input tensor for thenn.Module, this is used to annotate the input tensor to\nbecome a DTensor that is sharded on the sequence dimension, default: 1.\nnn.Module\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module output, default: False.\ntorch.Tensor\nDTensor\nAParallelStyleobject that represents Sequence Parallel of thenn.Module.\nParallelStyle\nnn.Module\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, SequenceParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> m = Model(...)  # m is a nn.Module that contains a \"norm\" nn.LayerNorm submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # By default, the input of the \"norm\" will be converted to DTensor that shards on the sequence dim\n>>> # and the output of \"norm\" will return a sharded on sequence dimension :class:`DTensor`.\n>>>\n>>> sharded_mod = parallelize_module(m, tp_mesh, {\"norm\": SequenceParallel()}),\n>>> ...\n\n```\n\nNote\nSequenceParallel style assumes ones initialization if there are weights in the nn.Module (i.e.nn.LayerNormorRMSNorm, and they by default have ones initialization). If you have custom\ninits for the weights on those modules, you need to broadcast the weights before/after parallelizing\nto ensure that they are replicated.\nnn.LayerNorm\nRMSNorm\nTo simply configure the nn.Module\u2019s inputs and outputs with DTensor layouts\nand perform necessary layout redistributions, without distribute the module\nparameters to DTensors, the followingParallelStyles can be used in\ntheparallelize_planwhen callingparallelize_module:\nParallelStyle\nparallelize_plan\nparallelize_module\nConfigure the nn.Module\u2019s inputs to convert the input tensors of the nn.Module to DTensors at runtime according toinput_layouts, and perform layout redistribution according to thedesired_input_layouts.\ninput_layouts\ndesired_input_layouts\ninput_layouts(Union[Placement,Tuple[Optional[Placement]]]) \u2013 The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to\nDTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,Noneneed to be specified\nas a placeholder. default: None.\nNone\ndesired_input_layouts(Union[Placement,Tuple[Optional[Placement]]]) \u2013 The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module\nhave the desired DTensor layouts. This argument needs to have the same length withinput_layouts. default: None.\ninput_layouts\ninput_kwarg_layouts(Dict[str,Placement]) \u2013 The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.\ndefault: None\ndesired_input_kwarg_layouts\u2013 (Dict[str, Placement]):\nThe desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module\nhave the desired DTensor layouts. default: None.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module inputs, default: False.\ntorch.Tensor\nDTensor\nAParallelStyleobject that prepares the sharding layouts of the nn.Module\u2019s inputs.\nParallelStyle\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> block = TransformerBlock(...)  # block is a nn.Module that contains an \"attn\" Attention submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # According to the style specified below, the first input of attn will be annotated to Sharded DTensor\n>>> # and then redistributed to Replicated DTensor.\n>>> parallelize_module(\n>>>     block, # this can be a submodule or module\n>>>     tp_mesh,\n>>>     parallelize_plan={\n>>>         \"attn\": PrepareModuleInput(\n>>>             input_layouts=(Shard(0), None, None, ...),\n>>>             desired_input_layouts=(Replicate(), None, None, ...)\n>>>         ),\n>>>     }\n>>> )\n\n```\n\nConfigure the nn.Module\u2019s outputs to convert the output tensors of the nn.Module to DTensors at runtime according tooutput_layouts, and perform layout redistribution according to thedesired_output_layouts.\noutput_layouts\ndesired_output_layouts\noutput_layouts(Union[Placement,Tuple[Placement]]) \u2013 The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to\nDTensors if they aretorch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,Noneneed to be specified as a placeholder.\ntorch.Tensor\nNone\ndesired_output_layouts(Union[Placement,Tuple[Placement]]) \u2013 The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module\nhave the desired DTensor layouts.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module outputs, default: True.\ntorch.Tensor\nDTensor\nA ParallelStyle object that prepares the sharding layouts of the nn.Module\u2019s outputs.\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleOutput\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> block = TransformerBlock(...)  # block is a nn.Module that contains an \"attn\" Attention submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # According to the style specified below, the output of the TransformerBlock will be converted to Replicated DTensor\n>>> # and then redistributed to Sharded DTensor.\n>>> parallelize_module(\n>>>     block, # this can be a submodule or module\n>>>     tp_mesh,\n>>>     parallelize_plan = PrepareModuleOutput(\n>>>         output_layouts=Replicate(),\n>>>         desired_output_layouts=Shard(0)\n>>>     )\n>>> )\n\n```\n\nConfigure the nn.Module\u2019s inputs (and outputs) to convert the input tensors (and output tensors, respectively) of the nn.Module\nto DTensors at runtime according toinput_layouts(and output_layouts, respectively), and perform layout redistribution\naccording to thedesired_input_layouts(anddesired_output_layouts, respectively). This is a combination ofPrepareModuleInputandPrepareModuleOutput.\ninput_layouts\ndesired_input_layouts\ndesired_output_layouts\nPrepareModuleInput\nPrepareModuleOutput\ninput_layouts(Union[Placement,Tuple[Optional[Placement]]]) \u2013 The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to\nDTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,Noneneed to be specified\nas a placeholder. default: None.\nNone\ndesired_input_layouts(Union[Placement,Tuple[Optional[Placement]]]) \u2013 The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module\nhave the desired DTensor layouts. This argument needs to have the same length withinput_layouts. default: None.\ninput_layouts\ninput_kwarg_layouts(Dict[str,Placement]) \u2013 The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.\ndefault: None\ndesired_input_kwarg_layouts\u2013 (Dict[str, Placement]):\nThe desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module\nhave the desired DTensor layouts. default: None.\nuse_local_input(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module inputs, default: False.\ntorch.Tensor\nDTensor\noutput_layouts(Union[Placement,Tuple[Placement]]) \u2013 The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to\nDTensors if they aretorch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,Noneneed to be specified as a placeholder.\ntorch.Tensor\nNone\ndesired_output_layouts(Union[Placement,Tuple[Placement]]) \u2013 The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module\nhave the desired DTensor layouts.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module outputs, default: True.\ntorch.Tensor\nDTensor\nAParallelStyleobject that prepares the sharding layouts of the nn.Module\u2019s inputs and outputs.\nParallelStyle\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInputOutput\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> block = TransformerBlock(...)  # block is a nn.Module that contains an \"attn\" Attention submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # According to the style specified below, the first input of attn will be annotated as Sharded DTensor\n>>> # and then redistributed to Replicated DTensor, and the output of the TransformerBlock will be annotated\n>>> # as Replicated DTensor and then redistributed to Sharded DTensor.\n>>> parallelize_module(\n>>>     block, # this can be a submodule or module\n>>>     tp_mesh,\n>>>     parallelize_plan={\n>>>         \"attn\": PrepareModuleInputOutput(\n>>>             input_layouts=(Shard(0), None, None, ...),\n>>>             desired_input_layouts=(Replicate(), None, None, ...),\n>>>             output_layouts=Replicate(),\n>>>             desired_output_layouts=Shard(0),\n>>>         ),\n>>>     }\n>>> )\n\n```\n\nNote\nwhen using theShard(dim)as the input/output layouts for the aboveParallelStyles, we assume the input/output activation tensors are evenly sharded on\nthe tensor dimensiondimon theDeviceMeshthat TP operates on. For instance,\nsinceRowwiseParallelaccepts input that is sharded on the last dimension, it assumes\nthe input tensor has already been evenly sharded on the last dimension. For the case of uneven sharded activation tensors, one could pass in DTensor directly to the partitioned modules, and useuse_local_output=Falseto return DTensor after eachParallelStyle, where DTensor could track the uneven sharding information.\nShard(dim)\nParallelStyle\ndim\nDeviceMesh\nRowwiseParallel\nuse_local_output=False\nParallelStyle\nFor models like Transformer, we recommend users to useColwiseParallelandRowwiseParalleltogether in the parallelize_plan for achieve the desired\nsharding for the entire model (i.e. Attention and MLP).\nColwiseParallel\nRowwiseParallel\nParallelized cross-entropy loss computation (loss parallelism), is supported via the following context manager:\nA context manager that enables loss parallelism, where efficient parallelized loss computation\ncan be performed when the input is sharded on the class dimension. Currently only the cross-entropy\nloss is supported.\nWithin this context manager, one can usecross_entropy()orCrossEntropyLossas usual, with the following assumptions on the input parameters.\nThe correspondingbackward()call, if any, also needs to happen under this context manager.\ncross_entropy()\nCrossEntropyLoss\nbackward()\ninput(DTensor) \u2013 Input logits. Assumed to be sharded on the class dimension.\nDTensor\ntarget(Union[torch.Tensor,DTensor]) \u2013 Must be ground truth class indices (class probabilities currently not supported).\nAssumed to be replicated across theDeviceMesh.\ntorch.Tensor\nDTensor\nDeviceMesh\nweight(Union[torch.Tensor,DTensor], optional) \u2013 If given, assumed to be replicated across theDeviceMesh.\ntorch.Tensor\nDTensor\nDeviceMesh\nlabel_smoothing\u2013 Currently not supported.\nA replicatedDTensor.\nDTensor\nExample\nA sharded DTensor is manually created here to showcase the usage.\nIn practice, it is usually the output of a TP module.\n\n```python\n>>> from torch.distributed.tensor.parallel import loss_parallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> device_mesh = init_device_mesh(\"cuda\", (8,))\n>>> input = torch.randn(4, 16, device=\"cuda\", requires_grad=True)\n>>> dist_input = distribute_tensor(input, device_mesh, placements=[Shard(1)])\n>>> target = torch.randint(16, (4,), device=\"cuda\")\n>>> with loss_parallel():\n>>>     loss = F.cross_entropy(dist_input, target, reduction=\"mean\")\n>>>     loss.backward()\n>>> ...\n\n```\n\nWarning\n\n```python\nThe loss_parallel API is experimental and subject to change.\n\n```\n",
  "url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
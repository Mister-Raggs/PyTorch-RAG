{
  "doc_id": "8740dcbf582bda7401caf2a2e009e180",
  "source": "pytorch_docs",
  "title": "torch.export API Reference \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.export API Reference#\n\nCreated On: Jul 17, 2025 | Last Updated On: Jul 17, 2025\nexport()takes any nn.Module along with example inputs, and produces a traced graph representing\nonly the Tensor computation of the function in an Ahead-of-Time (AOT) fashion,\nwhich can subsequently be executed with different inputs or serialized.  The\ntraced graph (1) produces normalized operators in the functional ATen operator set\n(as well as any user-specified custom operators), (2) has eliminated all Python control\nflow and data structures (with certain exceptions), and (3) records the set of\nshape constraints needed to show that this normalization and control-flow elimination\nis sound for future inputs.\nexport()\nSoundness Guarantee\nWhile tracing,export()takes note of shape-related assumptions\nmade by the user program and the underlying PyTorch operator kernels.\nThe outputExportedProgramis considered valid only when these\nassumptions hold true.\nexport()\nExportedProgram\nTracing makes assumptions on the shapes (not values) of input tensors.\nSuch assumptions must be validated at graph capture time forexport()to succeed. Specifically:\nexport()\nAssumptions on static shapes of input tensors are automatically validated without additional effort.\nAssumptions on dynamic shape of input tensors require explicit specification\nby using theDim()API to construct dynamic dimensions and by associating\nthem with example inputs through thedynamic_shapesargument.\nDim()\ndynamic_shapes\nIf any assumption can not be validated, a fatal error will be raised. When that happens,\nthe error message will include suggested fixes to the specification that are needed\nto validate the assumptions. For exampleexport()might suggest the\nfollowing fix to the definition of a dynamic dimensiondim0_x, say appearing in the\nshape associated with inputx, that was previously defined asDim(\"dim0_x\"):\nexport()\ndim0_x\nx\nDim(\"dim0_x\")\n\n```python\ndim = Dim(\"dim0_x\", max=5)\n\n```\n\nThis example means the generated code requires dimension 0 of inputxto be less\nthan or equal to 5 to be valid. You can inspect the suggested fixes to dynamic dimension\ndefinitions and then copy them verbatim into your code without needing to change thedynamic_shapesargument to yourexport()call.\nx\ndynamic_shapes\nexport()\nmod(Module) \u2013 We will trace the forward method of this module.\nargs(tuple[Any,...]) \u2013 Example positional inputs.\nkwargs(Optional[Mapping[str,Any]]) \u2013 Optional example keyword inputs.\ndynamic_shapes(Optional[Union[dict[str,Any],tuple[Any,...],list[Any]]]) \u2013An optional argument where the type should either be:\n1) a dict from argument names offto their dynamic shape specifications,\n2) a tuple that specifies dynamic shape specifications for each input in original order.\nIf you are specifying dynamism on keyword args, you will need to pass them in the order that\nis defined in the original function signature.The dynamic shape of a tensor argument can be specified as either\n(1) a dict from dynamic dimension indices toDim()types, where it is\nnot required to include static dimension indices in this dict, but when they are,\nthey should be mapped to None; or (2) a tuple / list ofDim()types or None,\nwhere theDim()types correspond to dynamic dimensions, and static dimensions\nare denoted by None. Arguments that are dicts or tuples / lists of tensors are\nrecursively specified by using mappings or sequences of contained specifications.\nAn optional argument where the type should either be:\n1) a dict from argument names offto their dynamic shape specifications,\n2) a tuple that specifies dynamic shape specifications for each input in original order.\nIf you are specifying dynamism on keyword args, you will need to pass them in the order that\nis defined in the original function signature.\nf\nThe dynamic shape of a tensor argument can be specified as either\n(1) a dict from dynamic dimension indices toDim()types, where it is\nnot required to include static dimension indices in this dict, but when they are,\nthey should be mapped to None; or (2) a tuple / list ofDim()types or None,\nwhere theDim()types correspond to dynamic dimensions, and static dimensions\nare denoted by None. Arguments that are dicts or tuples / lists of tensors are\nrecursively specified by using mappings or sequences of contained specifications.\nDim()\nDim()\nDim()\nstrict(bool) \u2013 When disabled (default), the export function will trace the program through\nPython runtime, which by itself will not validate some of the implicit assumptions\nbaked into the graph. It will still validate most critical assumptions like shape\nsafety. When enabled (by settingstrict=True), the export function will trace\nthe program through TorchDynamo which will ensure the soundness of the resulting\ngraph. TorchDynamo has limited Python feature coverage, thus you may experience more\nerrors. Note that toggling this argument does not affect the resulting IR spec to be\ndifferent and the model will be serialized in the same way regardless of what value\nis passed here.\nstrict=True\npreserve_module_call_signature(tuple[str,...]) \u2013 A list of submodule paths for which the original\ncalling conventions are preserved as metadata. The metadata will be used when calling\ntorch.export.unflatten to preserve the original calling conventions of modules.\nAnExportedProgramcontaining the traced callable.\nExportedProgram\nExportedProgram\nAcceptable input/output types\nAcceptable types of inputs (forargsandkwargs) and outputs include:\nargs\nkwargs\nPrimitive types, i.e.torch.Tensor,int,float,boolandstr.\ntorch.Tensor\nint\nfloat\nbool\nstr\nDataclasses, but they must be registered by callingregister_dataclass()first.\nregister_dataclass()\n(Nested) Data structures comprising ofdict,list,tuple,namedtupleandOrderedDictcontaining all above types.\ndict\nlist\ntuple\nnamedtuple\nOrderedDict\nPackage of a program fromexport(). It contains\nantorch.fx.Graphthat represents Tensor computation, a state_dict containing\ntensor values of all lifted parameters and buffers, and various metadata.\nexport()\ntorch.fx.Graph\nYou can call an ExportedProgram like the original callable traced byexport()with the same calling convention.\nexport()\nTo perform transformations on the graph, use.moduleproperty to access\nantorch.fx.GraphModule. You can then useFX transformationto rewrite the graph. Afterwards, you can simply useexport()again to construct a correct ExportedProgram.\n.module\ntorch.fx.GraphModule\nexport()\nReturns an iterator over original module buffers.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nIterator[Tensor]\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nReturns a self contained GraphModule with all the parameters/buffers inlined.\nWhencheck_guards=True(default), a_guards_fnsubmodule is generated\nand a call to a_guards_fnsubmodule is inserted right after placeholders\nin the graph. This module checks guards on inputs.\nWhencheck_guards=False, a subset of these checks are performed by a\nforward pre-hook on the graph module. No_guards_fnsubmodule is generated.\nGraphModule\nWarning\nThis API is experimental and isNOTbackward-compatible.\nReturns an iterator over original module buffers, yielding\nboth the name of the buffer as well as the buffer itself.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nIterator[tuple[str,torch.Tensor]]\nReturns an iterator over original module parameters, yielding\nboth the name of the parameter as well as the parameter itself.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nIterator[tuple[str,torch.nn.parameter.Parameter]]\nReturns an iterator over original module\u2019s parameters.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nIterator[Parameter]\nWarning\nThis API is experimental and isNOTbackward-compatible.\nRun a set of decompositions on the exported program and returns a new\nexported program. By default we will run the Core ATen decompositions to\nget operators in theCore ATen Operator Set.\nFor now, we do not decompose joint graphs.\ndecomp_table(Optional[dict[torch._ops.OperatorBase,Callable]]) \u2013 An optional argument that specifies decomp behaviour for Aten ops\n(1) If None, we decompose to core aten decompositions\n(2) If empty, we don\u2019t decompose any operator\nExportedProgram\nSome examples:\nIf you don\u2019t want to decompose anything\n\n```python\nep = torch.export.export(model, ...)\nep = ep.run_decompositions(decomp_table={})\n\n```\n\nIf you want to get a core aten operator set except for certain operator, you can do following:\n\n```python\nep = torch.export.export(model, ...)\ndecomp_table = torch.export.default_decompositions()\ndecomp_table[your_op] = your_custom_decomp\nep = ep.run_decompositions(decomp_table=decomp_table)\n\n```\n\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInfers dynamic_shapes based on additional inputs.\nThis is useful particularly for deployment engineers who, on the one hand, may\nhave access to ample testing or profiling data that can provide a fair sense of\nrepresentative inputs for a model, but on the other hand, may not know enough\nabout the model to guess which input shapes should be dynamic.\nInput shapes that are different than the original are considered dynamic; conversely,\nthose that are the same as the original are considered static. Moreover, we verify\nthat the additional inputs are valid for the exported program. This guarantees that\ntracing with them instead of the original would have generated the same graph.\nExample:\n\n```python\nargs0, kwargs0 = ...  # example inputs for export\n\n# other representative inputs that the exported program will run on\ndynamic_shapes = torch.export.AdditionalInputs()\ndynamic_shapes.add(args1, kwargs1)\n...\ndynamic_shapes.add(argsN, kwargsN)\n\ntorch.export(..., args0, kwargs0, dynamic_shapes=dynamic_shapes)\n\n```\n\nAdditional inputargs()andkwargs().\nargs()\nkwargs()\nInfers adynamic_shapes()pytree structure by merging shapes of the\noriginal inputargs()andkwargs()and of each additional input\nargs and kwargs.\ndynamic_shapes()\nargs()\nkwargs()\nVerifies that an exported program is valid for each additional input.\nTheDimclass allows users to specify dynamism in their exported\nprograms. By marking a dimension with aDim, the compiler associates the\ndimension with a symbolic integer containing a dynamic range.\nDim\nDim\nThe API can be used in 2 ways: Dim hints (i.e. automatic dynamic shapes:Dim.AUTO,Dim.DYNAMIC,Dim.STATIC), or named Dims (i.e.Dim(\"name\",min=1,max=2)).\nDim.AUTO\nDim.DYNAMIC\nDim.STATIC\nDim(\"name\",min=1,max=2)\nDim hints provide the lowest barrier to exportability, with the user only\nneeding to specify if a dimension if dynamic, static, or left for the\ncompiler to decide (Dim.AUTO). The export process will automatically\ninfer the remaining constraints on min/max ranges and relationships between\ndimensions.\nDim.AUTO\nExample:\n\n```python\nclass Foo(nn.Module):\n    def forward(self, x, y):\n        assert x.shape[0] == 4\n        assert y.shape[0] >= 16\n        return x @ y\n\n\nx = torch.randn(4, 8)\ny = torch.randn(8, 16)\ndynamic_shapes = {\n    \"x\": {0: Dim.AUTO, 1: Dim.AUTO},\n    \"y\": {0: Dim.AUTO, 1: Dim.AUTO},\n}\nep = torch.export(Foo(), (x, y), dynamic_shapes=dynamic_shapes)\n\n```\n\nHere, export would raise an exception if we replaced all uses ofDim.AUTOwithDim.DYNAMIC,\nasx.shape[0]is constrained to be static by the model.\nDim.AUTO\nDim.DYNAMIC\nx.shape[0]\nMore complex relations between dimensions may also be codegened as runtime assertion nodes by the compiler,\ne.g.(x.shape[0]+y.shape[1])%4==0, to be raised if runtime inputs do not satisfy such constraints.\n(x.shape[0]+y.shape[1])%4==0\nYou may also specify min-max bounds for Dim hints, e.g.Dim.AUTO(min=16,max=32),Dim.DYNAMIC(max=64),\nwith the compiler inferring the remaining constraints within the ranges. An exception will be raised if\nthe valid range is entirely outside the user-specified range.\nDim.AUTO(min=16,max=32)\nDim.DYNAMIC(max=64)\nNamed Dims provide a stricter way of specifying dynamism, where exceptions are raised if the compiler\ninfers constraints that do not match the user specification. For example, exporting the previous\nmodel, the user would need the followingdynamic_shapesargument:\ndynamic_shapes\n\n```python\ns0 = Dim(\"s0\")\ns1 = Dim(\"s1\", min=16)\ndynamic_shapes = {\n    \"x\": {0: 4, 1: s0},\n    \"y\": {0: s0, 1: s1},\n}\nep = torch.export(Foo(), (x, y), dynamic_shapes=dynamic_shapes)\n\n```\n\nNamed Dims also allow specification of relationships between dimensions, up\nto univariate linear relations.  For example, the following indicates one\ndimension is a multiple of another plus 4:\n\n```python\ns0 = Dim(\"s0\")\ns1 = 3 * s0 + 4\n\n```\n\nBuilder for dynamic_shapes.\nUsed to assign dynamic shape specifications to tensors that appear in inputs.\nThis is useful particularly whenargs()is a nested input structure, and it\u2019s\neasier to index the input tensors, than to replicate the structure ofargs()in\nthedynamic_shapes()specification.\nargs()\nargs()\ndynamic_shapes()\nExample:\n\n```python\nargs = {\"x\": tensor_x, \"others\": [tensor_y, tensor_z]}\n\ndim = torch.export.Dim(...)\ndynamic_shapes = torch.export.ShapesCollection()\ndynamic_shapes[tensor_x] = (dim, dim + 1, 8)\ndynamic_shapes[tensor_y] = {0: dim * 2}\n# This is equivalent to the following (now auto-generated):\n# dynamic_shapes = {\"x\": (dim, dim + 1, 8), \"others\": [{0: dim * 2}, None]}\n\ntorch.export(..., args, dynamic_shapes=dynamic_shapes)\n\n```\n\nTo specify dynamism for integers, we need to first wrap the integers using\n_IntWrapper so that we have a \u201cunique identification tag\u201d for each integer.\nExample:\n\n```python\nargs = {\"x\": tensor_x, \"others\": [int_x, int_y]}\n# Wrap all ints with _IntWrapper\nmapped_args = pytree.tree_map_only(int, lambda a: _IntWrapper(a), args)\n\ndynamic_shapes = torch.export.ShapesCollection()\ndynamic_shapes[tensor_x] = (dim, dim + 1, 8)\ndynamic_shapes[mapped_args[\"others\"][0]] = Dim.DYNAMIC\n\n# This is equivalent to the following (now auto-generated):\n# dynamic_shapes = {\"x\": (dim, dim + 1, 8), \"others\": [Dim.DYNAMIC, None]}\n\ntorch.export(..., args, dynamic_shapes=dynamic_shapes)\n\n```\n\nGenerates thedynamic_shapes()pytree structure according toargs()andkwargs().\ndynamic_shapes()\nargs()\nkwargs()\nWhen exporting withdynamic_shapes(), export may fail with a ConstraintViolation error if the specification\ndoesn\u2019t match the constraints inferred from tracing the model. The error message may provide suggested fixes -\nchanges that can be made todynamic_shapes()to export successfully.\ndynamic_shapes()\ndynamic_shapes()\nExample ConstraintViolation error message:\n\n```python\nSuggested fixes:\n\n    dim = Dim('dim', min=3, max=6)  # this just refines the dim's range\n    dim = 4  # this specializes to a constant\n    dy = dx + 1  # dy was specified as an independent dim, but is actually tied to dx with this relation\n\n```\n\nThis is a helper function that takes the ConstraintViolation error message and the originaldynamic_shapes()spec,\nand returns a newdynamic_shapes()spec that incorporates the suggested fixes.\ndynamic_shapes()\ndynamic_shapes()\nExample usage:\n\n```python\ntry:\n    ep = export(mod, args, dynamic_shapes=dynamic_shapes)\nexcept torch._dynamo.exc.UserError as exc:\n    new_shapes = refine_dynamic_shapes_from_suggested_fixes(\n        exc.msg, dynamic_shapes\n    )\n    ep = export(mod, args, dynamic_shapes=new_shapes)\n\n```\n\nUnion[dict[str,Any],tuple[Any],list[Any]]\nWarning\nUnder active development, saved files may not be usable in newer versions\nof PyTorch.\nSaves anExportedProgramto a file-like object. It can then be\nloaded using the Python APItorch.export.load.\nExportedProgram\ntorch.export.load\nep(ExportedProgram) \u2013 The exported program to save.\nf(str|os.PathLike[str]|IO[bytes]) \u2013 implement write and flush) or a string containing a file name.\nextra_files(Optional[Dict[str,Any]]) \u2013 Map from filename to contents\nwhich will be stored as part of f.\nopset_version(Optional[Dict[str,int]]) \u2013 A map of opset names\nto the version of this opset\npickle_protocol(int) \u2013 can be specified to override the default protocol\nExample:\n\n```python\nimport torch\nimport io\n\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\n\nep = torch.export.export(MyModule(), (torch.randn(5),))\n\n# Save to file\ntorch.export.save(ep, \"exported_program.pt2\")\n\n# Save to io.BytesIO buffer\nbuffer = io.BytesIO()\ntorch.export.save(ep, buffer)\n\n# Save with extra files\nextra_files = {\"foo.txt\": b\"bar\".decode(\"utf-8\")}\ntorch.export.save(ep, \"exported_program.pt2\", extra_files=extra_files)\n\n```\n\nWarning\nUnder active development, saved files may not be usable in newer versions\nof PyTorch.\nLoads anExportedProgrampreviously saved withtorch.export.save.\nExportedProgram\ntorch.export.save\nf(str|os.PathLike[str]|IO[bytes]) \u2013 A file-like object (has to\nimplement write and flush) or a string containing a file name.\nextra_files(Optional[Dict[str,Any]]) \u2013 The extra filenames given in\nthis map would be loaded and their content would be stored in the\nprovided map.\nexpected_opset_version(Optional[Dict[str,int]]) \u2013 A map of opset names\nto expected opset versions\nAnExportedProgramobject\nExportedProgram\nExportedProgram\nExample:\n\n```python\nimport torch\nimport io\n\n# Load ExportedProgram from file\nep = torch.export.load(\"exported_program.pt2\")\n\n# Load ExportedProgram from io.BytesIO object\nwith open(\"exported_program.pt2\", \"rb\") as f:\n    buffer = io.BytesIO(f.read())\nbuffer.seek(0)\nep = torch.export.load(buffer)\n\n# Load with extra files.\nextra_files = {\"foo.txt\": \"\"}  # values will be replaced with data\nep = torch.export.load(\"exported_program.pt2\", extra_files=extra_files)\nprint(extra_files[\"foo.txt\"])\nprint(ep(torch.randn(5)))\n\n```\n\nSaves the artifacts to a PT2Archive format. The artifact can then be loaded\nusingload_pt2.\nload_pt2\nf(str|os.PathLike[str]|IO[bytes]) \u2013 A file-like object (has to\nimplement write and flush) or a string containing a file name.\nexported_programs(Union[ExportedProgram,dict[str,ExportedProgram]]) \u2013 The exported program to save, or a dictionary mapping model name to an\nexported program to save. The exported program will be saved under\nmodels/*.json. If only one ExportedProgram is specified, this will\nautomatically be named \u201cmodel\u201d.\naoti_files(Union[list[str],dict[str,list[str]]]) \u2013 A list of files\ngenerated by AOTInductor viatorch._inductor.aot_compile(...,{\"aot_inductor.package\":True}),\nor a dictionary mapping model name to its AOTInductor generated files.\nIf only one set of files is specified, this will automatically be named\n\u201cmodel\u201d.\ntorch._inductor.aot_compile(...,{\"aot_inductor.package\":True})\nextra_files(Optional[Dict[str,Any]]) \u2013 Map from filename to contents\nwhich will be stored as part of the pt2.\nopset_version(Optional[Dict[str,int]]) \u2013 A map of opset names\nto the version of this opset\npickle_protocol(int) \u2013 can be specified to override the default protocol\nUnion[str,PathLike[str],IO[bytes]]\nLoads all the artifacts previously saved withpackage_pt2.\npackage_pt2\nf(str|os.PathLike[str]|IO[bytes]) \u2013 A file-like object (has to\nimplement write and flush) or a string containing a file name.\nexpected_opset_version(Optional[Dict[str,int]]) \u2013 A map of opset names\nto expected opset versions\nnum_runners(int) \u2013 Number of runners to load AOTInductor artifacts\nrun_single_threaded(bool) \u2013 Whether the model should be run without\nthread synchronization logic. This is useful to avoid conflicts with\nCUDAGraphs.\ndevice_index(int) \u2013 The index of the device to which the PT2 package is\nto be loaded. By default,device_index=-1is used, which corresponds\nto the devicecudawhen using CUDA. Passingdevice_index=1would\nload the package tocuda:1, for example.\nAPT2ArchiveContentsobject which contains all the objects in the PT2.\nPT2ArchiveContents\nPT2ArchiveContents\nA version of torch.export.export which is designed to consistently produce\nan ExportedProgram, even if there are potential soundness issues, and to\ngenerate a report listing the issues found.\nExportedProgram\nAdapts input arguments withinput_specto aligntarget_spec.\ninput_spec\ntarget_spec\nNOTE: This adapter may mutate giveninput_args_with_path.\ninput_args_with_path\nlist[Any]\nReturns a list of paths that are used to access the flat args.\nlist[str]\nA module that uses torch.fx.Interpreter to execute instead of the usual\ncodegen that GraphModule uses. This provides better stack trace information\nand makes it easier to debug execution.\nA module that carries a sequence of InterpreterModules corresponding to\na sequence of calls of that module. Each call to the module dispatches\nto the next InterpreterModule, and wraps back around after the last.\nUnflatten an ExportedProgram, producing a module with the same module\nhierarchy as the original eager module. This can be useful if you are trying\nto usetorch.exportwith another system that expects a module\nhierarchy instead of the flat graph thattorch.exportusually produces.\ntorch.export\ntorch.export\nNote\nThe args/kwargs of unflattened modules will not necessarily match\nthe eager module, so doing a module swap (e.g.self.submod=new_mod) will not necessarily work. If you need to swap a module out, you\nneed to set thepreserve_module_call_signatureparameter oftorch.export.export().\nself.submod=new_mod\npreserve_module_call_signature\ntorch.export.export()\nmodule(ExportedProgram) \u2013 The ExportedProgram to unflatten.\nflat_args_adapter(Optional[FlatArgsAdapter]) \u2013 Adapt flat args if input TreeSpec does not match with exported module\u2019s.\nAn instance ofUnflattenedModule, which has the same module\nhierarchy as the original eager module pre-export.\nUnflattenedModule\nUnflattenedModule\nRegisters a dataclass as a valid input/output type fortorch.export.export().\ntorch.export.export()\ncls(type[Any]) \u2013 the dataclass type to register\nserialized_type_name(Optional[str]) \u2013 The serialized name for the dataclass. This is\nthis(required if you want to serialize the pytree TreeSpec containing) \u2013\ndataclass.\u2013\nExample:\n\n```python\nimport torch\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass InputDataClass:\n    feature: torch.Tensor\n    bias: int\n\n\n@dataclass\nclass OutputDataClass:\n    res: torch.Tensor\n\n\ntorch.export.register_dataclass(InputDataClass)\ntorch.export.register_dataclass(OutputDataClass)\n\n\nclass Mod(torch.nn.Module):\n    def forward(self, x: InputDataClass) -> OutputDataClass:\n        res = x.feature + x.bias\n        return OutputDataClass(res=res)\n\n\nep = torch.export.export(Mod(), (InputDataClass(torch.ones(2, 2), 1),))\nprint(ep)\n\n```\n\nThis is a custom dictionary that is specifically used for handling decomp_table in export.\nThe reason we need this is because in the new world, you can onlydeletean op from decomp\ntable to preserve it. This is problematic for custom ops because we don\u2019t know when the custom\nop will actually be loaded to the dispatcher. As a result, we need to record the custom ops operations\nuntil we really need to materialize it (which is when we run decomposition pass.)\nAll aten decomp is loaded at the init time\nWe materialize ALL ops when user ever reads from the table to make it more likely\nthat dispatcher picks up the custom op.\nIf it is write operation, we don\u2019t necessarily materialize\nWe load the final time during export, right before calling run_decompositions()\nCustomDecompTable\ndict[torch._ops.OperatorBase,Callable]\nMove the exported program to the given device.\nep(ExportedProgram) \u2013 The exported program to move.\nlocation(Union[torch.device,str,Dict[str,str]]) \u2013 The device to move the exported program to.\nIf a string, it is interpreted as a device name.\nIf a dict, it is interpreted as a mapping from\nthe existing device to the intended one\nThe moved exported program.\nExportedProgram\nContext manager for reading a PT2 archive.\nGet the archive version.\nint\nGet the file names in the archive.\nlist[str]\nRead a bytes object from the archive.\nname: The source file inside the archive.\nbytes\nRead a string object from the archive.\nname: The source file inside the archive.\nstr\nContext manager for writing a PT2 archive.\nClose the archive.\nCount the number of records that start with a given prefix.\nint\nCheck if a record exists in the archive.\nbool\nWrite a bytes object to the archive.\nname: The destination file inside the archive.\ndata: The bytes object to write.\nCopy a file into the archive.\nname: The destination file inside the archive.\nfile_path: The source file on disk.\nCopy a folder into the archive.\narchive_dir: The destination folder inside the archive.\nfolder_dir: The source folder on disk.\nWrite a string object to the archive.\nname: The destination file inside the archive.\ndata: The string object to write.\nCheck if the serialized model is a PT2 Archive package.\nbool\nThis is the default decomposition table which contains decomposition of\nall ATEN operators to core aten opset. Use this API together withrun_decompositions()\nrun_decompositions()\nCustomDecompTable\nMetadata which is stored on nodes representing ScriptObjects.\nExportGraphSignaturemodels the input/output signature of Export Graph,\nwhich is a fx.Graph with stronger invariants guarantees.\nExportGraphSignature\nExport Graph is functional and does not access \u201cstates\u201d like parameters\nor buffers within the graph viagetattrnodes. Instead,export()guarantees that parameters, buffers, and constant tensors are lifted out of\nthe graph as inputs.  Similarly, any mutations to buffers are not included\nin the graph either, instead the updated values of mutated buffers are\nmodeled as additional outputs of Export Graph.\ngetattr\nexport()\nThe ordering of all inputs and outputs are:\n\n```python\nInputs = [*parameters_buffers_constant_tensors, *flattened_user_inputs]\nOutputs = [*mutated_inputs, *flattened_user_outputs]\n\n```\n\ne.g. If following module is exported:\n\n```python\nclass CustomModule(nn.Module):\n    def __init__(self) -> None:\n        super(CustomModule, self).__init__()\n\n        # Define a parameter\n        self.my_parameter = nn.Parameter(torch.tensor(2.0))\n\n        # Define two buffers\n        self.register_buffer(\"my_buffer1\", torch.tensor(3.0))\n        self.register_buffer(\"my_buffer2\", torch.tensor(4.0))\n\n    def forward(self, x1, x2):\n        # Use the parameter, buffers, and both inputs in the forward method\n        output = (\n            x1 + self.my_parameter\n        ) * self.my_buffer1 + x2 * self.my_buffer2\n\n        # Mutate one of the buffers (e.g., increment it by 1)\n        self.my_buffer2.add_(1.0)  # In-place addition\n\n        return output\n\n\nmod = CustomModule()\nep = torch.export.export(mod, (torch.tensor(1.0), torch.tensor(2.0)))\n\n```\n\nResulting Graph is non-functional:\n\n```python\ngraph():\n    %p_my_parameter : [num_users=1] = placeholder[target=p_my_parameter]\n    %b_my_buffer1 : [num_users=1] = placeholder[target=b_my_buffer1]\n    %b_my_buffer2 : [num_users=2] = placeholder[target=b_my_buffer2]\n    %x1 : [num_users=1] = placeholder[target=x1]\n    %x2 : [num_users=1] = placeholder[target=x2]\n    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x1, %p_my_parameter), kwargs = {})\n    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, %b_my_buffer1), kwargs = {})\n    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%x2, %b_my_buffer2), kwargs = {})\n    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, %mul_1), kwargs = {})\n    %add_ : [num_users=0] = call_function[target=torch.ops.aten.add_.Tensor](args = (%b_my_buffer2, 1.0), kwargs = {})\n    return (add_1,)\n\n```\n\nResulting ExportGraphSignature of the non-functional Graph would be:\n\n```python\n# inputs\np_my_parameter: PARAMETER target='my_parameter'\nb_my_buffer1: BUFFER target='my_buffer1' persistent=True\nb_my_buffer2: BUFFER target='my_buffer2' persistent=True\nx1: USER_INPUT\nx2: USER_INPUT\n\n# outputs\nadd_1: USER_OUTPUT\n\n```\n\nTo get a functional Graph, you can userun_decompositions():\nrun_decompositions()\n\n```python\nmod = CustomModule()\nep = torch.export.export(mod, (torch.tensor(1.0), torch.tensor(2.0)))\nep = ep.run_decompositions()\n\n```\n\nResulting Graph is functional:\n\n```python\ngraph():\n    %p_my_parameter : [num_users=1] = placeholder[target=p_my_parameter]\n    %b_my_buffer1 : [num_users=1] = placeholder[target=b_my_buffer1]\n    %b_my_buffer2 : [num_users=2] = placeholder[target=b_my_buffer2]\n    %x1 : [num_users=1] = placeholder[target=x1]\n    %x2 : [num_users=1] = placeholder[target=x2]\n    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x1, %p_my_parameter), kwargs = {})\n    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, %b_my_buffer1), kwargs = {})\n    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%x2, %b_my_buffer2), kwargs = {})\n    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, %mul_1), kwargs = {})\n    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_my_buffer2, 1.0), kwargs = {})\n    return (add_2, add_1)\n\n```\n\nResulting ExportGraphSignature of the functional Graph would be:\n\n```python\n# inputs\np_my_parameter: PARAMETER target='my_parameter'\nb_my_buffer1: BUFFER target='my_buffer1' persistent=True\nb_my_buffer2: BUFFER target='my_buffer2' persistent=True\nx1: USER_INPUT\nx2: USER_INPUT\n\n# outputs\nadd_2: BUFFER_MUTATION target='my_buffer2'\nadd_1: USER_OUTPUT\n\n```\n\nReplace all uses of the old name with new name in the signature.\nAn enumeration.\nAn enumeration.",
  "url": "https://pytorch.org/docs/stable/export/api_reference.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
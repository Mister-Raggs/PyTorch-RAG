{
  "doc_id": "a4452cb439969ca647f3741252f336e2",
  "source": "pytorch_docs",
  "title": "Multiprocessing \u2014 PyTorch 2.9 documentation",
  "text": "\n## Multiprocessing#\n\nCreated On: May 04, 2021 | Last Updated On: Feb 29, 2024\nLibrary that launches and managesncopies of worker subprocesses either specified by a function or a binary.\nn\nFor functions, it usestorch.multiprocessing(and therefore pythonmultiprocessing) to spawn/fork worker processes. For binaries it uses pythonsubprocessing.Popento create worker processes.\ntorch.multiprocessing\nmultiprocessing\nsubprocessing.Popen\nUsage 1: Launching two trainers as a function\n\n```python\nfrom torch.distributed.elastic.multiprocessing import Std, start_processes\n\n\ndef trainer(a, b, c):\n    pass  # train\n\n\n# runs two trainers\n# LOCAL_RANK=0 trainer(1,2,3)\n# LOCAL_RANK=1 trainer(4,5,6)\nctx = start_processes(\n    name=\"trainer\",\n    entrypoint=trainer,\n    args={0: (1, 2, 3), 1: (4, 5, 6)},\n    envs={0: {\"LOCAL_RANK\": 0}, 1: {\"LOCAL_RANK\": 1}},\n    log_dir=\"/tmp/foobar\",\n    redirects=Std.ALL,  # write all worker stdout/stderr to a log file\n    tee={0: Std.ERR},  # tee only local rank 0's stderr to console\n)\n\n# waits for all copies of trainer to finish\nctx.wait()\n\n```\n\nUsage 2: Launching 2 echo workers as a binary\n\n```python\n# same as invoking\n# echo hello\n# echo world > stdout.log\nctx = start_processes(\n        name=\"echo\"\n        entrypoint=\"echo\",\n        log_dir=\"/tmp/foobar\",\n        args={0: \"hello\", 1: \"world\"},\n        redirects={1: Std.OUT},\n       )\n\n```\n\nJust liketorch.multiprocessing, the return value of the functionstart_processes()is a process context (api.PContext). If a function\nwas launched, aapi.MultiprocessContextis returned and if a binary\nwas launched aapi.SubprocessContextis returned. Both are specific\nimplementations of the parentapi.PContextclass.\ntorch.multiprocessing\nstart_processes()\napi.PContext\napi.MultiprocessContext\napi.SubprocessContext\napi.PContext\n\n## Starting Multiple Workers#\n\nStartncopies ofentrypointprocesses with the provided options.\nn\nentrypoint\nentrypointis either aCallable(function) or astr(binary).\nThe number of copies is determined by the number of entries forargsandenvsarguments, which need to have the same key set.\nentrypoint\nCallable\nstr\nargs\nenvs\nargsandenvparameters are the arguments and environment variables\nto pass down to the entrypoint mapped by the replica index (local rank).\nAll local ranks must be accounted for.\nThat is, the keyset should be{0,1,...,(nprocs-1)}.\nargs\nenv\n{0,1,...,(nprocs-1)}\nNote\nWhen theentrypointis a binary (str),argscan only be strings.\nIf any other type is given, then it is casted to a string representation\n(e.g.str(arg1)). Furthermore, a binary failure will only write\nanerror.jsonerror file if the main function is annotated withtorch.distributed.elastic.multiprocessing.errors.record. For function launches,\nthis is done by default and there is no need to manually annotate\nwith the@recordannotation.\nentrypoint\nstr\nargs\nstr(arg1)\nerror.json\ntorch.distributed.elastic.multiprocessing.errors.record\n@record\nredirectsandteeare bitmasks specifying which std stream(s) to redirect\nto a log file in thelog_dir. Valid mask values are defined inStd.\nTo redirect/tee only certain local ranks, passredirectsas a map with the key as\nthe local rank to specify the redirect behavior for.\nAny missing local ranks will default toStd.NONE.\nredirects\ntee\nlog_dir\nStd\nredirects\nStd.NONE\nteeacts like the unix \u201ctee\u201d command in that it redirects + prints to console.\nTo avoid worker stdout/stderr from printing to console, use theredirectsparameter.\ntee\nredirects\nFor each process, thelog_dirwill contain:\nlog_dir\n{local_rank}/error.json: if the process failed, a file with the error info\n{local_rank}/error.json\n{local_rank}/stdout.log: ifredirect&STDOUT==STDOUT\n{local_rank}/stdout.log\nredirect&STDOUT==STDOUT\n{local_rank}/stderr.log: ifredirect&STDERR==STDERR\n{local_rank}/stderr.log\nredirect&STDERR==STDERR\nNote\nIt is expected that thelog_direxists, is empty, and is a directory.\nlog_dir\nExample:\n\n```python\nlog_dir = \"/tmp/test\"\n\n# ok; two copies of foo: foo(\"bar0\"), foo(\"bar1\")\nstart_processes(\n   name=\"trainer\",\n   entrypoint=foo,\n   args:{0:(\"bar0\",), 1:(\"bar1\",),\n   envs:{0:{}, 1:{}},\n   log_dir=log_dir\n)\n\n# invalid; envs missing for local rank 1\nstart_processes(\n   name=\"trainer\",\n   entrypoint=foo,\n   args:{0:(\"bar0\",), 1:(\"bar1\",),\n   envs:{0:{}},\n   log_dir=log_dir\n)\n\n# ok; two copies of /usr/bin/touch: touch file1, touch file2\nstart_processes(\n   name=\"trainer\",\n   entrypoint=\"/usr/bin/touch\",\n   args:{0:(\"file1\",), 1:(\"file2\",),\n   envs:{0:{}, 1:{}},\n   log_dir=log_dir\n )\n\n# caution; arguments casted to string, runs:\n# echo \"1\" \"2\" \"3\" and echo \"[1, 2, 3]\"\nstart_processes(\n   name=\"trainer\",\n   entrypoint=\"/usr/bin/echo\",\n   args:{0:(1,2,3), 1:([1,2,3],),\n   envs:{0:{}, 1:{}},\n   log_dir=log_dir\n )\n\n```\n\nname(str) \u2013 a human readable short name that describes what the processes are\n(used as header when tee\u2019ing stdout/stderr outputs)\nentrypoint(Union[Callable,str]) \u2013 either aCallable(function) orcmd(binary)\nCallable\ncmd\nargs(dict[int,tuple]) \u2013 arguments to each replica\nenvs(dict[int,dict[str,str]]) \u2013 env vars to each replica\nlog_dir\u2013 directory used to write log files\nstart_method(str) \u2013 multiprocessing start method (spawn, fork, forkserver)\nignored for binaries\nredirects\u2013 which std streams to redirect to a log file\ntee\u2013 which std streams to redirect + print to console\nlocal_ranks_filter\u2013 which ranks\u2019 logs to print to console\nPContext\n\n## Process Context#\n\nThe base class that standardizes operations over a set of processes that are launched via different mechanisms.\nThe namePContextis intentional to disambiguate withtorch.multiprocessing.ProcessContext.\nPContext\ntorch.multiprocessing.ProcessContext\nWarning\nstdouts and stderrs should ALWAYS be a superset of\ntee_stdouts and tee_stderrs (respectively) this is b/c\ntee is implemented as a redirect + tail -f <stdout/stderr.log>\nPContextholding worker processes invoked as a function.\nPContext\nPContextholding worker processes invoked as a binary.\nPContext\nResults of a completed run of processes started withstart_processes(). Returned byPContext.\nstart_processes()\nPContext\nNote the following:\nAll fields are mapped by local rank\nreturn_values- only populated for functions (not the binaries).\nreturn_values\nstdouts- path to stdout.log (empty string if no redirect)\nstdouts\nstderrs- path to stderr.log (empty string if no redirect)\nstderrs\nDefault LogsSpecs implementation:\nlog_dirwill be created if it doesn\u2019t exist\nGenerates nested folders for each attempt and rank.\nUses following scheme to build log destination paths:\n<log_dir>/<rdzv_run_id>/attempt_<attempt>/<rank>/stdout.log\n<log_dir>/<rdzv_run_id>/attempt_<attempt>/<rank>/stderr.log\n<log_dir>/<rdzv_run_id>/attempt_<attempt>/<rank>/error.json\nLogsDest\nFor each log type, holds mapping of local rank ids to file paths.\nDefines logs processing and redirection for each worker process.\nlog_dir(Optional[str]) \u2013 Base directory where logs will be written.\nredirects(Union[Std,dict[int,torch.distributed.elastic.multiprocessing.api.Std]]) \u2013 Streams to redirect to files. Pass a singleStdenum to redirect for all workers, or a mapping keyed\nby local_rank to selectively redirect.\nStd\ntee(Union[Std,dict[int,torch.distributed.elastic.multiprocessing.api.Std]]) \u2013 Streams to duplicate to stdout/stderr.\nPass a singleStdenum to duplicate streams for all workers,\nor a mapping keyed by local_rank to selectively duplicate.\nStd\nGiven the environment variables, builds destination of log files for each of the local ranks.\nEnvs parameter contains env variables dict for each of the local ranks, where entries are defined in:_start_workers().\n_start_workers()\nLogsDest",
  "url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "1329429f2238f9c5069112c5dca0bb06",
  "source": "pytorch_docs",
  "title": "Accelerator Integration \u2014 PyTorch 2.9 documentation",
  "text": "\n## Accelerator Integration#\n\nCreated On: Sep 02, 2025 | Last Updated On: Sep 02, 2025\nSince PyTorch 2.1, the community has made significant progress in streamlining the process of integrating new accelerators into the PyTorch ecosystem. These improvements include, but are not limited to: refinements to thePrivateUse1Dispatch Key, the introduction and enhancement of core subsystem extension mechanisms, and the device-agnostic refactoring of key modules (e.g.,torch.accelerator,memorymanagement). Taken together, these advances provide the foundation for arobust,flexible, anddeveloper-friendlypathway for accelerator integration.\nPrivateUse1\ntorch.accelerator\nmemorymanagement\n\n## Why Does This Matter?#\n\nThis integration pathway offers several major benefits:\nSpeed: Extensibility is built into all core PyTorch modules. Developers can integrate new accelerators into their downstream codebases independently\u2014without modifying upstream code and without being limited by community review bandwidth.\nFuture-proofing: This is the default integration path for all future PyTorch features, meaning that as new modules and features are added, they will automatically support scaling to new accelerators if this path is followed.\nAutonomy: Vendors maintain full control over their accelerator integration timelines, enabling fast iteration cycles and reducing reliance on upstream coordination.\n\n## About This Document#\n\nThis guide aims to provide acomprehensive overview of the modern integration pathwayfor new accelerator in PyTorch. It walks through the full integration surface, from low-level device primitives to higher-level domain modules like compilation and quantization. The structure follows amodular and scenario-driven approach, where each topic is paired with corresponding code examples fromtorch_openreg, an official reference implementation.\nThe goal is to help developers:\nUnderstand the full scope of accelerator integration;\nFollow best practices to quickly launch new accelerators;\nAvoid common pitfalls through clear, targeted examples.\n\n## Target Audience#\n\nThis document is intended for:\nAccelerator Developerswho are integrating accelerator into PyTorch;\nAdvanced PyTorch Usersinterested in the inner workings of key modules;\n\n## Quick Overview#\n\nThis document outlines the key processes and practical scenarios involved in integrating new devices into PyTorch, providing developers with a comprehensive and detailed guide for bringing up new backends. The discussion is structured around four major axes:\nRuntime: Covers core components such as Event, Stream, Memory, Generator, Guard, Hooks, as well as the supporting C++ scaffolding.\nOperators: Involve the minimum necessary set of operators, forward and backward operators, fallback operators, fallthroughs, STUBs, etc. in both C++ and Python implementations.\nPython Frontend: Focuses on Python bindings for modules and device-agnostic APIs.\nHigh-level Modules: Explores integration with major subsystems such asAMP,Compiler,ONNX, andDistributedand so on.\nAMP\nCompiler\nONNX\nDistributed\nNext, we will officially embark on the integration journey for a new PyTorch accelerator.\nNote\nThis guide is a work in progress. For more details, please refer to theroadmap.",
  "url": "https://pytorch.org/docs/stable/accelerator/index.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
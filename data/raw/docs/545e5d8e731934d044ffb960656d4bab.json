{
  "doc_id": "545e5d8e731934d044ffb960656d4bab",
  "source": "pytorch_docs",
  "title": "Named Tensors \u2014 PyTorch 2.9 documentation",
  "text": "\n## Named Tensors#\n\nCreated On: Oct 08, 2019 | Last Updated On: Jun 14, 2025\nNamed Tensors allow users to give explicit names to tensor dimensions.\nIn most cases, operations that take dimension parameters will accept\ndimension names, avoiding the need to track dimensions by position.\nIn addition, named tensors use names to automatically check that APIs\nare being used correctly at runtime, providing extra safety. Names can\nalso be used to rearrange dimensions, for example, to support\n\u201cbroadcasting by name\u201d rather than \u201cbroadcasting by position\u201d.\nWarning\n\n```python\nThe named tensor API is a prototype feature and subject to change.\n\n```\n\n\n## Creating named tensors#\n\nFactory functions now take a newnamesargument that associates a name\nwith each dimension.\nnames\n\n```python\n    >>> torch.zeros(2, 3, names=('N', 'C'))\n    tensor([[0., 0., 0.],\n            [0., 0., 0.]], names=('N', 'C'))\n\n```\n\nNamed dimensions, like regular Tensor dimensions, are ordered.tensor.names[i]is the name of dimensionioftensor.\ntensor.names[i]\ni\ntensor\nThe following factory functions support named tensors:\ntorch.empty()\ntorch.empty()\ntorch.rand()\ntorch.rand()\ntorch.randn()\ntorch.randn()\ntorch.ones()\ntorch.ones()\ntorch.tensor()\ntorch.tensor()\ntorch.zeros()\ntorch.zeros()\n\n## Named dimensions#\n\nSeenamesfor restrictions on tensor names.\nnames\nUsenamesto access the dimension names of a tensor andrename()to rename named dimensions.\nnames\nrename()\n\n```python\n    >>> imgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W'))\n    >>> imgs.names\n    ('N', 'C', 'H', 'W')\n\n    >>> renamed_imgs = imgs.rename(H='height', W='width')\n    >>> renamed_imgs.names\n    ('N', 'C', 'height', 'width)\n\n```\n\nNamed tensors can coexist with unnamed tensors; named tensors are instances oftorch.Tensor. Unnamed tensors haveNone-named dimensions. Named\ntensors do not require all dimensions to be named.\ntorch.Tensor\nNone\n\n```python\n    >>> imgs = torch.randn(1, 2, 2, 3 , names=(None, 'C', 'H', 'W'))\n    >>> imgs.names\n    (None, 'C', 'H', 'W')\n\n```\n\n\n## Name propagation semantics#\n\nNamed tensors use names to automatically check that APIs are being called\ncorrectly at runtime. This occurs in a process calledname inference.\nMore formally, name inference consists of the following two steps:\nCheck names: an operator may perform automatic checks at runtime that\ncheck that certain dimension names must match.\nPropagate names: name inference propagates names to output tensors.\nAll operations that support named tensors propagate names.\n\n```python\n    >>> x = torch.randn(3, 3, names=('N', 'C'))\n    >>> x.abs().names\n    ('N', 'C')\n\n```\n\n\n## match semantics#\n\nTwo namesmatchif they are equal (string equality) or if at least one isNone.\nNones are essentially a special \u201cwildcard\u201d name.\nNone\nunify(A,B)determines which of the namesAandBto propagate to the outputs.\nIt returns the morespecificof the two names, if they match. If the names do not match,\nthen it errors.\nunify(A,B)\nA\nB\nNote\nIn practice, when working with named tensors, one should avoid having unnamed\ndimensions because their handling can be complicated. It is recommended to lift\nall unnamed dimensions to be named dimensions by usingrefine_names().\nrefine_names()\n\n## Basic name inference rules#\n\nLet\u2019s see howmatchandunifyare used in name inference in the case of\nadding two one-dim tensors with no broadcasting.\nmatch\nunify\n\n```python\n    x = torch.randn(3, names=('X',))\n    y = torch.randn(3)\n    z = torch.randn(3, names=('Z',))\n\n```\n\nCheck names: check that the names of the two tensorsmatch.\nFor the following examples:\n\n```python\n    >>> # x + y  # match('X', None) is True\n    >>> # x + z  # match('X', 'Z') is False\n    >>> # x + x  # match('X', 'X') is True\n\n    >>> x + z\n    Error when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match.\n\n```\n\nPropagate names:unifythe names to select which one to propagate.\nIn the case ofx+y,unify('X',None)='X'because'X'is more\nspecific thanNone.\nx+y\nunify('X',None)='X'\n'X'\nNone\n\n```python\n    >>> (x + y).names\n    ('X',)\n    >>> (x + x).names\n    ('X',)\n\n```\n\nFor a comprehensive list of name inference rules, seeNamed Tensors operator coverage.\nHere are two common operations that may be useful to go over:\nBinary arithmetic ops:Unifies names from inputs\nMatrix multiplication ops:Contracts away dims\n\n## Explicit alignment by names#\n\nUsealign_as()oralign_to()to align tensor dimensions\nby name to a specified ordering. This is useful for performing \u201cbroadcasting by names\u201d.\nalign_as()\nalign_to()\n\n```python\n    # This function is agnostic to the dimension ordering of `input`,\n    # as long as it has a `C` dimension somewhere.\n    def scale_channels(input, scale):\n        scale = scale.refine_names('C')\n        return input * scale.align_as(input)\n\n    >>> num_channels = 3\n    >>> scale = torch.randn(num_channels, names=('C',))\n    >>> imgs = torch.rand(3, 3, 3, num_channels, names=('N', 'H', 'W', 'C'))\n    >>> more_imgs = torch.rand(3, num_channels, 3, 3, names=('N', 'C', 'H', 'W'))\n    >>> videos = torch.randn(3, num_channels, 3, 3, 3, names=('N', 'C', 'H', 'W', 'D')\n\n    >>> scale_channels(imgs, scale)\n    >>> scale_channels(more_imgs, scale)\n    >>> scale_channels(videos, scale)\n\n```\n\n\n## Manipulating dimensions#\n\nUsealign_to()to permute large amounts of dimensions without\nmentioning all of them as in required bypermute().\nalign_to()\npermute()\n\n```python\n    >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n    >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n    # Move the F (dim 5) and E dimension (dim 4) to the front while keeping\n    # the rest in the same order\n    >>> tensor.permute(5, 4, 0, 1, 2, 3)\n    >>> named_tensor.align_to('F', 'E', ...)\n\n```\n\nUseflatten()andunflatten()to flatten and unflatten\ndimensions, respectively. These methods are more verbose thanview()andreshape(), but have more semantic meaning to someone reading the code.\nflatten()\nunflatten()\nview()\nreshape()\n\n```python\n    >>> imgs = torch.randn(32, 3, 128, 128)\n    >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n\n    >>> flat_imgs = imgs.view(32, -1)\n    >>> named_flat_imgs = named_imgs.flatten(['C', 'H', 'W'], 'features')\n    >>> named_flat_imgs.names\n    ('N', 'features')\n\n    >>> unflattened_named_imgs = named_flat_imgs.unflatten('features', [('C', 3), ('H', 128), ('W', 128)])\n    >>> unflattened_named_imgs.names\n    ('N', 'C', 'H', 'W')\n\n```\n\n\n## Autograd support#\n\nAutograd currently supports named tensors in a limited manner: autograd ignores\nnames on all tensors. Gradient computation is still correct but we lose the\nsafety that names give us.\n\n```python\n    >>> x = torch.randn(3, names=('D',))\n    >>> weight = torch.randn(3, names=('D',), requires_grad=True)\n    >>> loss = (x - weight).abs()\n    >>> grad_loss = torch.randn(3)\n    >>> loss.backward(grad_loss)\n    >>> weight.grad  # Unnamed for now. Will be named in the future\n    tensor([-1.8107, -0.6357,  0.0783])\n\n    >>> weight.grad.zero_()\n    >>> grad_loss = grad_loss.refine_names('C')\n    >>> loss = (x - weight).abs()\n    # Ideally we'd check that the names of loss and grad_loss match but we don't yet.\n    >>> loss.backward(grad_loss)\n    >>> weight.grad\n    tensor([-1.8107, -0.6357,  0.0783])\n\n```\n\n\n## Currently supported operations and subsystems#\n\n\n## Operators#\n\nSeeNamed Tensors operator coveragefor a full list of the supported torch and\ntensor operations. We do not yet support the following that is not covered by the link:\nindexing, advanced indexing.\nFortorch.nn.functionaloperators, we support the following:\ntorch.nn.functional\ntorch.nn.functional.relu()\ntorch.nn.functional.relu()\ntorch.nn.functional.softmax()\ntorch.nn.functional.softmax()\ntorch.nn.functional.log_softmax()\ntorch.nn.functional.log_softmax()\ntorch.nn.functional.tanh()\ntorch.nn.functional.tanh()\ntorch.nn.functional.sigmoid()\ntorch.nn.functional.sigmoid()\ntorch.nn.functional.dropout()\ntorch.nn.functional.dropout()\n\n## Subsystems#\n\nAutograd is supported, seeAutograd support.\nBecause gradients are currently unnamed, optimizers may work but are untested.\nNN modules are currently unsupported. This can lead to the following when calling\nmodules with named tensor inputs:\nNN module parameters are unnamed, so outputs may be partially named.\nNN module forward passes have code that don\u2019t support named tensors and will\nerror out appropriately.\nWe also do not support the following subsystems, though some may work out\nof the box:\ndistributions\nserialization (torch.load(),torch.save())\ntorch.load()\ntorch.save()\nmultiprocessing\nJIT\ndistributed\nONNX\nIf any of these would help your use case, pleasesearch if an issue has already been filedand if not,file one.\n\n## Named tensor API reference#\n\nIn this section please find the documentation for named tensor specific APIs.\nFor a comprehensive reference for how names are propagated through other PyTorch\noperators, seeNamed Tensors operator coverage.\nStores names for each of this tensor\u2019s dimensions.\nnames[idx]corresponds to the name of tensor dimensionidx.\nNames are either a string if the dimension is named orNoneif the\ndimension is unnamed.\nnames[idx]\nidx\nNone\nDimension names may contain characters or underscore. Furthermore, a dimension\nname must be a valid Python variable name (i.e., does not start with underscore).\nTensors may not have two named dimensions with the same name.\nWarning\nThe named tensor API is experimental and subject to change.\nRenames dimension names ofself.\nself\nThere are two main usages:\nself.rename(**rename_map)returns a view on tensor that has dims\nrenamed as specified in the mappingrename_map.\nself.rename(**rename_map)\nrename_map\nself.rename(*names)returns a view on tensor, renaming all\ndimensions positionally usingnames.\nUseself.rename(None)to drop names on a tensor.\nself.rename(*names)\nnames\nself.rename(None)\nOne cannot specify both positional argsnamesand keyword argsrename_map.\nnames\nrename_map\nExamples:\n\n```python\n>>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n>>> renamed_imgs = imgs.rename(N='batch', C='channels')\n>>> renamed_imgs.names\n('batch', 'channels', 'H', 'W')\n\n>>> renamed_imgs = imgs.rename(None)\n>>> renamed_imgs.names\n(None, None, None, None)\n\n>>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n>>> renamed_imgs.names\n('batch', 'channel', 'height', 'width')\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.\nIn-place version ofrename().\nrename()\nRefines the dimension names ofselfaccording tonames.\nself\nnames\nRefining is a special case of renaming that \u201clifts\u201d unnamed dimensions.\nANonedim can be refined to have any name; a named dim can only be\nrefined to have the same name.\nNone\nBecause named tensors can coexist with unnamed tensors, refining names\ngives a nice way to write named-tensor-aware code that works with both\nnamed and unnamed tensors.\nnamesmay contain up to one Ellipsis (...).\nThe Ellipsis is expanded greedily; it is expanded in-place to fillnamesto the same length asself.dim()using names from the\ncorresponding indices ofself.names.\nnames\n...\nnames\nself.dim()\nself.names\nPython 2 does not support Ellipsis but one may use a string literal\ninstead ('...').\n'...'\nnames(iterableofstr) \u2013 The desired names of the output tensor. May\ncontain up to one Ellipsis.\nExamples:\n\n```python\n>>> imgs = torch.randn(32, 3, 128, 128)\n>>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n>>> named_imgs.names\n('N', 'C', 'H', 'W')\n\n>>> tensor = torch.randn(2, 3, 5, 7, 11)\n>>> tensor = tensor.refine_names('A', ..., 'B', 'C')\n>>> tensor.names\n('A', None, None, 'B', 'C')\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.\nPermutes the dimensions of theselftensor to match the dimension order\nin theothertensor, adding size-one dims for any new names.\nself\nother\nThis operation is useful for explicit broadcasting by names (see examples).\nAll of the dims ofselfmust be named in order to use this method.\nThe resulting tensor is a view on the original tensor.\nself\nAll dimension names ofselfmust be present inother.names.othermay contain named dimensions that are not inself.names;\nthe output tensor has a size-one dimension for each of those new names.\nself\nother.names\nother\nself.names\nTo align a tensor to a specific order, usealign_to().\nalign_to()\nExamples:\n\n```python\n# Example 1: Applying a mask\n>>> mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')\n>>> imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))\n>>> imgs.masked_fill_(mask.align_as(imgs), 0)\n\n\n# Example 2: Applying a per-channel-scale\n>>> def scale_channels(input, scale):\n>>>    scale = scale.refine_names('C')\n>>>    return input * scale.align_as(input)\n\n>>> num_channels = 3\n>>> scale = torch.randn(num_channels, names=('C',))\n>>> imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))\n>>> more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))\n>>> videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))\n\n# scale_channels is agnostic to the dimension order of the input\n>>> scale_channels(imgs, scale)\n>>> scale_channels(more_imgs, scale)\n>>> scale_channels(videos, scale)\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.\nPermutes the dimensions of theselftensor to match the order\nspecified innames, adding size-one dims for any new names.\nself\nnames\nAll of the dims ofselfmust be named in order to use this method.\nThe resulting tensor is a view on the original tensor.\nself\nAll dimension names ofselfmust be present innames.namesmay contain additional names that are not inself.names;\nthe output tensor has a size-one dimension for each of those new names.\nself\nnames\nnames\nself.names\nnamesmay contain up to one Ellipsis (...).\nThe Ellipsis is expanded to be equal to all dimension names ofselfthat are not mentioned innames, in the order that they appear\ninself.\nnames\n...\nself\nnames\nself\nPython 2 does not support Ellipsis but one may use a string literal\ninstead ('...').\n'...'\nnames(iterableofstr) \u2013 The desired dimension ordering of the\noutput tensor. May contain up to one Ellipsis that is expanded\nto all unmentioned dim names ofself.\nself\nExamples:\n\n```python\n>>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n>>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n# Move the F and E dims to the front while keeping the rest in order\n>>> named_tensor.align_to('F', 'E', ...)\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.\nFlattensdimsinto a single dimension with nameout_dim.\ndims\nout_dim\nAll ofdimsmust be consecutive in order in theselftensor,\nbut not necessary contiguous in memory.\nself\nExamples:\n\n```python\n>>> imgs = torch.randn(32, 3, 128, 128, names=('N', 'C', 'H', 'W'))\n>>> flat_imgs = imgs.flatten(['C', 'H', 'W'], 'features')\n>>> flat_imgs.names, flat_imgs.shape\n(('N', 'features'), torch.Size([32, 49152]))\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.",
  "url": "https://pytorch.org/docs/stable/named_tensor.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
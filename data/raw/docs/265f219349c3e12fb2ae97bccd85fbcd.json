{
  "doc_id": "265f219349c3e12fb2ae97bccd85fbcd",
  "source": "pytorch_docs",
  "title": "Distributed Optimizers \u2014 PyTorch 2.9 documentation",
  "text": "\n## Distributed Optimizers#\n\nCreated On: Mar 01, 2021 | Last Updated On: Jun 16, 2025\nWarning\nDistributed optimizer is not currently supported when using CUDA tensors\ntorch.distributed.optimexposes DistributedOptimizer, which takes a list\nof remote parameters (RRef) and runs the\noptimizer locally on the workers where the parameters live.  The distributed\noptimizer can use any of the local optimizerBase classto\napply the gradients on each worker.\ntorch.distributed.optim\nRRef\nDistributedOptimizer takes remote references to parameters scattered\nacross workers and applies the given optimizer locally for each parameter.\nThis class usesget_gradients()in order\nto retrieve the gradients for specific parameters.\nget_gradients()\nConcurrent calls tostep(),\neither from the same or different clients, will\nbe serialized on each worker \u2013 as each worker\u2019s optimizer can only work\non one set of gradients at a time. However, there is no guarantee that\nthe full forward-backward-optimizer sequence will execute for one client\nat a time. This means that the gradients being applied may not correspond\nto the latest forward pass executed on a given worker. Also, there is no\nguaranteed ordering across workers.\nstep()\nDistributedOptimizercreates the local optimizer with TorchScript enabled\nby default, so that optimizer updates are not blocked by the Python Global\nInterpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed\nModel Parallel). This feature is currently enabled for most optimizers. You\ncan also followthe recipein PyTorch tutorials to enable TorchScript support\nfor your own custom optimizers.\noptimizer_class(optim.Optimizer) \u2013 the class of optimizer to\ninstantiate on each worker.\nparams_rref(list[RRef]) \u2013 list of RRefs to local or remote parameters\nto optimize.\nargs\u2013 arguments to pass to the optimizer constructor on each worker.\nkwargs\u2013 arguments to pass to the optimizer constructor on each worker.\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> import torch.distributed.rpc as rpc\n>>> from torch import optim\n>>> from torch.distributed.optim import DistributedOptimizer\n>>>\n>>> with dist_autograd.context() as context_id:\n>>>   # Forward pass.\n>>>   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>>   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>>   loss = rref1.to_here() + rref2.to_here()\n>>>\n>>>   # Backward pass.\n>>>   dist_autograd.backward(context_id, [loss.sum()])\n>>>\n>>>   # Optimizer.\n>>>   dist_optim = DistributedOptimizer(\n>>>      optim.SGD,\n>>>      [rref1, rref2],\n>>>      lr=0.05,\n>>>   )\n>>>   dist_optim.step(context_id)\n\n```\n\nPerforms a single optimization step.\nThis will calltorch.optim.Optimizer.step()on each worker\ncontaining parameters to be optimized, and will block until all workers\nreturn. The providedcontext_idwill be used to retrieve the\ncorrespondingcontextthat\ncontains the gradients that should be applied to the parameters.\ntorch.optim.Optimizer.step()\ncontext_id\ncontext\ncontext_id\u2013 the autograd context id for which we should run the\noptimizer step.\nWraps an arbitrarytorch.optim.Optimizerand runspost-local SGD,\nThis optimizer runs local optimizer at every step.\nAfter the warm-up stage, it averages parameters periodically after the local optimizer is applied.\ntorch.optim.Optimizer\noptim(Optimizer) \u2013 The local optimizer.\naverager(ModelAverager) \u2013 A model averager instance to run post-localSGD algorithm.\nExample:\n\n```python\n>>> import torch\n>>> import torch.distributed as dist\n>>> import torch.distributed.algorithms.model_averaging.averagers as averagers\n>>> import torch.nn as nn\n>>> from torch.distributed.optim import PostLocalSGDOptimizer\n>>> from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (\n>>>   PostLocalSGDState,\n>>>   post_localSGD_hook,\n>>> )\n>>>\n>>> model = nn.parallel.DistributedDataParallel(\n>>>    module, device_ids=[rank], output_device=rank\n>>> )\n>>>\n>>> # Register a post-localSGD communication hook.\n>>> state = PostLocalSGDState(process_group=None, subgroup=None, start_localSGD_iter=100)\n>>> model.register_comm_hook(state, post_localSGD_hook)\n>>>\n>>> # Create a post-localSGD optimizer that wraps a local optimizer.\n>>> # Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as\n>>> # ``start_localSGD_iter`` used in ``PostLocalSGDState``.\n>>> local_optim = torch.optim.SGD(params=model.parameters(), lr=0.01)\n>>> opt = PostLocalSGDOptimizer(\n>>>     optim=local_optim,\n>>>     averager=averagers.PeriodicModelAverager(period=4, warmup_steps=100)\n>>> )\n>>>\n>>> # In the first 100 steps, DDP runs global gradient averaging at every step.\n>>> # After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),\n>>> # and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.\n>>> for step in range(0, 200):\n>>>    opt.zero_grad()\n>>>    loss = loss_fn(output, labels)\n>>>    loss.backward()\n>>>    opt.step()\n\n```\n\nThis is the same astorch.optim.Optimizerload_state_dict(),\nbut also restores model averager\u2019s step value to the one\nsaved in the providedstate_dict.\ntorch.optim.Optimizer\nload_state_dict()\nstate_dict\nIf there is no\"step\"entry instate_dict,\nit will raise a warning and initialize the model averager\u2019s step to 0.\n\"step\"\nstate_dict\nThis is the same astorch.optim.Optimizerstate_dict(),\nbut adds an extra entry to record model averager\u2019s step to the checkpoint\nto ensure reload does not cause unnecessary warm up again.\ntorch.optim.Optimizer\nstate_dict()\nPerforms a single optimization step (parameter update).\nWrap an arbitraryoptim.Optimizerand shards its states across ranks in the group.\noptim.Optimizer\nThe sharing is done as described byZeRO.\nThe local optimizer instance in each rank is only\nresponsible for updating approximately1/world_sizeparameters and\nhence only needs to keep1/world_sizeoptimizer states. After\nparameters are updated locally, each rank will broadcast its parameters to\nall other peers to keep all model replicas in the same state.ZeroRedundancyOptimizercan be used in conjunction withtorch.nn.parallel.DistributedDataParallelto reduce per-rank peak\nmemory consumption.\n1/world_size\n1/world_size\nZeroRedundancyOptimizer\ntorch.nn.parallel.DistributedDataParallel\nZeroRedundancyOptimizeruses a sorted-greedy algorithm to pack a number\nof parameters at each rank. Each parameter belongs to a single rank and is\nnot divided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order.\nZeroRedundancyOptimizer\nparams(Iterable) \u2013 anIterableoftorch.Tensors\nordicts giving all parameters, which will be sharded\nacross ranks.\nIterable\nIterable\ntorch.Tensor\ndict\noptimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer.\ntorch.nn.Optimizer\nprocess_group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:dist.group.WORLDinitialized bytorch.distributed.init_process_group()).\nProcessGroup\ntorch.distributed\nProcessGroup\ndist.group.WORLD\ntorch.distributed.init_process_group()\nparameters_as_bucket_view(bool,optional) \u2013 ifTrue, parameters are\npacked into buckets to speed up communication, andparam.datafields point to bucket views at different offsets; ifFalse,\neach individual parameter is communicated separately, and eachparams.datastays intact (default:False).\nTrue\nparam.data\nFalse\nparams.data\nFalse\noverlap_with_ddp(bool,optional) \u2013 ifTrue,step()is\noverlapped withDistributedDataParallel\u2018s gradient\nsynchronization; this requires (1) either a functional optimizer\nfor theoptimizer_classargument or one with a functional\nequivalent and (2) registering a DDP communication hook\nconstructed from one of the functions inddp_zero_hook.py;\nparameters are packed into buckets matching those inDistributedDataParallel, meaning that theparameters_as_bucket_viewargument is ignored.\nIfFalse,step()runs disjointly after the backward pass\n(per normal).\n(default:False)\nTrue\nstep()\nDistributedDataParallel\noptimizer_class\nddp_zero_hook.py\nDistributedDataParallel\nparameters_as_bucket_view\nFalse\nstep()\nFalse\n**defaults\u2013 any trailing arguments, which are forwarded to the local\noptimizer.\nExample:\n\n```python\n>>> import torch.nn as nn\n>>> from torch.distributed.optim import ZeroRedundancyOptimizer\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n>>> ddp = DDP(model, device_ids=[rank])\n>>> opt = ZeroRedundancyOptimizer(\n>>>     ddp.parameters(),\n>>>     optimizer_class=torch.optim.Adam,\n>>>     lr=0.01\n>>> )\n>>> ddp(inputs).sum().backward()\n>>> opt.step()\n\n```\n\nWarning\nCurrently,ZeroRedundancyOptimizerrequires that all of the\npassed-in parameters are the same dense type.\nZeroRedundancyOptimizer\nWarning\nIf you passoverlap_with_ddp=True, be wary of the following: Given\nthe way that overlappingDistributedDataParallelwithZeroRedundancyOptimizeris currently implemented, the first\ntwo or three training iterations do not perform parameter updates in\nthe optimizer step, depending on ifstatic_graph=Falseorstatic_graph=True, respectively. This is because it needs\ninformation about the gradient bucketing strategy used byDistributedDataParallel, which is not finalized until the\nsecond forward pass ifstatic_graph=Falseor until the third\nforward pass ifstatic_graph=True. To adjust for this, one option\nis to prepend dummy inputs.\noverlap_with_ddp=True\nDistributedDataParallel\nZeroRedundancyOptimizer\nstatic_graph=False\nstatic_graph=True\nDistributedDataParallel\nstatic_graph=False\nstatic_graph=True\nWarning\nZeroRedundancyOptimizer is experimental and subject to change.\nAdd a parameter group to theOptimizer\u2018sparam_groups.\nOptimizer\nparam_groups\nThis can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses.\nOptimizer\nparam_group(dict) \u2013 specifies the parameters to be optimized and\ngroup-specific optimization options.\nWarning\nThis method handles updating the shards on all partitions\nbut needs to be called on all ranks. Calling this on a subset of\nthe ranks will cause the training to hang because communication\nprimitives are called depending on the managed parameters and\nexpect all the ranks to participate on the same set of parameters.\nConsolidate a list ofstate_dicts (one per rank) on the target rank.\nstate_dict\nto(int) \u2013 the rank that receives the optimizer states (default: 0).\nRuntimeError\u2013 ifoverlap_with_ddp=Trueand this method is\n    called before thisZeroRedundancyOptimizerinstance\n    has been fully initialized, which happens onceDistributedDataParallelgradient buckets have been\n    rebuilt.\noverlap_with_ddp=True\nZeroRedundancyOptimizer\nDistributedDataParallel\nWarning\nThis needs to be called on all ranks.\nReturn default device.\nReturn the ZeRO join hook.\nIt enables training on uneven inputs by\nshadowing the collective communications in the optimizer step.\nGradients must be properly set before this hook is called.\nkwargs(dict) \u2013 adictcontaining any keyword arguments\nto modify the behavior of the join hook at run time; allJoinableinstances sharing the same join context\nmanager are forwarded the same value forkwargs.\ndict\nJoinable\nkwargs\nThis hook does not support any keyword arguments; i.e.kwargsis\nunused.\nkwargs\nReturn process group.\nLoad the state pertaining to the given rank from the inputstate_dict, updating the local optimizer as needed.\nstate_dict\nstate_dict(dict) \u2013 optimizer state; should be an object returned\nfrom a call tostate_dict().\nstate_dict()\nRuntimeError\u2013 ifoverlap_with_ddp=Trueand this method is\n    called before thisZeroRedundancyOptimizerinstance\n    has been fully initialized, which happens onceDistributedDataParallelgradient buckets have been\n    rebuilt.\noverlap_with_ddp=True\nZeroRedundancyOptimizer\nDistributedDataParallel\nReturn the last global optimizer state known to this rank.\nRuntimeError\u2013 ifoverlap_with_ddp=Trueand this method is\n    called before thisZeroRedundancyOptimizerinstance\n    has been fully initialized, which happens onceDistributedDataParallelgradient buckets have been\n    rebuilt; or if this method is called without a preceding call\n    toconsolidate_state_dict().\noverlap_with_ddp=True\nZeroRedundancyOptimizer\nDistributedDataParallel\nconsolidate_state_dict()\ndict[str,Any]\nPerform a single optimizer step and syncs parameters across all ranks.\nclosure(Callable) \u2013 a closure that re-evaluates the model and\nreturns the loss; optional for most optimizers.\nOptional loss depending on the underlying local optimizer.\nOptional[float]\nNote\nAny extra parameters are passed to the base optimizer as-is.",
  "url": "https://pytorch.org/docs/stable/distributed.optim.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "d226b8a7284bc31c54f0a0bc78311f59",
  "source": "pytorch_docs",
  "title": "Generic Join Context Manager \u2014 PyTorch 2.9 documentation",
  "text": "\n## Generic Join Context Manager#\n\nCreated On: Jun 06, 2025 | Last Updated On: Jun 06, 2025\nThe generic join context manager facilitates distributed training on uneven\ninputs. This page outlines the API of the relevant classes:Join,Joinable, andJoinHook. For a tutorial, seeDistributed Training with Uneven Inputs Using the Join Context Manager.\nJoin\nJoinable\nJoinHook\nThis class defines the generic join context manager, which allows custom hooks to be called after a process joins.\nThese hooks should shadow the\ncollective communications of non-joined processes to prevent hanging and\nerroring and to ensure algorithmic correctness. Refer toJoinHookfor details about the hook definition.\nJoinHook\nWarning\nThe context manager requires each participatingJoinableto\ncall the methodnotify_join_context()before its own per-\niteration collective communications to ensure correctness.\nJoinable\nnotify_join_context()\nWarning\nThe context manager requires that allprocess_groupattributes in\ntheJoinHookobjects are the same. If there are multipleJoinHookobjects, then thedeviceof the first is used.\nThe process group and device information is used for checking for non-\njoined processes and for notifying processes to throw an exception ifthrow_on_early_terminationis enabled, both of which using an all-\nreduce.\nprocess_group\nJoinHook\nJoinHook\ndevice\nthrow_on_early_termination\njoinables(List[Joinable]) \u2013 a list of the participatingJoinables; their hooks are iterated over in the given\norder.\nJoinable\nenable(bool) \u2013 a flag enabling uneven input detection; setting toFalsedisables the context manager\u2019s functionality and should\nonly be set when the user knows the inputs will not be uneven\n(default:True).\nFalse\nTrue\nthrow_on_early_termination(bool) \u2013 a flag controlling whether to throw an\nexception upon detecting uneven inputs (default:False).\nFalse\nExample:\n\n```python\n>>> import os\n>>> import torch\n>>> import torch.distributed as dist\n>>> import torch.multiprocessing as mp\n>>> import torch.nn.parallel.DistributedDataParallel as DDP\n>>> import torch.distributed.optim.ZeroRedundancyOptimizer as ZeRO\n>>> from torch.distributed.algorithms.join import Join\n>>>\n>>> # On each spawned worker\n>>> def worker(rank):\n>>>     dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n>>>     model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n>>>     optim = ZeRO(model.parameters(), torch.optim.Adam, lr=0.01)\n>>>     # Rank 1 gets one more input than rank 0\n>>>     inputs = [torch.tensor([1.]).to(rank) for _ in range(10 + rank)]\n>>>     with Join([model, optim]):\n>>>         for input in inputs:\n>>>             loss = model(input).sum()\n>>>             loss.backward()\n>>>             optim.step()\n>>>     # All ranks reach here without hanging/erroring\n\n```\n\nNotifies the join context manager that the calling process has not yet joined.\nThen, ifthrow_on_early_termination=True, checks if uneven inputs have been detected\n(i.e. if one process has already joined) and throws an exception if so.\nthrow_on_early_termination=True\nThis method should be called from aJoinableobject before\nits per-iteration collective communications. For example, this should\nbe called at the beginning of the forward pass inDistributedDataParallel.\nJoinable\nDistributedDataParallel\nOnly the firstJoinableobject passed into the context\nmanager performs the collective communications in this method, and\nfor the others, this method is vacuous.\nJoinable\njoinable(Joinable) \u2013 theJoinableobject calling this\nmethod.\nJoinable\nAn async work handle for the all-reduce meant to notify the context\nmanager that the process has not yet joined ifjoinableis the\nfirst one passed into the context manager;Noneotherwise.\njoinable\nNone\nThis defines an abstract base class for joinable classes.\nA joinable class\n(inheriting fromJoinable) should implementjoin_hook(),\nwhich returns aJoinHookinstance, in addition tojoin_device()andjoin_process_group()that return device and\nprocess group information, respectively.\nJoinable\njoin_hook()\nJoinHook\njoin_device()\njoin_process_group()\nReturn the device from which to perform collective communications needed by the join context manager.\nReturn aJoinHookinstance for the givenJoinable.\nJoinHook\nJoinable\nkwargs(dict) \u2013 adictcontaining any keyword arguments\nto modify the behavior of the join hook at run time; allJoinableinstances sharing the same join context\nmanager are forwarded the same value forkwargs.\ndict\nJoinable\nkwargs\nJoinHook\nReturns the process group for the collective communications needed by the join context manager itself.\nThis defines a join hook, which provides two entry points in the join context manager.\nEntry points : a main hook, which is called repeatedly while there exists a non-joined\nprocess, and a post-hook, which is called once all processes have joined.\nTo implement a join hook for the generic join context manager, define a\nclass that inherits fromJoinHookand overridemain_hook()andpost_hook()as appropriate.\nJoinHook\nmain_hook()\npost_hook()\nCall this hook while there exists a non-joined process to shadow collective communications in a training iteration.\nTraining iteration i.e., in one forward pass, backward pass, and optimizer step.\nCall hook after all processes have joined.\nIt is passed an additionalboolargumentis_last_joiner, which indicates if the rank is one of the last to join.\nbool\nis_last_joiner\nis_last_joiner(bool) \u2013Trueif the rank is one of the last to\njoin;Falseotherwise.\nTrue\nFalse",
  "url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
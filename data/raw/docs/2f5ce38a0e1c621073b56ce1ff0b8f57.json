{
  "doc_id": "2f5ce38a0e1c621073b56ce1ff0b8f57",
  "source": "pytorch_docs",
  "title": "Migrating from functorch to torch.func \u2014 PyTorch 2.9 documentation",
  "text": "\n## Migrating from functorch to torch.func#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\ntorch.func, previously known as \u201cfunctorch\u201d, isJAX-likecomposable function transforms for PyTorch.\nfunctorch started as an out-of-tree library over at\nthepytorch/functorchrepository.\nOur goal has always been to upstream functorch directly into PyTorch and provide\nit as a core PyTorch library.\nAs the final step of the upstream, we\u2019ve decided to migrate from being a top level package\n(functorch) to being a part of PyTorch to reflect how the function transforms are\nintegrated directly into PyTorch core. As of PyTorch 2.0, we are deprecatingimportfunctorchand ask that users migrate to the newest APIs, which we\nwill maintain going forward.importfunctorchwill be kept around to maintain\nbackwards compatibility for a couple of releases.\nfunctorch\nimportfunctorch\nimportfunctorch\n\n## function transforms#\n\nThe following APIs are a drop-in replacement for the followingfunctorch APIs.\nThey are fully backwards compatible.\nfunctorch API\nPyTorch API (as of PyTorch 2.0)\nfunctorch.vmap\ntorch.vmap()ortorch.func.vmap()\ntorch.vmap()\ntorch.func.vmap()\nfunctorch.grad\ntorch.func.grad()\ntorch.func.grad()\nfunctorch.vjp\ntorch.func.vjp()\ntorch.func.vjp()\nfunctorch.jvp\ntorch.func.jvp()\ntorch.func.jvp()\nfunctorch.jacrev\ntorch.func.jacrev()\ntorch.func.jacrev()\nfunctorch.jacfwd\ntorch.func.jacfwd()\ntorch.func.jacfwd()\nfunctorch.hessian\ntorch.func.hessian()\ntorch.func.hessian()\nfunctorch.functionalize\ntorch.func.functionalize()\ntorch.func.functionalize()\nFurthermore, if you are using torch.autograd.functional APIs, please try out\nthetorch.funcequivalents instead.torch.funcfunction\ntransforms are more composable and more performant in many cases.\ntorch.func\ntorch.func\ntorch.autograd.functional API\ntorch.func API (as of PyTorch 2.0)\ntorch.autograd.functional.vjp()\ntorch.autograd.functional.vjp()\ntorch.func.grad()ortorch.func.vjp()\ntorch.func.grad()\ntorch.func.vjp()\ntorch.autograd.functional.jvp()\ntorch.autograd.functional.jvp()\ntorch.func.jvp()\ntorch.func.jvp()\ntorch.autograd.functional.jacobian()\ntorch.autograd.functional.jacobian()\ntorch.func.jacrev()ortorch.func.jacfwd()\ntorch.func.jacrev()\ntorch.func.jacfwd()\ntorch.autograd.functional.hessian()\ntorch.autograd.functional.hessian()\ntorch.func.hessian()\ntorch.func.hessian()\n\n## NN module utilities#\n\nWe\u2019ve changed the APIs to apply function transforms over NN modules to make them\nfit better into the PyTorch design philosophy. The new API is different, so\nplease read this section carefully.\n\n## functorch.make_functional#\n\ntorch.func.functional_call()is the replacement forfunctorch.make_functionalandfunctorch.make_functional_with_buffers.\nHowever, it is not a drop-in replacement.\ntorch.func.functional_call()\nIf you\u2019re in a hurry, you can usehelper functions in this gistthat emulate the behavior of functorch.make_functional and functorch.make_functional_with_buffers.\nWe recommend usingtorch.func.functional_call()directly because it is a more explicit\nand flexible API.\ntorch.func.functional_call()\nConcretely, functorch.make_functional returns a functional module and parameters.\nThe functional module accepts parameters and inputs to the model as arguments.torch.func.functional_call()allows one to call the forward pass of an existing\nmodule using new parameters and buffers and inputs.\ntorch.func.functional_call()\nHere\u2019s an example of how to compute gradients of parameters of a model using functorch\nvstorch.func:\ntorch.func\n\n```python\n# ---------------\n# using functorch\n# ---------------\nimport torch\nimport functorch\ninputs = torch.randn(64, 3)\ntargets = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nfmodel, params = functorch.make_functional(model)\n\ndef compute_loss(params, inputs, targets):\n    prediction = fmodel(params, inputs)\n    return torch.nn.functional.mse_loss(prediction, targets)\n\ngrads = functorch.grad(compute_loss)(params, inputs, targets)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport torch\ninputs = torch.randn(64, 3)\ntargets = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nparams = dict(model.named_parameters())\n\ndef compute_loss(params, inputs, targets):\n    prediction = torch.func.functional_call(model, params, (inputs,))\n    return torch.nn.functional.mse_loss(prediction, targets)\n\ngrads = torch.func.grad(compute_loss)(params, inputs, targets)\n\n```\n\nAnd here\u2019s an example of how to compute jacobians of model parameters:\n\n```python\n# ---------------\n# using functorch\n# ---------------\nimport torch\nimport functorch\ninputs = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nfmodel, params = functorch.make_functional(model)\njacobians = functorch.jacrev(fmodel)(params, inputs)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport torch\nfrom torch.func import jacrev, functional_call\ninputs = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nparams = dict(model.named_parameters())\n# jacrev computes jacobians of argnums=0 by default.\n# We set it to 1 to compute jacobians of params\njacobians = jacrev(functional_call, argnums=1)(model, params, (inputs,))\n\n```\n\nNote that it is important for memory consumption that you should only carry\naround a single copy of your parameters.model.named_parameters()does not copy\nthe parameters. If in your model training you update the parameters of the model\nin-place, then thenn.Modulethat is your model has the single copy of the\nparameters and everything is OK.\nmodel.named_parameters()\nnn.Module\nHowever, if you want to carry your parameters around in a dictionary and update\nthem out-of-place, then there are two copies of parameters: the one in the\ndictionary and the one in themodel. In this case, you should changemodelto not hold memory by converting it to the meta device viamodel.to('meta').\nmodel\nmodel\nmodel.to('meta')\n\n## functorch.combine_state_for_ensemble#\n\nPlease usetorch.func.stack_module_state()instead offunctorch.combine_state_for_ensembletorch.func.stack_module_state()returns two dictionaries, one of stacked parameters, and\none of stacked buffers, that can then be used withtorch.vmap()andtorch.func.functional_call()for ensembling.\ntorch.func.stack_module_state()\ntorch.func.stack_module_state()\ntorch.vmap()\ntorch.func.functional_call()\nFor example, here is an example of how to ensemble over a very simple model:\n\n```python\nimport torch\nnum_models = 5\nbatch_size = 64\nin_features, out_features = 3, 3\nmodels = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]\ndata = torch.randn(batch_size, 3)\n\n# ---------------\n# using functorch\n# ---------------\nimport functorch\nfmodel, params, buffers = functorch.combine_state_for_ensemble(models)\noutput = functorch.vmap(fmodel, (0, 0, None))(params, buffers, data)\nassert output.shape == (num_models, batch_size, out_features)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport copy\n\n# Construct a version of the model with no memory by putting the Tensors on\n# the meta device.\nbase_model = copy.deepcopy(models[0])\nbase_model.to('meta')\n\nparams, buffers = torch.func.stack_module_state(models)\n\n# It is possible to vmap directly over torch.func.functional_call,\n# but wrapping it in a function makes it clearer what is going on.\ndef call_single_model(params, buffers, data):\n    return torch.func.functional_call(base_model, (params, buffers), (data,))\n\noutput = torch.vmap(call_single_model, (0, 0, None))(params, buffers, data)\nassert output.shape == (num_models, batch_size, out_features)\n\n```\n\n\n## functorch.compile#\n\nWe are no longer supporting functorch.compile (also known as AOTAutograd)\nas a frontend for compilation in PyTorch; we have integrated AOTAutograd\ninto PyTorch\u2019s compilation story. If you are a user, please usetorch.compile()instead.\ntorch.compile()",
  "url": "https://pytorch.org/docs/stable/func.migrating.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
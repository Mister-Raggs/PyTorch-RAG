{
  "doc_id": "501f17aa69d4b9299a39518c0502397f",
  "source": "pytorch_docs",
  "title": "Extending torch.func with autograd.Function \u2014 PyTorch 2.9 documentation",
  "text": "\n## Extending torch.func with autograd.Function#\n\nCreated On: Jan 03, 2023 | Last Updated On: Sep 14, 2023\nSo you\u2019d like to usetorch.autograd.Functionwith thetorch.functransforms liketorch.vmap(),torch.func.grad(), etc.\ntorch.autograd.Function\ntorch.func\ntorch.vmap()\ntorch.func.grad()\nThere are two main use cases:\nyou wish to call code that does not contain PyTorch operations and\nhave it work with function transforms. That is, thetorch.autograd.Function\u2019s\nforward/backward/etc calls into functions from other systems like C++, CUDA, numpy.\ntorch.autograd.Function\nyou wish to specify custom gradient rules, like\nJAX\u2019scustom_vjp/custom_jvp\nPyTorch combines both of these concepts intotorch.autograd.Function.\ntorch.autograd.Function\n\n## Basic Usage#\n\nThis guide assumes you are familiar withExtending torch.autograd,\nwhich explains how to usetorch.autograd.Function.\ntorch.autograd.Function\ntorch.autograd.Functioncan either have aforward()that accepts a ctx object,\nor it can have separateforward()(that does not acceptctx) and asetup_context()staticmethod that modifies thectxobject.\ntorch.autograd.Function\nforward()\nforward()\nctx\nsetup_context()\nctx\nOnly the latter is supported with function transforms:\nforward()is the code that performs the operation and it should not accept\nactxobject.\nforward()\nctx\nsetup_context(ctx,inputs,output)is the code where you can\ncall methods onctx. Here is where you should save Tensors for backward\n(by callingctx.save_for_backward(*tensors)), or save non-Tensors\n(by assigning them to thectxobject).\nsetup_context(ctx,inputs,output)\nctx\nctx.save_for_backward(*tensors)\nctx\nBecausesetup_context()accepts onlyinputsandoutput,\nthe only quantities that can be saved are either objects (such as Tensors) in\nthe inputs or outputs or quantities (likeTensor.shape) derived from them.\nIf you wish to save a non-input intermediate activation fromFunction.forward()for backward, then you\u2019ll need to return it as an\noutput fromforward()so that it gets passed tosetup_context().\nsetup_context()\ninputs\noutput\nTensor.shape\nFunction.forward()\nforward()\nsetup_context()\nDepending on the transform,\nto support reverse-mode AD (torch.func.grad(),torch.func.vjp()),\nthetorch.autograd.Functionneeds abackward()staticmethod.\ntorch.func.grad()\ntorch.func.vjp()\ntorch.autograd.Function\nbackward()\nto supporttorch.vmap(), thetorch.autograd.Functionneeds avmap()staticmethod.\ntorch.vmap()\ntorch.autograd.Function\nvmap()\nto supporttorch.func.jvp(), thetorch.autograd.Functionneeds ajvp()staticmethod.\ntorch.func.jvp()\ntorch.autograd.Function\njvp()\nto support compositions of transforms (liketorch.func.jacrev(),torch.func.jacfwd(),torch.func.hessian()) \u2013 you may need multiple\nof the above.\ntorch.func.jacrev()\ntorch.func.jacfwd()\ntorch.func.hessian()\nIn order for thetorch.autograd.Functionto be arbitrarily composable with function\ntransforms, we recommend that all other staticmethods other thanforward()andsetup_context()must be transformable: that is, they must consist of only PyTorch\noperators or call othertorch.autograd.Function(that may call into C++/CUDA/etc).\ntorch.autograd.Function\nforward()\nsetup_context()\ntorch.autograd.Function\nLet\u2019s go over some examples of common use cases.\n\n## Example 1: autograd.Function calls into another system#\n\nA common case is atorch.autograd.Functionwith both forward() and backward() calling\ninto another system (like C++, CUDA, numpy, triton).\ntorch.autograd.Function\n\n```python\nimport torch\nimport numpy as np\n\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    # Note that forward does not take ctx\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        # Any intermediates to be saved in backward must be returned as\n        # outputs.\n        return (\n            # The desired output\n            torch.tensor(result, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind_inv, device=device),\n        )\n\n    # setup_context is responsible for calling methods and/or assigning to\n    # the ctx object. Please do not do additional compute (e.g. add\n    # Tensors together) in setup_context.\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        # Note that output is whatever you returned from forward.\n        # If you returned multiple values, then output is a Tuple of multiple values.\n        # If you returned a single Tensor, then output is a Tensor.\n        # If you returned a Tuple with a single Tensor, then output is a\n        # Tuple with a single Tensor.\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        # Tensors must be saved via ctx.save_for_backward. Please do not\n        # assign them directly onto the ctx object.\n        ctx.save_for_backward(ind, ind_inv)\n        # Non-tensors may be saved by assigning them as attributes on the ctx object.\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        # For the autograd.Function to be arbitrarily composable with function\n        # transforms, all staticmethod other than forward and setup_context\n        # must be implemented in a \"transformable\" way; that is, they must\n        # only consist of PyTorch operations or autograd.Function.\n        #\n        # For example, this allows us to do double backwards and/or compute\n        # second order gradients.\n        #\n        # We've written the backward pass of NumpySort in terms of another\n        # autograd.Function, NumpyTake.\n        ind, ind_inv = ctx.saved_tensors\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None\n\n```\n\nNow, to make it easier to useNumpySort(to hide away the intermediates we\nreturned as outputs, as well as allow default args and kwargs), we create a new\nfunction that invokes it:\nNumpySort\n\n```python\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result\n\n```\n\nAnd here\u2019s a sanity check:\n\n```python\nx = torch.randn(2, 3)\ngrad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)\nassert torch.allclose(grad_x, torch.ones_like(x))\n\n```\n\n\n## Example 2: autograd.Function specifies custom gradient rules#\n\nAnother common case is antorch.autograd.Functionthat is implemented with PyTorch\noperations. PyTorch is able to compute gradients for PyTorch operations automatically,\nbut perhaps we wish to customize how the gradients are computed. Some reasons why\nwe may want a custom backward different from the one PyTorch gives us are:\ntorch.autograd.Function\nimproving numeric stability\nchanging the performance characteristics of the backward\nchanging how edge cases are handled (e.g. nans, inf)\nmodifying the gradient (e.g. gradient clipping)\nHere\u2019s an example of antorch.autograd.Functionfor the functiony=x**3where we\nchange the performance characteristics (some computation that would normally happen\nduring the backward pass, computing dx, happens in the forward pass).\ntorch.autograd.Function\ny=x**3\n\n```python\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        # In regular PyTorch, if we had just run y = x ** 3, then the backward\n        # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done\n        # that computation here in the forward pass instead.\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\n```\n\nNow, to make it easier to useNumpySort(and hide away the intermediates we\nreturned as outputs) we create a new function that invokes it:\nNumpySort\n\n```python\ndef my_cube(x):\n    result, _ = MyCube.apply(x)\n    return result\n\n```\n\nHere\u2019s a sanity check computing the second-order gradients:\n\n```python\nx = torch.randn([])\nggx = torch.func.grad(torch.func.grad(my_cube))(x)\nassert torch.allclose(ggx, 6 * x)\n\n```\n\n\n## Limitations and gotchas#\n\nWarning\nPlease read these limitations oftorch.autograd.Functionwith torch.func transforms\ncarefully. We are not able to catch many of these situations and error out\ngracefully so they will lead to undefined behavior.\ntorch.autograd.Function\nPlease do not capture Tensors that are being transformed over, have\nrequires_grad=True, or are dual tensors, into the methods of thetorch.autograd.Function. The way to be completely safe is to ensure that the only\nTensors being used inside any method of thetorch.autograd.Functionmust be directly\npassed as inputs (or via the ctx object) rather than come from outside\nthetorch.autograd.Function.\ntorch.autograd.Function\ntorch.autograd.Function\ntorch.autograd.Function\ntorch.autograd.Functiondoes not handle Tensors in pytrees (arbitrary nested\nPython data structures that may or may not contain Tensors). For\nthose Tensors to be tracked by autograd, they must be passed directly as\nan argument totorch.autograd.Function. This is in contrast to\njax.{custom_vjp, custom_jvp}, which do accept pytrees.\ntorch.autograd.Function\ntorch.autograd.Function\nPlease only usesave_for_backward()orsave_for_forward()to save Tensors.\nPlease do not assign Tensors or collections of Tensors directly onto the ctx object -\nthese Tensors will not get tracked\nsave_for_backward()\nsave_for_forward()\n\n## torch.vmap()Support#\n\ntorch.vmap()\nTo use antorch.autograd.Functionwithtorch.vmap(), you must either:\ntorch.autograd.Function\ntorch.vmap()\nprovide avmap()staticmethod that tells us the behavior of thetorch.autograd.Functionundertorch.vmap()\nvmap()\ntorch.autograd.Function\ntorch.vmap()\nask us to autogenerate it by settinggenerate_vmap_rule=True.\ngenerate_vmap_rule=True\n\n## Automatically generate a vmap rule#\n\nIf yourtorch.autograd.Functionfulfills the following additional constraints, then we\nare able to generate a vmap rule for it. If it doesn\u2019t fulfill the constraints or if you\nwant custom behavior under vmap, please manually define a vmap staticmethod (see next section).\ntorch.autograd.Function\nWarning\nWe are not easily able to check for the following constraints and error\nout gracefully. Violation of the constraints may lead to undefined\nbehavior.\nThetorch.autograd.Function\u2019sforward(),backward()(if it exists) andjvp()(if it exists) staticmethods must be transformable viatorch.vmap(). That\nis, they must consist of only PyTorch operations (as opposed to e.g. NumPy or custom\nCUDA kernels).\ntorch.autograd.Function\nforward()\nbackward()\njvp()\ntorch.vmap()\nExample:\n\n```python\nclass MyCube(torch.autograd.Function):\n    # Set generate_vmap_rule to True to ask PyTorch to automatically generate\n    # a vmap rule.\n    generate_vmap_rule = True\n\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result\n\nx = torch.randn(3)\nresult = torch.vmap(my_cube)(x)\nassert torch.allclose(result, x ** 3)\n\n```\n\n\n## Defining the vmap staticmethod#\n\nIf yourtorch.autograd.Functioncalls into another system (like NumPy, C++, CUDA, triton),\nthen to get it to work withtorch.vmap()or transforms that use it, you\u2019ll\nneed to manually define avmap()staticmethod.\ntorch.autograd.Function\ntorch.vmap()\nvmap()\nDepending on what transforms you want to use and your use case, you may not need\nto add avmap()staticmethod to all of yourtorch.autograd.Function:\nvmap()\ntorch.autograd.Function\nFor example,torch.func.jacrev()performsvmap()over the backward pass.\nSo if you\u2019re only interested in usingtorch.func.jacrev(), only\nthebackward()staticmethod needs to be vmappable.\ntorch.func.jacrev()\nvmap()\ntorch.func.jacrev()\nbackward()\nWe do recommend ensuring all of yourtorch.autograd.Functionhave support fortorch.vmap()though, especially if you are writing a third-party library and you want yourtorch.autograd.Functionto work with all combinations oftorch.func()transforms.\ntorch.autograd.Function\ntorch.vmap()\ntorch.autograd.Function\ntorch.func()\nConceptually, the vmap staticmethod is responsible for defining how theforward()should behave undertorch.vmap(). That is, it defines how to transform\ntheforward()to run over inputs with an additional dimension (the dimension\nbeing vmapped over). This is similar to howtorch.vmap()is implemented over\nPyTorch operations: for each operation, we define a vmap rule (sometimes also\nreferred to as a \u201cbatching rule\u201d).\nforward()\ntorch.vmap()\nforward()\ntorch.vmap()\nHere\u2019s how to define thevmap()staticmethod:\nvmap()\nthe signature isvmap(info,in_dims:Tuple[Optional[int]],*args), where*argsis the same as the args toforward().\nvmap(info,in_dims:Tuple[Optional[int]],*args)\n*args\nforward()\nThe vmap staticmethod is responsible for defining how theforward()should behave\nundertorch.vmap(). That is, given inputs with an additional dimension\n(specified byin_dims), how do we compute the batched version offorward()?\nforward()\ntorch.vmap()\nin_dims\nforward()\nFor each arg inargs,in_dimshas a correspondingOptional[int].\nIt isNoneif the arg is not a Tensor or if the arg is not being vmapped over,\notherwise, it is an integer specifying what dimension of the Tensor is being vmapped\nover.\nargs\nin_dims\nOptional[int]\nNone\ninfois a collection of additional metadata that may be helpful:info.batch_sizespecifies the size of the dimension being vmapped over, whileinfo.randomnessis therandomnessoption that was passed totorch.vmap().\ninfo\ninfo.batch_size\ninfo.randomness\nrandomness\ntorch.vmap()\nThe return of the vmap staticmethod is a tuple of(output,out_dims). Similar\ntoin_dims,out_dimsshould be of the same structure asoutputand contain\noneout_dimper output that specifies if the output has the vmapped\ndimension and what index it is in.\n(output,out_dims)\nin_dims\nout_dims\noutput\nout_dim\nExample:\n\n```python\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        return (\n            torch.tensor(result, device=device),\n            torch.tensor(ind, device=device),\n            torch.tensor(ind_inv, device=device),\n        )\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        ind, ind_inv = ctx.saved_tensors\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\n    # The signature of the vmap staticmethod is:\n    # vmap(info, in_dims: Tuple[Optional[int]], *args)\n    # where *args is the same as the arguments to `forward`.\n    @staticmethod\n    def vmap(info, in_dims, x, dim):\n        # For every input (x and dim), in_dims stores an Optional[int]\n        # that is:\n        # - None if the input is not being vmapped over or if the input\n        #   is not a Tensor\n        # - an integer if the input is being vmapped over that represents\n        #   the index of the dimension being vmapped over.\n        x_bdim, _ = in_dims\n\n        # A \"vmap rule\" is the logic of how to perform the operation given\n        # inputs with one additional dimension. In NumpySort, x has an\n        # additional dimension (x_bdim). The vmap rule is simply\n        # to call NumpySort again but pass it a different `dim`.\n        x = x.movedim(x_bdim, 0)\n        # Handle negative dims correctly\n        dim = dim if dim >= 0 else dim + x.dim() - 1\n        result = NumpySort.apply(x, dim + 1)\n\n        # The vmap rule must return a tuple of two things\n        # 1. the output. Should be the same amount of things\n        #    as returned by the forward().\n        # 2. one Optional[int] for each output specifying if each output\n        # is being vmapped over, and if so, the index of the\n        # dimension being vmapped over.\n        #\n        # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the\n        # dimension being vmapped over to the front of `x`, that appears at\n        # dimension 0 of all outputs.\n        # The return is (output, out_dims) -- output is a tuple of 3 Tensors\n        # and out_dims is a Tuple of 3 Optional[int]\n        return NumpySort.apply(x, dim + 1), (0, 0, 0)\n\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None\n\n    @staticmethod\n    def vmap(info, in_dims, x, ind, ind_inv, dim):\n        x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims\n\n        # The strategy is: expand {x, ind, ind_inv} to all have the dimension\n        # being vmapped over.\n        # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim).\n\n        # Handle negative dims by wrapping them to be positive\n        logical_dim = x.dim() if x_bdim is None else x_bdim - 1\n        dim = dim if dim >= 0 else dim + logical_dim\n\n        def maybe_expand_bdim_at_front(x, x_bdim):\n            if x_bdim is None:\n                return x.expand(info.batch_size, *x.shape)\n            return x.movedim(x_bdim, 0)\n\n        # If the Tensor doesn't have the dimension being vmapped over,\n        # expand it out. Otherwise, move it to the front of the Tensor\n        x = maybe_expand_bdim_at_front(x, x_bdim)\n        ind = maybe_expand_bdim_at_front(ind, ind_bdim)\n        ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)\n\n        # The return is a tuple (output, out_dims). Since output is a Tensor,\n        # then out_dims is an Optional[int] (instead of being a Tuple).\n        return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0\n\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result\n\nx = torch.randn(2, 3)\nresult = torch.vmap(numpy_sort)(x)\nassert torch.allclose(result, numpy_sort(result, 1))\n\n```\n\nNote\nThe vmap staticmethod should aim to preserve the semantics of the\nentireFunction. That is, (pseudocode)grad(vmap(MyFunc))should be replaceable with agrad(map(MyFunc)).\nFunction\ngrad(vmap(MyFunc))\ngrad(map(MyFunc))\nIf your autograd.Function has any custom behavior in the backward pass, please\nkeep this in mind.\nNote\nIt is a legitimate use case to write a custom vmap staticmethod for aFunctionthat PyTorch is able to generate a vmap\nrule for viagenerate_vmap_rule=True. You may wish to do this if the\ngenerated vmap rule doesn\u2019t have the semantics you\u2019re looking for.\nFunction\ngenerate_vmap_rule=True\n\n## torch.func.jvp()Support#\n\ntorch.func.jvp()\nTo support forward-mode AD, atorch.autograd.Functionmust have ajvp()staticmethod.\nPlease seeForward mode ADfor details.\ntorch.autograd.Function\njvp()",
  "url": "https://pytorch.org/docs/stable/notes/extending.func.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
{
  "doc_id": "54d09665dcfe6e2bb3e73c5be6da28be",
  "source": "pytorch_docs",
  "title": "torch.accelerator \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.accelerator#\n\nCreated On: Oct 27, 2024 | Last Updated On: Aug 08, 2025\nThis package introduces support for the currentacceleratorin python.\ndevice_count\n\ndevice_count\nReturn the number of currentacceleratoravailable.\nis_available\n\nis_available\nCheck if the current accelerator is available at runtime: it was build, all the required drivers are available and at least one device is visible.\ncurrent_accelerator\n\ncurrent_accelerator\nReturn the device of the accelerator available at compilation time.\nset_device_index\n\nset_device_index\nSet the current device index to a given device.\nset_device_idx\n\nset_device_idx\n(Deprecated) Set the current device index to a given device.\ncurrent_device_index\n\ncurrent_device_index\nReturn the index of a currently selected device for the currentaccelerator.\ncurrent_device_idx\n\ncurrent_device_idx\n(Deprecated) Return the index of a currently selected device for the currentaccelerator.\nset_stream\n\nset_stream\nSet the current stream to a given stream.\ncurrent_stream\n\ncurrent_stream\nReturn the currently selected stream for a given device.\nsynchronize\n\nsynchronize\nWait for all kernels in all streams on the given device to complete.\ndevice_index\n\ndevice_index\nContext manager to set the current device index for the currentaccelerator.\n\n## Memory management#\n\nempty_cache\n\nempty_cache\nRelease all unoccupied cached memory currently held by the caching allocator so that those can be used in other application.\nmax_memory_allocated\n\nmax_memory_allocated\nReturn the currentacceleratormaximum device memory occupied by tensors in bytes for a given device index.\nmax_memory_reserved\n\nmax_memory_reserved\nReturn the currentacceleratormaximum device memory managed by the caching allocator in bytes for a given device index.\nmemory_allocated\n\nmemory_allocated\nReturn the currentacceleratordevice memory occupied by tensors in bytes for a given device index.\nmemory_reserved\n\nmemory_reserved\nReturn the currentacceleratordevice memory managed by the caching allocator in bytes for a given device index.\nmemory_stats\n\nmemory_stats\nReturn a dictionary of accelerator device memory allocator statistics for a given device index.\nreset_accumulated_memory_stats\n\nreset_accumulated_memory_stats\nReset the \"accumulated\" (historical) stats tracked by the currentacceleratormemory allocator for a given device index.\nreset_peak_memory_stats\n\nreset_peak_memory_stats\nReset the \"peak\" stats tracked by the currentacceleratormemory allocator for a given device index.",
  "url": "https://pytorch.org/docs/stable/accelerator.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
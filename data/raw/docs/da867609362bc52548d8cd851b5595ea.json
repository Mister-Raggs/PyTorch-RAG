{
  "doc_id": "da867609362bc52548d8cd851b5595ea",
  "source": "pytorch_docs",
  "title": "Distributed Data Parallel \u2014 PyTorch 2.9 documentation",
  "text": "\n## Distributed Data Parallel#\n\nCreated On: Jan 15, 2020 | Last Updated On: Jan 25, 2024\nWarning\nThe implementation oftorch.nn.parallel.DistributedDataParallelevolves over time. This design note is written based on the state as of v1.4.\ntorch.nn.parallel.DistributedDataParallel\ntorch.nn.parallel.DistributedDataParallel(DDP) transparently performs\ndistributed data parallel training. This page describes how it works and reveals\nimplementation details.\ntorch.nn.parallel.DistributedDataParallel\n\n## Example#\n\nLet us start with a simpletorch.nn.parallel.DistributedDataParallelexample. This example uses atorch.nn.Linearas the local model, wraps\nit with DDP, and then runs one forward pass, one backward pass, and an optimizer\nstep on the DDP model. After that, parameters on the local model will be\nupdated, and all models on different processes should be exactly the same.\ntorch.nn.parallel.DistributedDataParallel\ntorch.nn.Linear\n\n```python\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\ndef example(rank, world_size):\n    # create default process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    # create local model\n    model = nn.Linear(10, 10).to(rank)\n    # construct DDP model\n    ddp_model = DDP(model, device_ids=[rank])\n    # define loss function and optimizer\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    # forward pass\n    outputs = ddp_model(torch.randn(20, 10).to(rank))\n    labels = torch.randn(20, 10).to(rank)\n    # backward pass\n    loss_fn(outputs, labels).backward()\n    # update parameters\n    optimizer.step()\n\ndef main():\n    world_size = 2\n    mp.spawn(example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True)\n\nif __name__==\"__main__\":\n    # Environment variables which need to be\n    # set when using c10d's default \"env\"\n    # initialization mode.\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    main()\n\n```\n\nDDP works with TorchDynamo.  When used with TorchDynamo, apply the DDP model wrapper\nbefore compiling the model, such that torchdynamo can applyDDPOptimizer(graph-break optimizations) based on DDP bucket sizes.  (SeeTorchDynamo DDPOptimizerfor more information.)\nDDPOptimizer\n\n```python\nddp_model = DDP(model, device_ids=[rank])\nddp_model = torch.compile(ddp_model)\n\n```\n\n\n## Internal Design#\n\nThis section reveals how it works under the hood oftorch.nn.parallel.DistributedDataParallelby diving into details of\nevery step in one iteration.\ntorch.nn.parallel.DistributedDataParallel\nPrerequisite: DDP relies on c10dProcessGroupfor communications.\nHence, applications must createProcessGroupinstances before constructing\nDDP.\nProcessGroup\nProcessGroup\nConstruction: The DDP constructor takes a reference to the local module,\nand broadcastsstate_dict()from the process with rank 0 to all other\nprocesses in the group to make sure that all model replicas start from the\nexact same state. Then, each DDP process creates a localReducer, which\nlater will take care of the gradients synchronization during the backward\npass. To improve communication efficiency, theReducerorganizes parameter\ngradients into buckets, and reduces one bucket at a time. Bucket size can be\nconfigured by setting thebucket_cap_mbargument in DDP constructor. The\nmapping from parameter gradients to buckets is determined at the construction\ntime, based on the bucket size limit and parameter sizes. Model parameters are\nallocated into buckets in (roughly) the reverse order ofModel.parameters()from the given model. The reason for using the reverse\norder is because DDP expects gradients to become ready during the backward\npass in approximately that order. The figure below shows an example. Note\nthat, thegrad0andgrad1are inbucket1, and the other two\ngradients are inbucket0. Of course, this assumption might not always\nbe true, and when that happens it could hurt DDP backward speed as theReducercannot kick off the communication at the earliest possible time.\nBesides bucketing, theReduceralso registers autograd hooks during\nconstruction, one hook per parameter. These hooks will be triggered during\nthe backward pass when the gradient becomes ready.\nstate_dict()\nReducer\nReducer\nModel.parameters()\ngrad0\ngrad1\nbucket1\nbucket0\nReducer\nReducer\nForward Pass: The DDP takes the input and passes it to the local model,\nand then analyzes the output from the local model iffind_unused_parametersis set toTrue. This mode allows running\nbackward on a subgraph of the model, and DDP finds out which parameters are\ninvolved in the backward pass by traversing the autograd graph from the model\noutput and marking all unused parameters as ready for reduction. During the\nbackward pass, theReducerwould only wait for unready parameters, but it\nwould still reduce all buckets. Marking a parameter gradient as ready does not\nhelp DDP skip buckets as for now, but it will prevent DDP from waiting for\nabsent gradients forever during the backward pass. Note that traversing the\nautograd graph introduces extra overheads, so applications should only setfind_unused_parameterstoTruewhen necessary.\nfind_unused_parameters\nTrue\nReducer\nfind_unused_parameters\nTrue\nBackward Pass: Thebackward()function is directly invoked on the lossTensor, which is out of DDP\u2019s control, and DDP uses autograd hooks\nregistered at construction time to trigger gradients synchronizations. When\none gradient becomes ready, its corresponding DDP hook on that grad\naccumulator will fire, and DDP will then mark that parameter gradient as\nready for reduction. When gradients in one bucket are all ready, theReducerkicks off an asynchronousallreduceon that bucket to\ncalculate mean of gradients across all processes. When all buckets are ready,\ntheReducerwill block waiting for allallreduceoperations to finish.\nWhen this is done, averaged gradients are written to theparam.gradfield\nof all parameters. So after the backward pass, thegradfield on the same\ncorresponding parameter across different DDP processes should be the same.\nbackward()\nTensor\nReducer\nallreduce\nReducer\nallreduce\nparam.grad\nOptimizer Step: From the optimizer\u2019s perspective, it is optimizing a local\nmodel. Model replicas on all DDP processes can keep in sync because they all\nstart from the same state and they have the same averaged gradients in\nevery iteration.\nNote\nDDP requiresReducerinstances on all processes to invokeallreducein exactly the same order, which is done by always runningallreducein the bucket index order instead of actual bucket ready order. Mismatchedallreduceorder across processes can lead to wrong results or DDP backward\nhang.\nReducer\nallreduce\nallreduce\nallreduce\n\n## Implementation#\n\nBelow are pointers to the DDP implementation components. The stacked graph shows\nthe structure of the code.\n\n## ProcessGroup#\n\nProcessGroup.hpp:\ncontains the abstract API of all process group implementations. Thec10dlibrary provides 3 implementations out of the box, namely,ProcessGroupGloo,ProcessGroupNCCL, andProcessGroupMPI.DistributedDataParallelusesProcessGroup::broadcast()to send\nmodel states from the process with rank 0 to others during initialization\nandProcessGroup::allreduce()to sum gradients.\nc10d\nDistributedDataParallel\nProcessGroup::broadcast()\nProcessGroup::allreduce()\nStore.hpp:\nassists the rendezvous service for process group instances to find each other.\n\n## DistributedDataParallel#\n\ndistributed.py:\nis the Python entry point for DDP. It implements the initialization steps and\ntheforwardfunction for thenn.parallel.DistributedDataParallelmodule which call into C++ libraries. Its_sync_paramfunction performs\nintra-process parameter synchronization when one DDP process works on multiple\ndevices, and it also broadcasts model buffers from the process with rank 0 to\nall other processes. The inter-process parameter synchronization happens inReducer.cpp.\nforward\nnn.parallel.DistributedDataParallel\n_sync_param\nReducer.cpp\ncomm.h:\nimplements the coalesced broadcast helper function which is invoked to\nbroadcast model states during initialization and synchronize model buffers\nbefore the forward pass.\nreducer.h:\nprovides the core implementation for gradient synchronization in the backward\npass. It has three entry point functions:\nReducer: The constructor is called indistributed.pywhich registersReducer::autograd_hook()to gradient accumulators.\nReducer\ndistributed.py\nReducer::autograd_hook()\nautograd_hook()function will be invoked by the autograd engine when\na gradient becomes ready.\nautograd_hook()\nprepare_for_backward()is called at the end of DDP forward pass indistributed.py. It traverses the autograd graph to find unused\nparameters whenfind_unused_parametersis set toTruein DDP\nconstructor.\nprepare_for_backward()\ndistributed.py\nfind_unused_parameters\nTrue\n\n## TorchDynamo DDPOptimizer#\n\nDDP\u2019s performance advantage comes from overlapping allreduce collectives with computations during backwards.\nAotAutograd prevents this overlap when used with TorchDynamo for compiling a whole forward and whole backward graph,\nbecause allreduce ops are launched by autograd hooks _after_ the whole optimized backwards computation finishes.\nTorchDynamo\u2019s DDPOptimizer helps by breaking the forward graph at the logical boundaries of DDP\u2019s allreduce buckets\nduring backwards.  Note: the goal is to break the graph during backwards, and the simplest implementation is to\nbreak the forward graphs and then call AotAutograd and compilation on each section.  This allows DDP\u2019s allreduce hooks\nto fire in-between sections of backwards, and schedule communications to overlap with compute.\nSeethis blog postfor\na more in-depth explanation and experimental results, or read the docs and code attorch/_dynamo/optimizations/distributed.py\nTo Debug DDPOptimizer, setTORCH_LOGS=\u2019ddp_graphs\u2019for full graph dumps. For logs without graphs, add any of \u2018dynamo\u2019, \u2018distributed\u2019, or \u2018dist_ddp\u2019 toTORCH_LOGS(for basic info about bucket boundaries).  To disable DDPOptimizer, settorch._dynamo.config.optimize_ddp=False.\nDDP and TorchDynamo should still work correctly without DDPOptimizer, but with performance degradation.",
  "url": "https://pytorch.org/docs/stable/notes/ddp.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
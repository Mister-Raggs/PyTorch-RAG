{
  "doc_id": "e718867b6395eaf39813ea54122393a4",
  "source": "pytorch_docs",
  "title": "torch.utils.bottleneck \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.utils.bottleneck#\n\nCreated On: Mar 23, 2018 | Last Updated On: Sep 28, 2022\ntorch.utils.bottleneckis a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch\u2019s autograd profiler.\nRun it on the command line with\n\n```python\npython -m torch.utils.bottleneck /path/to/source/script.py [args]\n\n```\n\nwhere [args] are any number of arguments toscript.py, or runpython-mtorch.utils.bottleneck-hfor more usage instructions.\npython-mtorch.utils.bottleneck-h\nWarning\nBecause your script will be profiled, please ensure that it exits in a\nfinite amount of time.\nWarning\nDue to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful.\nNote\nTo decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.\nOf course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result oftorch.autograd.profiler.emit_nvtx()withnvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Similarly,Intel\u00aeVTune\u2122Profilerhelps to analyze performance on Intel platforms further withtorch.autograd.profiler.emit_itt().\ntorch.autograd.profiler.emit_nvtx()\nnvprof\nIntel\u00aeVTune\u2122Profiler\ntorch.autograd.profiler.emit_itt()\nWarning\nIf you are profiling CUDA code, the first profiler thatbottleneckruns\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time.\nbottleneck\nFor more complicated uses of the profilers (like in a multi-GPU case),\nplease seehttps://docs.python.org/3/library/profile.htmlortorch.autograd.profiler.profile()for more information.\ntorch.autograd.profiler.profile()",
  "url": "https://pytorch.org/docs/stable/bottleneck.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
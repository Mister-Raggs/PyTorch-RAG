{
  "doc_id": "91ac5dd9187ce04274c9fd9a724dccf3",
  "source": "pytorch_docs",
  "title": "torch.utils.checkpoint \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch.utils.checkpoint#\n\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\nNote\nCheckpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward propagation.  This can cause persistent\nstates like the RNG state to be more advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supplypreserve_rng_state=Falsetocheckpointorcheckpoint_sequentialto omit stashing and\nrestoring the RNG state during each checkpoint.\npreserve_rng_state=False\ncheckpoint\ncheckpoint_sequential\nThe stashing logic saves and restores the RNG state for CPU and another\ndevice type (infer the device type from Tensor arguments excluding CPU\ntensors by_infer_device_type) to therun_fn. If there are multiple\ndevice, device state will only be saved for devices of a single device type,\nand the remaining devices will be ignored. Consequently, if any checkpointed\nfunctions involve randomness, this may result in incorrect gradients. (Note\nthat if CUDA devices are among the devices detected, it will be prioritized;\notherwise, the first device encountered will be selected.) If there are no\nCPU-tensors, the default device type state (default value iscuda, and it\ncould be set to other device byDefaultDeviceType) will be saved and restored.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within therun_fnitself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) withinrun_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed.\n_infer_device_type\nrun_fn\ncuda\nDefaultDeviceType\nrun_fn\nrun_fn\nCheckpoint a model or part of the model.\nActivation checkpointing is a technique that trades compute for memory.\nInstead of keeping tensors needed for backward alive until they are used in\ngradient computation during backward, forward computation in checkpointed\nregions omits saving tensors for backward and recomputes them during the\nbackward pass. Activation checkpointing can be applied to any part of a\nmodel.\nThere are currently two checkpointing implementations available, determined\nby theuse_reentrantparameter. It is recommended that you useuse_reentrant=False. Please refer the note below for a discussion of\ntheir differences.\nuse_reentrant\nuse_reentrant=False\nWarning\nIf thefunctioninvocation during the backward pass differs\nfrom the forward pass, e.g., due to a global variable, the checkpointed\nversion may not be equivalent, potentially causing an\nerror being raised or leading to silently incorrect gradients.\nfunction\nWarning\nTheuse_reentrantparameter should be passed explicitly. In version\n2.9 we will raise an exception ifuse_reentrantis not passed.\nIf you are using theuse_reentrant=Truevariant, please refer to the\nnote below for important considerations and potential limitations.\nuse_reentrant\nuse_reentrant\nuse_reentrant=True\nNote\nThe reentrant variant of checkpoint (use_reentrant=True) and\nthe non-reentrant variant of checkpoint (use_reentrant=False)\ndiffer in the following ways:\nuse_reentrant=True\nuse_reentrant=False\nNon-reentrant checkpoint stops recomputation as soon as all needed\nintermediate activations have been recomputed. This feature is enabled\nby default, but can be disabled withset_checkpoint_early_stop().\nReentrant checkpoint always recomputesfunctionin its\nentirety during the backward pass.\nset_checkpoint_early_stop()\nfunction\nThe reentrant variant does not record the autograd graph during the\nforward pass, as it runs with the forward pass undertorch.no_grad(). The non-reentrant version does record the\nautograd graph, allowing one to perform backward on the graph within\ncheckpointed regions.\ntorch.no_grad()\nThe reentrant checkpoint only supports thetorch.autograd.backward()API for the backward pass without itsinputsargument, while the non-reentrant version supports all ways\nof performing the backward pass.\ntorch.autograd.backward()\nAt least one input and output must haverequires_grad=Truefor the\nreentrant variant. If this condition is unmet, the checkpointed part\nof the model will not have gradients. The non-reentrant version does\nnot have this requirement.\nrequires_grad=True\nThe reentrant version does not consider tensors in nested structures\n(e.g., custom objects, lists, dicts, etc) as participating in\nautograd, while the non-reentrant version does.\nThe reentrant checkpoint does not support checkpointed regions with\ndetached tensors from the computational graph, whereas the\nnon-reentrant version does. For the reentrant variant, if the\ncheckpointed segment contains tensors detached usingdetach()or\nwithtorch.no_grad(), the backward pass will raise an error.\nThis is becausecheckpointmakes all the outputs require gradients\nand this causes issues when a tensor is defined to have no gradient in\nthe model. To avoid this, detach the tensors outside of thecheckpointfunction.\ndetach()\ntorch.no_grad()\ncheckpoint\ncheckpoint\nfunction\u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes(activation,hidden),functionshould correctly use the\nfirst input asactivationand the second input ashidden\n(activation,hidden)\nfunction\nactivation\nhidden\nargs\u2013 tuple containing inputs to thefunction\nfunction\npreserve_rng_state(bool,optional) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Note that under torch.compile,\nthis flag doesn\u2019t take effect and we always preserve RNG state.\nDefault:True\nTrue\nuse_reentrant(bool) \u2013 specify whether to use the activation checkpoint variant that\nrequires reentrant autograd. This parameter should be passed\nexplicitly. In version 2.9 we will raise an exception ifuse_reentrantis not passed. Ifuse_reentrant=False,checkpointwill use an implementation that does not require\nreentrant autograd. This allowscheckpointto support additional\nfunctionality, such as working as expected withtorch.autograd.gradand support for keyword arguments input into\nthe checkpointed function.\nuse_reentrant\nuse_reentrant=False\ncheckpoint\ncheckpoint\ntorch.autograd.grad\ncontext_fn(Callable,optional) \u2013 A callable returning a tuple of two\ncontext managers. The function and its recomputation will be run\nunder the first and second context managers respectively.\nThis argument is only supported ifuse_reentrant=False.\nuse_reentrant=False\ndeterminism_check(str,optional) \u2013 A string specifying the determinism\ncheck to perform. By default it is set to\"default\"which\ncompares the shapes, dtypes, and devices of the recomputed tensors\nagainst those the saved tensors. To turn off this check, specify\"none\". Currently these are the only two supported values.\nPlease open an issue if you would like to see more determinism\nchecks. This argument is only supported ifuse_reentrant=False,\nifuse_reentrant=True, the determinism check is always disabled.\n\"default\"\n\"none\"\nuse_reentrant=False\nuse_reentrant=True\ndebug(bool,optional) \u2013 IfTrue, error messages will also include\na trace of the operators ran during the original forward computation\nas well as the recomputation. This argument is only supported ifuse_reentrant=False.\nTrue\nuse_reentrant=False\nearly_stop(bool,optional) \u2013 IfTrue, non-reentrant checkpoint stops\nrecomputation as soon as it has computed all needed Tensors. This\nargument is ignored ifuse_reentrant=True. Can be overridden\nglobally usingset_checkpoint_early_stop()context manager.\nDefault:True.\nTrue\nuse_reentrant=True\nset_checkpoint_early_stop()\nTrue\nOutput of runningfunctionon*args\nfunction\n*args\nCheckpoint a sequential model to save memory.\nSequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will not store\nthe intermediate activations. The inputs of each checkpointed segment will\nbe saved for re-running the segment in the backward pass.\nWarning\nTheuse_reentrantparameter should be passed explicitly. In version\n2.9 we will raise an exception ifuse_reentrantis not passed.\nIf you are using theuse_reentrant=True`variant,pleasesee:func:`~torch.utils.checkpoint.checkpoint`fortheimportantconsiderationsandlimitationsofthisvariant.Itisrecommendedthatyouuse``use_reentrant=False.\nuse_reentrant\nuse_reentrant\nuse_reentrant=True`variant,pleasesee:func:`~torch.utils.checkpoint.checkpoint`fortheimportantconsiderationsandlimitationsofthisvariant.Itisrecommendedthatyouuse``use_reentrant=False\nfunctions\u2013 Atorch.nn.Sequentialor the list of modules or\nfunctions (comprising the model) to run sequentially.\ntorch.nn.Sequential\nsegments\u2013 Number of chunks to create in the model\ninput\u2013 A Tensor that is input tofunctions\nfunctions\npreserve_rng_state(bool,optional) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint.\nDefault:True\nTrue\nuse_reentrant(bool) \u2013 specify whether to use the activation checkpoint variant that\nrequires reentrant autograd. This parameter should be passed\nexplicitly. In version 2.5 we will raise an exception ifuse_reentrantis not passed. Ifuse_reentrant=False,checkpointwill use an implementation that does not require\nreentrant autograd. This allowscheckpointto support additional\nfunctionality, such as working as expected withtorch.autograd.gradand support for keyword arguments input into\nthe checkpointed function.\nuse_reentrant\nuse_reentrant=False\ncheckpoint\ncheckpoint\ntorch.autograd.grad\nOutput of runningfunctionssequentially on*inputs\nfunctions\n*inputs\nExample\n\n```python\n>>> model = nn.Sequential(...)\n>>> input_var = checkpoint_sequential(model, chunks, input_var)\n\n```\n\nContext manager that sets whether checkpoint should print additional debug\ninformation when running. See thedebugflag forcheckpoint()for more information. Note that\nwhen set, this context manager overrides the value ofdebugpassed to\ncheckpoint. To defer to the local setting, passNoneto this context.\ndebug\ncheckpoint()\ndebug\nNone\nenabled(bool) \u2013 Whether checkpoint should print debug information.\nDefault is \u2018None\u2019.\nEnum for specifying the policy for checkpointing during backpropagation.\nThe following policies are supported:\n{MUST,PREFER}_SAVE: The operation\u2019s output will be saved during the forward\npass and will not be recomputed during the backward pass\n{MUST,PREFER}_SAVE\n{MUST,PREFER}_RECOMPUTE: The operation\u2019s output will not be saved during the\nforward pass and will be recomputed during the backward pass\n{MUST,PREFER}_RECOMPUTE\nUseMUST_*overPREFER_*to indicate that the policy should not be overridden\nby other subsystems liketorch.compile.\nMUST_*\nPREFER_*\nNote\nA policy function that always returnsPREFER_RECOMPUTEis\nequivalent to vanilla checkpointing.\nPREFER_RECOMPUTE\nA policy function that returnsPREFER_SAVEevery op is\nNOT equivalent to not using checkpointing. Using such a policy would\nsave additional tensors not limited to ones that are actually needed for\ngradient computation.\nPREFER_SAVE\nContext passed to policy function during selective checkpointing.\nThis class is used to pass relevant metadata to the policy function during\nselective checkpointing. The metadata includes whether the current invocation\nof the policy function is during recomputation or not.\nExample\n\n```python\n>>>\n>>> def policy_fn(ctx, op, *args, **kwargs):\n>>>    print(ctx.is_recompute)\n>>>\n>>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n>>>\n>>> out = torch.utils.checkpoint.checkpoint(\n>>>     fn, x, y,\n>>>     use_reentrant=False,\n>>>     context_fn=context_fn,\n>>> )\n\n```\n\nHelper to avoid recomputing certain ops during activation checkpointing.\nUse this withtorch.utils.checkpoint.checkpointto control which\noperations are recomputed during the backward pass.\npolicy_fn_or_list(CallableorList) \u2013If a policy function is provided, it should accept aSelectiveCheckpointContext, theOpOverload, args and\nkwargs to the op, and return aCheckpointPolicyenum value\nindicating whether the execution of the op should be recomputed or not.If a list of operations is provided, it is equivalent to a policy\nreturningCheckpointPolicy.MUST_SAVEfor the specified\noperations andCheckpointPolicy.PREFER_RECOMPUTEfor all other\noperations.\nIf a policy function is provided, it should accept aSelectiveCheckpointContext, theOpOverload, args and\nkwargs to the op, and return aCheckpointPolicyenum value\nindicating whether the execution of the op should be recomputed or not.\nSelectiveCheckpointContext\nOpOverload\nCheckpointPolicy\nIf a list of operations is provided, it is equivalent to a policy\nreturningCheckpointPolicy.MUST_SAVEfor the specified\noperations andCheckpointPolicy.PREFER_RECOMPUTEfor all other\noperations.\nallow_cache_entry_mutation(bool,optional) \u2013 By default, an error is\nraised if any tensors cached by selective activation checkpoint are\nmutated in order to ensure correctness. If set toTrue, this check\nis disabled.\nA tuple of two context managers.\nExample\n\n```python\n>>> import functools\n>>>\n>>> x = torch.rand(10, 10, requires_grad=True)\n>>> y = torch.rand(10, 10, requires_grad=True)\n>>>\n>>> ops_to_save = [\n>>>    torch.ops.aten.mm.default,\n>>> ]\n>>>\n>>> def policy_fn(ctx, op, *args, **kwargs):\n>>>    if op in ops_to_save:\n>>>        return CheckpointPolicy.MUST_SAVE\n>>>    else:\n>>>        return CheckpointPolicy.PREFER_RECOMPUTE\n>>>\n>>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n>>>\n>>> # or equivalently\n>>> context_fn = functools.partial(create_selective_checkpoint_contexts, ops_to_save)\n>>>\n>>> def fn(x, y):\n>>>     return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y\n>>>\n>>> out = torch.utils.checkpoint.checkpoint(\n>>>     fn, x, y,\n>>>     use_reentrant=False,\n>>>     context_fn=context_fn,\n>>> )\n\n```\n",
  "url": "https://pytorch.org/docs/stable/checkpoint.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
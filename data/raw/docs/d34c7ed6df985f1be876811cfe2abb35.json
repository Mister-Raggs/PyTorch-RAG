{
  "doc_id": "d34c7ed6df985f1be876811cfe2abb35",
  "source": "pytorch_docs",
  "title": "Distributed RPC Framework \u2014 PyTorch 2.9 documentation",
  "text": "\n## Distributed RPC Framework#\n\nCreated On: Nov 14, 2019 | Last Updated On: Jul 09, 2025\nThe distributed RPC framework provides mechanisms for multi-machine model\ntraining through a set of primitives to allow for remote communication, and a\nhigher-level API to automatically differentiate models split across several\nmachines.\nWarning\nAPIs in the RPC package are stable and in maintenance mode.\nWarning\nCUDA support is abetafeature.\nNot all features of the RPC package are yet compatible with CUDA support and\nthus their use is discouraged. These unsupported features include: RRefs,\nJIT compatibility, dist autograd and dist optimizer, and profiling.\nNote\nPlease refer toPyTorchDistributedOverview<https://pytorch.org/tutorials/beginner/dist_overview.html>__\nfor a brief introduction to all features related to distributed training.\nPyTorchDistributedOverview<https://pytorch.org/tutorials/beginner/dist_overview.html>\n\n## Basics#\n\nThe distributed RPC framework makes it easy to run functions remotely, supports\nreferencing remote objects without copying the real data around, and provides\nautograd and optimizer APIs to transparently run backward and update parameters\nacross RPC boundaries. These features can be categorized into four sets of APIs.\nRemote Procedure Call (RPC)supports running a function on the specified\ndestination worker with the given arguments and getting the return value back\nor creating a reference to the return value. There are three main RPC APIs:rpc_sync()(synchronous),rpc_async()(asynchronous), andremote()(asynchronous and returns a reference\nto the remote return value). Use the synchronous API if the user code cannot\nproceed without the return value. Otherwise, use the asynchronous API to get\na future, and wait on the future when the return value is needed on the\ncaller. Theremote()API is useful when the\nrequirement is to create something remotely but never need to fetch it to\nthe caller. Imagine the case that a driver process is setting up a parameter\nserver and a trainer. The driver can create an embedding table on the\nparameter server and then share the reference to the embedding table with the\ntrainer, but itself will never use the embedding table locally. In this case,rpc_sync()andrpc_async()are no longer appropriate, as they\nalways imply that the return value will be returned to the caller\nimmediately or in the future.\nrpc_sync()\nrpc_async()\nremote()\nremote()\nrpc_sync()\nrpc_async()\nRemote Reference (RRef)serves as a distributed shared pointer to a local\nor remote object. It can be shared with other workers and reference counting\nwill be handled transparently. Each RRef only has one owner and the object\nonly lives on that owner. Non-owner workers holding RRefs can get copies of\nthe object from the owner by explicitly requesting it. This is useful when\na worker needs to access some data object, but itself is neither the creator\n(the caller ofremote()) or the owner of the\nobject. The distributed optimizer, as we will discuss below, is one example\nof such use cases.\nremote()\nDistributed Autogradstitches together local autograd engines on all the\nworkers involved in the forward pass, and automatically reach out to them\nduring the backward pass to compute gradients. This is especially helpful if\nthe forward pass needs to span multiple machines when conducting, e.g.,\ndistributed model parallel training, parameter-server training, etc. With\nthis feature, user code no longer needs to worry about how to send gradients\nacross RPC boundaries and in which order should the local autograd engines\nbe launched, which can become quite complicated where there are nested and\ninter-dependent RPC calls in the forward pass.\nDistributed Optimizer\u2019s constructor takes aOptimizer()(e.g.,SGD(),Adagrad(), etc.) and a list of parameter RRefs, creates anOptimizer()instance on each distinct RRef owner, and\nupdates parameters accordingly when runningstep(). When you have\ndistributed forward and backward passes, parameters and gradients will be\nscattered across multiple workers, and hence it requires an optimizer on each\nof the involved workers. Distributed Optimizer wraps all those local\noptimizers into one, and provides a concise constructor andstep()API.\nOptimizer()\nSGD()\nAdagrad()\nOptimizer()\nstep()\nstep()\n\n## RPC#\n\nBefore using RPC and distributed autograd primitives, initialization must take\nplace. To initialize the RPC framework we need to useinit_rpc()which would initialize the RPC\nframework, RRef framework and distributed autograd.\ninit_rpc()\nInitializes RPC primitives such as the local RPC agent\nand distributed autograd, which immediately makes the current\nprocess ready to send and receive RPCs.\nname(str) \u2013 a globally unique name of this node. (e.g.,Trainer3,ParameterServer2,Master,Worker1)\nName can only contain number, alphabet, underscore, colon,\nand/or dash, and must be shorter than 128 characters.\nTrainer3\nParameterServer2\nMaster\nWorker1\nbackend(BackendType,optional) \u2013 The type of RPC backend\nimplementation. Supported values isBackendType.TENSORPIPE(the default).\nSeeBackendsfor more information.\nBackendType.TENSORPIPE\nrank(int) \u2013 a globally unique id/rank of this node.\nworld_size(int) \u2013 The number of workers in the group.\nrpc_backend_options(RpcBackendOptions,optional) \u2013 The options\npassed to the RpcAgent constructor. It must be an agent-specific\nsubclass ofRpcBackendOptionsand contains agent-specific initialization configurations. By\ndefault, for all agents, it sets the default timeout to 60\nseconds and performs the rendezvous with an underlying process\ngroup initialized usinginit_method=\"env://\",\nmeaning that environment variablesMASTER_ADDRandMASTER_PORTneed to be set properly. SeeBackendsfor more information and find which options\nare available.\nRpcBackendOptions\ninit_method=\"env://\"\nMASTER_ADDR\nMASTER_PORT\nThe following APIs allow users to remotely execute functions as well as create\nreferences (RRefs) to remote data objects. In these APIs, when passing aTensoras an argument or a return value, the destination worker will try to\ncreate aTensorwith the same meta (i.e., shape, stride, etc.). We\nintentionally disallow transmitting CUDA tensors because it might crash if the\ndevice lists on source and destination workers do not match. In such cases,\napplications can always explicitly move the input tensors to CPU on the caller\nand move it to the desired devices on the callee if necessary.\nTensor\nTensor\nMake a blocking RPC call to run functionfuncon workerto. RPC\nmessages are sent and received in parallel to execution of Python code. This\nmethod is thread-safe.\nfunc\nto\nto(strorWorkerInfoorint) \u2013 name/rank/WorkerInfoof the destination worker.\nWorkerInfo\nfunc(Callable) \u2013 a callable function, such as Python callables, builtin\noperators (e.g.add()) and annotated\nTorchScript functions.\nadd()\nargs(tuple) \u2013 the argument tuple for thefuncinvocation.\nfunc\nkwargs(dict) \u2013 is a dictionary of keyword arguments for thefuncinvocation.\nfunc\ntimeout(float,optional) \u2013 timeout in seconds to use for this RPC. If\nthe RPC does not complete in this amount of\ntime, an exception indicating it has\ntimed out will be raised. A value of 0\nindicates an infinite timeout, i.e. a timeout\nerror will never be raised. If not provided,\nthe default value set during initialization\nor with_set_rpc_timeoutis used.\n_set_rpc_timeout\nReturns the result of runningfuncwithargsandkwargs.\nfunc\nargs\nkwargs\nMake sure thatMASTER_ADDRandMASTER_PORTare set properly\non both workers. Refer toinit_process_group()API for more details. For example,\nMASTER_ADDR\nMASTER_PORT\ninit_process_group()\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\nThen run the following code in two different processes:\n\n```python\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nBelow is an example of running a TorchScript function using RPC.\n\n```python\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar)\n\n```\n\n\n```python\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nMake a non-blocking RPC call to run functionfuncon workerto. RPC\nmessages are sent and received in parallel to execution of Python code. This\nmethod is thread-safe. This method will immediately return aFuturethat can be awaited on.\nfunc\nto\nFuture\nto(strorWorkerInfoorint) \u2013 name/rank/WorkerInfoof the destination worker.\nWorkerInfo\nfunc(Callable) \u2013 a callable function, such as Python callables, builtin\noperators (e.g.add()) and annotated\nTorchScript functions.\nadd()\nargs(tuple) \u2013 the argument tuple for thefuncinvocation.\nfunc\nkwargs(dict) \u2013 is a dictionary of keyword arguments for thefuncinvocation.\nfunc\ntimeout(float,optional) \u2013 timeout in seconds to use for this RPC. If\nthe RPC does not complete in this amount of\ntime, an exception indicating it has\ntimed out will be raised. A value of 0\nindicates an infinite timeout, i.e. a timeout\nerror will never be raised. If not provided,\nthe default value set during initialization\nor with_set_rpc_timeoutis used.\n_set_rpc_timeout\nReturns aFutureobject that can be waited\non. When completed, the return value offunconargsandkwargscan be retrieved from theFutureobject.\nFuture\nfunc\nargs\nkwargs\nFuture\nWarning\nUsing GPU tensors as arguments or return values offuncis not\nsupported since we don\u2019t support sending GPU tensors over the wire. You\nneed to explicitly copy GPU tensors to CPU before using them as\narguments or return values offunc.\nfunc\nfunc\nWarning\nTherpc_asyncAPI does not copy storages of argument tensors until\nsending them over the wire, which could be done by a different thread\ndepending on the RPC backend type. The caller should make sure that the\ncontents of those tensors stay intact until the returnedFuturecompletes.\nrpc_async\nFuture\nMake sure thatMASTER_ADDRandMASTER_PORTare set properly\non both workers. Refer toinit_process_group()API for more details. For example,\nMASTER_ADDR\nMASTER_PORT\ninit_process_group()\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\nThen run the following code in two different processes:\n\n```python\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2))\n>>> result = fut1.wait() + fut2.wait()\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nBelow is an example of running a TorchScript function using RPC.\n\n```python\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar)\n\n```\n\n\n```python\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut = rpc.rpc_async(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> ret = fut.wait()\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nMake a remote call to runfuncon workertoand return anRRefto the result value immediately.\nWorkertowill be the owner of the returnedRRef, and the worker callingremoteis\na user. The owner manages the global reference count of itsRRef, and the ownerRRefis only destructed when globally there\nare no living references to it.\nfunc\nto\nRRef\nto\nRRef\nremote\nRRef\nRRef\nto(strorWorkerInfoorint) \u2013 name/rank/WorkerInfoof the destination worker.\nWorkerInfo\nfunc(Callable) \u2013 a callable function, such as Python callables, builtin\noperators (e.g.add()) and annotated\nTorchScript functions.\nadd()\nargs(tuple) \u2013 the argument tuple for thefuncinvocation.\nfunc\nkwargs(dict) \u2013 is a dictionary of keyword arguments for thefuncinvocation.\nfunc\ntimeout(float,optional) \u2013 timeout in seconds for this remote call. If the\ncreation of thisRRefon workertois not successfully processed on this\nworker within this timeout, then the next time\nthere is an attempt to use the RRef (such asto_here()), a timeout will be raised\nindicating this failure. A value of 0 indicates\nan infinite timeout, i.e. a timeout error will\nnever be raised. If not provided, the default\nvalue set during initialization or with_set_rpc_timeoutis used.\nRRef\nto\nto_here()\n_set_rpc_timeout\nA userRRefinstance to the result\nvalue. Use the blocking APItorch.distributed.rpc.RRef.to_here()to retrieve the result value locally.\nRRef\ntorch.distributed.rpc.RRef.to_here()\nWarning\nTheremoteAPI does not copy storages of argument tensors until\nsending them over the wire, which could be done by a different thread\ndepending on the RPC backend type. The caller should make sure that the\ncontents of those tensors stay intact until the returned RRef is\nconfirmed by the owner, which can be checked using thetorch.distributed.rpc.RRef.confirmed_by_owner()API.\nremote\ntorch.distributed.rpc.RRef.confirmed_by_owner()\nWarning\nErrors such as timeouts for theremoteAPI are handled on a\nbest-effort basis. This means that when remote calls initiated byremotefail, such as with a timeout error, we take a best-effort\napproach to error handling. This means that errors are handled and set\non the resulting RRef on an asynchronous basis. If the RRef has not been\nused by the application before this handling (such asto_hereor\nfork call), then future uses of theRRefwill appropriately raise\nerrors. However, it is possible that the user application will use theRRefbefore the errors are handled. In this case, errors may not be\nraised as they have not yet been handled.\nremote\nremote\nto_here\nRRef\nRRef\nExample:\n\n```python\nMake sure that ``MASTER_ADDR`` and ``MASTER_PORT`` are set properly\non both workers. Refer to :meth:`~torch.distributed.init_process_group`\nAPI for more details. For example,\n\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\n\nThen run the following code in two different processes:\n\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>> x = rref1.to_here() + rref2.to_here()\n>>> rpc.shutdown()\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\nBelow is an example of running a TorchScript function using RPC.\n\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar)\n\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref = rpc.remote(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rref.to_here()\n>>> rpc.shutdown()\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nGetWorkerInfoof a given worker name.\nUse thisWorkerInfoto avoid passing an\nexpensive string on every invocation.\nWorkerInfo\nWorkerInfo\nworker_name(str) \u2013 the string name of a worker. IfNone, return the\nthe id of the current worker. (defaultNone)\nNone\nNone\nWorkerInfoinstance for the givenworker_nameorWorkerInfoof the\ncurrent worker ifworker_nameisNone.\nWorkerInfo\nworker_name\nWorkerInfo\nworker_name\nNone\nPerform a shutdown of the RPC agent, and then destroy the RPC agent. This\nstops the local agent from accepting outstanding requests, and shuts\ndown the RPC framework by terminating all RPC threads. Ifgraceful=True,\nthis will block until all local and remote RPC processes reach this method\nand wait for all outstanding work to complete. Otherwise, ifgraceful=False, this is a local shutdown, and it does not wait for other\nRPC processes to reach this method.\ngraceful=True\ngraceful=False\nWarning\nForFutureobjects returned byrpc_async(),future.wait()should not\nbe called aftershutdown().\nFuture\nrpc_async()\nfuture.wait()\nshutdown()\ngraceful(bool) \u2013 Whether to do a graceful shutdown or not. If True,\nthis will 1) wait until there is no pending system\nmessages forUserRRefsand delete them; 2) block\nuntil all local and remote RPC processes have reached\nthis method and wait for all outstanding work to\ncomplete.\nUserRRefs\nMake sure thatMASTER_ADDRandMASTER_PORTare set properly\non both workers. Refer toinit_process_group()API for more details. For example,\nMASTER_ADDR\nMASTER_PORT\ninit_process_group()\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\nThen run the following code in two different processes:\n\n```python\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> # do some work\n>>> result = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(1), 1))\n>>> # ready to shutdown\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> # wait for worker 0 to finish work, and then shutdown.\n>>> rpc.shutdown()\n\n```\n\nA structure that encapsulates information of a worker in the system.\nContains the name and ID of the worker. This class is not meant to\nbe constructed directly, rather, an instance can be retrieved\nthroughget_worker_info()and the\nresult can be passed in to functions such asrpc_sync(),rpc_async(),remote()to avoid copying a string on\nevery invocation.\nget_worker_info()\nrpc_sync()\nrpc_async()\nremote()\nGlobally unique id to identify the worker.\nThe name of the worker.\nThe RPC package also provides decorators which allow applications to specify\nhow a given function should be treated on the callee side.\nA decorator for a function indicating that the return value of the function\nis guaranteed to be aFutureobject and this\nfunction can run asynchronously on the RPC callee. More specifically, the\ncallee extracts theFuturereturned by the wrapped\nfunction and installs subsequent processing steps as a callback to thatFuture. The installed callback will read the value\nfrom theFuturewhen completed and send the\nvalue back as the RPC response. That also means the returnedFutureonly exists on the callee side and is never\nsent through RPC. This decorator is useful when the wrapped function\u2019s\n(fn) execution needs to pause and resume due to, e.g., containingrpc_async()or waiting for other signals.\nFuture\nFuture\nFuture\nFuture\nFuture\nfn\nrpc_async()\nNote\nTo enable asynchronous execution, applications must pass the\nfunction object returned by this decorator to RPC APIs. If RPC detected\nattributes installed by this decorator, it knows that this function\nreturns aFutureobject and will handle that accordingly.\nHowever, this does not mean this decorator has to be outmost one when\ndefining a function. For example, when combined with@staticmethodor@classmethod,@rpc.functions.async_executionneeds to be the\ninner decorator to allow the target function be recognized as a static\nor class function. This target function can still execute asynchronously\nbecause, when accessed, the static or class method preserves attributes\ninstalled by@rpc.functions.async_execution.\nFuture\n@staticmethod\n@classmethod\n@rpc.functions.async_execution\n@rpc.functions.async_execution\nThe returnedFutureobject can come fromrpc_async(),then(), orFutureconstructor. The example below shows directly using theFuturereturned bythen().\nFuture\nrpc_async()\nthen()\nFuture\nFuture\nthen()\n\n```python\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @rpc.functions.async_execution\n>>> def async_add_chained(to, x, y, z):\n>>>     # This function runs on \"worker1\" and returns immediately when\n>>>     # the callback is installed through the `then(cb)` API. In the\n>>>     # mean time, the `rpc_async` to \"worker2\" can run concurrently.\n>>>     # When the return value of that `rpc_async` arrives at\n>>>     # \"worker1\", \"worker1\" will run the lambda function accordingly\n>>>     # and set the value for the previously returned `Future`, which\n>>>     # will then trigger RPC to send the result back to \"worker0\".\n>>>     return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>         lambda fut: fut.wait() + z\n>>>     )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add_chained,\n>>>     args=(\"worker2\", torch.ones(2), 1, 1)\n>>> )\n>>> print(ret)  # prints tensor([3., 3.])\n\n```\n\nWhen combined with TorchScript decorators, this decorator must be the\noutmost one.\n\n```python\n>>> from torch import Tensor\n>>> from torch.futures import Future\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @torch.jit.script\n>>> def script_add(x: Tensor, y: Tensor) -> Tensor:\n>>>     return x + y\n>>>\n>>> @rpc.functions.async_execution\n>>> @torch.jit.script\n>>> def async_add(to: str, x: Tensor, y: Tensor) -> Future[Tensor]:\n>>>     return rpc.rpc_async(to, script_add, (x, y))\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1)\n>>> )\n>>> print(ret)  # prints tensor([2., 2.])\n\n```\n\nWhen combined with static or class method, this decorator must be the\ninner one.\n\n```python\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> class AsyncExecutionClass:\n>>>\n>>>     @staticmethod\n>>>     @rpc.functions.async_execution\n>>>     def static_async_add(to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>>     @classmethod\n>>>     @rpc.functions.async_execution\n>>>     def class_async_add(cls, to, x, y, z):\n>>>         ret_fut = torch.futures.Future()\n>>>         rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: ret_fut.set_result(fut.wait() + z)\n>>>         )\n>>>         return ret_fut\n>>>\n>>>     @rpc.functions.async_execution\n>>>     def bound_async_add(self, to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.static_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.class_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n\n```\n\nThis decorator also works with RRef helpers, i.e., .torch.distributed.rpc.RRef.rpc_sync(),torch.distributed.rpc.RRef.rpc_async(), andtorch.distributed.rpc.RRef.remote().\ntorch.distributed.rpc.RRef.rpc_sync()\ntorch.distributed.rpc.RRef.rpc_async()\ntorch.distributed.rpc.RRef.remote()\n\n```python\n>>> from torch.distributed import rpc\n>>>\n>>> # reuse the AsyncExecutionClass class above\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_sync().static_async_add(\"worker2\", torch.ones(2), 1, 2)\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_async().static_async_add(\"worker2\", torch.ones(2), 1, 2).wait()\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.remote().static_async_add(\"worker2\", torch.ones(2), 1, 2).to_here()\n>>> print(ret)  # prints tensor([4., 4.])\n\n```\n\n\n## Backends#\n\nThe RPC module can leverage different backends to perform the communication\nbetween the nodes. The backend to be used can be specified in theinit_rpc()function, by passing a certain value of\ntheBackendTypeenum. Regardless of what backend\nis used, the rest of the RPC API won\u2019t change. Each backend also defines its own\nsubclass of theRpcBackendOptionsclass, an\ninstance of which can also be passed toinit_rpc()to configure the backend\u2019s behavior.\ninit_rpc()\nBackendType\nRpcBackendOptions\ninit_rpc()\nAn enum class of available backends.\nPyTorch ships with a builtinBackendType.TENSORPIPEbackend.\nAdditional ones can be registered using theregister_backend()function.\nBackendType.TENSORPIPE\nregister_backend()\nAn abstract structure encapsulating the options passed into the RPC\nbackend. An instance of this class can be passed in toinit_rpc()in order to initialize RPC\nwith specific configurations, such as the RPC timeout andinit_methodto be used.\ninit_rpc()\ninit_method\nURL specifying how to initialize the process group.\nDefault isenv://\nenv://\nA float indicating the timeout to use for all\nRPCs. If an RPC does not complete in this timeframe, it will\ncomplete with an exception indicating that it has timed out.\nThe TensorPipe agent, which is the default, leveragesthe TensorPipe library, which provides a natively\npoint-to-point communication primitive specifically suited for machine learning\nthat fundamentally addresses some of the limitations of Gloo. Compared to Gloo,\nit has the advantage of being asynchronous, which allows a large number of\ntransfers to occur simultaneously, each at their own speed, without blocking\neach other. It will only open pipes between pairs of nodes when needed, on\ndemand, and when one node fails only its incident pipes will be closed, while\nall other ones will keep working as normal. In addition, it is able to support\nmultiple different transports (TCP, of course, but also shared memory, NVLink,\nInfiniBand, \u2026) and can automatically detect their availability and negotiate\nthe best transport to use for each pipe.\nThe TensorPipe backend comes with a TCP-based transport, just like Gloo. It is also able to\nautomatically chunk and multiplex large tensors over multiple sockets and\nthreads in order to achieve very high bandwidths. The agent will be able to pick\nthe best transport on its own, with no intervention required.\nExample:\n\n```python\nimport os\nfrom torch.distributed import rpc\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '29500'\n\nrpc.init_rpc(\n    \"worker1\",\n    rank=0,\n    world_size=2,\n    rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n        num_worker_threads=8,\n        rpc_timeout=20 # 20 second timeout\n    )\n)\n\n# omitting init_rpc invocation on worker2\n\n```\n\nThe backend options forTensorPipeAgent, derived fromRpcBackendOptions.\nTensorPipeAgent\nRpcBackendOptions\nnum_worker_threads(int,optional) \u2013 The number of threads in the\nthread-pool used byTensorPipeAgentto execute\nrequests (default: 16).\nTensorPipeAgent\nrpc_timeout(float,optional) \u2013 The default timeout, in seconds,\nfor RPC requests (default: 60 seconds). If the RPC has not\ncompleted in this timeframe, an exception indicating so will\nbe raised. Callers can override this timeout for individual\nRPCs inrpc_sync()andrpc_async()if necessary.\nrpc_sync()\nrpc_async()\ninit_method(str,optional) \u2013 The URL to initialize the distributed\nstore used for rendezvous. It takes any value accepted for the\nsame argument ofinit_process_group()(default:env://).\ninit_process_group()\nenv://\ndevice_maps(Dict[str,Dict],optional) \u2013 Device placement mappings from\nthis worker to the callee. Key is the callee worker name and value\nthe dictionary (Dictofint,str, ortorch.device)\nthat maps this worker\u2019s devices to the callee worker\u2019s devices.\n(default:None)\nDict\nint\nstr\ntorch.device\nNone\ndevices(List[int, str, ortorch.device], optional) \u2013 all local\nCUDA devices used by RPC agent. By Default, it will be initialized\nto all local devices from its owndevice_mapsand corresponding\ndevices from its peers\u2019device_maps. When processing CUDA RPC\nrequests, the agent will properly synchronize CUDA streams for\nall devices in thisList.\ntorch.device\ndevice_maps\ndevice_maps\nList\nThe device map locations.\nAll devices used by the local agent.\nURL specifying how to initialize the process group.\nDefault isenv://\nenv://\nThe number of threads in the thread-pool used byTensorPipeAgentto execute\nrequests.\nTensorPipeAgent\nA float indicating the timeout to use for all\nRPCs. If an RPC does not complete in this timeframe, it will\ncomplete with an exception indicating that it has timed out.\nSet device mapping between each RPC caller and callee pair. This\nfunction can be called multiple times to incrementally add\ndevice placement configurations.\nto(str) \u2013 Callee name.\ndevice_map(Dictofint,str, ortorch.device) \u2013 Device placement\nmappings from this worker to the callee. This map must be\ninvertible.\nExample\n\n```python\n>>> # both workers\n>>> def add(x, y):\n>>>     print(x)  # tensor([1., 1.], device='cuda:1')\n>>>     return x + y, (x + y).to(2)\n>>>\n>>> # on worker 0\n>>> options = TensorPipeRpcBackendOptions(\n>>>     num_worker_threads=8,\n>>>     device_maps={\"worker1\": {0: 1}}\n>>> # maps worker0's cuda:0 to worker1's cuda:1\n>>> )\n>>> options.set_device_map(\"worker1\", {1: 2})\n>>> # maps worker0's cuda:1 to worker1's cuda:2\n>>>\n>>> rpc.init_rpc(\n>>>     \"worker0\",\n>>>     rank=0,\n>>>     world_size=2,\n>>>     backend=rpc.BackendType.TENSORPIPE,\n>>>     rpc_backend_options=options\n>>> )\n>>>\n>>> x = torch.ones(2)\n>>> rets = rpc.rpc_sync(\"worker1\", add, args=(x.to(0), 1))\n>>> # The first argument will be moved to cuda:1 on worker1. When\n>>> # sending the return value back, it will follow the invert of\n>>> # the device map, and hence will be moved back to cuda:0 and\n>>> # cuda:1 on worker0\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:0')\n>>> print(rets[1])  # tensor([2., 2.], device='cuda:1')\n\n```\n\nSet local devices used by the TensorPipe RPC agent. When processing\nCUDA RPC requests, the TensorPipe RPC agent will properly synchronize\nCUDA streams for all devices in thisList.\nList\ndevices(Listofint,str, ortorch.device) \u2013 local devices used by\nthe TensorPipe RPC agent.\nNote\nThe RPC framework does not automatically retry anyrpc_sync(),rpc_async()andremote()calls. The reason being that there is\nno way the RPC framework can determine whether an operation is idempotent or\nnot and whether it is safe to retry. As a result, it is the application\u2019s\nresponsibility to deal with failures and retry if necessary. RPC communication\nis based on TCP and as a result failures could happen due to network failures\nor intermittent network connectivity issues. In such scenarios, the application\nneeds to retry appropriately with reasonable backoffs to ensure the network\nisn\u2019t overwhelmed by aggressive retries.\nrpc_sync()\nrpc_async()\nremote()\n\n## RRef#\n\nWarning\nRRefs are not currently supported when using CUDA tensors\nAnRRef(Remote REFerence) is a reference to a value of some typeT(e.g.Tensor) on a remote worker. This handle keeps the referenced remote\nvalue alive on the owner, but there is no implication that the value will be\ntransferred to the local worker in the future. RRefs can be used in\nmulti-machine training by holding references tonn.Modulesthat exist on\nother workers, and calling the appropriate functions to retrieve or modify their\nparameters during training. SeeRemote Reference Protocolfor more\ndetails.\nRRef\nT\nTensor\nA class encapsulating a reference to a value of some type on a remote\nworker. This handle will keep the referenced remote value alive on the\nworker. AUserRRefwill be deleted when 1) no references to it in\nboth the application code and in the local RRef context, or 2) the\napplication has called a graceful shutdown. Invoking methods on a\ndeleted RRef leads to undefined behaviors. RRef implementation only\noffers best-effort error detection, and applications should not useUserRRefsafterrpc.shutdown().\nUserRRef\nUserRRefs\nrpc.shutdown()\nWarning\nRRefs can only be serialized and deserialized by the RPC module.\nSerializing and deserializing RRefs without RPC (e.g., Python\npickle, torchsave()/load(),\nJITsave()/load(), etc.) will\nlead to errors.\nsave()\nload()\nsave()\nload()\nvalue(object) \u2013 The value to be wrapped by this RRef.\ntype_hint(Type,optional) \u2013 Python type that should be passed toTorchScriptcompiler as type hint forvalue.\nTorchScript\nvalue\nFollowing examples skip RPC initialization and shutdown code\nfor simplicity. Refer to RPC docs for those details.\nCreate an RRef using rpc.remote\n\n```python\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> # get a copy of value from the RRef\n>>> x = rref.to_here()\n\n```\n\nCreate an RRef from a local object\n\n```python\n>>> import torch\n>>> from torch.distributed.rpc import RRef\n>>> x = torch.zeros(2, 2)\n>>> rref = RRef(x)\n\n```\n\nShare an RRef with other workers\n\n```python\n>>> # On both worker0 and worker1:\n>>> def f(rref):\n>>>   return rref.to_here() + 1\n\n```\n\n\n```python\n>>> # On worker0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> from torch.distributed.rpc import RRef\n>>> rref = RRef(torch.zeros(2, 2))\n>>> # the following RPC shares the rref with worker1, reference\n>>> # count is automatically updated.\n>>> rpc.rpc_sync(\"worker1\", f, args=(rref,))\n\n```\n\nRuns the backward pass using the RRef as the root of the\nbackward pass. Ifdist_autograd_ctx_idis provided,\nwe perform a distributed backward pass using the provided\nctx_id starting from the owner of the RRef. In this case,get_gradients()should be\nused to retrieve the gradients. Ifdist_autograd_ctx_idisNone, it is assumed that this is a local autograd graph\nand we only perform a local backward pass. In the local case,\nthe node calling this API has to be the owner of the RRef.\nThe value of the RRef is expected to be a scalar Tensor.\ndist_autograd_ctx_id\nget_gradients()\ndist_autograd_ctx_id\nNone\ndist_autograd_ctx_id(int,optional) \u2013 The distributed\nautograd context id for which we should retrieve the\ngradients (default: -1).\nretain_graph(bool,optional) \u2013 IfFalse, the graph used to\ncompute the grad will be freed. Note that in nearly all\ncases setting this option toTrueis not needed and\noften can be worked around in a much more efficient way.\nUsually, you need to set this toTrueto run backward\nmultiple times (default: False).\nFalse\nTrue\nTrue\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n\n```\n\nReturns whether thisRRefhas been confirmed by the owner.OwnerRRefalways returns true, whileUserRRefonly\nreturns true when the owner knowns about thisUserRRef.\nRRef\nOwnerRRef\nUserRRef\nUserRRef\nReturns whether or not the current node is the owner of thisRRef.\nRRef\nIf the current node is the owner, returns a reference to the\nlocal value. Otherwise, throws an exception.\nReturns worker information of the node that owns thisRRef.\nRRef\nReturns worker name of the node that owns thisRRef.\nRRef\nCreate a helper proxy to easily launch aremoteusing\nthe owner of the RRef as the destination to run functions on\nthe object referenced by this RRef. More specifically,rref.remote().func_name(*args,**kwargs)is the same as\nthe following:\nremote\nrref.remote().func_name(*args,**kwargs)\n\n```python\n>>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n\n```\n\ntimeout(float,optional) \u2013 Timeout forrref.remote(). If\nthe creation of thisRRefis not successfully completed within the timeout, then the\nnext time there is an attempt to use the RRef\n(such asto_here), a timeout will be raised. If not\nprovided, the default RPC timeout will be used. Please seerpc.remote()for specific timeout semantics forRRef.\nrref.remote()\nRRef\nto_here\nrpc.remote()\nRRef\n\n```python\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n\n```\n\nCreate a helper proxy to easily launch anrpc_asyncusing\nthe owner of the RRef as the destination to run functions on\nthe object referenced by this RRef. More specifically,rref.rpc_async().func_name(*args,**kwargs)is the same as\nthe following:\nrpc_async\nrref.rpc_async().func_name(*args,**kwargs)\n\n```python\n>>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))\n\n```\n\ntimeout(float,optional) \u2013 Timeout forrref.rpc_async().\nIf the call does not complete within this timeframe, an\nexception indicating so will be raised. If this argument\nis not provided, the default RPC timeout will be used.\nrref.rpc_async()\n\n```python\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n\n```\n\nCreate a helper proxy to easily launch anrpc_syncusing\nthe owner of the RRef as the destination to run functions on\nthe object referenced by this RRef. More specifically,rref.rpc_sync().func_name(*args,**kwargs)is the same as\nthe following:\nrpc_sync\nrref.rpc_sync().func_name(*args,**kwargs)\n\n```python\n>>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))\n\n```\n\ntimeout(float,optional) \u2013 Timeout forrref.rpc_sync().\nIf the call does not complete within this timeframe, an\nexception indicating so will be raised. If this argument\nis not provided, the default RPC timeout will be used.\nrref.rpc_sync()\n\n```python\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n\n```\n\nBlocking call that copies the value of the RRef from the owner\nto the local node and returns it. If the current node is the\nowner, returns a reference to the local value.\ntimeout(float,optional) \u2013 Timeout forto_here. If\nthe call does not complete within this timeframe, an\nexception indicating so will be raised. If this\nargument is not provided, the default RPC timeout\n(60s) will be used.\nto_here\nMore Information about RRef\n\n## RemoteModule#\n\nWarning\nRemoteModule is not currently supported when using CUDA tensors\nRemoteModuleis an easy way to create an nn.Module remotely on a different\nprocess. The actual module resides on a remote host, but the local host has a\nhandle to this module and invoke this module similar to a regular nn.Module.\nThe invocation however incurs RPC calls to the remote end and can be performed\nasynchronously if needed via additional APIs supported by RemoteModule.\nRemoteModule\nA RemoteModule instance can only be created after RPC initialization.\nIt creates a user-specified module on a specified remote node.\nIt behaves like a regularnn.Moduleexcept that theforwardmethod is\nexecuted on the remote node.\nIt takes care of autograd recording to ensure the backward pass propagates\ngradients back to the corresponding remote module.\nnn.Module\nforward\nIt generates two methodsforward_asyncandforwardbased on the\nsignature of theforwardmethod ofmodule_cls.forward_asyncruns asynchronously and returns a Future. The arguments offorward_asyncandforwardare the same as theforwardmethod of the module\nreturned by themodule_cls.\nforward_async\nforward\nforward\nmodule_cls\nforward_async\nforward_async\nforward\nforward\nmodule_cls\nFor example, ifmodule_clsreturns an instance ofnn.Linear,\nthat hasforwardmethod signature:defforward(input:Tensor)->Tensor:,\nthe generatedRemoteModulewill have 2 methods with the signatures:\nmodule_cls\nnn.Linear\nforward\ndefforward(input:Tensor)->Tensor:\nRemoteModule\ndefforward(input:Tensor)->Tensor:\ndefforward_async(input:Tensor)->Future[Tensor]:\nremote_device(str) \u2013 Device on the destination worker where we\u2019d like to place this module.\nThe format should be \u201c<workername>/<device>\u201d, where the device field can be parsed as torch.device type.\nE.g., \u201ctrainer0/cpu\u201d, \u201ctrainer0\u201d, \u201cps0/cuda:0\u201d.\nIn addition, the device field can be optional and the default value is \u201ccpu\u201d.\nmodule_cls(nn.Module) \u2013Class for the module to be created remotely. For example,>>>classMyModule(nn.Module):>>>defforward(input):>>>returninput+1>>>>>>module_cls=MyModule\nClass for the module to be created remotely. For example,\n\n```python\n>>> class MyModule(nn.Module):\n>>>     def forward(input):\n>>>         return input + 1\n>>>\n>>> module_cls = MyModule\n\n```\n\nargs(Sequence,optional) \u2013 args to be passed tomodule_cls.\nmodule_cls\nkwargs(Dict,optional) \u2013 kwargs to be passed tomodule_cls.\nmodule_cls\nA remote module instance which wraps theModulecreated by the\nuser-providedmodule_cls, it has a blockingforwardmethod and an\nasynchronousforward_asyncmethod that returns a future of theforwardcall\non the user-provided module on the remote side.\nModule\nmodule_cls\nforward\nforward_async\nforward\nRun the following code in two different processes:\n\n```python\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> from torch import nn, Tensor\n>>> from torch.distributed.nn.api.remote_module import RemoteModule\n>>>\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> remote_linear_module = RemoteModule(\n>>>     \"worker1/cpu\", nn.Linear, args=(20, 30),\n>>> )\n>>> input = torch.randn(128, 20)\n>>> ret_fut = remote_linear_module.forward_async(input)\n>>> ret = ret_fut.wait()\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>>\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nFurthermore, a more practical example that is combined withDistributedDataParallel(DDP)\ncan be found in thistutorial.\nReturn anRRef(RRef[nn.Module]) pointing to the remote module.\nRRef\nRRef[nn.Module]\nRRef[Module]\nReturn a list ofRRefpointing to the remote module\u2019s parameters.\nRRef\nThis can typically be used in conjunction\nwithDistributedOptimizer.\nDistributedOptimizer\nrecurse(bool) \u2013 if True, then returns parameters of the remote\nmodule and all submodules of the remote module. Otherwise,\nreturns only parameters that are direct members of the\nremote module.\nA list ofRRef(List[RRef[nn.Parameter]])\nto remote module\u2019s parameters.\nRRef\nList[RRef[nn.Parameter]]\nlist[torch.distributed.rpc.api.RRef[torch.nn.parameter.Parameter]]\n\n## Distributed Autograd Framework#\n\nWarning\nDistributed autograd is not currently supported when using CUDA tensors\nThis module provides an RPC-based distributed autograd framework that can be\nused for applications such as model parallel training. In short, applications\nmay send and receive gradient recording tensors over RPC. In the forward pass,\nwe record when gradient recording tensors are sent over RPC and during the\nbackward pass we use this information to perform a distributed backward pass\nusing RPC. For more details seeDistributed Autograd Design.\nKicks off the distributed backward pass using the provided roots. This\ncurrently implements theFAST mode algorithmwhich\nassumes all RPC messages sent in the same distributed autograd context\nacross workers would be part of the autograd graph during the backward pass.\nWe use the provided roots to discover the autograd graph and compute\nappropriate dependencies. This method blocks until the entire\nautograd computation is done.\nWe accumulate the gradients in the appropriatetorch.distributed.autograd.contexton each of the nodes. The autograd\ncontext to be used is looked up given thecontext_idthat is passed in whentorch.distributed.autograd.backward()is called. If there is no valid\nautograd context corresponding to the given ID, we throw an error. You can\nretrieve the accumulated gradients using theget_gradients()API.\ntorch.distributed.autograd.context\ncontext_id\ntorch.distributed.autograd.backward()\nget_gradients()\ncontext_id(int) \u2013 The autograd context id for which we should retrieve the gradients.\nroots(list) \u2013 Tensors which represent the roots of the autograd\ncomputation. All the tensors should be scalars.\nretain_graph(bool,optional) \u2013 If False, the graph used to compute the grad\nwill be freed. Note that in nearly all cases setting this\noption to True is not needed and often can be worked around\nin a much more efficient way. Usually, you need to set this\nto True to run backward multiple times.\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     pred = model.forward()\n>>>     loss = loss_func(pred, loss)\n>>>     dist_autograd.backward(context_id, loss)\n\n```\n\nContext object to wrap forward and backward passes when using\ndistributed autograd. Thecontext_idgenerated in thewithstatement  is required to uniquely identify a distributed backward pass\non all workers. Each worker stores metadata associated with thiscontext_id, which is required to correctly execute a distributed\nautograd pass.\ncontext_id\nwith\ncontext_id\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = rpc.rpc_sync(\"worker1\", torch.add, args=(t1, t2)).sum()\n>>>     dist_autograd.backward(context_id, [loss])\n\n```\n\nRetrieves a map from Tensor to the appropriate gradient for that Tensor\naccumulated in the provided context corresponding to the givencontext_idas part of the distributed autograd backward pass.\ncontext_id\ncontext_id(int) \u2013 The autograd context id for which we should retrieve the\ngradients.\nA map where the key is the Tensor and the value is the associated gradient\nfor that Tensor.\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = t1 + t2\n>>>     dist_autograd.backward(context_id, [loss.sum()])\n>>>     grads = dist_autograd.get_gradients(context_id)\n>>>     print(grads[t1])\n>>>     print(grads[t2])\n\n```\n\nMore Information about RPC Autograd\n\n## Distributed Optimizer#\n\nSee thetorch.distributed.optimpage for documentation on distributed optimizers.\n\n## Design Notes#\n\nThe distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.\nDistributed Autograd Design\nThe RRef design note covers the design of theRRef(Remote REFerence) protocol used to refer to values on remote workers by the framework.\nRemote Reference Protocol\n\n## Tutorials#\n\nThe RPC tutorials introduce users to the RPC framework, provide several example applications\nusingtorch.distributed.rpcAPIs, and demonstrate how\nto usethe profilerto profile RPC-based workloads.\nGetting started with Distributed RPC Framework\nImplementing a Parameter Server using Distributed RPC Framework\nCombining Distributed DataParallel with Distributed RPC Framework(coversRemoteModuleas well)\nImplementing batch RPC processing",
  "url": "https://pytorch.org/docs/stable/rpc.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
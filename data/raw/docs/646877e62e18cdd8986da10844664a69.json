{
  "doc_id": "646877e62e18cdd8986da10844664a69",
  "source": "pytorch_docs",
  "title": "Use fullgraph=True to Identify and Eliminate Graph Breaks \u2014 PyTorch 2.9 documentation",
  "text": "\n## Usefullgraph=Trueto Identify and Eliminate Graph Breaks#\n\nfullgraph=True\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nUsingtorch.compile(fullgraph=False)(the default) is a good way to get started withtorch.compile: it supports all Python programs out-of-the-box via the ability to graph break and gives good performance on common cases.\ntorch.compile(fullgraph=False)\ntorch.compile\nHowever, if you\u2019re trying to get more performance out of your model, you should explicitly think about what regions of code should be compiled:\nWe recommend usingtorch.compile(fullgraph=True)to find and eliminate graph breaks in your code.\ntorch.compile(fullgraph=True)\nIf you\u2019re a library developer (or testing if your code \u201cworks\u201d withtorch.compile), we recommend testing usingtorch.compile(fullgraph=True).\ntorch.compile\ntorch.compile(fullgraph=True)\ntorch.compile(fullgraph=True)offers stronger guarantees overfullgraph=False:\nwe will always capture a single FX graph to be compiled (or error if we cannot due to a graph break).In particular, you are forced to resolve every graph break that is encountered.\ntorch.compile(fullgraph=True)\nfullgraph=False\nThere are a number of strategies for resolving a graph break.\n\n## Strategy 1:  Rewrite the unsupported code to use features supported by Dynamo#\n\nMany graph break error messages will give some suggestions on how to rewrite code to avoid the graph break.\nIf the graph break is still difficult to resolve, then please move on to the next strategy\nor submit an issue to thePyTorch GitHub repo.\nMore graph break examples and how to resolve them can be found inCommon Graph Breaks.\nExample: Dynamo does not support callingnexton alist_iteratorobject that was an input to the function being compiled.\nnext\nlist_iterator\n\n```python\n@torch.compile(fullgraph=True)\ndef f(xs):\n    a = next(xs)\n    b = next(xs)\n    return a + b\n\nxs = [torch.tensor(1.), torch.tensor(2.)]\ntry:\n    out = f(iter(xs))\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nUnsupported method call\n  Explanation: Dynamo does not know how to trace method `__next__` of class `list_iterator`\n  Hint: Avoid calling `list_iterator.__next__` in your code.\n  Hint: Please report an issue to PyTorch.\n  Hint: Dynamo does not fully support tracing builtin iterators (e.g. `map`, `zip`, `enumerate`) passed in from uncompiled to compiled regions (e.g. `torch.compile(fn)(enumerate(...))`). This can happen unintentionally if a previous graph break happens with a builtin iterator in the local scope.\n  Hint: List/dict comprehensions in Python <= 3.11 result in implicit function calls, which Dynamo cannot trace as a top level frame. Possible workarounds are (1) use a loop instead of a comprehension, (2) fix any graph breaks in the function above the comprehension, (3) wrap the comprehension in a function, or (4) use Python 3.12+.\n\n  Developer debug context: call_method UserDefinedObjectVariable(list_iterator) __next__ [] {}\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0156.html\n\nfrom user code:\n   File \"/tmp/ipykernel_387/1195637716.py\", line 3, in f\n    a = next(xs)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\nInstead, rewrite the compiled function to accept a list.\n\n```python\n@torch.compile(fullgraph=True)\ndef f_rewritten(xs):\n    it = iter(xs)\n    a = next(it)\n    b = next(it)\n    return a + b\n\nf_rewritten(xs)\n\n```\n\n\n```python\ntensor(3.)\n\n```\n\n\n## Strategy 2: Pure functions can always be compiled via an escape hatch.#\n\nSummary: The space of all Python functions is vast and thus it is impractical for Dynamo to be able to trace\nthrough every Python function without graph breaks. For Python functions considered to be \u201cpure\u201d\nthat Dynamo cannot trace through without graph breaks, we provide some escape hatches to attempt\nto trace through these functions anyway:\nUsecustom_oportriton_opon pure triton kernels.\ncustom_op\ntriton_op\nUsenonstrict_tracefor pure functions that only use PyTorch Tensor ops.\nnonstrict_trace\nUsecustom_opfor all other pure functions.\ncustom_op\nA \u201cpure function\u201d is a function with the following properties:\nDeterminism. Given the same inputs, the pure function will always return the same output\nNo external side effects. A pure function does not have any externally-visible side effects,\nsuch as modifying external state or performing I/O operations.\nSide effects that remain internal to the function are allowed (e.g. mutating intermediate tensors).\nOne notable exception is that mutatingtorch.*ops on function input Tensors are generally allowed.\ntorch.*\nExplicit input/output. All the input data must be passed through the function parameters and all of the outputs are returned from the function.\nSeePure Functionsfor examples.\nDynamo is theoretically able to handle a wide variety of impure functions, but may be lacking coverage for specific\nPython language features. However, pure functions can always be compiled via an escape hatch.\nIf you have a graph break it may be possible to refactor the code around it into a pure function and use an escape hatch that bypasses Dynamo tracing:\nUsetorch._dynamo.nonstrict_traceif you want the Tensor operations in the function to show up in the Dynamo output graph (and therefore be optimizable).nonstrict_tracetells Dynamo to usenon-strict tracing.\ntorch._dynamo.nonstrict_trace\nnonstrict_trace\nUse custom operators if you want the function to be opaque w.r.t. totorch.compile(both the frontend Dynamo and the backend).\ntorch.compile\nNote that there is nothing preventing these escape hatches from being applied to impure functions,\nbutwe do not provide any soundness guarantees.\nExample: If Dynamo doesn\u2019t support some Python feature or API that is non-strict traceable (e.g. it uses PyTorch operations),usetorch._dynamo.nonstrict_traceto capture it instead.\ntorch._dynamo.nonstrict_trace\n\n```python\n# this is a function that Dynamo doesn't support (due to the graph_break() call).\ndef g(x):\n    y = x.sin()\n    torch._dynamo.graph_break()\n    z = y.sin()\n    return z\n\n@torch.compile(fullgraph=True)\ndef f(x):\n    w = x.sin()\n    return g(w)\n\nx = torch.randn(3)\ntry:\n    f(x)  # Graph Break: there was a call to torch._dynamo.graph_break()\nexcept Exception as e:\n    print(e)\n\n@torch.compile(fullgraph=True)\ndef f_rewritten(x):\n    w = x.sin()\n    return torch._dynamo.nonstrict_trace(g)(w)\nf_rewritten(x)  # works\n\n```\n\n\n```python\nCall to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_387/2422769198.py\", line 11, in f\n    return g(w)\n  File \"/tmp/ipykernel_387/2422769198.py\", line 4, in g\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\ntensor([ 0.4607, -0.7399, -0.5501])\n\n```\n\nExample: usecustom operatorsto create opaque functions w.r.t. totorch.compile\ntorch.compile\n\n```python\nfrom torch.utils.cpp_extension import load_inline\n\n# C++ source code for the square operation\ncpp_source = \"\"\"\ntorch::Tensor square_cpu(torch::Tensor input) {\n    // Check that input is a CPU tensor\n    TORCH_CHECK(input.device().is_cpu(), \"Input must be a CPU tensor\");\n\n    // Create output tensor with same shape and dtype as input\n    torch::Tensor output = torch::empty_like(input);\n\n    // Get data pointers\n    float* input_data = input.data_ptr<float>();\n    float* output_data = output.data_ptr<float>();\n\n    // Get total number of elements\n    int64_t numel = input.numel();\n\n    // For loop to compute square of each element\n    for (int64_t i = 0; i < numel; i++) {\n        output_data[i] = input_data[i] * input_data[i];\n    }\n\n    return output;\n}\n\"\"\"\n\n# Load the extension inline\nsquare_module = load_inline(\n    name=\"square_cpu_kernel\",\n    cpp_sources=cpp_source,\n    functions=[\"square_cpu\"],\n    verbose=True\n)\n\ndef square(x):\n    return square_module.square_cpu(x)\n\n@torch.compile(fullgraph=True)\ndef f(x):\n    return square(x)\n\ntry:\n    f(torch.randn(3, 3))  # graph break\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\n[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=square_cpu_kernel -DTORCH_API_INCLUDE_EXTENSION_H -isystem /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include -isystem /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/envs/py_3.10/include/python3.10 -fPIC -std=c++17 -c /var/lib/jenkins/.cache/torch_extensions/py310_cpu/square_cpu_kernel/main.cpp -o main.o \n[2/2] c++ main.o -shared -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o square_cpu_kernel.so\nAttempted to call function marked as skipped\n  Explanation: Dynamo does not know how to trace the builtin `square_cpu_kernel.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1.square_cpu.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).\n  Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.\n  Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.\n\n  Developer debug context: module: square_cpu_kernel, qualname: pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1.square_cpu, skip reason: <missing reason>\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html\n\nfrom user code:\n   File \"/tmp/ipykernel_387/2059008136.py\", line 41, in f\n    return square(x)\n  File \"/tmp/ipykernel_387/2059008136.py\", line 37, in square\n    return square_module.square_cpu(x)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `square_cpu_kernel.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1.square_cpu.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).\nIf it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.\nIf it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.\n  torch._dynamo.utils.warn_once(explanation + \"\\n\" + \"\\n\".join(hints))\n\n```\n\n\n```python\n# Use torch.library.custom_op to define a new custom operator.\n# Custom operators are opaque with respect to torch.compile:\n# that is, torch.compile does not peek into them.\n\n@torch.library.custom_op(\"mylib::square\", mutates_args=())\ndef square(x: torch.Tensor) -> torch.Tensor:\n    return square_module.square_cpu(x)\n\n# Use register_fake to add a ``FakeTensor`` kernel for the operator\n@square.register_fake\ndef _(x):\n    return x.new_empty(x.size())\n\nprint(f(torch.randn(3, 3)))  # no graph break\n\n```\n\n\n```python\ntensor([[5.2260e-01, 1.9521e+00, 8.7664e-01],\n        [6.8205e-01, 5.1948e-03, 2.5824e-03],\n        [5.2988e+00, 5.3917e-01, 2.5636e+00]])\n\n```\n\nFor more information ontriton_opfor custom triton kernels, see theuser-defined triton kernel tutorial.\ntriton_op\n\n## Strategy 3: Don\u2019t compile the code#\n\nNot all code is amenable to being compiled.torch.compileis a compiler for Tensor computation;\nit will not be able to optimize things like disk IO. Try to refactor the code such that the unsupported\ncode is not called in the compiled region.\ntorch.compile\n\n```python\n@torch.compile(fullgraph=True)\ndef f(x):\n   y = x ** 2  / 2\n   torch.save(y, \"foo.pt\")\n   z = y ** 3 / 6\n   return z\n\nx = torch.randn(3)\ntry:\n    f(x)  # Graph Break: torch.save not supported\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nAttempted to call function marked as skipped\n  Explanation: Dynamo developers have intentionally marked that the function `save` in file `/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/serialization.py` should not be traced.\n  Hint: Avoid calling the function `save`.\n  Hint: Apply `@torch._dynamo.dont_skip_tracing` to the function `save` to force tracing into the function. More graph breaks may occur as a result of attempting to trace into the function.\n  Hint: Please file an issue to PyTorch.\n\n  Developer debug context: module: torch.serialization, qualname: save, skip reason: <missing reason>\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html\n\nfrom user code:\n   File \"/tmp/ipykernel_387/150060719.py\", line 4, in f\n    torch.save(y, \"foo.pt\")\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\ndef f_rewritten(x):\n   y = g(x)\n   torch.save(y, \"foo.pt\")\n   z = h(y)\n   return z\n\n@torch.compile(fullgraph=True)\ndef g(x):\n   y = x ** 2  / 2\n   return y\n\n@torch.compile(fullgraph=True)\ndef h(y):\n   z = y ** 3 / 6\n   return z\n\nf_rewritten(x)\n\n```\n\n\n```python\ntensor([3.1630e-04, 2.0966e-05, 2.8076e-02])\n\n```\n",
  "url": "https://pytorch.org/docs/stable/compile/programming_model.fullgraph_true.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
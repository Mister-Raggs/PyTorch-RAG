{
  "doc_id": "9f1e22e39f6c6d4b8d26d19c239a973a",
  "source": "pytorch_docs",
  "title": "torch \u2014 PyTorch 2.9 documentation",
  "text": "\n## torch#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jul 22, 2025\nThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serialization of\nTensors and arbitrary types, and other useful utilities.\nIt has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0.\n\n## Tensors#\n\nis_tensor\n\nis_tensor\nReturns True ifobjis a PyTorch tensor.\nis_storage\n\nis_storage\nReturns True ifobjis a PyTorch storage object.\nis_complex\n\nis_complex\nReturns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.\ninput\ntorch.complex64\ntorch.complex128\nis_conj\n\nis_conj\nReturns True if theinputis a conjugated tensor, i.e. its conjugate bit is set toTrue.\ninput\nis_floating_point\n\nis_floating_point\nReturns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.\ninput\ntorch.float64\ntorch.float32\ntorch.float16\ntorch.bfloat16\nis_nonzero\n\nis_nonzero\nReturns True if theinputis a single element tensor which is not equal to zero after type conversions.\ninput\nset_default_dtype\n\nset_default_dtype\nSets the default floating point dtype tod.\nd\nget_default_dtype\n\nget_default_dtype\nGet the current default floating pointtorch.dtype.\ntorch.dtype\nset_default_device\n\nset_default_device\nSets the defaulttorch.Tensorto be allocated ondevice.\ntorch.Tensor\ndevice\nget_default_device\n\nget_default_device\nGets the defaulttorch.Tensorto be allocated ondevice\ntorch.Tensor\ndevice\nset_default_tensor_type\n\nset_default_tensor_type\n\nnumel\n\nnumel\nReturns the total number of elements in theinputtensor.\ninput\nset_printoptions\n\nset_printoptions\nSet options for printing.\nset_flush_denormal\n\nset_flush_denormal\nDisables denormal floating numbers on CPU.\n\n## Creation Ops#\n\nNote\nRandom sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\ntorch.empty()\ntorch.Tensor\ntensor\n\ntensor\nConstructs a tensor with no autograd history (also known as a \"leaf tensor\", seeAutograd mechanics) by copyingdata.\ndata\nsparse_coo_tensor\n\nsparse_coo_tensor\nConstructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.\nindices\nsparse_csr_tensor\n\nsparse_csr_tensor\nConstructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices.\ncrow_indices\ncol_indices\nsparse_csc_tensor\n\nsparse_csc_tensor\nConstructs asparse tensor in CSC (Compressed Sparse Column)with specified values at the givenccol_indicesandrow_indices.\nccol_indices\nrow_indices\nsparse_bsr_tensor\n\nsparse_bsr_tensor\nConstructs asparse tensor in BSR (Block Compressed Sparse Row))with specified 2-dimensional blocks at the givencrow_indicesandcol_indices.\ncrow_indices\ncol_indices\nsparse_bsc_tensor\n\nsparse_bsc_tensor\nConstructs asparse tensor in BSC (Block Compressed Sparse Column))with specified 2-dimensional blocks at the givenccol_indicesandrow_indices.\nccol_indices\nrow_indices\nasarray\n\nasarray\nConvertsobjto a tensor.\nobj\nas_tensor\n\nas_tensor\nConvertsdatainto a tensor, sharing data and preserving autograd history if possible.\ndata\nas_strided\n\nas_strided\nCreate a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.\ninput\nsize\nstride\nstorage_offset\nfrom_file\n\nfrom_file\nCreates a CPU tensor with a storage backed by a memory-mapped file.\nfrom_numpy\n\nfrom_numpy\nCreates aTensorfrom anumpy.ndarray.\nTensor\nnumpy.ndarray\nfrom_dlpack\n\nfrom_dlpack\nConverts a tensor from an external library into atorch.Tensor.\ntorch.Tensor\nfrombuffer\n\nfrombuffer\nCreates a 1-dimensionalTensorfrom an object that implements the Python buffer protocol.\nTensor\nzeros\n\nzeros\nReturns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.\nsize\nzeros_like\n\nzeros_like\nReturns a tensor filled with the scalar value0, with the same size asinput.\ninput\nones\n\nones\nReturns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.\nsize\nones_like\n\nones_like\nReturns a tensor filled with the scalar value1, with the same size asinput.\ninput\narange\n\narange\nReturns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.\n[start,end)\nstep\nrange\n\nrange\nReturns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.\nstart\nend\nstep\nlinspace\n\nlinspace\nCreates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.\nsteps\nstart\nend\nlogspace\n\nlogspace\nCreates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.\nsteps\nbase\neye\n\neye\nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.\nempty\n\nempty\nReturns a tensor filled with uninitialized data.\nempty_like\n\nempty_like\nReturns an uninitialized tensor with the same size asinput.\ninput\nempty_strided\n\nempty_strided\nCreates a tensor with the specifiedsizeandstrideand filled with undefined data.\nsize\nstride\nfull\n\nfull\nCreates a tensor of sizesizefilled withfill_value.\nsize\nfill_value\nfull_like\n\nfull_like\nReturns a tensor with the same size asinputfilled withfill_value.\ninput\nfill_value\nquantize_per_tensor\n\nquantize_per_tensor\nConverts a float tensor to a quantized tensor with given scale and zero point.\nquantize_per_channel\n\nquantize_per_channel\nConverts a float tensor to a per-channel quantized tensor with given scales and zero points.\ndequantize\n\ndequantize\nReturns an fp32 Tensor by dequantizing a quantized Tensor\ncomplex\n\ncomplex\nConstructs a complex tensor with its real part equal torealand its imaginary part equal toimag.\nreal\nimag\npolar\n\npolar\nConstructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.\nabs\nangle\nheaviside\n\nheaviside\nComputes the Heaviside step function for each element ininput.\ninput\n\n## Indexing, Slicing, Joining, Mutating Ops#\n\nadjoint\n\nadjoint\nReturns a view of the tensor conjugated and with the last two dimensions transposed.\nargwhere\n\nargwhere\nReturns a tensor containing the indices of all non-zero elements ofinput.\ninput\ncat\n\ncat\nConcatenates the given sequence of tensors intensorsin the given dimension.\ntensors\nconcat\n\nconcat\nAlias oftorch.cat().\ntorch.cat()\nconcatenate\n\nconcatenate\nAlias oftorch.cat().\ntorch.cat()\nconj\n\nconj\nReturns a view ofinputwith a flipped conjugate bit.\ninput\nchunk\n\nchunk\nAttempts to split a tensor into the specified number of chunks.\ndsplit\n\ndsplit\nSplitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.\ninput\nindices_or_sections\ncolumn_stack\n\ncolumn_stack\nCreates a new tensor by horizontally stacking the tensors intensors.\ntensors\ndstack\n\ndstack\nStack tensors in sequence depthwise (along third axis).\ngather\n\ngather\nGathers values along an axis specified bydim.\nhsplit\n\nhsplit\nSplitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.\ninput\nindices_or_sections\nhstack\n\nhstack\nStack tensors in sequence horizontally (column wise).\nindex_add\n\nindex_add\nSeeindex_add_()for function description.\nindex_add_()\nindex_copy\n\nindex_copy\nSeeindex_add_()for function description.\nindex_add_()\nindex_reduce\n\nindex_reduce\nSeeindex_reduce_()for function description.\nindex_reduce_()\nindex_select\n\nindex_select\nReturns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.\ninput\ndim\nindex\nmasked_select\n\nmasked_select\nReturns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.\ninput\nmask\nmovedim\n\nmovedim\nMoves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.\ninput\nsource\ndestination\nmoveaxis\n\nmoveaxis\nAlias fortorch.movedim().\ntorch.movedim()\nnarrow\n\nnarrow\nReturns a new tensor that is a narrowed version ofinputtensor.\ninput\nnarrow_copy\n\nnarrow_copy\nSame asTensor.narrow()except this returns a copy rather than shared storage.\nTensor.narrow()\nnonzero\n\nnonzero\n\npermute\n\npermute\nReturns a view of the original tensorinputwith its dimensions permuted.\ninput\nreshape\n\nreshape\nReturns a tensor with the same data and number of elements asinput, but with the specified shape.\ninput\nrow_stack\n\nrow_stack\nAlias oftorch.vstack().\ntorch.vstack()\nselect\n\nselect\nSlices theinputtensor along the selected dimension at the given index.\ninput\nscatter\n\nscatter\nOut-of-place version oftorch.Tensor.scatter_()\ntorch.Tensor.scatter_()\ndiagonal_scatter\n\ndiagonal_scatter\nEmbeds the values of thesrctensor intoinputalong the diagonal elements ofinput, with respect todim1anddim2.\nsrc\ninput\ninput\ndim1\ndim2\nselect_scatter\n\nselect_scatter\nEmbeds the values of thesrctensor intoinputat the given index.\nsrc\ninput\nslice_scatter\n\nslice_scatter\nEmbeds the values of thesrctensor intoinputat the given dimension.\nsrc\ninput\nscatter_add\n\nscatter_add\nOut-of-place version oftorch.Tensor.scatter_add_()\ntorch.Tensor.scatter_add_()\nscatter_reduce\n\nscatter_reduce\nOut-of-place version oftorch.Tensor.scatter_reduce_()\ntorch.Tensor.scatter_reduce_()\nsegment_reduce\n\nsegment_reduce\nPerform a segment reduction operation on the input tensor along the specified axis.\nsplit\n\nsplit\nSplits the tensor into chunks.\nsqueeze\n\nsqueeze\nReturns a tensor with all specified dimensions ofinputof size1removed.\ninput\nstack\n\nstack\nConcatenates a sequence of tensors along a new dimension.\nswapaxes\n\nswapaxes\nAlias fortorch.transpose().\ntorch.transpose()\nswapdims\n\nswapdims\nAlias fortorch.transpose().\ntorch.transpose()\nt\n\nt\nExpectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.\ninput\ntake\n\ntake\nReturns a new tensor with the elements ofinputat the given indices.\ninput\ntake_along_dim\n\ntake_along_dim\nSelects values frominputat the 1-dimensional indices fromindicesalong the givendim.\ninput\nindices\ndim\ntensor_split\n\ntensor_split\nSplits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.\ninput\ndim\nindices_or_sections\ntile\n\ntile\nConstructs a tensor by repeating the elements ofinput.\ninput\ntranspose\n\ntranspose\nReturns a tensor that is a transposed version ofinput.\ninput\nunbind\n\nunbind\nRemoves a tensor dimension.\nunravel_index\n\nunravel_index\nConverts a tensor of flat indices into a tuple of coordinate tensors that index into an arbitrary tensor of the specified shape.\nunsqueeze\n\nunsqueeze\nReturns a new tensor with a dimension of size one inserted at the specified position.\nvsplit\n\nvsplit\nSplitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.\ninput\nindices_or_sections\nvstack\n\nvstack\nStack tensors in sequence vertically (row wise).\nwhere\n\nwhere\nReturn a tensor of elements selected from eitherinputorother, depending oncondition.\ninput\nother\ncondition\n\n## Accelerators#\n\nWithin the PyTorch repo, we define an \u201cAccelerator\u201d as atorch.devicethat is being used\nalongside a CPU to speed up computation. These device use an asynchronous execution scheme,\nusingtorch.Streamandtorch.Eventas their main way to perform synchronization.\nWe also assume that only one such accelerator can be available at once on a given host. This allows\nus to use the current accelerator as the default device for relevant concepts such as pinned memory,\nStream device_type, FSDP, etc.\ntorch.device\ntorch.Stream\ntorch.Event\nAs of today, accelerator devices are (in no particular order)\u201cCUDA\u201d,\u201cMTIA\u201d,\u201cXPU\u201d,\u201cMPS\u201d, \u201cHPU\u201d, and PrivateUse1 (many device not in the PyTorch repo itself).\nMany tools in the PyTorch Ecosystem use fork to create subprocesses (for example dataloading\nor intra-op parallelism), it is thus important to delay as much as possible any\noperation that would prevent further forks. This is especially important here as most accelerator\u2019s initialization has such effect.\nIn practice, you should keep in mind that checkingtorch.accelerator.current_accelerator()is a compile-time check by default, it is thus always fork-safe.\nOn the contrary, passing thecheck_available=Trueflag to this function or callingtorch.accelerator.is_available()will usually prevent later fork.\ntorch.accelerator.current_accelerator()\ncheck_available=True\ntorch.accelerator.is_available()\nSome backends provide an experimental opt-in option to make the runtime availability\ncheck fork-safe. When using the CUDA devicePYTORCH_NVML_BASED_CUDA_CHECK=1can be\nused for example.\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nStream\n\nStream\nAn in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.\nEvent\n\nEvent\nQuery and record Stream status to identify or control dependencies across Stream and measure timing.\n\n## Generators#\n\nGenerator\n\nGenerator\nCreates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.\n\n## Random sampling#\n\nseed\n\nseed\nSets the seed for generating random numbers to a non-deterministic random number on all devices.\nmanual_seed\n\nmanual_seed\nSets the seed for generating random numbers on all devices.\ninitial_seed\n\ninitial_seed\nReturns the initial seed for generating random numbers as a Pythonlong.\nget_rng_state\n\nget_rng_state\nReturns the random number generator state as atorch.ByteTensor.\nset_rng_state\n\nset_rng_state\nSets the random number generator state.\nbernoulli\n\nbernoulli\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\nmultinomial\n\nmultinomial\nReturns a tensor where each row containsnum_samplesindices sampled from the multinomial (a stricter definition would be multivariate, refer totorch.distributions.multinomial.Multinomialfor more details) probability distribution located in the corresponding row of tensorinput.\nnum_samples\ntorch.distributions.multinomial.Multinomial\ninput\nnormal\n\nnormal\nReturns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.\npoisson\n\npoisson\nReturns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,\ninput\ninput\nrand\n\nrand\nReturns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)\nrand_like\n\nrand_like\nReturns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).\ninput\nrandint\n\nrandint\nReturns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).\nlow\nhigh\nrandint_like\n\nrandint_like\nReturns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).\ninput\nlow\nhigh\nrandn\n\nrandn\nReturns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).\nrandn_like\n\nrandn_like\nReturns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.\ninput\nrandperm\n\nrandperm\nReturns a random permutation of integers from0ton-1.\n0\nn-1\n\n## In-place random sampling#\n\nThere are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:\ntorch.Tensor.bernoulli_()- in-place version oftorch.bernoulli()\ntorch.Tensor.bernoulli_()\ntorch.bernoulli()\ntorch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution\ntorch.Tensor.cauchy_()\ntorch.Tensor.exponential_()- numbers drawn from the exponential distribution\ntorch.Tensor.exponential_()\ntorch.Tensor.geometric_()- elements drawn from the geometric distribution\ntorch.Tensor.geometric_()\ntorch.Tensor.log_normal_()- samples from the log-normal distribution\ntorch.Tensor.log_normal_()\ntorch.Tensor.normal_()- in-place version oftorch.normal()\ntorch.Tensor.normal_()\ntorch.normal()\ntorch.Tensor.random_()- numbers sampled from the discrete uniform distribution\ntorch.Tensor.random_()\ntorch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution\ntorch.Tensor.uniform_()\n\n## Quasi-random sampling#\n\nquasirandom.SobolEngine\nquasirandom.SobolEngine\nThetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences.\ntorch.quasirandom.SobolEngine\n\n## Serialization#\n\nsave\n\nsave\nSaves an object to a disk file.\nload\n\nload\nLoads an object saved withtorch.save()from a file.\ntorch.save()\n\n## Parallelism#\n\nget_num_threads\n\nget_num_threads\nReturns the number of threads used for parallelizing CPU operations\nset_num_threads\n\nset_num_threads\nSets the number of threads used for intraop parallelism on CPU.\nget_num_interop_threads\n\nget_num_interop_threads\nReturns the number of threads used for inter-op parallelism on CPU (e.g.\nset_num_interop_threads\n\nset_num_interop_threads\nSets the number of threads used for interop parallelism (e.g.\n\n## Locally disabling gradient computation#\n\nThe context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\ngradient computation. SeeLocally disabling gradient computationfor more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using thethreadingmodule, etc.\ntorch.no_grad()\ntorch.enable_grad()\ntorch.set_grad_enabled()\nthreading\nExamples:\n\n```python\n>>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n\n```\n\nno_grad\n\nno_grad\nContext-manager that disables gradient calculation.\nenable_grad\n\nenable_grad\nContext-manager that enables gradient calculation.\nautograd.grad_mode.set_grad_enabled\nautograd.grad_mode.set_grad_enabled\nContext-manager that sets gradient calculation on or off.\nis_grad_enabled\n\nis_grad_enabled\nReturns True if grad mode is currently enabled.\nautograd.grad_mode.inference_mode\nautograd.grad_mode.inference_mode\nContext manager that enables or disables inference mode.\nis_inference_mode_enabled\n\nis_inference_mode_enabled\nReturns True if inference mode is currently enabled.\n\n## Math operations#\n\n\n## Constants#\n\ninf\ninf\nA floating-point positive infinity. Alias formath.inf.\nmath.inf\nnan\nnan\nA floating-point \u201cnot a number\u201d value. This value is not a legal number. Alias formath.nan.\nmath.nan\n\n## Pointwise Ops#\n\nabs\n\nabs\nComputes the absolute value of each element ininput.\ninput\nabsolute\n\nabsolute\nAlias fortorch.abs()\ntorch.abs()\nacos\n\nacos\nComputes the inverse cosine of each element ininput.\ninput\narccos\n\narccos\nAlias fortorch.acos().\ntorch.acos()\nacosh\n\nacosh\nReturns a new tensor with the inverse hyperbolic cosine of the elements ofinput.\ninput\narccosh\n\narccosh\nAlias fortorch.acosh().\ntorch.acosh()\nadd\n\nadd\nAddsother, scaled byalpha, toinput.\nother\nalpha\ninput\naddcdiv\n\naddcdiv\nPerforms the element-wise division oftensor1bytensor2, multiplies the result by the scalarvalueand adds it toinput.\ntensor1\ntensor2\nvalue\ninput\naddcmul\n\naddcmul\nPerforms the element-wise multiplication oftensor1bytensor2, multiplies the result by the scalarvalueand adds it toinput.\ntensor1\ntensor2\nvalue\ninput\nangle\n\nangle\nComputes the element-wise angle (in radians) of the giveninputtensor.\ninput\nasin\n\nasin\nReturns a new tensor with the arcsine of the elements ofinput.\ninput\narcsin\n\narcsin\nAlias fortorch.asin().\ntorch.asin()\nasinh\n\nasinh\nReturns a new tensor with the inverse hyperbolic sine of the elements ofinput.\ninput\narcsinh\n\narcsinh\nAlias fortorch.asinh().\ntorch.asinh()\natan\n\natan\nReturns a new tensor with the arctangent of the elements ofinput.\ninput\narctan\n\narctan\nAlias fortorch.atan().\ntorch.atan()\natanh\n\natanh\nReturns a new tensor with the inverse hyperbolic tangent of the elements ofinput.\ninput\narctanh\n\narctanh\nAlias fortorch.atanh().\ntorch.atanh()\natan2\n\natan2\nElement-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.\narctan2\n\narctan2\nAlias fortorch.atan2().\ntorch.atan2()\nbitwise_not\n\nbitwise_not\nComputes the bitwise NOT of the given input tensor.\nbitwise_and\n\nbitwise_and\nComputes the bitwise AND ofinputandother.\ninput\nother\nbitwise_or\n\nbitwise_or\nComputes the bitwise OR ofinputandother.\ninput\nother\nbitwise_xor\n\nbitwise_xor\nComputes the bitwise XOR ofinputandother.\ninput\nother\nbitwise_left_shift\n\nbitwise_left_shift\nComputes the left arithmetic shift ofinputbyotherbits.\ninput\nother\nbitwise_right_shift\n\nbitwise_right_shift\nComputes the right arithmetic shift ofinputbyotherbits.\ninput\nother\nceil\n\nceil\nReturns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.\ninput\nclamp\n\nclamp\nClamps all elements ininputinto the range[min,max].\ninput\nmin\nmax\nclip\n\nclip\nAlias fortorch.clamp().\ntorch.clamp()\nconj_physical\n\nconj_physical\nComputes the element-wise conjugate of the giveninputtensor.\ninput\ncopysign\n\ncopysign\nCreate a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.\ninput\nother\ncos\n\ncos\nReturns a new tensor with the cosine  of the elements ofinput.\ninput\ncosh\n\ncosh\nReturns a new tensor with the hyperbolic cosine  of the elements ofinput.\ninput\ndeg2rad\n\ndeg2rad\nReturns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.\ninput\ndiv\n\ndiv\nDivides each element of the inputinputby the corresponding element ofother.\ninput\nother\ndivide\n\ndivide\nAlias fortorch.div().\ntorch.div()\ndigamma\n\ndigamma\nAlias fortorch.special.digamma().\ntorch.special.digamma()\nerf\n\nerf\nAlias fortorch.special.erf().\ntorch.special.erf()\nerfc\n\nerfc\nAlias fortorch.special.erfc().\ntorch.special.erfc()\nerfinv\n\nerfinv\nAlias fortorch.special.erfinv().\ntorch.special.erfinv()\nexp\n\nexp\nReturns a new tensor with the exponential of the elements of the input tensorinput.\ninput\nexp2\n\nexp2\nAlias fortorch.special.exp2().\ntorch.special.exp2()\nexpm1\n\nexpm1\nAlias fortorch.special.expm1().\ntorch.special.expm1()\nfake_quantize_per_channel_affine\n\nfake_quantize_per_channel_affine\nReturns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.\ninput\nscale\nzero_point\nquant_min\nquant_max\naxis\nfake_quantize_per_tensor_affine\n\nfake_quantize_per_tensor_affine\nReturns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.\ninput\nscale\nzero_point\nquant_min\nquant_max\nfix\n\nfix\nAlias fortorch.trunc()\ntorch.trunc()\nfloat_power\n\nfloat_power\nRaisesinputto the power ofexponent, elementwise, in double precision.\ninput\nexponent\nfloor\n\nfloor\nReturns a new tensor with the floor of the elements ofinput, the largest integer less than or equal to each element.\ninput\nfloor_divide\n\nfloor_divide\n\nfmod\n\nfmod\nApplies C++'sstd::fmodentrywise.\nfrac\n\nfrac\nComputes the fractional portion of each element ininput.\ninput\nfrexp\n\nfrexp\nDecomposesinputinto mantissa and exponent tensors such thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent.\ninput\ngradient\n\ngradient\nEstimates the gradient of a functiong:Rn\u2192Rg : \\mathbb{R}^n \\rightarrow \\mathbb{R}g:Rn\u2192Rin one or more dimensions using thesecond-order accurate central differences methodand either first or second order estimates at the boundaries.\nimag\n\nimag\nReturns a new tensor containing imaginary values of theselftensor.\nself\nldexp\n\nldexp\nMultipliesinputby 2 **other.\ninput\nother\nlerp\n\nlerp\nDoes a linear interpolation of two tensorsstart(given byinput) andendbased on a scalar or tensorweightand returns the resultingouttensor.\nstart\ninput\nend\nweight\nout\nlgamma\n\nlgamma\nComputes the natural logarithm of the absolute value of the gamma function oninput.\ninput\nlog\n\nlog\nReturns a new tensor with the natural logarithm of the elements ofinput.\ninput\nlog10\n\nlog10\nReturns a new tensor with the logarithm to the base 10 of the elements ofinput.\ninput\nlog1p\n\nlog1p\nReturns a new tensor with the natural logarithm of (1 +input).\ninput\nlog2\n\nlog2\nReturns a new tensor with the logarithm to the base 2 of the elements ofinput.\ninput\nlogaddexp\n\nlogaddexp\nLogarithm of the sum of exponentiations of the inputs.\nlogaddexp2\n\nlogaddexp2\nLogarithm of the sum of exponentiations of the inputs in base-2.\nlogical_and\n\nlogical_and\nComputes the element-wise logical AND of the given input tensors.\nlogical_not\n\nlogical_not\nComputes the element-wise logical NOT of the given input tensor.\nlogical_or\n\nlogical_or\nComputes the element-wise logical OR of the given input tensors.\nlogical_xor\n\nlogical_xor\nComputes the element-wise logical XOR of the given input tensors.\nlogit\n\nlogit\nAlias fortorch.special.logit().\ntorch.special.logit()\nhypot\n\nhypot\nGiven the legs of a right triangle, return its hypotenuse.\ni0\n\ni0\nAlias fortorch.special.i0().\ntorch.special.i0()\nigamma\n\nigamma\nAlias fortorch.special.gammainc().\ntorch.special.gammainc()\nigammac\n\nigammac\nAlias fortorch.special.gammaincc().\ntorch.special.gammaincc()\nmul\n\nmul\nMultipliesinputbyother.\ninput\nother\nmultiply\n\nmultiply\nAlias fortorch.mul().\ntorch.mul()\nmvlgamma\n\nmvlgamma\nAlias fortorch.special.multigammaln().\ntorch.special.multigammaln()\nnan_to_num\n\nnan_to_num\nReplacesNaN, positive infinity, and negative infinity values ininputwith the values specified bynan,posinf, andneginf, respectively.\nNaN\ninput\nnan\nposinf\nneginf\nneg\n\nneg\nReturns a new tensor with the negative of the elements ofinput.\ninput\nnegative\n\nnegative\nAlias fortorch.neg()\ntorch.neg()\nnextafter\n\nnextafter\nReturn the next floating-point value afterinputtowardsother, elementwise.\ninput\nother\npolygamma\n\npolygamma\nAlias fortorch.special.polygamma().\ntorch.special.polygamma()\npositive\n\npositive\nReturnsinput.\ninput\npow\n\npow\nTakes the power of each element ininputwithexponentand returns a tensor with the result.\ninput\nexponent\nquantized_batch_norm\n\nquantized_batch_norm\nApplies batch normalization on a 4D (NCHW) quantized tensor.\nquantized_max_pool1d\n\nquantized_max_pool1d\nApplies a 1D max pooling over an input quantized tensor composed of several input planes.\nquantized_max_pool2d\n\nquantized_max_pool2d\nApplies a 2D max pooling over an input quantized tensor composed of several input planes.\nrad2deg\n\nrad2deg\nReturns a new tensor with each of the elements ofinputconverted from angles in radians to degrees.\ninput\nreal\n\nreal\nReturns a new tensor containing real values of theselftensor.\nself\nreciprocal\n\nreciprocal\nReturns a new tensor with the reciprocal of the elements ofinput\ninput\nremainder\n\nremainder\nComputesPython's modulus operationentrywise.\nround\n\nround\nRounds elements ofinputto the nearest integer.\ninput\nrsqrt\n\nrsqrt\nReturns a new tensor with the reciprocal of the square-root of each of the elements ofinput.\ninput\nsigmoid\n\nsigmoid\nAlias fortorch.special.expit().\ntorch.special.expit()\nsign\n\nsign\nReturns a new tensor with the signs of the elements ofinput.\ninput\nsgn\n\nsgn\nThis function is an extension of torch.sign() to complex tensors.\nsignbit\n\nsignbit\nTests if each element ofinputhas its sign bit set or not.\ninput\nsin\n\nsin\nReturns a new tensor with the sine of the elements ofinput.\ninput\nsinc\n\nsinc\nAlias fortorch.special.sinc().\ntorch.special.sinc()\nsinh\n\nsinh\nReturns a new tensor with the hyperbolic sine of the elements ofinput.\ninput\nsoftmax\n\nsoftmax\nAlias fortorch.nn.functional.softmax().\ntorch.nn.functional.softmax()\nsqrt\n\nsqrt\nReturns a new tensor with the square-root of the elements ofinput.\ninput\nsquare\n\nsquare\nReturns a new tensor with the square of the elements ofinput.\ninput\nsub\n\nsub\nSubtractsother, scaled byalpha, frominput.\nother\nalpha\ninput\nsubtract\n\nsubtract\nAlias fortorch.sub().\ntorch.sub()\ntan\n\ntan\nReturns a new tensor with the tangent of the elements ofinput.\ninput\ntanh\n\ntanh\nReturns a new tensor with the hyperbolic tangent of the elements ofinput.\ninput\ntrue_divide\n\ntrue_divide\nAlias fortorch.div()withrounding_mode=None.\ntorch.div()\nrounding_mode=None\ntrunc\n\ntrunc\nReturns a new tensor with the truncated integer values of the elements ofinput.\ninput\nxlogy\n\nxlogy\nAlias fortorch.special.xlogy().\ntorch.special.xlogy()\n\n## Reduction Ops#\n\nargmax\n\nargmax\nReturns the indices of the maximum value of all elements in theinputtensor.\ninput\nargmin\n\nargmin\nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension\namax\n\namax\nReturns the maximum value of each slice of theinputtensor in the given dimension(s)dim.\ninput\ndim\namin\n\namin\nReturns the minimum value of each slice of theinputtensor in the given dimension(s)dim.\ninput\ndim\naminmax\n\naminmax\nComputes the minimum and maximum values of theinputtensor.\ninput\nall\n\nall\nTests if all elements ininputevaluate toTrue.\ninput\nany\n\nany\nTests if any element ininputevaluates toTrue.\ninput\nmax\n\nmax\nReturns the maximum value of all elements in theinputtensor.\ninput\nmin\n\nmin\nReturns the minimum value of all elements in theinputtensor.\ninput\ndist\n\ndist\nReturns the p-norm of (input-other)\ninput\nother\nlogsumexp\n\nlogsumexp\nReturns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.\ninput\ndim\nmean\n\nmean\n\nnanmean\n\nnanmean\nComputes the mean of allnon-NaNelements along the specified dimensions.\nmedian\n\nmedian\nReturns the median of the values ininput.\ninput\nnanmedian\n\nnanmedian\nReturns the median of the values ininput, ignoringNaNvalues.\ninput\nNaN\nmode\n\nmode\nReturns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often in that row, andindicesis the index location of each mode value found.\n(values,indices)\nvalues\ninput\ndim\nindices\nnorm\n\nnorm\nReturns the matrix norm or vector norm of a given tensor.\nnansum\n\nnansum\nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.\nprod\n\nprod\nReturns the product of all elements in theinputtensor.\ninput\nquantile\n\nquantile\nComputes the q-th quantiles of each row of theinputtensor along the dimensiondim.\ninput\ndim\nnanquantile\n\nnanquantile\nThis is a variant oftorch.quantile()that \"ignores\"NaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.\ntorch.quantile()\nNaN\nq\nNaN\ninput\nstd\n\nstd\nCalculates the standard deviation over the dimensions specified bydim.\ndim\nstd_mean\n\nstd_mean\nCalculates the standard deviation and mean over the dimensions specified bydim.\ndim\nsum\n\nsum\nReturns the sum of all elements in theinputtensor.\ninput\nunique\n\nunique\nReturns the unique elements of the input tensor.\nunique_consecutive\n\nunique_consecutive\nEliminates all but the first element from every consecutive group of equivalent elements.\nvar\n\nvar\nCalculates the variance over the dimensions specified bydim.\ndim\nvar_mean\n\nvar_mean\nCalculates the variance and mean over the dimensions specified bydim.\ndim\ncount_nonzero\n\ncount_nonzero\nCounts the number of non-zero values in the tensorinputalong the givendim.\ninput\ndim\nhash_tensor\n\nhash_tensor\nReturns a hash of all elements in theinputtensor.\ninput\n\n## Comparison Ops#\n\nallclose\n\nallclose\nThis function checks ifinputandothersatisfy the condition:\ninput\nother\nargsort\n\nargsort\nReturns the indices that sort a tensor along a given dimension in ascending order by value.\neq\n\neq\nComputes element-wise equality\nequal\n\nequal\nTrueif two tensors have the same size and elements,Falseotherwise.\nTrue\nFalse\nge\n\nge\nComputesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.\ngreater_equal\n\ngreater_equal\nAlias fortorch.ge().\ntorch.ge()\ngt\n\ngt\nComputesinput>other\\text{input} > \\text{other}input>otherelement-wise.\ngreater\n\ngreater\nAlias fortorch.gt().\ntorch.gt()\nisclose\n\nisclose\nReturns a new tensor with boolean elements representing if each element ofinputis \"close\" to the corresponding element ofother.\ninput\nother\nisfinite\n\nisfinite\nReturns a new tensor with boolean elements representing if each element isfiniteor not.\nisin\n\nisin\nTests if each element ofelementsis intest_elements.\nelements\ntest_elements\nisinf\n\nisinf\nTests if each element ofinputis infinite (positive or negative infinity) or not.\ninput\nisposinf\n\nisposinf\nTests if each element ofinputis positive infinity or not.\ninput\nisneginf\n\nisneginf\nTests if each element ofinputis negative infinity or not.\ninput\nisnan\n\nisnan\nReturns a new tensor with boolean elements representing if each element ofinputis NaN or not.\ninput\nisreal\n\nisreal\nReturns a new tensor with boolean elements representing if each element ofinputis real-valued or not.\ninput\nkthvalue\n\nkthvalue\nReturns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.\n(values,indices)\nvalues\nk\ninput\ndim\nle\n\nle\nComputesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.\nless_equal\n\nless_equal\nAlias fortorch.le().\ntorch.le()\nlt\n\nlt\nComputesinput<other\\text{input} < \\text{other}input<otherelement-wise.\nless\n\nless\nAlias fortorch.lt().\ntorch.lt()\nmaximum\n\nmaximum\nComputes the element-wise maximum ofinputandother.\ninput\nother\nminimum\n\nminimum\nComputes the element-wise minimum ofinputandother.\ninput\nother\nfmax\n\nfmax\nComputes the element-wise maximum ofinputandother.\ninput\nother\nfmin\n\nfmin\nComputes the element-wise minimum ofinputandother.\ninput\nother\nne\n\nne\nComputesinput\u2260other\\text{input} \\neq \\text{other}input\ue020=otherelement-wise.\nnot_equal\n\nnot_equal\nAlias fortorch.ne().\ntorch.ne()\nsort\n\nsort\nSorts the elements of theinputtensor along a given dimension in ascending order by value.\ninput\ntopk\n\ntopk\nReturns theklargest elements of the giveninputtensor along a given dimension.\nk\ninput\nmsort\n\nmsort\nSorts the elements of theinputtensor along its first dimension in ascending order by value.\ninput\n\n## Spectral Ops#\n\nstft\n\nstft\nShort-time Fourier transform (STFT).\nistft\n\nistft\nInverse short time Fourier Transform.\nbartlett_window\n\nbartlett_window\nBartlett window function.\nblackman_window\n\nblackman_window\nBlackman window function.\nhamming_window\n\nhamming_window\nHamming window function.\nhann_window\n\nhann_window\nHann window function.\nkaiser_window\n\nkaiser_window\nComputes the Kaiser window with window lengthwindow_lengthand shape parameterbeta.\nwindow_length\nbeta\n\n## Other Operations#\n\natleast_1d\n\natleast_1d\nReturns a 1-dimensional view of each input tensor with zero dimensions.\natleast_2d\n\natleast_2d\nReturns a 2-dimensional view of each input tensor with zero dimensions.\natleast_3d\n\natleast_3d\nReturns a 3-dimensional view of each input tensor with zero dimensions.\nbincount\n\nbincount\nCount the frequency of each value in an array of non-negative ints.\nblock_diag\n\nblock_diag\nCreate a block diagonal matrix from provided tensors.\nbroadcast_tensors\n\nbroadcast_tensors\nBroadcasts the given tensors according toBroadcasting semantics.\nbroadcast_to\n\nbroadcast_to\nBroadcastsinputto the shapeshape.\ninput\nshape\nbroadcast_shapes\n\nbroadcast_shapes\nSimilar tobroadcast_tensors()but for shapes.\nbroadcast_tensors()\nbucketize\n\nbucketize\nReturns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.\ninput\nboundaries\ncartesian_prod\n\ncartesian_prod\nDo cartesian product of the given sequence of tensors.\ncdist\n\ncdist\nComputes batched the p-norm distance between each pair of the two collections of row vectors.\nclone\n\nclone\nReturns a copy ofinput.\ninput\ncombinations\n\ncombinations\nCompute combinations of lengthrrrof the given tensor.\ncorrcoef\n\ncorrcoef\nEstimates the Pearson product-moment correlation coefficient matrix of the variables given by theinputmatrix, where rows are the variables and columns are the observations.\ninput\ncov\n\ncov\nEstimates the covariance matrix of the variables given by theinputmatrix, where rows are the variables and columns are the observations.\ninput\ncross\n\ncross\nReturns the cross product of vectors in dimensiondimofinputandother.\ndim\ninput\nother\ncummax\n\ncummax\nReturns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.\n(values,indices)\nvalues\ninput\ndim\ncummin\n\ncummin\nReturns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.\n(values,indices)\nvalues\ninput\ndim\ncumprod\n\ncumprod\nReturns the cumulative product of elements ofinputin the dimensiondim.\ninput\ndim\ncumsum\n\ncumsum\nReturns the cumulative sum of elements ofinputin the dimensiondim.\ninput\ndim\ndiag\n\ndiag\nIfinputis a vector (1-D tensor), then returns a 2-D square tensor\nIfinputis a vector (1-D tensor), then returns a 2-D square tensor\ninput\ndiag_embed\n\ndiag_embed\nCreates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.\ndim1\ndim2\ninput\ndiagflat\n\ndiagflat\nIfinputis a vector (1-D tensor), then returns a 2-D square tensor\nIfinputis a vector (1-D tensor), then returns a 2-D square tensor\ninput\ndiagonal\n\ndiagonal\nReturns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.\ninput\ndim1\ndim2\ndiff\n\ndiff\nComputes the n-th forward difference along the given dimension.\neinsum\n\neinsum\nSums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.\noperands\nflatten\n\nflatten\nFlattensinputby reshaping it into a one-dimensional tensor.\ninput\nflip\n\nflip\nReverse the order of an n-D tensor along given axis in dims.\nfliplr\n\nfliplr\nFlip tensor in the left/right direction, returning a new tensor.\nflipud\n\nflipud\nFlip tensor in the up/down direction, returning a new tensor.\nkron\n\nkron\nComputes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.\ninput\nother\nrot90\n\nrot90\nRotate an n-D tensor by 90 degrees in the plane specified by dims axis.\ngcd\n\ngcd\nComputes the element-wise greatest common divisor (GCD) ofinputandother.\ninput\nother\nhistc\n\nhistc\nComputes the histogram of a tensor.\nhistogram\n\nhistogram\nComputes a histogram of the values in a tensor.\nhistogramdd\n\nhistogramdd\nComputes a multi-dimensional histogram of the values in a tensor.\nmeshgrid\n\nmeshgrid\nCreates grids of coordinates specified by the 1D inputs inattr:tensors.\nlcm\n\nlcm\nComputes the element-wise least common multiple (LCM) ofinputandother.\ninput\nother\nlogcumsumexp\n\nlogcumsumexp\nReturns the logarithm of the cumulative summation of the exponentiation of elements ofinputin the dimensiondim.\ninput\ndim\nravel\n\nravel\nReturn a contiguous flattened tensor.\nrenorm\n\nrenorm\nReturns a tensor where each sub-tensor ofinputalong dimensiondimis normalized such that thep-norm of the sub-tensor is lower than the valuemaxnorm\ninput\ndim\nmaxnorm\nrepeat_interleave\n\nrepeat_interleave\nRepeat elements of a tensor.\nroll\n\nroll\nRoll the tensorinputalong the given dimension(s).\ninput\nsearchsorted\n\nsearchsorted\nFind the indices from theinnermostdimension ofsorted_sequencesuch that, if the corresponding values invalueswere inserted before the indices, when sorted, the order of the correspondinginnermostdimension withinsorted_sequencewould be preserved.\nsorted_sequence\nvalues\nsorted_sequence\ntensordot\n\ntensordot\nReturns a contraction of a and b over multiple dimensions.\ntrace\n\ntrace\nReturns the sum of the elements of the diagonal of the input 2-D matrix.\ntril\n\ntril\nReturns the lower triangular part of the matrix (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0.\ninput\nout\ntril_indices\n\ntril_indices\nReturns the indices of the lower triangular part of arow-by-colmatrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.\nrow\ncol\ntriu\n\ntriu\nReturns the upper triangular part of a matrix (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0.\ninput\nout\ntriu_indices\n\ntriu_indices\nReturns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.\nrow\ncol\nunflatten\n\nunflatten\nExpands a dimension of the input tensor over multiple dimensions.\nvander\n\nvander\nGenerates a Vandermonde matrix.\nview_as_real\n\nview_as_real\nReturns a view ofinputas a real tensor.\ninput\nview_as_complex\n\nview_as_complex\nReturns a view ofinputas a complex tensor.\ninput\nresolve_conj\n\nresolve_conj\nReturns a new tensor with materialized conjugation ifinput's conjugate bit is set toTrue, else returnsinput.\ninput\ninput\nresolve_neg\n\nresolve_neg\nReturns a new tensor with materialized negation ifinput's negative bit is set toTrue, else returnsinput.\ninput\ninput\n\n## BLAS and LAPACK Operations#\n\naddbmm\n\naddbmm\nPerforms a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).\nbatch1\nbatch2\naddmm\n\naddmm\nPerforms a matrix multiplication of the matricesmat1andmat2.\nmat1\nmat2\naddmv\n\naddmv\nPerforms a matrix-vector product of the matrixmatand the vectorvec.\nmat\nvec\naddr\n\naddr\nPerforms the outer-product of vectorsvec1andvec2and adds it to the matrixinput.\nvec1\nvec2\ninput\nbaddbmm\n\nbaddbmm\nPerforms a batch matrix-matrix product of matrices inbatch1andbatch2.\nbatch1\nbatch2\nbmm\n\nbmm\nPerforms a batch matrix-matrix product of matrices stored ininputandmat2.\ninput\nmat2\nchain_matmul\n\nchain_matmul\nReturns the matrix product of theNNN2-D tensors.\ncholesky\n\ncholesky\nComputes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.\ncholesky_inverse\n\ncholesky_inverse\nComputes the inverse of a complex Hermitian or real symmetric positive-definite matrix given its Cholesky decomposition.\ncholesky_solve\n\ncholesky_solve\nComputes the solution of a system of linear equations with complex Hermitian or real symmetric positive-definite lhs given its Cholesky decomposition.\ndot\n\ndot\nComputes the dot product of two 1D tensors.\ngeqrf\n\ngeqrf\nThis is a low-level function for calling LAPACK's geqrf directly.\nger\n\nger\nAlias oftorch.outer().\ntorch.outer()\ninner\n\ninner\nComputes the dot product for 1D tensors.\ninverse\n\ninverse\nAlias fortorch.linalg.inv()\ntorch.linalg.inv()\ndet\n\ndet\nAlias fortorch.linalg.det()\ntorch.linalg.det()\nlogdet\n\nlogdet\nCalculates log determinant of a square matrix or batches of square matrices.\nslogdet\n\nslogdet\nAlias fortorch.linalg.slogdet()\ntorch.linalg.slogdet()\nlu\n\nlu\nComputes the LU factorization of a matrix or batches of matricesA.\nA\nlu_solve\n\nlu_solve\nReturns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromlu_factor().\nlu_factor()\nlu_unpack\n\nlu_unpack\nUnpacks the LU decomposition returned bylu_factor()into theP, L, Umatrices.\nlu_factor()\nmatmul\n\nmatmul\nMatrix product of two tensors.\nmatrix_power\n\nmatrix_power\nAlias fortorch.linalg.matrix_power()\ntorch.linalg.matrix_power()\nmatrix_exp\n\nmatrix_exp\nAlias fortorch.linalg.matrix_exp().\ntorch.linalg.matrix_exp()\nmm\n\nmm\nPerforms a matrix multiplication of the matricesinputandmat2.\ninput\nmat2\nmv\n\nmv\nPerforms a matrix-vector product of the matrixinputand the vectorvec.\ninput\nvec\norgqr\n\norgqr\nAlias fortorch.linalg.householder_product().\ntorch.linalg.householder_product()\normqr\n\normqr\nComputes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.\nouter\n\nouter\nOuter product ofinputandvec2.\ninput\nvec2\npinverse\n\npinverse\nAlias fortorch.linalg.pinv()\ntorch.linalg.pinv()\nqr\n\nqr\nComputes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.\ninput\nsvd\n\nsvd\nComputes the singular value decomposition of either a matrix or batch of matricesinput.\ninput\nsvd_lowrank\n\nsvd_lowrank\nReturn the singular value decomposition(U,S,V)of a matrix, batches of matrices, or a sparse matrixAAAsuch thatA\u2248Udiag\u2061(S)VHA \\approx U \\operatorname{diag}(S) V^{\\text{H}}A\u2248Udiag(S)VH.\n(U,S,V)\npca_lowrank\n\npca_lowrank\nPerforms linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.\nlobpcg\n\nlobpcg\nFind the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.\ntrapz\n\ntrapz\nAlias fortorch.trapezoid().\ntorch.trapezoid()\ntrapezoid\n\ntrapezoid\nComputes thetrapezoidal rulealongdim.\ndim\ncumulative_trapezoid\n\ncumulative_trapezoid\nCumulatively computes thetrapezoidal rulealongdim.\nCumulatively computes thetrapezoidal rulealongdim.\ndim\ntriangular_solve\n\ntriangular_solve\nSolves a system of equations with a square upper or lower triangular invertible matrixAAAand multiple right-hand sidesbbb.\nvdot\n\nvdot\nComputes the dot product of two 1D vectors along a dimension.\n\n## Foreach Operations#\n\nWarning\nThis API is in beta and subject to future changes.\nForward-mode AD is not supported.\n_foreach_abs\n\n_foreach_abs\nApplytorch.abs()to each Tensor of the input list.\ntorch.abs()\n_foreach_abs_\n\n_foreach_abs_\nApplytorch.abs()to each Tensor of the input list.\ntorch.abs()\n_foreach_acos\n\n_foreach_acos\nApplytorch.acos()to each Tensor of the input list.\ntorch.acos()\n_foreach_acos_\n\n_foreach_acos_\nApplytorch.acos()to each Tensor of the input list.\ntorch.acos()\n_foreach_asin\n\n_foreach_asin\nApplytorch.asin()to each Tensor of the input list.\ntorch.asin()\n_foreach_asin_\n\n_foreach_asin_\nApplytorch.asin()to each Tensor of the input list.\ntorch.asin()\n_foreach_atan\n\n_foreach_atan\nApplytorch.atan()to each Tensor of the input list.\ntorch.atan()\n_foreach_atan_\n\n_foreach_atan_\nApplytorch.atan()to each Tensor of the input list.\ntorch.atan()\n_foreach_ceil\n\n_foreach_ceil\nApplytorch.ceil()to each Tensor of the input list.\ntorch.ceil()\n_foreach_ceil_\n\n_foreach_ceil_\nApplytorch.ceil()to each Tensor of the input list.\ntorch.ceil()\n_foreach_cos\n\n_foreach_cos\nApplytorch.cos()to each Tensor of the input list.\ntorch.cos()\n_foreach_cos_\n\n_foreach_cos_\nApplytorch.cos()to each Tensor of the input list.\ntorch.cos()\n_foreach_cosh\n\n_foreach_cosh\nApplytorch.cosh()to each Tensor of the input list.\ntorch.cosh()\n_foreach_cosh_\n\n_foreach_cosh_\nApplytorch.cosh()to each Tensor of the input list.\ntorch.cosh()\n_foreach_erf\n\n_foreach_erf\nApplytorch.erf()to each Tensor of the input list.\ntorch.erf()\n_foreach_erf_\n\n_foreach_erf_\nApplytorch.erf()to each Tensor of the input list.\ntorch.erf()\n_foreach_erfc\n\n_foreach_erfc\nApplytorch.erfc()to each Tensor of the input list.\ntorch.erfc()\n_foreach_erfc_\n\n_foreach_erfc_\nApplytorch.erfc()to each Tensor of the input list.\ntorch.erfc()\n_foreach_exp\n\n_foreach_exp\nApplytorch.exp()to each Tensor of the input list.\ntorch.exp()\n_foreach_exp_\n\n_foreach_exp_\nApplytorch.exp()to each Tensor of the input list.\ntorch.exp()\n_foreach_expm1\n\n_foreach_expm1\nApplytorch.expm1()to each Tensor of the input list.\ntorch.expm1()\n_foreach_expm1_\n\n_foreach_expm1_\nApplytorch.expm1()to each Tensor of the input list.\ntorch.expm1()\n_foreach_floor\n\n_foreach_floor\nApplytorch.floor()to each Tensor of the input list.\ntorch.floor()\n_foreach_floor_\n\n_foreach_floor_\nApplytorch.floor()to each Tensor of the input list.\ntorch.floor()\n_foreach_log\n\n_foreach_log\nApplytorch.log()to each Tensor of the input list.\ntorch.log()\n_foreach_log_\n\n_foreach_log_\nApplytorch.log()to each Tensor of the input list.\ntorch.log()\n_foreach_log10\n\n_foreach_log10\nApplytorch.log10()to each Tensor of the input list.\ntorch.log10()\n_foreach_log10_\n\n_foreach_log10_\nApplytorch.log10()to each Tensor of the input list.\ntorch.log10()\n_foreach_log1p\n\n_foreach_log1p\nApplytorch.log1p()to each Tensor of the input list.\ntorch.log1p()\n_foreach_log1p_\n\n_foreach_log1p_\nApplytorch.log1p()to each Tensor of the input list.\ntorch.log1p()\n_foreach_log2\n\n_foreach_log2\nApplytorch.log2()to each Tensor of the input list.\ntorch.log2()\n_foreach_log2_\n\n_foreach_log2_\nApplytorch.log2()to each Tensor of the input list.\ntorch.log2()\n_foreach_neg\n\n_foreach_neg\nApplytorch.neg()to each Tensor of the input list.\ntorch.neg()\n_foreach_neg_\n\n_foreach_neg_\nApplytorch.neg()to each Tensor of the input list.\ntorch.neg()\n_foreach_tan\n\n_foreach_tan\nApplytorch.tan()to each Tensor of the input list.\ntorch.tan()\n_foreach_tan_\n\n_foreach_tan_\nApplytorch.tan()to each Tensor of the input list.\ntorch.tan()\n_foreach_sin\n\n_foreach_sin\nApplytorch.sin()to each Tensor of the input list.\ntorch.sin()\n_foreach_sin_\n\n_foreach_sin_\nApplytorch.sin()to each Tensor of the input list.\ntorch.sin()\n_foreach_sinh\n\n_foreach_sinh\nApplytorch.sinh()to each Tensor of the input list.\ntorch.sinh()\n_foreach_sinh_\n\n_foreach_sinh_\nApplytorch.sinh()to each Tensor of the input list.\ntorch.sinh()\n_foreach_round\n\n_foreach_round\nApplytorch.round()to each Tensor of the input list.\ntorch.round()\n_foreach_round_\n\n_foreach_round_\nApplytorch.round()to each Tensor of the input list.\ntorch.round()\n_foreach_sqrt\n\n_foreach_sqrt\nApplytorch.sqrt()to each Tensor of the input list.\ntorch.sqrt()\n_foreach_sqrt_\n\n_foreach_sqrt_\nApplytorch.sqrt()to each Tensor of the input list.\ntorch.sqrt()\n_foreach_lgamma\n\n_foreach_lgamma\nApplytorch.lgamma()to each Tensor of the input list.\ntorch.lgamma()\n_foreach_lgamma_\n\n_foreach_lgamma_\nApplytorch.lgamma()to each Tensor of the input list.\ntorch.lgamma()\n_foreach_frac\n\n_foreach_frac\nApplytorch.frac()to each Tensor of the input list.\ntorch.frac()\n_foreach_frac_\n\n_foreach_frac_\nApplytorch.frac()to each Tensor of the input list.\ntorch.frac()\n_foreach_reciprocal\n\n_foreach_reciprocal\nApplytorch.reciprocal()to each Tensor of the input list.\ntorch.reciprocal()\n_foreach_reciprocal_\n\n_foreach_reciprocal_\nApplytorch.reciprocal()to each Tensor of the input list.\ntorch.reciprocal()\n_foreach_sigmoid\n\n_foreach_sigmoid\nApplytorch.sigmoid()to each Tensor of the input list.\ntorch.sigmoid()\n_foreach_sigmoid_\n\n_foreach_sigmoid_\nApplytorch.sigmoid()to each Tensor of the input list.\ntorch.sigmoid()\n_foreach_trunc\n\n_foreach_trunc\nApplytorch.trunc()to each Tensor of the input list.\ntorch.trunc()\n_foreach_trunc_\n\n_foreach_trunc_\nApplytorch.trunc()to each Tensor of the input list.\ntorch.trunc()\n_foreach_zero_\n\n_foreach_zero_\nApplytorch.zero()to each Tensor of the input list.\ntorch.zero()\n\n## Utilities#\n\ncompiled_with_cxx11_abi\n\ncompiled_with_cxx11_abi\nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\nresult_type\n\nresult_type\nReturns thetorch.dtypethat would result from performing an arithmetic operation on the provided input tensors.\ntorch.dtype\ncan_cast\n\ncan_cast\nDetermines if a type conversion is allowed under PyTorch casting rules described in the type promotiondocumentation.\npromote_types\n\npromote_types\nReturns thetorch.dtypewith the smallest size and scalar kind that is not smaller nor of lower kind than eithertype1ortype2.\ntorch.dtype\nuse_deterministic_algorithms\n\nuse_deterministic_algorithms\nSets whether PyTorch operations must use \"deterministic\" algorithms.\nare_deterministic_algorithms_enabled\n\nare_deterministic_algorithms_enabled\nReturns True if the global deterministic flag is turned on.\nis_deterministic_algorithms_warn_only_enabled\n\nis_deterministic_algorithms_warn_only_enabled\nReturns True if the global deterministic flag is set to warn only.\nset_deterministic_debug_mode\n\nset_deterministic_debug_mode\nSets the debug mode for deterministic operations.\nget_deterministic_debug_mode\n\nget_deterministic_debug_mode\nReturns the current value of the debug mode for deterministic operations.\nset_float32_matmul_precision\n\nset_float32_matmul_precision\nSets the internal precision of float32 matrix multiplications.\nget_float32_matmul_precision\n\nget_float32_matmul_precision\nReturns the current value of float32 matrix multiplication precision.\nset_warn_always\n\nset_warn_always\nWhen this flag is False (default) then some PyTorch warnings may only appear once per process.\nget_device_module\n\nget_device_module\nReturns the module associated with a given device(e.g., torch.device('cuda'), \"mtia:0\", \"xpu\", ...).\nis_warn_always_enabled\n\nis_warn_always_enabled\nReturns True if the global warn_always flag is turned on.\nvmap\n\nvmap\nvmap is the vectorizing map;vmap(func)returns a new function that mapsfuncover some dimension of the inputs.\nvmap(func)\nfunc\n_assert\n\n_assert\nA wrapper around Python's assert which is symbolically traceable.\n\n## Symbolic Numbers#\n\nLike an int (including magic methods), but redirects all operations on the\nwrapped node. This is used in particular to symbolically record operations\nin the symbolic shape workflow.\nRepresent this int as an exact integer ratio\ntuple[\u2018SymInt\u2019,int]\nLike a float (including magic methods), but redirects all operations on the\nwrapped node. This is used in particular to symbolically record operations\nin the symbolic shape workflow.\nRepresent this float as an exact integer ratio\ntuple[int,int]\nReturns the complex conjugate of the float.\nSymFloat\nReturns the hexadecimal representation of the float.\nstr\nReturn True if the float is an integer.\nLike a bool (including magic methods), but redirects all operations on the\nwrapped node. This is used in particular to symbolically record operations\nin the symbolic shape workflow.\nUnlike regular bools, regular boolean operators will force extra guards instead\nof symbolically evaluate.  Use the bitwise operators instead to handle this.\nsym_float\n\nsym_float\nSymInt-aware utility for float casting.\nsym_fresh_size\n\nsym_fresh_size\n\nsym_int\n\nsym_int\nSymInt-aware utility for int casting.\nsym_max\n\nsym_max\nSymInt-aware utility for max which avoids branching on a < b.\nsym_min\n\nsym_min\nSymInt-aware utility for min().\nsym_not\n\nsym_not\nSymInt-aware utility for logical negation.\nsym_ite\n\nsym_ite\nSymInt-aware utility for ternary operator (tifbelsef.)\ntifbelsef\nsym_sum\n\nsym_sum\nN-ary add which is faster to compute for long lists than iterated binary addition.\n\n## Export Path#\n\nWarning\nThis feature is a prototype and may have compatibility breaking changes in the future.\nexport\ngenerated/exportdb/index\n\n## Control Flow#\n\nWarning\nThis feature is a prototype and may have compatibility breaking changes in the future.\ncond\n\ncond\nConditionally appliestrue_fnorfalse_fn.\n\n## Optimizations#\n\ncompile\n\ncompile\nOptimizes given model/function using TorchDynamo and specified backend.\ntorch.compile documentation\n\n## Operator Tags#\n\nMembers:\ncore\ncudagraph_unsafe\ndata_dependent_output\ndynamic_output_shape\nflexible_layout\ngenerated\ninplace_view\nmaybe_aliasing_or_mutating\nneeds_contiguous_strides\nneeds_exact_strides\nneeds_fixed_stride_order\nnondeterministic_bitwise\nnondeterministic_seeded\npointwise\npt2_compliant_tag\nview_copy",
  "url": "https://pytorch.org/docs/stable/torch.html",
  "metadata": {
    "section": null,
    "issue_number": null,
    "labels": null,
    "answer_author": null
  }
}
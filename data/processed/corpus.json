[
  {
    "doc_id": "ac37897fc21c8e0d8924352fd7e0862a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/numa.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e3ee9617a32576473b44bacc74117e89",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/amp_examples.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2e32df37bbdaa74a6260cbbaf57ff057",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/gradcheck.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4a7cffddc6a5131265ad4d34d3246253",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/func.ux_limitations.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "fc1f09c8f8d065ffc7dac4d01021bb3f",
    "source": "pytorch_docs",
    "title": "LibTorch Stable ABI \u2014 PyTorch 2.9 documentation",
    "text": "\n## LibTorch Stable ABI#\n\nCreated On: Mar 17, 2025 | Last Updated On: Nov 07, 2025\n\n## Overview#\n\nThe LibTorch Stable ABI (Application Binary Interface) provides a limited interface for extending PyTorch functionality without being tightly coupled to specific PyTorch versions. This enables the development of custom operators and extensions that remain compatible across PyTorch releases. This limited set of APIs is not intended to replace existing LibTorch, but rather to provide a stable foundation for a majority of custom extension use cases. If there is any API you would like to see added to the stable ABI, please file a request through anew issue on the PyTorch repo.\nThe limited stable ABI consists of three main components:\nStable C headers- Low-level C API implemented by libtorch (primarilytorch/csrc/inductor/aoti_torch/c/shim.h)\ntorch/csrc/inductor/aoti_torch/c/shim.h\nHeader-only C++ library- Standalone utilities implemented in only headers such that there is no dependence on libtorch (torch/headeronly/*)\ntorch/headeronly/*\nStable C++ wrappers- High-level C++ convenience wrappers (torch/csrc/stable/*)\ntorch/csrc/stable/*\nWe discuss each of these in detail\n\n## torch/headeronly#\n\ntorch/headeronly\nThe inlined C++ headers living intorch/headeronlyare completely decoupled from LibTorch. The headers consist of certain utilities that might be familiar to custom extension writers. For example, thec10::ScalarTypeenum lives here astorch::headeronly::ScalarType, as well as a libtorch-independent version ofTORCH_CHECKthat isSTD_TORCH_CHECK. You can trust all APIs in thetorch::headeronlynamespace to not depend onlibtorch.so. These APIs are also globally listed intorch/header_only_apis.txt.\ntorch/headeronly\nc10::ScalarType\ntorch::headeronly::ScalarType\nTORCH_CHECK\nSTD_TORCH_CHECK\ntorch::headeronly\nlibtorch.so\n\n## torch/csrc/stable#\n\ntorch/csrc/stable\nThis is a set of inlined C++ headers that provide wrappers around the C API that handle the rough edges\ndiscussed below.\nIt consists of\ntorch/csrc/stable/library.h: Provides a stable version of TORCH_LIBRARY and similar macros.\ntorch/csrc/stable/tensor_struct.h: Provides torch::stable::Tensor, a stable version of at::Tensor.\ntorch/csrc/stable/ops.h: Provides a stable interface for calling ATen ops fromnative_functions.yaml.\nnative_functions.yaml\ntorch/csrc/stable/accelerator.h: Provides a stable interface for device-generic objects and APIs\n(e.g.getCurrentStream,DeviceGuard).\ngetCurrentStream\nDeviceGuard\nWe are continuing to improve coverage in ourtorch/csrc/stableAPIs. Please file an issue if you\u2019d like to see support for particular APIs in your custom extension.\ntorch/csrc/stable\n\n## Stable C headers#\n\nThe stable C headers started by AOTInductor form the foundation of the stable ABI. Presently, the available C headers include:\ntorch/csrc/inductor/aoti_torch/c/shim.h: Includes C-style shim APIs for commonly used regarding Tensors, dtypes, CUDA, and the like.\ntorch/csrc/inductor/aoti_torch/generated/c_shim_aten.h: Includes C-style shim APIs for ATen ops fromnative_functions.yaml(e.g.aoti_torch_aten_new_empty).\nnative_functions.yaml\naoti_torch_aten_new_empty\ntorch/csrc/inductor/aoti_torch/generated/c_shim_*.h: Includes C-style shim APIs for specific backend kernels dispatched fromnative_functions.yaml(e.g.aoti_torch_cuda_pad). These APIs should only be used for the specific backend they are named after (e.g.aoti_torch_cuda_padshould only be used within CUDA kernels), as they opt out of the dispatcher.\nnative_functions.yaml\naoti_torch_cuda_pad\naoti_torch_cuda_pad\ntorch/csrc/stable/c/shim.h: We are building out more ABIs to logically live intorch/csrc/stable/cinstead of continuing the AOTI naming that no longer makes sense for our general use case.\ntorch/csrc/stable/c\nThese headers are promised to be ABI stable across releases and adhere to a stronger backwards compatibility policy than LibTorch. Specifically, we promise not to modify them for at least 2 years after they are released. However, this isuse at your own risk. For example, users must handle the memory lifecycle of objects returned by certain APIs. Further, the stack-based APIs discussed below which allow the user to call into the PyTorch dispatcher do not provide strong guarantees on forward and backward compatibility of the underlying op that is called.\nUnless absolutely necessary, we recommend the high-level C++ API intorch/csrc/stablewhich will handle all the rough edges of the C API for the user.\ntorch/csrc/stable\n\n## Migrating your kernel to the LibTorch stable ABI#\n\nIf you\u2019d like your kernel to be ABI stable with LibTorch, meaning you\u2019d the ability to build for one version and run on another, your kernel must only use the limited stable ABI. This following section goes through some steps of migrating an existing kernel and APIs we imagine you would need to swap over.\nFirstly, instead of registering kernels throughTORCH_LIBRARY, LibTorch ABI stable kernels must be registered viaSTABLE_TORCH_LIBRARY. Note that, for the time being, implementations registered viaSTABLE_TORCH_LIBRARYmust be boxed unlikeTORCH_LIBRARY. See the simple example below or our docs onStack-based APIsfor more details. For kernels that are registered viapybind, before using the stable ABI, it would be useful to migrate to register them viaTORCH_LIBRARY.\nTORCH_LIBRARY\nSTABLE_TORCH_LIBRARY\nSTABLE_TORCH_LIBRARY\nTORCH_LIBRARY\npybind\nTORCH_LIBRARY\nWhile previously your kernels might have included APIs from<torch/*.h>(for example,<torch/all.h>), they are now limited to including from the 3 categories of headers mentioned above (torch/csrc/stable/*.h,torch/headeronly/*.hand the stable C headers). This means that your extension should no longer use any utilities from theat::orc10::namespaces but instead use their replacements intorch::stableandtorch::headeronly. To provide a couple examples of the necessary migrations:\n<torch/*.h>\n<torch/all.h>\ntorch/csrc/stable/*.h\ntorch/headeronly/*.h\nat::\nc10::\ntorch::stable\ntorch::headeronly\nall uses ofat::Tensormust be replaced withtorch::stable::Tensor\nat::Tensor\ntorch::stable::Tensor\nall uses ofTORCH_CHECKmust be replaced withSTD_TORCH_CHECK\nTORCH_CHECK\nSTD_TORCH_CHECK\nall uses ofat::kCUDAmust be replaced withtorch::headeronly::kCUDAetc.\nat::kCUDA\ntorch::headeronly::kCUDA\nnative functions such asat::padmust be replaced withtorch::stable::pad\nat::pad\ntorch::stable::pad\nnative functions that are called as Tensor methods (e.g.,Tensor.pad) must be replaced with the ATen variant throughtorch::stable::pad.\nTensor.pad\ntorch::stable::pad\nAs mentioned above, the LibTorch stable ABI is still under development. If there is any API or feature you would like to see added to the stable ABI/torch::headeronly/torch::stable, please file a request through anew issue on the PyTorch repo.\ntorch::headeronly\ntorch::stable\nBelow is a simple example of migrating an existing kernel that usesTORCH_LIBRARYto the stable ABI (TORCH_STABLE_LIBRARY). For a larger end to end example you can take a look at the FA3 repository. Specifically the diff betweenflash_api.cppand the stable variantflash_api_stable.cpp.\nTORCH_LIBRARY\nTORCH_STABLE_LIBRARY\nflash_api.cpp\nflash_api_stable.cpp\n\n## Original Version withTORCH_LIBRARY#\n\nTORCH_LIBRARY\n\n```python\n// original_kernel.cpp - Using TORCH_LIBRARY (not stable ABI)\n#include <torch/torch.h>\n#include <ATen/ATen.h>\n\nnamespace myops {\n\n// Simple kernel that adds a scalar value to each element of a tensor\nat::Tensor add_scalar(const at::Tensor& input, double scalar) {\n  TORCH_CHECK(input.scalar_type() == at::kFloat, \"Input must be float32\");\n\n  return input.add(scalar);\n}\n\n// Register the operator\nTORCH_LIBRARY(myops, m) {\n  m.def(\"add_scalar(Tensor input, float scalar) -> Tensor\", &add_scalar);\n}\n\n// Register the implementation\nTORCH_LIBRARY_IMPL(myops, CompositeExplicitAutograd, m) {\n  m.impl(\"add_scalar\", &add_scalar);\n}\n\n} // namespace myops\n\n```\n\n\n## Migrated Version withSTABLE_TORCH_LIBRARY#\n\nSTABLE_TORCH_LIBRARY\n\n```python\n// stable_kernel.cpp - Using STABLE_TORCH_LIBRARY (stable ABI)\n\n// (1) Don't include <torch/torch.h> <ATen/ATen.h>\n//     only include APIs from torch/csrc/stable, torch/headeronly and C-shims\n#include <torch/csrc/stable/library.h>\n#include <torch/csrc/stable/tensor_struct.h>\n#include <torch/csrc/stable/ops.h>\n#include <torch/csrc/stable/stableivalue_conversions.h>\n#include <torch/headeronly/core/ScalarType.h>\n#include <torch/headeronly/macros/Macros.h>\n\nnamespace myops {\n\n// Simple kernel that adds a scalar value to each element of a tensor\ntorch::stable::Tensor add_scalar(const torch::stable::Tensor& input, double scalar) {\n  // (2) use STD_TORCH_CHECK instead of TORCH_CHECK\n  STD_TORCH_CHECK(\n      // (3) use torch::headeronly::kFloat instead of at:kFloat\n      input.scalar_type() == torch::headeronly::kFloat,\n      \"Input must be float32\");\n\n  // (4) Use stable ops namespace instead of input.add\n  return torch::stable::add(input, scalar);\n}\n\n// (5) Add Boxed wrapper required for STABLE_TORCH_LIBRARY\nvoid boxed_add_scalar(StableIValue* stack, uint64_t num_args, uint64_t num_outputs) {\n  // Extract arguments from stack using `to<T>`\n  auto input = to<torch::stable::Tensor>(stack[0]);\n  auto scalar = to<double>(stack[1]);\n\n  // Call the actual kernel\n  auto result = add_scalar(input, scalar);\n\n  // Put result back on stack using `from()`\n  // Stack slot 0 now holds the return value\n  stack[0] = from(result);\n}\n\n// (6) Register the operator using STABLE_TORCH_LIBRARY\nSTABLE_TORCH_LIBRARY(myops, m) {\n  m.def(\"add_scalar(Tensor input, float scalar) -> Tensor\", &boxed_add_scalar);\n}\n\n// (7) Register the implementation using STABLE_TORCH_LIBRARY_IMPL\nSTABLE_TORCH_LIBRARY_IMPL(myops, CompositeExplicitAutograd, m) {\n  m.impl(\"add_scalar\", &boxed_add_scalar);\n}\n\n} // namespace myops\n\n```\n\n\n## How are objects passed across the ABI boundary when interacting with the dispatcher?#\n\nWhen interacting with the dispatcher via the stable APIs (STABLE_TORCH_LIBRARYetc.) we use a boxed convention. Arguments and returns are represented as a stack ofStableIValuewhich correlates with atorch::jit::stackof IValues. We discuss the following below\nSTABLE_TORCH_LIBRARY\nStableIValue\ntorch::jit::stack\nStableIValue Conversions\nStableIValue stack Conventions\nStable APIs that interact with the dispatcher\n\n## StableIValue Conversions#\n\nWe provide utilities for users to convert objects to and from StableIValues with the synonymoustoandfromAPIs intorch/csrc/stable/stableivalue_conversions.h. We document the stable custom extension representation, libtorch representation and StableIValue\nrepresentations below. Our confidently supported types are the ones in the table that have completed\nrows. You can rely on this subset for proper ABI stability, meaning that you can callto<T_custom_ext>(arg/ret)orfrom(T)on these types.\nto\nfrom\ntorch/csrc/stable/stableivalue_conversions.h\nto<T_custom_ext>(arg/ret)\nfrom(T)\nFor a limited set of use cases, we also implicitly support any literal type that is representable within 64 bits as StableIValues, as the default reinterpret_cast will succeed. (For example: c10::Device.) These types are currently ABI-stable on best effort but might break in the future and thus should be used for short term testing only.\nYou can always work with StableIValue abstractions in your custom kernel for types such as c10::Device even if there is no standard defined representation of device in custom extensions by not introspecting into the StableIValue. For example, a custom operator can take as argument a StableIValue device and directly pass it through to an aten operator withaoti_torch_call_dispatcher.\naoti_torch_call_dispatcher\ntype in custom extension: type used within the end user custom library.\nStableIValue representation: a stable conversion of the type to liaison between the user model vs libtorch.so in an ABI-stable manner.\ntype in libtorch: type used within libtorch.so (or any code binary locked with libtorch).\nSchema Type: type as described by the schema, which we hail as the source of truth for both ATen ops in native_functions.yaml and for user defined custom operators registered to the dispatcher via TORCH_LIBRARY or torch.library.\ntype in custom extension\nStableIValue representation\ntype in libtorch\nSchema Type\nstd::optional<S>\nif there is a value, raw bitwise copy into leading bytes of uint64_t of pointer to a new StableIValue representing S. if there is no value, nullptr.\nstd::optional<T>\nType?\ntorch::stable::Tensor\nraw bitwise copy of underlying AtenTensorHandle into leading bytes of uint64_t\nat::Tensor\nTensor\nRAIIATH (outdated)\nraw bitwise copy of underlying AtenTensorHandle into leading bytes of uint64_t\nat::Tensor\nTensor\ntorch::headeronly::ScalarType\nraw bitwise copy of the translated underlying enum into leading bytes of uint64_t\ntorch::headeronly::ScalarType\nScalarType\nint32_t\nraw bitwise copy into leading bytes of uint64_t\nat::Layout\nLayout\nint32_t\nraw bitwise copy into leading bytes of uint64_t\nat::MemoryFormat\nMemoryFormat\nbool\nraw bitwise copy into leading bytes of uint64_t\nbool\nbool\nint64_t\nraw bitwise copy into leading bytes of uint64_t\nint64_t\nint\ndouble\nraw bitwise copy into leading bytes of uint64_t\ndouble\nfloat\n?\n?\nc10::Device\nDevice\n?\n?\nc10::Stream\nStream\n?\n?\nc10::complex\ncomplex\n?\n?\nat::Scalar\nScalar\n?\n?\nstd::string/const char*/ivalue::ConstantString\nstr\n?\n?\nat::Storage\nStorage\n?\n?\nat::Generator\nGenerator\n?\n?\nc10::List<T>\nType[]\n?\n?\nivalue::Tuple<T>\n(Type, \u2026)\n?\n?\nc10::SymInt\nSymInt\n?\n?\nc10::SymFloat\nSymFloat\n?\n?\nc10::SymBool\nSymBool\n?\n?\nat::QScheme\nQScheme\n\n## Stack Conventions#\n\nThere are two invariants for the stack:\nThe stack is populated left to right.\na. For example, a stack representing argumentsarg0,arg1, andarg2will havearg0at index 0,arg1at index 1, andarg2at index 2.\nb. Returns are also populated left to right, e.g.,ret0will be at index 0 andret1will be at index 1, and so on.\narg0\narg1\narg2\narg0\narg1\narg2\nret0\nret1\nThe stack always has ownership of the objects it holds.\na. When calling a stack-based API, you must give owning references to the calling stack and steal references from the returned stack.\nb. When registering your function to be called with a stack, you must steal references from your argument stack and push onto the stack new references.\n\n## Stack-based APIs#\n\nThe above is relevant in two places:\nSTABLE_TORCH_LIBRARYUnlikeTORCH_LIBRARY, the dispatcher expects kernels registered viaSTABLE_TORCH_LIBRARYto be boxed. This means they must have the signature(StableIValue*stack,uint64_tnum_args,uint64_tnum_outputs)->void.We plan to eventually abstract away the need for manual boxing, but, for the time being, please usefromandto.\nSTABLE_TORCH_LIBRARY\nTORCH_LIBRARY\nSTABLE_TORCH_LIBRARY\n(StableIValue*stack,uint64_tnum_args,uint64_tnum_outputs)->void\nfrom\nto\n\n```python\nTensor my_amax_vec(Tensor t) {\n    std::vector<int64_t> v = {0,1};\n    return amax(t, v, false);\n}\n\nvoid boxed_my_amax_vec(StableIValue* stack, uint64_t num_args, uint64_t num_outputs) {\n    auto res = my_amax_vec(to<Tensor>(stack[0]));\n    stack[0] = from(res);\n}\n\n```\n\naoti_torch_call_dispatcherThis API allows you to call the PyTorch dispatcher from C/C++ code. It has the following signature:\naoti_torch_call_dispatcher\n\n```python\naoti_torch_call_dispatcher(const char* opName, const char* overloadName, StableIValue* stack);\n\n```\n\naoti_torch_call_dispatcherwill call the op overload defined by a givenopName,overloadName, and a stack of\nStableIValues. This call will populate any return values of the op into the stack in their StableIValue form,\nwithret0at index 0,ret1at index 1, and so on.\naoti_torch_call_dispatcher\nopName\noverloadName\nret0\nret1",
    "url": "https://pytorch.org/docs/stable/notes/libtorch_stable_abi.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e95dcb0536060ca47a1fb82f488ac3ce",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/data.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "85db1f194fa808885557ca6a06677b52",
    "source": "pytorch_docs",
    "title": "Examples \u2014 PyTorch 2.9 documentation",
    "text": "\n## Examples#\n\nCreated On: May 04, 2021 | Last Updated On: May 04, 2021\nPlease refer to theelastic/examples README.",
    "url": "https://pytorch.org/docs/stable/elastic/examples.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ee74c254fb9de1f2b2a510680c49f894",
    "source": "pytorch_docs",
    "title": "torch.futures \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.futures#\n\nCreated On: Jun 12, 2025 | Last Updated On: Jun 12, 2025\nThis package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework.\nFuture\nFuture\nFuture\nWrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results.\ntorch._C.Future\nrpc_async()\nWarning\nGPU support is a beta feature, subject to changes.\nAppend the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline.\nFuture\nFuture\nFuture\nFuture\nvalue()\nFuture\nWe recommend that you use thethen()method as it provides a way\nto synchronize after your callback has completed.add_done_callbackcan be cheaper if your callback does not return anything. But boththen()andadd_done_callbackuse the same callback\nregistration API under the hood.\nthen()\nadd_done_callback\nthen()\nadd_done_callback\nWith respect to GPU tensors, this method behaves in the same way asthen().\nthen()\ncallback(Future) \u2013 aCallablethat takes in one argument,\nwhich is the reference to thisFuture.\nFuture\nCallable\nFuture\nNote\nNote that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently.\nfut.wait()\nExample:\n\n```python\n>>> def callback(fut):\n...     print(\"This will run after the future has finished.\")\n...     print(fut.wait())\n>>> fut = torch.futures.Future()\n>>> fut.add_done_callback(callback)\n>>> fut.set_result(5)\nThis will run after the future has finished.\n5\n\n```\n\nReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception.\nTrue\nFuture\nFuture\nIf the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()).\nFuture.done()\nTrue\nwait()\nbool\nSet an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline.\nFuture\nFuture\nFuture\nresult(BaseException) \u2013 the exception for thisFuture.\nFuture\nExample:\n\n```python\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\nTraceback (most recent call last):\n...\nValueError: foo\n\n```\n\nSet the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice.\nFuture\nFuture\nFuture\nIf the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture.\nFuture\nresult(object) \u2013 the result object of thisFuture.\nFuture\nExample:\n\n```python\n>>> import threading\n>>> import time\n>>> def slow_set_future(fut, value):\n...     time.sleep(0.5)\n...     fut.set_result(value)\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n...     target=slow_set_future,\n...     args=(fut, torch.ones(2) * 3)\n... )\n>>> t.start()\n>>> print(fut.wait())\ntensor([3., 3.])\n>>> t.join()\n\n```\n\nAppend the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline.\nFuture\nFuture\nFuture\nfut.then(cb1).then(cb2)\nFuture\nvalue()\nFuture\nIf theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait().\nFuture\nwait()\nSimilarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked.\ncallback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument.\nCallable\nCallable\nFuture\nA newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes.\nFuture\ncallback\ncallback\nFuture[S]\nNote\nNote that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently.\nfut.wait()\nthen\nExample:\n\n```python\n>>> def callback(fut):\n...     print(f\"RPC return value is {fut.wait()}.\")\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n...     lambda x : print(f\"Chained cb done. {x.wait()}\")\n... )\n>>> fut.set_result(5)\nRPC return value is 5.\nChained cb done. None\n\n```\n\nObtain the value of an already-completed future.\nThis method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail.\nwait()\nthen()\nFuture\nvalue()\nIf the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()).\nwait()\nthen()\nThe value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error.\nFuture\nvalue()\nT\nBlock until the value of thisFutureis ready.\nFuture\nIf the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams.\nwait()\nwait()\nThe value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error.\nFuture\nwait\nT\nCollects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed.\nFuture\nFuture\nfutures(list) \u2013 a list ofFutureobjects.\nFuture\nReturns aFutureobject to a list of the passed\nin Futures.\nFuture\nFuture[list[torch.jit.Future]]\n\n```python\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\nfut0 result = 0\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\nfut1 result = 1\n\n```\n\nWaits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete.\nfutures(list) \u2013 a list ofFutureobject.\nFuture\nA list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows.\nFuture\nwait\nFuture\nlist",
    "url": "https://pytorch.org/docs/stable/futures.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2b15d427cbc68f392853635d7c95dfdf",
    "source": "pytorch_docs",
    "title": "python.assert \u2014 PyTorch 2.9 documentation",
    "text": "\n## python.assert#\n\n\n## dynamic_shape_assert#\n\nNote\nTags:python.assert\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeAssert(torch.nn.Module):\n    \"\"\"\n    A basic usage of python assertion.\n    \"\"\"\n\n    def forward(self, x):\n        # assertion with error message\n        assert x.shape[0] > 2, f\"{x.shape[0]} is greater than 2\"\n        # assertion without error message\n        assert x.shape[0] > 1\n        return x\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.assert\"}\nmodel = DynamicShapeAssert()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n            return (x,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    x: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_contains#\n\nNote\nTags:python.data-structure,torch.dynamic-shape,python.assert\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass ListContains(torch.nn.Module):\n    \"\"\"\n    List containment relation can be checked on a dynamic shape or constants.\n    \"\"\"\n\n    def forward(self, x):\n        assert x.size(-1) in [6, 2]\n        assert x.size(0) not in [4, 5, 6]\n        assert \"monkey\" not in [\"cow\", \"pig\"]\n        return x + x\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"}\nmodel = ListContains()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/python.assert.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "da867609362bc52548d8cd851b5595ea",
    "source": "pytorch_docs",
    "title": "Distributed Data Parallel \u2014 PyTorch 2.9 documentation",
    "text": "\n## Distributed Data Parallel#\n\nCreated On: Jan 15, 2020 | Last Updated On: Jan 25, 2024\nWarning\nThe implementation oftorch.nn.parallel.DistributedDataParallelevolves over time. This design note is written based on the state as of v1.4.\ntorch.nn.parallel.DistributedDataParallel\ntorch.nn.parallel.DistributedDataParallel(DDP) transparently performs\ndistributed data parallel training. This page describes how it works and reveals\nimplementation details.\ntorch.nn.parallel.DistributedDataParallel\n\n## Example#\n\nLet us start with a simpletorch.nn.parallel.DistributedDataParallelexample. This example uses atorch.nn.Linearas the local model, wraps\nit with DDP, and then runs one forward pass, one backward pass, and an optimizer\nstep on the DDP model. After that, parameters on the local model will be\nupdated, and all models on different processes should be exactly the same.\ntorch.nn.parallel.DistributedDataParallel\ntorch.nn.Linear\n\n```python\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n\ndef example(rank, world_size):\n    # create default process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n    # create local model\n    model = nn.Linear(10, 10).to(rank)\n    # construct DDP model\n    ddp_model = DDP(model, device_ids=[rank])\n    # define loss function and optimizer\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    # forward pass\n    outputs = ddp_model(torch.randn(20, 10).to(rank))\n    labels = torch.randn(20, 10).to(rank)\n    # backward pass\n    loss_fn(outputs, labels).backward()\n    # update parameters\n    optimizer.step()\n\ndef main():\n    world_size = 2\n    mp.spawn(example,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True)\n\nif __name__==\"__main__\":\n    # Environment variables which need to be\n    # set when using c10d's default \"env\"\n    # initialization mode.\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29500\"\n    main()\n\n```\n\nDDP works with TorchDynamo.  When used with TorchDynamo, apply the DDP model wrapper\nbefore compiling the model, such that torchdynamo can applyDDPOptimizer(graph-break optimizations) based on DDP bucket sizes.  (SeeTorchDynamo DDPOptimizerfor more information.)\nDDPOptimizer\n\n```python\nddp_model = DDP(model, device_ids=[rank])\nddp_model = torch.compile(ddp_model)\n\n```\n\n\n## Internal Design#\n\nThis section reveals how it works under the hood oftorch.nn.parallel.DistributedDataParallelby diving into details of\nevery step in one iteration.\ntorch.nn.parallel.DistributedDataParallel\nPrerequisite: DDP relies on c10dProcessGroupfor communications.\nHence, applications must createProcessGroupinstances before constructing\nDDP.\nProcessGroup\nProcessGroup\nConstruction: The DDP constructor takes a reference to the local module,\nand broadcastsstate_dict()from the process with rank 0 to all other\nprocesses in the group to make sure that all model replicas start from the\nexact same state. Then, each DDP process creates a localReducer, which\nlater will take care of the gradients synchronization during the backward\npass. To improve communication efficiency, theReducerorganizes parameter\ngradients into buckets, and reduces one bucket at a time. Bucket size can be\nconfigured by setting thebucket_cap_mbargument in DDP constructor. The\nmapping from parameter gradients to buckets is determined at the construction\ntime, based on the bucket size limit and parameter sizes. Model parameters are\nallocated into buckets in (roughly) the reverse order ofModel.parameters()from the given model. The reason for using the reverse\norder is because DDP expects gradients to become ready during the backward\npass in approximately that order. The figure below shows an example. Note\nthat, thegrad0andgrad1are inbucket1, and the other two\ngradients are inbucket0. Of course, this assumption might not always\nbe true, and when that happens it could hurt DDP backward speed as theReducercannot kick off the communication at the earliest possible time.\nBesides bucketing, theReduceralso registers autograd hooks during\nconstruction, one hook per parameter. These hooks will be triggered during\nthe backward pass when the gradient becomes ready.\nstate_dict()\nReducer\nReducer\nModel.parameters()\ngrad0\ngrad1\nbucket1\nbucket0\nReducer\nReducer\nForward Pass: The DDP takes the input and passes it to the local model,\nand then analyzes the output from the local model iffind_unused_parametersis set toTrue. This mode allows running\nbackward on a subgraph of the model, and DDP finds out which parameters are\ninvolved in the backward pass by traversing the autograd graph from the model\noutput and marking all unused parameters as ready for reduction. During the\nbackward pass, theReducerwould only wait for unready parameters, but it\nwould still reduce all buckets. Marking a parameter gradient as ready does not\nhelp DDP skip buckets as for now, but it will prevent DDP from waiting for\nabsent gradients forever during the backward pass. Note that traversing the\nautograd graph introduces extra overheads, so applications should only setfind_unused_parameterstoTruewhen necessary.\nfind_unused_parameters\nTrue\nReducer\nfind_unused_parameters\nTrue\nBackward Pass: Thebackward()function is directly invoked on the lossTensor, which is out of DDP\u2019s control, and DDP uses autograd hooks\nregistered at construction time to trigger gradients synchronizations. When\none gradient becomes ready, its corresponding DDP hook on that grad\naccumulator will fire, and DDP will then mark that parameter gradient as\nready for reduction. When gradients in one bucket are all ready, theReducerkicks off an asynchronousallreduceon that bucket to\ncalculate mean of gradients across all processes. When all buckets are ready,\ntheReducerwill block waiting for allallreduceoperations to finish.\nWhen this is done, averaged gradients are written to theparam.gradfield\nof all parameters. So after the backward pass, thegradfield on the same\ncorresponding parameter across different DDP processes should be the same.\nbackward()\nTensor\nReducer\nallreduce\nReducer\nallreduce\nparam.grad\nOptimizer Step: From the optimizer\u2019s perspective, it is optimizing a local\nmodel. Model replicas on all DDP processes can keep in sync because they all\nstart from the same state and they have the same averaged gradients in\nevery iteration.\nNote\nDDP requiresReducerinstances on all processes to invokeallreducein exactly the same order, which is done by always runningallreducein the bucket index order instead of actual bucket ready order. Mismatchedallreduceorder across processes can lead to wrong results or DDP backward\nhang.\nReducer\nallreduce\nallreduce\nallreduce\n\n## Implementation#\n\nBelow are pointers to the DDP implementation components. The stacked graph shows\nthe structure of the code.\n\n## ProcessGroup#\n\nProcessGroup.hpp:\ncontains the abstract API of all process group implementations. Thec10dlibrary provides 3 implementations out of the box, namely,ProcessGroupGloo,ProcessGroupNCCL, andProcessGroupMPI.DistributedDataParallelusesProcessGroup::broadcast()to send\nmodel states from the process with rank 0 to others during initialization\nandProcessGroup::allreduce()to sum gradients.\nc10d\nDistributedDataParallel\nProcessGroup::broadcast()\nProcessGroup::allreduce()\nStore.hpp:\nassists the rendezvous service for process group instances to find each other.\n\n## DistributedDataParallel#\n\ndistributed.py:\nis the Python entry point for DDP. It implements the initialization steps and\ntheforwardfunction for thenn.parallel.DistributedDataParallelmodule which call into C++ libraries. Its_sync_paramfunction performs\nintra-process parameter synchronization when one DDP process works on multiple\ndevices, and it also broadcasts model buffers from the process with rank 0 to\nall other processes. The inter-process parameter synchronization happens inReducer.cpp.\nforward\nnn.parallel.DistributedDataParallel\n_sync_param\nReducer.cpp\ncomm.h:\nimplements the coalesced broadcast helper function which is invoked to\nbroadcast model states during initialization and synchronize model buffers\nbefore the forward pass.\nreducer.h:\nprovides the core implementation for gradient synchronization in the backward\npass. It has three entry point functions:\nReducer: The constructor is called indistributed.pywhich registersReducer::autograd_hook()to gradient accumulators.\nReducer\ndistributed.py\nReducer::autograd_hook()\nautograd_hook()function will be invoked by the autograd engine when\na gradient becomes ready.\nautograd_hook()\nprepare_for_backward()is called at the end of DDP forward pass indistributed.py. It traverses the autograd graph to find unused\nparameters whenfind_unused_parametersis set toTruein DDP\nconstructor.\nprepare_for_backward()\ndistributed.py\nfind_unused_parameters\nTrue\n\n## TorchDynamo DDPOptimizer#\n\nDDP\u2019s performance advantage comes from overlapping allreduce collectives with computations during backwards.\nAotAutograd prevents this overlap when used with TorchDynamo for compiling a whole forward and whole backward graph,\nbecause allreduce ops are launched by autograd hooks _after_ the whole optimized backwards computation finishes.\nTorchDynamo\u2019s DDPOptimizer helps by breaking the forward graph at the logical boundaries of DDP\u2019s allreduce buckets\nduring backwards.  Note: the goal is to break the graph during backwards, and the simplest implementation is to\nbreak the forward graphs and then call AotAutograd and compilation on each section.  This allows DDP\u2019s allreduce hooks\nto fire in-between sections of backwards, and schedule communications to overlap with compute.\nSeethis blog postfor\na more in-depth explanation and experimental results, or read the docs and code attorch/_dynamo/optimizations/distributed.py\nTo Debug DDPOptimizer, setTORCH_LOGS=\u2019ddp_graphs\u2019for full graph dumps. For logs without graphs, add any of \u2018dynamo\u2019, \u2018distributed\u2019, or \u2018dist_ddp\u2019 toTORCH_LOGS(for basic info about bucket boundaries).  To disable DDPOptimizer, settorch._dynamo.config.optimize_ddp=False.\nDDP and TorchDynamo should still work correctly without DDPOptimizer, but with performance degradation.",
    "url": "https://pytorch.org/docs/stable/notes/ddp.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "097802f6655c74458727bd5f7dd2a4a4",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/python.context-manager.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1c028fc146584d8819f8802b81af99be",
    "source": "pytorch_docs",
    "title": "Frequently Asked Questions \u2014 PyTorch 2.9 documentation",
    "text": "\n## Frequently Asked Questions#\n\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\nAuthor:Mark Saroufim\n\n## Doestorch.compilesupport training?#\n\ntorch.compile\ntorch.compilesupports training, using AOTAutograd to capture backwards:\ntorch.compile\nThe.forward()graph andoptimizer.step()is captured by\nTorchDynamo\u2019s pythonevalframefrontend.\n.forward()\noptimizer.step()\nevalframe\nFor each segment of.forward()that torchdynamo captures, it uses\nAOTAutograd to generate a backward graph segment.\n.forward()\nEach pair of forward and backward graph are (optionally) min-cut\npartitioned to save the minimal state between forward and backward.\nThe forward and backward pairs are wrapped inautograd.functionmodules.\nautograd.function\nUser code calling.backward()still triggers eager\u2019s autograd engine,\nwhich runs eachcompiled backwardgraph as if it were one op, also running\nany non-compiled eager ops\u2019.backward()functions.\n.backward()\n.backward()\n\n## Do you support Distributed code?#\n\ntorch.compilesupportsDistributedDataParallel(DDP).\nSupport for other distributed training libraries is being considered.\ntorch.compile\nDistributedDataParallel\nThe main reason why Distributed code is challenging with dynamo is\nbecause AOTAutograd unrolls both the forward and backward pass and\nprovides 2 graphs for backends to optimize. This is a problem for\ndistributed code because we\u2019d like to ideally overlap communication\noperations with computations. Eager pytorch accomplishes this in\ndifferent ways for DDP/FSDP- using autograd hooks, module hooks, and\nmodifications/mutations of module states. In a naive application of\ndynamo, hooks that should run directly after an operation during\nbackwards may be delayed until after the entire compiled region of\nbackwards ops, due to how AOTAutograd compiled functions interact with\ndispatcher hooks.\nThe basic strategy for optimizing DDP with Dynamo is outlined indistributed.pywhere the main idea will be to graph break onDDP bucket\nboundaries.\nWhen each node in DDP needs to synchronize its weights with the other\nnodes it organizes its gradients and parameters into buckets which\nreduces communication times and allows a node to broadcast a fraction of\nits gradients to other waiting nodes.\nGraph breaks in distributed code mean you can expect dynamo and its\nbackends to optimize the compute overhead of a distributed program but\nnot its communication overhead. Graph-breaks may interfere with\ncompilation speedups, if the reduced graph-size robs the compiler of\nfusion opportunities. However, there are diminishing returns with\nincreasing graph size since most of the current compute optimizations\nare local fusions. So in practice this approach may be sufficient.\n\n## Do I still need to export whole graphs?#\n\nFor the vast majority of models you probably don\u2019t and you can usetorch.compile()as is but there are a few situations where\nfull graphs are necessary and you can can ensure a full graph by simply\nrunningtorch.compile(...,fullgraph=True). These situations include:\ntorch.compile()\ntorch.compile(...,fullgraph=True)\nLarge scale training runs, such as $250K+ that require pipeline parallelism\nand other advanced sharding strategies.\nInference optimizers likeTensorRTorAITemplatethat\nrely on fusing much more aggressively than training optimizers.\nMobile training or inference.\nFuture work will include tracing communication operations into graphs,\ncoordinating these operations with compute optimizations, and optimizing\nthe communication operations.\n\n## Why is my code crashing?#\n\nIf your code ran just fine withouttorch.compileand started to\ncrash with it is enabled, then the most important first step is figuring\nout which part of the stack your failure occurred. To troubleshoot that,\nfollow the steps below and only try the next step if the previous one\nsucceeded.\ntorch.compile\ntorch.compile(...,backend=\"eager\")which only runs TorchDynamo\nforward graph capture and then runs the captured graph with PyTorch.\nIf this fails then there\u2019s an issue with TorchDynamo.\ntorch.compile(...,backend=\"eager\")\ntorch.compile(...,backend=\"aot_eager\")which runs TorchDynamo to capture a forward graph, and then AOTAutograd\nto trace the backward graph without any additional backend compiler\nsteps. PyTorch eager will then be used to run the forward and backward\ngraphs. If this fails then there\u2019s an issue with AOTAutograd.\ntorch.compile(...,backend=\"aot_eager\")\ntorch.compile(...,backend=\"inductor\")which runs TorchDynamo to capture a\nforward graph, and then AOTAutograd to trace the backward graph with the\nTorchInductor compiler. If this fails then there\u2019s an issue with TorchInductor\ntorch.compile(...,backend=\"inductor\")\n\n## Why is compilation slow?#\n\nDynamo Compilation\u2013 TorchDynamo has a builtin stats function for\ncollecting and displaying the time spent in each compilation phase.\nThese stats can be accessed by callingtorch._dynamo.utils.compile_times()after executingtorch._dynamo. By default, this returns a string\nrepresentation of the compile times spent in each TorchDynamo function by name.\ntorch._dynamo.utils.compile_times()\ntorch._dynamo\nInductor Compilation\u2013 TorchInductor has a builtin stats and trace function\nfor displaying time spent in each compilation phase, output code, output\ngraph visualization and IR dump.envTORCH_COMPILE_DEBUG=1pythonrepro.py.\nThis is a debugging tool designed to make it easier to debug/understand the\ninternals of TorchInductor with an output that will look something likethisEach file in that debug trace can be enabled/disabled viatorch._inductor.config.trace.*. The profile and the diagram are both\ndisabled by default since they are expensive to generate. See theexample debug directory\noutputfor more examples.\nenvTORCH_COMPILE_DEBUG=1pythonrepro.py\ntorch._inductor.config.trace.*\nExcessive RecompilationWhen TorchDynamo compiles a function (or part of one), it makes certain\nassumptions about locals and globals in order to allow compiler\noptimizations, and expresses these assumptions as guards that check\nparticular values at runtime. If any of these guards fail, Dynamo will\nrecompile that function (or part) up totorch._dynamo.config.recompile_limittimes. If your program is\nhitting the cache limit, you will first need to determine which guard is\nfailing and what part of your program is triggering it. The\nUseTORCH_TRACE/tlparseorTORCH_LOGS=recompilesto trace the root of the issue, checktorch.compile Troubleshootingfor more details.\ntorch._dynamo.config.recompile_limit\nTORCH_TRACE/tlparse\nTORCH_LOGS=recompiles\n\n## Why are you recompiling in production?#\n\nIn some cases, you may not want unexpected compiles after a program has\nwarmed up. For example, if you are serving production traffic in a\nlatency critical application. For this, TorchDynamo provides an\nalternate mode where prior compiled graphs are used, but no new ones are\ngenerated:\n\n```python\nfrozen_toy_example = dynamo.run(toy_example)\nfrozen_toy_example(torch.randn(10), torch.randn(10))\n\n```\n\n\n## How are you speeding up my code?#\n\nThere are 3 major ways to accelerate PyTorch code:\nKernel fusion via vertical fusions which fuse sequential operations to avoid\nexcessive read/writes. For example, fuse 2 subsequent cosines means you\ncan can do 1 read 1 write instead 2 reads 2 writes 2. Horizontal fusion:\nthe simplest example being batching where a single matrix is multiplied\nwith a batch of examples but the more general scenario is a grouped GEMM\nwhere a group of matrix multiplications are scheduled together\nOut of order execution: A general optimization for compilers, by looking ahead\nat the exact data dependencies within a graph we can decide on the most\nopportune time to execute a node and which buffers can be reused\nAutomatic work placement: Similar of the out of order execution point,\nbut by matching nodes of a graph to resources like physical hardware or\nmemory we can design an appropriate schedule\nThe above are general principles for accelerating PyTorch code but\ndifferent backends will each make different tradeoffs on what to\noptimize. For example Inductor first takes care of fusing whatever it\ncan and only then generatesTritonkernels.\nTriton in addition offers speedups because of automatic memory\ncoalescing, memory management and scheduling within each Streaming\nMultiprocessor and has been designed to handle tiled computations.\nHowever, regardless of the backend you use it\u2019s best to use a benchmark\nand see approach so try out the PyTorch profiler, visually inspect the\ngenerated kernels and try to see what\u2019s going on for yourself.\n\n## Why am I not seeing speedups?#\n\n\n## Graph Breaks#\n\nThe main reason you won\u2019t see the speedups you\u2019d like to by using dynamo\nis excessive graph breaks. So what\u2019s a graph break?\nGiven a program like:\n\n```python\ndef some_fun(x):\n    ...\n\ntorch.compile(some_fun)(x)\n...\n\n```\n\nTorchdynamo will attempt to compile all of the torch/tensor operations\nwithinsome_fun()into a single FX graph, but it may fail to capture\neverything into one graph.\nsome_fun()\nSome graph break reasons are insurmountable to TorchDynamo like calling\ninto a C extension other than PyTorch is invisible to TorchDynamo, and\ncould do arbitrary things without TorchDynamo being able to introduce\nnecessary guards to ensure that the compiled program would be safe to reuse.\nTo maximize performance, it\u2019s important to have as few graph breaks\nas possible.\n\n## Identifying the cause of a graph break#\n\nTo identify all graph breaks in a program and the associated reasons for\nthe breaks,torch._dynamo.explaincan be used. This tool runs\nTorchDynamo on the supplied function and aggregates the graph breaks\nthat are encountered. Here is an example usage:\ntorch._dynamo.explain\n\n```python\nimport torch\nimport torch._dynamo as dynamo\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    print(\"woo\")\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nexplanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))\nprint(explanation)\n\"\"\"\nGraph Count: 3\nGraph Break Count: 2\nOp Count: 5\nBreak Reasons:\n  Break Reason 1:\n    Reason: builtin: print [<class 'torch._dynamo.variables.constant.ConstantVariable'>] False\n    User Stack:\n      <FrameSummary file foo.py, line 5 in toy_example>\n  Break Reason 2:\n    Reason: generic_jump TensorVariable()\n    User Stack:\n      <FrameSummary file foo.py, line 6 in torch_dynamo_resume_in_toy_example_at_5>\nOps per Graph:\n  ...\nOut Guards:\n  ...\n\"\"\"\n\n```\n\nTo throw an error on the first graph break encountered you can\ndisable python fallbacks by usingfullgraph=True, this should be\nfamiliar if you\u2019ve worked with export based compilers.\nfullgraph=True\n\n```python\ndef toy_example(a, b):\n   ...\n\ntorch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)\n\n```\n\n\n## Why didn\u2019t my code recompile when I changed it?#\n\nIf you enabled dynamic shapes by settingenvTORCHDYNAMO_DYNAMIC_SHAPES=1pythonmodel.pythen your code\nwon\u2019t recompile on shape changes. We\u2019ve added support for dynamic shapes\nwhich avoids recompilations in the case when shapes vary by less than a\nfactor of 2. This is especially useful in scenarios like varying image\nsizes in CV or variable sequence length in NLP. In inference scenarios\nit\u2019s often not possible to know what a batch size will be beforehand\nbecause you take what you can get from different client apps.\nenvTORCHDYNAMO_DYNAMIC_SHAPES=1pythonmodel.py\nIn general, TorchDynamo tries very hard not to recompile things\nunnecessarily so if for example TorchDynamo finds 3 graphs and your\nchange only modified one graph then only that graph will recompile. So\nanother tip to avoid potentially slow compilation times is to warmup a\nmodel by compiling it once after which subsequent compilations will be\nmuch faster. Cold start compile times is still a metric we track\nvisibly.\n\n## Why am I getting incorrect results?#\n\nAccuracy issues can also be minified if you set the environment variableTORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect\nmodel and a full repro might be something likeTORCHDYNAMO_REPRO_AFTER=\"aot\"TORCHDYNAMO_REPRO_LEVEL=4the reason\nwe need this is downstream compilers will codegen code whether it\u2019s\nTriton code or the C++ backend, the numerics from those downstream\ncompilers can be different in subtle ways yet have dramatic impact on\nyour training stability. So the accuracy debugger is very useful for us\nto detect bugs in our codegen or with a backend compiler.\nTORCHDYNAMO_REPRO_LEVEL=4\nTORCHDYNAMO_REPRO_AFTER=\"aot\"TORCHDYNAMO_REPRO_LEVEL=4\nIf you\u2019d like to ensure that random number generation is the same across both torch\nand triton then you can enabletorch._inductor.config.fallback_random=True\ntorch._inductor.config.fallback_random=True\n\n## Why am I getting OOMs?#\n\nDynamo is still an alpha product so there\u2019s a few sources of OOMs and if\nyou\u2019re seeing an OOM try disabling the following configurations in this\norder and then open an issue on GitHub so we can solve the root problem\n1. If you\u2019re using dynamic shapes try disabling them, we\u2019ve disabled\nthem by default:envTORCHDYNAMO_DYNAMIC_SHAPES=0pythonmodel.py2.\nCUDA graphs with Triton are enabled by default in inductor but removing\nthem may alleviate some OOM issues:torch._inductor.config.triton.cudagraphs=False.\nenvTORCHDYNAMO_DYNAMIC_SHAPES=0pythonmodel.py\ntorch._inductor.config.triton.cudagraphs=False\n\n## Doestorch.funcwork withtorch.compile(forgradandvmaptransforms)?#\n\ntorch.func\ntorch.compile\ngrad\nvmap\nApplying atorch.functransform to a function that usestorch.compiledoes work:\ntorch.func\ntorch.compile\n\n```python\nimport torch\n\n@torch.compile\ndef f(x):\n    return torch.sin(x)\n\ndef g(x):\n    return torch.grad(f)(x)\n\nx = torch.randn(2, 3)\ng(x)\n\n```\n\n\n## Callingtorch.functransform inside of a function handled withtorch.compile#\n\ntorch.func\ntorch.compile\n\n## Compilingtorch.func.gradwithtorch.compile#\n\ntorch.func.grad\ntorch.compile\n\n```python\nimport torch\n\ndef wrapper_fn(x):\n    return torch.func.grad(lambda x: x.sin().sum())(x)\n\nx = torch.randn(3, 3, 3)\ngrad_x = torch.compile(wrapper_fn)(x)\n\n```\n\n\n## Compilingtorch.vmapwithtorch.compile#\n\ntorch.vmap\ntorch.compile\n\n```python\nimport torch\n\ndef my_fn(x):\n    return torch.vmap(lambda x: x.sum(1))(x)\n\nx = torch.randn(3, 3, 3)\noutput = torch.compile(my_fn)(x)\n\n```\n\n\n## Compiling functions besides the ones which are supported (escape hatch)#\n\nFor other transforms, as a workaround, usetorch._dynamo.allow_in_graph\ntorch._dynamo.allow_in_graph\nallow_in_graphis an escape hatch. If your code does not work withtorch.compile, which introspects Python bytecode, but you believe it\nwill work via a symbolic tracing approach (likejax.jit), then useallow_in_graph.\nallow_in_graph\ntorch.compile\njax.jit\nallow_in_graph\nBy usingallow_in_graphto annotate a function, you must make sure\nyour code meets the following requirements:\nallow_in_graph\nAll outputs in your function only depend on the inputs and\ndo not depend on any captured Tensors.\nYour function is functional. That is, it does not mutate any state. This may\nbe relaxed; we actually support functions that appear to be functional from\nthe outside: they may have in-place PyTorch operations, but may not mutate\nglobal state or inputs to the function.\nYour function does not raise data-dependent errors.\n\n```python\nimport torch\n\n@torch.compile\ndef f(x):\n    return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x)\n\nx = torch.randn(2, 3)\nf(x)\n\n```\n\nA common pitfall is usingallow_in_graphto annotate a function that\ninvokes annn.Module. This is because the outputs now depend on the\nparameters of thenn.Module. To get this to work, usetorch.func.functional_callto extract the module state.\nallow_in_graph\nnn.Module\nnn.Module\ntorch.func.functional_call\n\n## Does NumPy work withtorch.compile?#\n\ntorch.compile\nStarting in 2.1,torch.compileunderstands native NumPy programs that\nwork on NumPy arrays, and mixed PyTorch-NumPy programs that convert from PyTorch\nto NumPy and back viax.numpy(),torch.from_numpy, and related functions.\ntorch.compile\nx.numpy()\ntorch.from_numpy\n\n## Which NumPy features doestorch.compilesupport?#\n\ntorch.compile\nNumPy withintorch.compilefollows NumPy 2.0 pre-release.\ntorch.compile\nGenerally,torch.compileis able to trace through most NumPy constructions,\nand when it cannot, it falls back to eager and lets NumPy execute that piece of\ncode. Even then, there are a few features wheretorch.compilesemantics\nslightly deviate from those of NumPy:\ntorch.compile\ntorch.compile\nNumPy scalars: We model them as 0-D arrays. That is,np.float32(3)returns\na 0-D array undertorch.compile. To avoid a graph break, it is best to use this 0-D\narray. If this breaks your code, you can workaround this by casting the NumPy scalar\nto the relevant Python scalar typebool/int/float.\nnp.float32(3)\ntorch.compile\nbool/int/float\nNegative strides:np.flipand slicing with a negative step return a copy.\nnp.flip\nType promotion: NumPy\u2019s type promotion will change in NumPy 2.0. The new rules\nare described inNEP 50.torch.compileimplements NEP 50 rather than the current soon-to-be deprecated rules.\ntorch.compile\n{tril,triu}_indices_from/{tril,triu}_indicesreturn arrays rather than a tuple of arrays.\n{tril,triu}_indices_from/{tril,triu}_indices\nThere are other features for which we do not support tracing and we gracefully\nfallback to NumPy for their execution:\nNon-numeric dtypes like datetimes, strings, chars, void, structured dtypes and recarrays.\nLong dtypesnp.float128/np.complex256and some unsigned dtypesnp.uint16/np.uint32/np.uint64.\nnp.float128/np.complex256\nnp.uint16/np.uint32/np.uint64\nndarraysubclasses.\nndarray\nMasked arrays.\nEsoteric ufunc machinery likeaxes=[(n,k),(k,m)->(n,m)]and ufunc methods (e.g.,np.add.reduce).\naxes=[(n,k),(k,m)->(n,m)]\nnp.add.reduce\nSorting / orderingcomplex64/complex128arrays.\ncomplex64/complex128\nNumPynp.poly1dandnp.polynomial.\nnp.poly1d\nnp.polynomial\nPositionalout1,out2args in functions with 2 or more returns (out=tupledoes work).\nout1,out2\nout=tuple\n__array_function__,__array_interface__and__array_wrap__.\n__array_function__\n__array_interface__\n__array_wrap__\nndarray.ctypesattribute.\nndarray.ctypes\n\n## Can I compile NumPy code usingtorch.compile?#\n\ntorch.compile\nOf course you do!torch.compileunderstands NumPy code natively, and treats it\nas if it were PyTorch code. To do so, simply wrap NumPy code with thetorch.compiledecorator.\ntorch.compile\ntorch.compile\n\n```python\nimport torch\nimport numpy as np\n\n@torch.compile\ndef numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = np.random.randn(1024, 64)\nY = np.random.randn(1024, 64)\nZ = numpy_fn(X, Y)\nassert isinstance(Z, np.ndarray)\n\n```\n\nExecuting this example with the environment variableTORCH_LOGS=output_code, we can see\nthattorch.compilewas able to fuse the multiplication and the sum into one C++ kernel.\nIt was also able to execute them in parallel using OpenMP (native NumPy is single-threaded).\nThis can easily make your NumPy codentimes faster, wherenis the number of cores\nin your processor!\nTORCH_LOGS=output_code\ntorch.compile\nn\nn\nTracing NumPy code this way also supports graph breaks within the compiled code.\n\n## Can I execute NumPy code on CUDA and compute gradients viatorch.compile?#\n\ntorch.compile\nYes you can! To do so, you may simply execute your code within atorch.device(\"cuda\")context. Consider the example\ntorch.device(\"cuda\")\n\n```python\nimport torch\nimport numpy as np\n\n@torch.compile\ndef numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = np.random.randn(1024, 64)\nY = np.random.randn(1024, 64)\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\nassert isinstance(Z, np.ndarray)\n\n```\n\nIn this example,numpy_fnwill be executed in CUDA. For this to be\npossible,torch.compileautomatically movesXandYfrom CPU\nto CUDA, and then it moves the resultZfrom CUDA to CPU. If we are\nexecuting this function several times in the same program run, we may want\nto avoid all these rather expensive memory copies. To do so, we just need\nto tweak ournumpy_fnso that it accepts cuda Tensors and returns tensors.\nWe can do so by usingtorch.compiler.wrap_numpy:\nnumpy_fn\ntorch.compile\nX\nY\nZ\nnumpy_fn\ntorch.compiler.wrap_numpy\n\n```python\n@torch.compile(fullgraph=True)\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = torch.randn(1024, 64, device=\"cuda\")\nY = torch.randn(1024, 64, device=\"cuda\")\nZ = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nassert Z.device.type == \"cuda\"\n\n```\n\nHere, we explicitly create the tensors in CUDA memory, and pass them to the\nfunction, which performs all the computations on the CUDA device.wrap_numpyis in charge of marking anytorch.Tensorinput as an input\nwithnp.ndarraysemantics at atorch.compilelevel. Marking tensors\ninside the compiler is a very cheap operation, so no data copy or data movement\nhappens during runtime.\nwrap_numpy\ntorch.Tensor\nnp.ndarray\ntorch.compile\nUsing this decorator, we can also differentiate through NumPy code!\n\n```python\n@torch.compile(fullgraph=True)\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    return np.mean(np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1)))\n\nX = torch.randn(1024, 64, device=\"cuda\", requires_grad=True)\nY = torch.randn(1024, 64, device=\"cuda\")\nZ = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nZ.backward()\n# X.grad now holds the gradient of the computation\nprint(X.grad)\n\n```\n\nWe have been usingfullgraph=Trueas graph break are problematic in this context.\nWhen a graph break occurs, we need to materialize the NumPy arrays. Since NumPy arrays\ndo not have a notion ofdeviceorrequires_grad, this information is lost during\na graph break.\nfullgraph=True\ndevice\nrequires_grad\nWe cannot propagate gradients through a graph break, as the graph break code may execute\narbitrary code that don\u2019t know how to differentiate. On the other hand, in the case of\nthe CUDA execution, we can work around this problem as we did in the first example, by\nusing thetorch.device(\"cuda\")context manager:\ntorch.device(\"cuda\")\n\n```python\n@torch.compile\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    prod = X[:, :, None] * Y[:, None, :]\n    print(\"oops, a graph break!\")\n    return np.sum(prod, axis=(-2, -1))\n\nX = torch.randn(1024, 64, device=\"cuda\")\nY = torch.randn(1024, 64, device=\"cuda\")\n\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nassert Z.device.type == \"cuda\"\n\n```\n\nDuring the graph break, the intermediary tensors still need to be moved to CPU, but when the\ntracing is resumed after the graph break, the rest of the graph is still traced on CUDA.\nGiven this CUDA <> CPU and CPU <> CUDA movement, graph breaks are fairly costly in the NumPy\ncontext and should be avoided, but at least they allow tracing through complex pieces of code.\n\n## How do I debug NumPy code undertorch.compile?#\n\ntorch.compile\nDebugging JIT compiled code is challenging, given the complexity of modern\ncompilers and the daunting errors that they raise.The torch.compile troubleshooting doccontains a few tips and tricks on how to tackle this task.\nIf the above is not enough to pinpoint the origin of the issue, there are still\na few other NumPy-specific tools we can use. We can discern whether the bug\nis entirely in the PyTorch code by disabling tracing through NumPy functions:\n\n```python\nfrom torch._dynamo import config\nconfig.trace_numpy = False\n\n```\n\nIf the bug lies in the traced NumPy code, we can execute the NumPy code eagerly (withouttorch.compile)\nusing PyTorch as a backend by importingimporttorch._numpyasnp.\nThis should just be used fordebugging purposesand is in no way a\nreplacement for the PyTorch API, as it ismuch less performantand, as a\nprivate API,may change without notice. At any rate,torch._numpyis a\nPython implementation of NumPy in terms of PyTorch and it is used internally bytorch.compileto\ntransform NumPy code into Pytorch code. It is rather easy to read and modify,\nso if you find any bug in it feel free to submit a PR fixing it or simply open\nan issue.\ntorch.compile\nimporttorch._numpyasnp\ntorch._numpy\ntorch.compile\nIf the program does work when importingtorch._numpyasnp, chances are\nthat the bug is in TorchDynamo. If this is the case, please feel free to open an issue\nwith aminimal reproducer.\ntorch._numpyasnp\n\n## Itorch.compilesome NumPy code and I did not see any speed-up.#\n\ntorch.compile\nThe best place to start is thetutorial with general advice for how to debug these sort of torch.compile issues.\nSome graph breaks may happen because of the use of unsupported features. SeeWhich NumPy features does torch.compile support?. More generally, it is useful to keep in mind\nthat some widely used NumPy features do not play well with compilers. For\nexample, in-place modifications make reasoning difficult within the compiler and\noften yield worse performance than their out-of-place counterparts.As such, it is best to avoid\nthem. Same goes for the use of theout=parameter. Instead, prefer\nout-of-place ops and lettorch.compileoptimize the memory use. Same goes\nfor data-dependent ops like masked indexing through boolean masks, or\ndata-dependent control flow likeiforwhileconstructions.\nout=\ntorch.compile\nif\nwhile\n\n## Which API to use for fine grain tracing?#\n\nIn some cases, you might need to exclude small parts of your code from the\ntorch.compile compilations. This section provides some of the answers and\nyou can find more information inTorchDynamo APIs for fine-grained tracing.\n\n## How do I graph break on a function?#\n\nGraph break on a function is not enough to sufficiently express what you want\nPyTorch to do. You need to be more specific about your use case. Some of the\nmost common use cases you might want to consider:\nIf you want to disable compilation on this function frame and the recursively\ninvoked frames, usetorch._dynamo.disable.\ntorch._dynamo.disable\nIf you want a particular operator, such asfbgemmto use the eager mode,\nusetorch._dynamo.disallow_in_graph.\nfbgemm\ntorch._dynamo.disallow_in_graph\nSome of the uncommon use cases include:\nIf you want to disable TorchDynamo on the function frame but enable it back\non the recursively invoked frames \u2013 usetorch._dynamo.disable(recursive=False).\ntorch._dynamo.disable(recursive=False)\nIf you want to prevent inlining of a function frame \u2013 usetorch._dynamo.graph_breakat the beginning of the function you want to prevent inlining.\ntorch._dynamo.graph_break\n\n## What\u2019s the difference betweentorch._dynamo.disableandtorch._dynamo.disallow_in_graph#\n\ntorch._dynamo.disable\ntorch._dynamo.disallow_in_graph\nDisallow-in-graph works at the level of operators, or more specifically,\nthe operators that you see in the TorchDynamo extracted graphs.\nDisable works at the function frame level and decides if TorchDynamo\nshould look into the function frame or not.\n\n## What\u2019s the difference betweentorch._dynamo.disableandtorch._dynamo_skip#\n\ntorch._dynamo.disable\ntorch._dynamo_skip\nNote\ntorch._dynamo_skipis deprecated.\ntorch._dynamo_skip\nYou most likely needtorch._dynamo.disable. But in an unlikely scenario, you\nmight need even finer control. Suppose you want to disable the tracing on just\nthea_fnfunction, but want to continue the tracing back inaa_fnandab_fn. The image below demonstrates this use case:\ntorch._dynamo.disable\na_fn\naa_fn\nab_fn\nIn this case, you can usetorch._dynamo.disable(recursive=False).\nIn previous versions, this functionality was provided bytorch._dynamo.skip.\nThis is now supported by therecursiveflag insidetorch._dynamo.disable.\ntorch._dynamo.disable(recursive=False)\ntorch._dynamo.skip\nrecursive\ntorch._dynamo.disable",
    "url": "https://pytorch.org/docs/stable/torch.compiler_faq.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "90eee2dcb5c43023001e7d49280a624d",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/export.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5936023e0e9a2eebd833bc68586db815",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/user_guide/pytorch_main_components.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a78db2f76798ba13a46fb6f8817fc524",
    "source": "pytorch_docs",
    "title": "torch.cpu \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.cpu#\n\nCreated On: Jul 11, 2023 | Last Updated On: Oct 11, 2023\nThis package implements abstractions found intorch.cudato facilitate writing device-agnostic code.\ntorch.cuda\ncurrent_device\n\ncurrent_device\nReturns current device for cpu.\ncurrent_stream\n\ncurrent_stream\nReturns the currently selectedStreamfor a given device.\nStream\nis_available\n\nis_available\nReturns a bool indicating if CPU is currently available.\nsynchronize\n\nsynchronize\nWaits for all kernels in all streams on the CPU device to complete.\nstream\n\nstream\nWrapper around the Context-manager StreamContext that selects a given stream.\nset_device\n\nset_device\nSets the current device, in CPU we do nothing.\ndevice_count\n\ndevice_count\nReturns number of CPU devices (not cores).\nStreamContext\n\nStreamContext\nContext-manager that selects a given stream.\n\n## Streams and events#\n\nStream\n\nStream\nN.B.",
    "url": "https://pytorch.org/docs/stable/cpu.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "de6411acb5c92f6d8b277c0a41360a66",
    "source": "pytorch_docs",
    "title": "PyTorch Governance | Mechanics \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch Governance | Mechanics#\n\nCreated On: Mar 11, 2019 | Last Updated On: Apr 16, 2025\n\n## Summary#\n\nPyTorch adopts a technical governance structure that is hierarchical.\nA community ofcontributorswho file issues, make pull requests,\nand contribute to the project.\nA small set ofmodule maintainersdrive each module of the PyTorch\nproject.\nThey are overseen bycore maintainers, who drive the\noverall project direction.\nThe core maintainers have alead core maintainerwho is the catch-all decision maker.\nAll maintainers are expected to have a strong bias towards\nPyTorch\u2019s design philosophy.\nBeyond the maintainers, the community is encouraged to contribute,\nfile issues, make proposals, review pull requests and be present\nin the community. Given contributions and willingness to invest,\nanyone can be accepted as a maintainer and provided write access\nor ownership of parts of the codebase.\nTechnical governance is strictly separated from business governance.\nSeparating technical from business governance ensures that there is\nno way for any person or company to \u201cbuy their way into\u201d the\ntechnical guidance of the project. Additionally, membership in\nthe technical governance process is forindividuals, not companies.\nThat is, there are no seats reserved for specific companies, and\nmembership is associated with the person rather than the company\nemploying that person.\n\n## Module Maintainers#\n\nModules are defined as GitHub repositories within the PyTorch org,\nor as directories within the core repositorypytorch/pytorch.\nEach module will have its own maintainer group. Maintainer\ngroups are responsible for reviewing and approving commits,\nimproving design, and changing the scope of the module.\nEach maintainer group may adopt its own rules and procedures\nfor making decisions (majority vote being default). Module\nmaintainers have the right to dispute decisions made by other\nmodule maintainers \u2013 especially if it affects them. When\ndisputes are made, the module maintainer group should\nprovide a reasonable and public explanation of the dispute,\nthe relevant arguments, and the resolution. In the exceptional\ncases where module maintainers cannot come to a conclusion\nthemselves, they will escalate to core maintainers for review.\nThe escalations are resolved by the core maintainers in\naccordance with their rules and procedures.\nEach maintainer group should publish publicly available\ncommunication for their module (a vision, rough roadmap,\ndesign docs, any disputes and dispute resolutions) so that\ncontributors and other interested parties understand the\nfuture direction of the project and can participate in discussion.\nResponsibilities of the maintainer includes:\nTriaging high priority issues of the module\nTriaging and reviewing and landing high priority pull requests of the module\nSupporting public documentation related to the module\nRunning public developer meetings\n\n## Core Maintainers#\n\nThe core maintainers are expected to have a deep understanding\nof the PyTorch code base and design philosophies. Their responsibilities\ninclude:\nArticulating a cohesive long-term vision for the project\nNegotiating and resolving contentious issues in ways\nacceptable to all parties involved\nReceiving broad requests for changes from stakeholders of\nPyTorch and evaluating / accepting them (small module-level\nrequests are handled by module maintainers)\nThe core maintainers as a group have the power to veto any\ndecision made at a Module maintainer level. The core\nmaintainers have power to resolve disputes as they see fit.\nThe core maintainers should publicly articulate their\ndecision-making, and give a clear reasoning for their\ndecisions, vetoes and dispute resolution.\nThe core maintainers are admins of the PyTorch GitHub Org\nand are listed inMaintainers.\n\n## Lead Core Maintainer (BDFL)#\n\nThere may be decisions in which the core maintainers cannot\ncome to a consensus. To make such difficult decisions, the\ncore maintainers have an assigned and publicly declared Lead\nCore Maintainer amongst them, also commonly known in open-source\ngovernance models as a BDFL.\nThe Lead Core Maintainer should publicly articulate their\ndecision-making, and give a clear reasoning for their\ndecisions. The Lead Core Maintainer is also responsible for\nconfirming or removing core maintainers.\n\n## Nominating, Confirming and Removing Maintainers#\n\n\n## The Principles#\n\nMembership in module maintainer groups is given toindividualsonmerit basisafter they demonstrated strong expertise of the\ncomponent through contributions, reviews and discussions and are\naligned with how the component fits in overall PyTorch direction.\nFor membership in the maintainer group the individual has to\ndemonstrate strong and continued alignment with the overall\nPyTorch principles.\nNo term limits for module maintainers or core maintainers\nLight criteria of moving module maintenance to \u2018emeritus\u2019\nstatus if they don\u2019t actively participate over long periods\nof time. Each module maintainer group may define the inactive\nperiod that\u2019s appropriate for that module.\nThe membership is for an individual, not a company.\n\n## The Process for Nomination#\n\nEach module has its own process. Please contact module maintainers for more information.\nHowever, if there is no process identified, you can file a request to the core\nmaintainers by submittingthis form.\nCore maintainers are meeting every three months.\nIf you are submitting a request to the core maintainers, the information in your request\nmust include the following items:\nThe nominees depth and breadth of code, review and design\ncontributions on the module\nTestimonials (positive and negative) of the nominee\u2019s interactions\nwith the maintainers, users, and the community\nGeneral testimonials of support from the maintainers\nThe core maintainers then evaluate all information and make\na final decision to Confirm or Decline the nomination. The\ndecision of the core maintainers has to be articulated well\nand would be public.\n\n## The Process for Removal#\n\nSimilar to the process for nomination, anyone in the community\ncan nominate a person to be removed from a Module maintainer\nposition or a Core maintainer position.\nA person can also self-nominate to be removed\nThe core maintainers (excluding persons with conflict of\ninterest) will request or put together more information around\nthe following:\nTheir activity (or lack of) on the project\nTheir changing thinking of the space, which results in\nconflict with the overall direction of the project\nOther information that makes them unfit to be a maintainer,\nsuch as Code of Conduct issues, their activity outside the\nscope of the project that conflicts with the project\u2019s values\nConflicts of interest: filial or romantic relationships\nThe core maintainers then evaluate all information and make\na final decision to Confirm or Decline the removal. The decision\nof the core maintainers has to be articulated well and would be\npublic.\n\n## Nominating Core Maintainers#\n\nAny core or module maintainer can nominate someone to become a\ncore maintainer\nThe lead maintainer (BDFL) is responsible for evaluating the\nnomination.\nThe lead maintainer requests or puts together more information\naround the strength of the candidate to be a core maintainer:\nLetters of support from other core and module maintainers\nGeneral letters of support from stakeholders within the PyTorch\ncommunity\nAny new relevant information that is befitting for the candidacy\nThe lead maintainer evaluates all information and makes a final\ndecision to Confirm or Decline the nomination, with a clear public\narticulation of their reasoning behind the decision.\n\n## Removing the Lead Core Maintainer and Nominating a New Lead Core Maintainer#\n\nA super-majority of core maintainers (75%) can choose to\nremove the Lead Core Maintainer\nAfter a removal of the Lead Core Maintainer or in unforeseen\ncircumstances (such as permanent unavailability of the Lead Core\nMaintainer), the core maintainers follow a Ranked-Choice voting\nmethod to elect a new Lead Core Maintainer.\n\n## Add, Remove, and Re-Scope Modules and Projects#\n\nThe core maintainers together are responsible for taking\ndecisions on adding, removing and re-scoping new modules\nin the PyTorch org, either as new repositories in the\nPyTorch GitHub org, or as folders in thepytorch/pytorchrepository.\nThey invite proposals from members in the community\n(including themselves) for such changes.\nThe proposals are open-ended, but should have some basic\nground-work to make a convincing case to make change. The\nfollowing is an example approach to this process:\nInterview researchers / stakeholders, talk to community, gather issues;\nRead papers, attend conferences, build example pipelines based on experience;\nCreate a state of the world - make sure this change is necessary,\nfor example adding a new project or module is worth the maintenance\ncost; or removing a project or module will not remove too much value\nfrom PyTorch;\nCreate a proposal; the proposal covers the maintainership, development\nand community plan once the proposal is approved.\nThe core maintainers take final decisions on the proposal, articulating\nthe reasoning behind the decision publicly.\n\n## Decision Making#\n\n\n## Uncontroversial Changes#\n\nPrimary work happens through issues and pull requests on\nGitHub. Maintainers should avoid pushing their changes directly to\nthe PyTorch repository, instead relying on pull requests. Approving a\npull request by a core or module maintainer allows it to be merged\nwithout further process. Core and module maintainers, as listed on\ntheMaintainerspage and withinCODEOWNERSultimately approve these changes.\nNotifying relevant experts about an issue or a pull request\nis important. Reviews from experts in the given interest area are\nstrongly preferred, especially on pull request approvals. Failure to do\nso might end up with the change being reverted by the relevant expert.\n\n## Controversial Decision Process#\n\nSubstantial changes in a given interest area require a GitHub issue to\nbe opened for discussion. This includes:\nAny semantic or syntactic change to the PyTorch framework or library.\nBackwards-incompatible changes to the Python or C++ API.\nAdditions to the core framework or library, including substantial new\nfunctionality within an existing library.\nRemoval of core features or platform support\nCore and module maintainers ultimately approve these changes.\n\n## General Project Policies#\n\nPyTorch has been established as PyTorch a Series of LF Projects, LLC.\nPolicies applicable to PyTorch and participants in PyTorch, including\nguidelines on the usage of trademarks, are located athttps://www.lfprojects.org/policies/.\nPyTorch participants acknowledge that the copyright in all new contributions\nwill be retained by the copyright holder as independent works of authorship\nand that no contributor or copyright holder will be required to assign copyrights\nto the project. Except as described below, all code contributions to the project\nmust be made using the 3-Clause-BSD License available here:https://opensource.org/licenses/BSD-3-Clause(the \u201cProject License\u201d).\nAll outbound code will be made available under the Project License.\nThe Maintainers may approve the use of an alternative open license or\nlicenses for inbound or outbound contributions on an exception basis.\n\n## FAQ#\n\nQ: What if I would like to own (or partly own) a part of the project\nsuch as a feature area or domain library, for exampleLinear AlgebraorTorch Vision?This is absolutely possible.\nThe first step is to start contributing to the existing project area and\nsupporting its health and success. In addition to this, you can\nmake a proposal through a GitHub issue for new functionality or changes\nto improve the project area.\nQ: What if I am a company looking to use PyTorch internally for\ndevelopment, can I be granted or purchase a board seat to drive the\nproject direction?No, the PyTorch project is strictly driven by the\na maintainer project philosophy and clearly separates technical\ngovernance from business governance. However, if you want to be\ninvolved in sponsorship and support, you can become involved in the\nPyTorch Foundation (PTF) and sponsorship through this. You can also\nhave individual engineers look to become maintainers, but this is\nnot guaranteed and is merit-based.\nQ: Does the PyTorch project support grants or ways to support\nindependent developers using or contributing to the project?No, not\nat this point. We are however looking at ways to better support the\ncommunity of independent developers around PyTorch. If you have\nsuggestions or inputs, please reach out on the PyTorch forums to\ndiscuss.\nQ: How do I contribute code to the project?If the change is\nrelatively minor, a pull request on GitHub can be opened up immediately\nfor review and merge by the project committers. For larger changes,\nplease open an issue to make a proposal to discuss prior. Please also\nsee thePyTorch Contributor\nWikifor contribution\nfor a walkthrough.\nQ: Can I become a committer on the project?Unfortunately, the\ncurrent commit process to PyTorch involves an interaction with Facebook\ninfrastructure that can only be triggered by Facebook employees. We are\nhowever looking at ways to expand the committer base to individuals\noutside of Facebook and will provide an update when the tooling exists\nto allow this.\nQ: What if I would like to deliver a PyTorch tutorial at a conference\nor otherwise? Do I need to be \u2018officially\u2019 a committer to do this?No,\nwe encourage community members to showcase their work wherever and\nwhenever they can. Please reach out tomarketing@pytorch.orgfor marketing support.",
    "url": "https://pytorch.org/docs/stable/community/governance.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "834e6766b05221dfdbe02475867f8e2f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/torch.compiler.cudagraph_mark_step_begin.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8a7ed49b2ac1e63fe4b58b9e52af2535",
    "source": "pytorch_docs",
    "title": "Writing Graph Transformations on ATen IR \u2014 PyTorch 2.9 documentation",
    "text": "\n## Writing Graph Transformations on ATen IR#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\n\n## Passes#\n\nSince the ATen IR sits at the FX Graph/GraphModule level, any\ntransformations written for FX Graphs can be easily applied onto the\nATen IR. If you\u2019re familiar with writing FX graph transformations, then\nthis will be the same.\nThe most direct way of writing transformations is by looping through the\ngiven graph and directly manipulating the nodes within the graph.\nFor example, let\u2019s say we want to replacetorch.ops.aten.add.Tensor()calls withtorch.ops.aten.mul.Tensor()calls:\ntorch.ops.aten.add.Tensor()\ntorch.ops.aten.mul.Tensor()\n\n```python\nimport torch\n\ndef replace_add_with_mul(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n    for node in gm.graph.nodes:\n        if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor:\n            node.target = torch.ops.aten.mul.Tensor\n\n```\n\nWe can also delete and append new nodes through FX utility functions\nthat can be found in theGraphdocumentation. For example, if we want to insert atorch.ops.aten.relu.default()after theaddcall:\ntorch.ops.aten.relu.default()\nadd\n\n```python\nimport torch\n\ndef insert_relu_after_add(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n    for node in gm.graph.nodes:\n        if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor:\n\n            # Specifies the insertion point. Any nodes added to the graph within\n            # this scope will be inserted after `node`\n            with gm.graph.inserting_after(node):\n                # Insert a new `call_function` node with op `torch.ops.aten.relu.default`\n                new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, args=(node,))\n                # Replace all the places that use `node` to now use the `new_relu_node`\n                node.replace_all_uses_with(new_relu_node)\n\n```\n\nIn general, transformations can be roughly categorized into a couple of\naxis:\nAxis A: 1. Creating one-to-X mapping (eg. decomposition) 2. Creating\nmany-to-one mapping (eg. fusion)\nAxis B: 1. Doing forwards iteration (eg. shape propagation) 2. Doing\nbackwards iteration (eg. dead code elimination)\nAxis C: 1. Dependent on local node information (eg. out-variant\nconversion) 2. Dependent on global graph information (eg. memory\nplanning)\nOur projection on the frequency of these use cases are: 1. A.1, B.1, C.1\n2. A.2 3. B.2, C.2\nAlthough we can make all graph transformations through directly\nmanipulating the graph, we also provide some helper utilities for some\nease of use for the level 1 and 2 use-cases.\n\n## Transformer#\n\nFor level 1 uses cases (creating one-to-X mappings, doing forwards\niterations, and looking at local node information), we can utilize theTransformerclass to execute each node and recreate a graph, except with the\ntransformations specified.\nAn example for one-to-one mappings, if we wanted to replace an op A with\nanother op B, we can run the GraphModule, and very time we see op A,\nreturn op B.\nAn example is:\n\n```python\nclass ReplaceAddWithMul(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        if target != torch.ops.aten.add.Tensor:\n            return super().call_function(target, args, kwargs)\n        return super().call_function(torch.ops.aten.mul.Tensor, args, kwargs)\n\ntransformed_graph_module = ReplaceAddWithMul(graph_module).transform()\n\n```\n\nThesuper().call_function(target,args,kwargs,meta)call creates acall_functionFX node, and returns the result of running the\noperator with the given arguments.\nsuper().call_function(target,args,kwargs,meta)\ncall_function\nIf we wanted to do one-to-X mappings, like replacing op A with 2 other\nops B and C, we would then make 2 calls tosuper().call_functionto\ncreate 2 FX nodes, one with op B and another with op C, and return the\nresult of running op C.\nsuper().call_function\nFor example:\n\n```python\nclass ReplaceAddWithMulSub(torch.fx.Transformer):\n    \"\"\"\n    Original:\n        def f(x, y):\n            return x + y\n\n    After pass:\n        def f(x, y):\n            z = x * y\n            return z - y\n    \"\"\"\n    def call_function(self, target, args, kwargs):\n        if target != torch.ops.aten.add.Tensor:\n            return super().call_function(target, args, kwargs)\n\n        x, y = args\n\n        mul_res = super().call_function(torch.ops.aten.mul.Tensor, args, {})\n        return super().call_function(torch.ops.aten.sub.Tensor, (mul_res, y), {})\n\ntransformed_graph_module = ReplaceAddWithMulSub(graph_module).transform()\n\n```\n\nIf we wanted to remove an op, we can just return the value passed into\nthe function:\n\n```python\nclass RemoveDetachPass(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        if target not in (\n            torch.ops.aten.detach.default,\n            torch.ops.aten.detach_copy.default,\n        ):\n            return super().call_function(target, args, kwargs, meta)\n\n        assert len(args) == 1\n        return args[0]\n\ntransformed_graph_module = RemoveDetachPass(graph_module).transform()\n\n```\n\nAn example of utilizing local node information is, if we wanted to\nconvert all the scalars within the graph to tensors, we can run the\ngivenfx.GraphModule, and for every argument that contains a scalar,\nwe convert it to a tensor. It might look something like:\nfx.GraphModule\n\n```python\ndef args_map(target, fn, args, kwargs):\n    assert isinstance(args, tuple)\n    assert isinstance(kwargs, dict)\n    args = list(args)\n    kwargs = kwargs.copy()\n\n    # Update the argument based on the function passed\n    def update(key, args, schema):\n        args[key] = fn(args[key], schema)\n\n    # Update each argument in the schema\n    for i, schema in enumerate(target._schema.arguments):\n        if schema.name in kwargs:\n            update(schema.name, kwargs, schema)\n        elif not schema.kwarg_only and i < len(args):\n            update(i, args, schema)\n    return tuple(args), kwargs\n\nclass ScalarToTensorPass(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        breakpoint()\n        def try_coerce(value, arg):\n            return (\n                torch.tensor(value)\n                if isinstance(value, (float, int, bool))\n                and type(arg.type) == torch.TensorType\n                else value\n            )\n\n        args, kwargs = args_map(target, try_coerce, args, kwargs)\n        return super().call_function(target, args, kwargs)\n\ntransformed_graph_module = ScalarToTensorPass(graph_module).transform()\n\n```\n\n\n## Subgraph Rewriter#\n\nFor creating many-to-one mappings, we can utilize FX\u2019ssubgraph\nrewriter.\nGiven apattern, it creates a subgraph of operators matching to the\npattern, and then replaces each matched subgraph with thereplacement.\npattern\nreplacement\nNote:\n\n```python\nThis is an inplace operation.\n\n```\n\nThepatternandreplacementinputs must be callable functions or\nGraphModules containing the same operators that are used within the\ngraph (ATen ops) so that the subgraph rewriter can find the correct\npattern in the graph. Inputs to the pattern/replacement callables will\nbe treated as wildcards when matching.\npattern\nreplacement\nAn example:\n\n```python\nfrom torch.fx import subgraph_rewriter\n\ndef replace_patterns(graph_module):\n    def pattern(x, y):\n        x = torch.ops.aten.add.Tensor(x, y)\n        x = torch.ops.aten.mul.Tensor(x, y)\n        return x\n\n    def replacement(x, y):\n        return torch.ops.aten.sub.Tensor(x, y)\n\nreplaced_patterns = subgraph_rewriter.replace_pattern_with_filters(\n    traced_module, pattern, replacement\n)\n\n```\n\nThe subgraph rewriter returns a list ofReplacedPatterns:\nReplacedPatterns\n\n```python\n@dataclass\nclass ReplacedPatterns:\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n    # List of nodes that were added into the graph\n    replacements: List[Node]\n\n```\n\nNote:\n\n```python\nThe nodes created by the subgraph rewriter will not have the metadata that\nis populated in the matched nodes, but you can use\n`ReplacedPatterns.nodes_map` to find the nodes in the original graph that\nwere matched, and `ReplacedPatterns.replacements` to find the nodes that\nwere replaced in the transformed graph.\n\n```\n\n\n## Pass Manager#\n\nThePassManageris a class used to run multiple passes on a given graph module. When\ninitializing aPassManagerinstance, we pass in a list of passes\nthat we want to run and set a couple of flags. To run the collection of\npasses on a graph module, we can pass the graph module directly to thePassManagerinstance.\nPassManager\nPassManager\nAn example:\n\n```python\nfrom torch.fx.passes.infra.pass_manager import PassManager\n\npm = PassManager(\n    passes=[replace_add_with_div, replace_div_with_mul],\n    run_checks_after_each_pass=True,\n    suppress_check_failures=False,\n)\ngraph_module_out = pm(graph_module)\n\n```\n\nTo add a common set of checks that are run after each pass, we can call\nthe functionset_checks(check:Callable)which takes in a callable\nfunction as input. If therun_checks_after_each_passflag is set,\nthecheckwill be called after each pass is run on the graph module.\nset_checks(check:Callable)\nrun_checks_after_each_pass\ncheck\nAn example:\n\n```python\npm = PassManager(passes=[replace_add_with_div, replace_div_with_mul])\n\ndef check_div_target(graph_module):\n    for node in graph_module.graph.nodes:\n        if node.op == \"call_function\" and node.target != torch.div:\n            raise ValueError(\"Target should be div!\")\n\npm.add_checks(check_div_target)\n\npm(graph_module)    # raises ValueError after replace_div_with_mul pass\n\n```\n\n\n## Partitioner#\n\nThere are a couple of common FX graph based partitioners we can use to\npartition the graph.\n\n## Subgraph Matcher#\n\nFor finding subgraphs within a graph that match a specific pattern, we\ncan utilize FX\u2019sSubgraphMatcher.\nSubgraphMatcher\nClass Attributes:\npattern(Graph): The targeted matching pattern. Placeholder nodes\nin the graph will be treated as wildcards when matching.\npattern(Graph)\nmatch_output(bool): If True, output node in the pattern graph\nwill be treated as a part of the targeted pattern. If False, output\nnode is ignored during match.\nmatch_output(bool)\nmatch_placeholder(bool): If True, placeholder node in the\npattern graph will be treated as a part of the targeted pattern. If\nFalse, placeholder nodes will be used a wildcard.\nmatch_placeholder(bool)\nremove_overlapping_matches(bool): If True, in the case of\noverlapping matches, only the first match will be returned.\nremove_overlapping_matches(bool)\nignore_literals(bool): If True, will not check if literals are\nequal and will instead treat them as wildcards.\nignore_literals(bool)\nAn example:\n\n```python\nfrom torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n\nclass LargeModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._weight = torch.nn.Parameter(torch.ones(3, 3))\n        self._bias = torch.nn.Parameter(torch.ones(3, 3))\n\n    def forward(self, x):\n        return torch.ops.aten.addmm.default(self._bias, x, self._weight)\n\nlarge_model_graph = torch.export(LargeModel(), inputs).graph\n\nclass PatternModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._weight_1 = torch.nn.Parameter(torch.ones(5, 5))\n        self._bias_1 = torch.nn.Parameter(torch.ones(5, 5))\n\n    def forward(self, x):\n        return torch.ops.aten.addmm.default(self._bias_1, x, self._weight_1)\n\npattern_graph = torch.export(PatternModel(), inputs).graph\n\nsubgraph_matcher = SubgraphMatcher(pattern_graph)\nmatch_result = subgraph_matcher.match(large_model_graph)\n\n```\n\nThematchfunction returns a list ofInternalMatch:\nmatch\nInternalMatch\n\n```python\n@dataclass\nclass InternalMatch():\n    # Nodes from which the match was found\n    anchors: List[Node]\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node] = field(default_factory=dict)\n    # Nodes in target graph that are matched placeholder in pattern\n    placeholder_nodes: List[Node] = field(default_factory=list)\n    # Nodes in matched subgraph returned by output\n    returning_nodes: List[Node] = field(default_factory=list)\n\n```\n\n\n## Capability Based Partitioner#\n\nTo find the largest subgraphs of nodes that support a specific\ninvariant, we can utilize FX\u2019sCapabilityBasedPartitioner.\nCapabilityBasedPartitioner\nClass Attributes\ngraph_module(torch.fx.GraphModule): The graph module we are\npartitioning on.\ngraph_module(torch.fx.GraphModule)\noperator_support(OperatorSupportBase): The object used to\ndetermine if a node in the graph is supported in the partition.\noperator_support(OperatorSupportBase)\nallows_single_node_partition(bool): If True, allows single node\npartitions to be formed.\nallows_single_node_partition(bool)\nnon_compute_ops(Optional[Sequence[str]]): A set of ops that are\nconsidered to be \u201cnon-compute\u201d (extorch.ops.aten.viewand_operator.getitem, so that the partitioner will not create graphs\nthat only contain these non-compute ops\nnon_compute_ops(Optional[Sequence[str]])\ntorch.ops.aten.view\n_operator.getitem\nallowed_single_node_partition_ops(Optional[Sequence[str]]): A\nset of ops that are allowed to be in a single node partition.\nallowed_single_node_partition_ops(Optional[Sequence[str]])\nTheOperatorSupportBaseclass is used by the partitioner to determine if a specific node in the\ngraph belongs in the partition. This is done by overriding theis_node_supportedfunction. You can chain multipleOperatorSupportBaseby usingchain(which\nreturns False if any of the OperatorSupportBase return False) andany_chain(which returns True if any of the OperatorSupportBase returns True).\nOperatorSupportBase\nis_node_supported\nOperatorSupportBase\nchain\nany_chain\nAn example:\n\n```python\nfrom torch.fx.passes.infra.partitioner import CapabilityBasedPartitioner\nfrom torch.fx.passes.operator_support import any_chain, OperatorSupportBase\n\nclass AddMulOperatorSupport(OperatorSupportBase):\n    def is_node_supported(self, submodules, node: torch.fx.Node) -> bool:\n        return node.op == \"call_function\" and node.target in [\n            torch.ops.aten.add.Tensor, torch.ops.aten.mul.Tensor,\n        ]\n\ncapability_partitioner = CapabilityBasedPartitioner(\n    graph_module,\n    op_support,\n)\n\n# Returns a list of partitions (list of nodes that belong in each partition)\npartition_list = capability_partitioner.propose_partitions()\n# Fuses the partitions into graph modules and inserts `call_module` nodes in the graph\nfused_graph_module = capability_partitioner.fuse_partitions(partition_list)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/torch.compiler_transformations.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "761ff63b4d0e06a2c56d1e20349c120f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/multiprocessing.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "03be4c21ed6bc8524b4124f1d0d55645",
    "source": "pytorch_docs",
    "title": "torch.monitor \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.monitor#\n\nCreated On: Jan 12, 2022 | Last Updated On: Jun 11, 2025\nWarning\nThis module is a prototype release, and its interfaces and functionality may\nchange without warning in future PyTorch releases.\ntorch.monitorprovides an interface for logging events and counters from\nPyTorch.\ntorch.monitor\nThe stat interfaces are designed to be used for tracking high level metrics that\nare periodically logged out to be used for monitoring system performance. Since\nthe stats aggregate with a specific window size you can log to them from\ncritical loops with minimal performance impact.\nFor more infrequent events or values such as loss, accuracy, usage tracking the\nevent interface can be directly used.\nEvent handlers can be registered to handle the events and pass them to an\nexternal event sink.\n\n## API Reference#\n\nThese are types of aggregations that can be used to accumulate stats.\nMembers:\nVALUE returns the last value to be added.\nMEAN computes the arithmetic mean of all the added values.\nCOUNT returns the total number of added values.\nSUM returns the sum of the added values.\nMAX returns the max of the added values.\nMIN returns the min of the added values.\nStat is used to compute summary statistics in a performant way over\nfixed intervals. Stat logs the statistics as an Event once everywindow_sizeduration. When the window closes the stats are logged\nvia the event handlers as atorch.monitor.Statevent.\nwindow_size\ntorch.monitor.Stat\nwindow_sizeshould be set to something relatively high to avoid a\nhuge number of events being logged. Ex: 60s. Stat uses millisecond\nprecision.\nwindow_size\nIfmax_samplesis set, the stat will cap the number of samples per\nwindow by discardingaddcalls oncemax_samplesadds have\noccurred. If it\u2019s not set, alladdcalls during the window will be\nincluded. This is an optional field to make aggregations more directly\ncomparable across windows when the number of samples might vary.\nmax_samples\nmax_samples\nadd\nWhen the Stat is destructed it will log any remaining data even if the\nwindow hasn\u2019t elapsed.\nConstructs theStat.\nStat\nAdds a value to the stat to be aggregated according to the\nconfigured stat type and aggregations.\nNumber of data points that have currently been collected. Resets\nonce the event has been logged.\nReturns the current value of the stat, primarily for testing\npurposes. If the stat has logged and no additional values have been\nadded this will be zero.\nThe name of the stat that was set during creation.\ndata_value_t is one ofstr,float,int,bool.\nstr\nfloat\nint\nbool\nEvent represents a specific typed event to be logged. This can represent\nhigh-level data points such as loss or accuracy per epoch or more\nlow-level aggregations such as through the Stats provided through this\nlibrary.\nAll Events of the same type should have the same name so downstream\nhandlers can correctly process them.\nConstructs theEvent.\nEvent\nThe structured data contained within theEvent.\nEvent\nThe name of theEvent.\nEvent\nThe timestamp when theEventhappened.\nEvent\nEventHandlerHandle is a wrapper type returned byregister_event_handlerused to unregister the handler viaunregister_event_handler. This cannot be directly initialized.\nregister_event_handler\nunregister_event_handler\nlog_event logs the specified event to all of the registered event\nhandlers. It\u2019s up to the event handlers to log the event out to the\ncorresponding event sink.\nIf there are no event handlers registered this method is a no-op.\nregister_event_handler registers a callback to be called whenever an\nevent is logged vialog_event. These handlers should avoid blocking\nthe main thread since that may interfere with training as they run\nduring thelog_eventcall.\nlog_event\nlog_event\nunregister_event_handler unregisters theEventHandlerHandlereturned\nafter callingregister_event_handler. After this returns the event\nhandler will no longer receive events.\nEventHandlerHandle\nregister_event_handler\nTensorboardEventHandler is an event handler that will write known events to\nthe provided SummaryWriter.\nThis currently only supportstorch.monitor.Statevents which are logged\nas scalars.\ntorch.monitor.Stat\nExample\n\n```python\n>>> from torch.utils.tensorboard import SummaryWriter\n>>> from torch.monitor import TensorboardEventHandler, register_event_handler\n>>> writer = SummaryWriter(\"log_dir\")\n>>> register_event_handler(TensorboardEventHandler(writer))\n\n```\n\nConstructs theTensorboardEventHandler.\nTensorboardEventHandler",
    "url": "https://pytorch.org/docs/stable/monitor.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "42796b583d6772bff0fab08e675779a9",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/metrics.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9a19722b0410d0c8f7952e8be963eb83",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/python.control-flow.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "98e7b9510fa3345e95c814c1063a98f7",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/custom_operators.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "35332e200263a6ac74da66d3fda8327c",
    "source": "pytorch_docs",
    "title": "PT2 Archive Spec \u2014 PyTorch 2.9 documentation",
    "text": "\n## PT2 Archive Spec#\n\nCreated On: Jul 16, 2025 | Last Updated On: Sep 09, 2025\nThe following specification defines the archive format which can be produced\nthrough the following methods:\ntorch.exportthrough callingtorch.export.save()\ntorch.export.save()\nAOTInductorthrough callingtorch._inductor.aoti_compile_and_package()\ntorch._inductor.aoti_compile_and_package()\nThe archive is a zipfile, and can be manipulated using standard zipfile APIs.\nThe following is a sample archive. We will walk through the archive folder by folder.\n\n```python\n.\n\u251c\u2500\u2500 archive_format\n\u251c\u2500\u2500 byteorder\n\u251c\u2500\u2500 .data\n\u2502   \u251c\u2500\u2500 serialization_id\n\u2502   \u2514\u2500\u2500 version\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 aotinductor\n\u2502   \u2502   \u2514\u2500\u2500 model1\n\u2502   \u2502       \u251c\u2500\u2500 cf5ez6ifexr7i2hezzz4s7xfusj4wtisvu2gddeamh37bw6bghjw.kernel_metadata.json\n\u2502   \u2502       \u251c\u2500\u2500 cf5ez6ifexr7i2hezzz4s7xfusj4wtisvu2gddeamh37bw6bghjw.kernel.cpp\n\u2502   \u2502       \u251c\u2500\u2500 cf5ez6ifexr7i2hezzz4s7xfusj4wtisvu2gddeamh37bw6bghjw.wrapper_metadata.json\n\u2502   \u2502       \u251c\u2500\u2500 cf5ez6ifexr7i2hezzz4s7xfusj4wtisvu2gddeamh37bw6bghjw.wrapper.cpp\n\u2502   \u2502       \u251c\u2500\u2500 cf5ez6ifexr7i2hezzz4s7xfusj4wtisvu2gddeamh37bw6bghjw.wrapper.so\n\u2502   \u2502       \u251c\u2500\u2500 cg7domx3woam3nnliwud7yvtcencqctxkvvcafuriladwxw4nfiv.cubin\n\u2502   \u2502       \u2514\u2500\u2500 cubaaxppb6xmuqdm4bej55h2pftbce3bjyyvljxbtdfuolmv45ex.cubin\n\u2502   \u251c\u2500\u2500 weights\n\u2502   \u2502  \u251c\u2500\u2500 model1_weights_config.json\n\u2502   \u2502  \u251c\u2500\u2500 model2_weights_config.json\n\u2502   \u2502  \u251c\u2500\u2500 weight_0\n\u2502   \u2502  \u251c\u2500\u2500 weight_1\n\u2502   \u2502  \u251c\u2500\u2500 weight_2\n\u2502   \u2514\u2500\u2500 constants\n\u2502   \u2502  \u251c\u2500\u2500 model1_constants_config.json\n\u2502   \u2502  \u251c\u2500\u2500 model2_constants_config.json\n\u2502   \u2502  \u251c\u2500\u2500 tensor_0\n\u2502   \u2502  \u251c\u2500\u2500 tensor_1\n\u2502   \u2502  \u251c\u2500\u2500 custom_obj_0\n\u2502   \u2502  \u251c\u2500\u2500 custom_obj_1\n\u2502   \u2514\u2500\u2500 sample_inputs\n\u2502       \u251c\u2500\u2500 model1.pt\n\u2502       \u2514\u2500\u2500 model2.pt\n\u251c\u2500\u2500 extra\n\u2502   \u2514\u2500\u2500 ....json\n\u2514\u2500\u2500 models\n    \u251c\u2500\u2500 model1.json\n    \u2514\u2500\u2500 model2.json\n\n```\n\n\n## Contents#\n\n\n## Archive Headers#\n\narchive_formatdeclares the format used by this archive. Currently, it can only be \u201cpt2\u201d.\narchive_format\nbyteorder. One of \u201clittle\u201d or \u201cbig\u201d, used by zip file reader\nbyteorder\n/.data/versioncontains the archive version. (Notice that this is neither export serialization\u2019s schema version, nor Aten Opset Version).\n/.data/version\n/.data/serialization_idis a hash generated for the current archive, used for verification.\n/.data/serialization_id\n\n## AOTInductor Compiled Artifact#\n\nPath:/data/aotinductor/<model_name>-<backend>/\n/data/aotinductor/<model_name>-<backend>/\nAOTInductor compilation artifacts are saved for each model-backend pair. For\nexample, compilation artifacts for themodel1model on A100 and H100 will be\nsaved inmodel1-a100andmodel1-h100folders separately.\nmodel1\nmodel1-a100\nmodel1-h100\nThe folder typically contains\n<uuid>.wrapper.so: Dynamic library compiled from.cpp.\n<uuid>.wrapper.so\n<uuid>.wrapper.cpp: AOTInductor generated cpp wrapper file.\n<uuid>.wrapper.cpp\n<uuid>.kernel.cpp: AOTInductor generated cpp kernel file.\n<uuid>.kernel.cpp\n*.cubin: Triton kernels compiled from triton codegen kernels\n*.cubin\n<uuid>.wrapper_metadata.json: Metadata which was passed in from theaot_inductor.metadatainductor config\n<uuid>.wrapper_metadata.json\naot_inductor.metadata\n(optional)<uuid>.json: External fallback nodes for custom ops to be executed byProxyExecutor, serialized according toExternKernelNodestruct. If the model doesn\u2019t use custom ops/ProxyExecutor, this file would be omitted.\n<uuid>.json\nProxyExecutor\nExternKernelNode\n\n## Weights#\n\nPath:/data/weights/*\n/data/weights/*\nModel parameters and buffers are saved in the/data/weights/folder. Each\ntensor is saved as a separated file. The file only contains the raw data blob,\ntensor metadata and mapping from model weight FQN to saved raw data blob are saved separately in the<model_name>_weights_config.json.\n/data/weights/\n<model_name>_weights_config.json\n\n## Constants#\n\nPath:/data/constants/*\n/data/constants/*\nTensorConstants, non-persistent buffers and TorchBind objects are saved in the/data/constants/folder. Metadata and mapping from model constant FQN to saved raw data blob are saved separately in the<model_name>_constants_config.json\n/data/constants/\n<model_name>_constants_config.json\n\n## Sample Inputs#\n\nPath:/data/sample_inputs/<model_name>.pt\n/data/sample_inputs/<model_name>.pt\nThesample_inputused bytorch.exportcould be included in the archive for\ndownstream use. Typically, it\u2019s a flattened list of Tensors, combining both args\nand kwargs of the forward() function.\nsample_input\ntorch.export\nThe .pt file is produced bytorch.save(sample_input), and can be loaded bytorch.load()in python andtorch::pickle_load()in c++.\ntorch.save(sample_input)\ntorch.load()\ntorch::pickle_load()\nWhen the model has multiple copies of sample input, it would be packaged as<model_name>_<index>.pt.\n<model_name>_<index>.pt\n\n## Models Definitions#\n\nPath:/models/<model_name>.json\n/models/<model_name>.json\nModel definition is the serialized json of the ExportedProgram fromtorch.export.save, and other model-level metadata.\ntorch.export.save\n\n## Multiple Models#\n\nThis archive spec supports multiple model definitions coexisting in the same\nfile, with<model_name>serving as a unique identifier for the models, and\nwill be used as reference in other folders of the archive.\n<model_name>\nLower level APIs liketorch.export.pt2_archive._package.package_pt2()andtorch.export.pt2_archive._package.load_pt2()allow you to have\nfiner-grained control over the packaging and loading process.\ntorch.export.pt2_archive._package.package_pt2()\ntorch.export.pt2_archive._package.load_pt2()",
    "url": "https://pytorch.org/docs/stable/export/pt2_archive.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a569bc780ac97e00b0f0a1e9c185cbbb",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/faq.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "cf82d8b6951600e0854f6dce07ec41bc",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_troubleshooting.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "761553a459b553fc4e2e59c5b9f2a262",
    "source": "pytorch_docs",
    "title": "torch.nn.functional.scaled_dot_product_attention \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.nn.functional.scaled_dot_product_attention#\n\nis_causal=False, scale=None, enable_gqa=False) -> Tensor:\nComputes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed,\nand applying dropout if a probability greater than 0.0 is specified. The optional scale argument can only be\nspecified as a keyword argument.\n\n```python\n# Efficient implementation equivalent to the following:\ndef scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n        is_causal=False, scale=None, enable_gqa=False) -> torch.Tensor:\n    L, S = query.size(-2), key.size(-2)\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n    if is_causal:\n        assert attn_mask is None\n        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n        else:\n            attn_bias = attn_mask + attn_bias\n\n    if enable_gqa:\n        key = key.repeat_interleave(query.size(-3)//key.size(-3), -3)\n        value = value.repeat_interleave(query.size(-3)//value.size(-3), -3)\n\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n    attn_weight += attn_bias\n    attn_weight = torch.softmax(attn_weight, dim=-1)\n    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n    return attn_weight @ value\n\n```\n\nWarning\nThis function is beta and subject to change.\nWarning\nThis function always applies dropout according to the specifieddropout_pargument.\nTo disable dropout during evaluation, be sure to pass a value of0.0when the module\nthat makes the function call is not in training mode.\ndropout_p\n0.0\nFor example:\n\n```python\nclass MyModel(nn.Module):\n    def __init__(self, p=0.5):\n        super().__init__()\n        self.p = p\n\n    def forward(self, ...):\n        return F.scaled_dot_product_attention(...,\n            dropout_p=(self.p if self.training else 0.0))\n\n```\n\nNote\nThere are currently three supported implementations of scaled dot product attention:\nFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\nMemory-Efficient Attention\nA PyTorch implementation defined in C++ matching the above formulation\nThe function may call optimized kernels for improved performance when using the CUDA backend.\nFor all other backends, the PyTorch implementation will be used.\nAll implementations are enabled by default. Scaled dot product attention attempts to automatically select the\nmost optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation\nis used, the following functions are provided for enabling and disabling implementations.\nThe context manager is the preferred mechanism:\ntorch.nn.attention.sdpa_kernel(): A context manager used to enable or disable any of the implementations.\ntorch.nn.attention.sdpa_kernel()\ntorch.backends.cuda.enable_flash_sdp(): Globally enables or disables FlashAttention.\ntorch.backends.cuda.enable_flash_sdp()\ntorch.backends.cuda.enable_mem_efficient_sdp(): Globally enables or disables  Memory-Efficient Attention.\ntorch.backends.cuda.enable_mem_efficient_sdp()\ntorch.backends.cuda.enable_math_sdp(): Globally enables or disables  the PyTorch C++ implementation.\ntorch.backends.cuda.enable_math_sdp()\nEach of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation,\ndisable the PyTorch C++ implementation usingtorch.nn.attention.sdpa_kernel().\nIn the event that a fused implementation is not available, a warning will be raised with the\nreasons why the fused implementation cannot run.\ntorch.nn.attention.sdpa_kernel()\nDue to the nature of fusing floating point operations, the output of this function may be different\ndepending on what backend kernel is chosen.\nThe c++ implementation supports torch.float64 and can be used when higher precision is required.\nFor math backend, all intermediates are kept in torch.float if inputs are in torch.half or torch.bfloat16.\nFor more information please seeNumerical accuracy\nGrouped Query Attention (GQA) is an experimental feature. It currently works only for Flash_attention\nand math kernel on CUDA tensor, and does not support Nested tensor.\nConstraints for GQA:\nnumber_of_heads_query % number_of_heads_key_value == 0 and,\nnumber_of_heads_key == number_of_heads_value\nNote\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by settingtorch.backends.cudnn.deterministic=True. SeeReproducibilityfor more information.\ntorch.backends.cudnn.deterministic=True\nquery(Tensor) \u2013 Query tensor; shape(N,...,Hq,L,E)(N, ..., Hq, L, E)(N,...,Hq,L,E).\nkey(Tensor) \u2013 Key tensor; shape(N,...,H,S,E)(N, ..., H, S, E)(N,...,H,S,E).\nvalue(Tensor) \u2013 Value tensor; shape(N,...,H,S,Ev)(N, ..., H, S, Ev)(N,...,H,S,Ev).\nattn_mask(optional Tensor) \u2013 Attention mask; shape must be broadcastable to the shape of attention weights,\nwhich is(N,...,L,S)(N,..., L, S)(N,...,L,S). Two types of masks are supported.\nA boolean mask where a value of True indicates that the elementshouldtake part in attention.\nA float mask of the same type as query, key, value that is added to the attention score.\ndropout_p(float) \u2013 Dropout probability; if greater than 0.0, dropout is applied\nis_causal(bool) \u2013 If set to true, the attention masking is a lower triangular matrix when the mask is a\nsquare matrix. The attention masking has the form of the upper left causal bias due to the alignment\n(seetorch.nn.attention.bias.CausalBias) when the mask is a non-square matrix.\nAn error is thrown if both attn_mask and is_causal are set.\ntorch.nn.attention.bias.CausalBias\nscale(optional python:float,keyword-only) \u2013 Scaling factor applied prior to softmax. If None, the default value is set\nto1E\\frac{1}{\\sqrt{E}}E\u200b1\u200b.\nenable_gqa(bool) \u2013 If set to True, Grouped Query Attention (GQA) is enabled, by default it is set to False.\nAttention output; shape(N,...,Hq,L,Ev)(N, ..., Hq, L, Ev)(N,...,Hq,L,Ev).\noutput (Tensor)\nN:Batch\u00a0size...:Any\u00a0number\u00a0of\u00a0other\u00a0batch\u00a0dimensions\u00a0(optional)N: \\text{Batch size} ... : \\text{Any number of other batch dimensions (optional)}N:Batch\u00a0size...:Any\u00a0number\u00a0of\u00a0other\u00a0batch\u00a0dimensions\u00a0(optional)\nS:Source\u00a0sequence\u00a0lengthS: \\text{Source sequence length}S:Source\u00a0sequence\u00a0length\nL:Target\u00a0sequence\u00a0lengthL: \\text{Target sequence length}L:Target\u00a0sequence\u00a0length\nE:Embedding\u00a0dimension\u00a0of\u00a0the\u00a0query\u00a0and\u00a0keyE: \\text{Embedding dimension of the query and key}E:Embedding\u00a0dimension\u00a0of\u00a0the\u00a0query\u00a0and\u00a0key\nEv:Embedding\u00a0dimension\u00a0of\u00a0the\u00a0valueEv: \\text{Embedding dimension of the value}Ev:Embedding\u00a0dimension\u00a0of\u00a0the\u00a0value\nHq:Number\u00a0of\u00a0heads\u00a0of\u00a0queryHq: \\text{Number of heads of query}Hq:Number\u00a0of\u00a0heads\u00a0of\u00a0query\nH:Number\u00a0of\u00a0heads\u00a0of\u00a0key\u00a0and\u00a0valueH: \\text{Number of heads of key and value}H:Number\u00a0of\u00a0heads\u00a0of\u00a0key\u00a0and\u00a0value\nExamples\n\n```python\n>>> # Optionally use the context manager to ensure one of the fused kernels is run\n>>> query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> with sdpa_kernel(backends=[SDPBackend.FLASH_ATTENTION]):\n>>>     F.scaled_dot_product_attention(query,key,value)\n\n```\n\n\n```python\n>>> # Sample for GQA for llama3\n>>> query = torch.rand(32, 32, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n>>> with sdpa_kernel(backends=[SDPBackend.MATH]):\n>>>     F.scaled_dot_product_attention(query,key,value,enable_gqa=True)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0f01b64c33a2e7cba9560a499acbc5af",
    "source": "pytorch_docs",
    "title": "CUDAGraph Trees \u2014 PyTorch 2.9 documentation",
    "text": "\n## CUDAGraph Trees#\n\nCreated On: May 19, 2023 | Last Updated On: Jul 30, 2025\n\n## Background#\n\n\n## CUDAGraph#\n\nFor a longer background on CUDAGraphs, readaccelerating pytorch with CUDAGraphs.\nCUDA Graphs, which made its debut in CUDA 10, let a series of CUDA kernels to be defined and encapsulated as a single unit, i.e., a graph of operations, rather than a sequence of individually-launched operations. It provides a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduces the launching overheads.\nCUDA Graphs can give large speedups, especially for models with high CPU overhead or small compute. There are a number of limitations from requiring the same kernels to be run with the same arguments and dependencies, and memory addresses.\nControl Flow is not possible\nKernels which trigger host to device syncs (such as .item()) errors\nAll input arguments to kernels are fixed to what they were recorded\nCUDA Memory addresses are fixed, however the values of the memory at those addresses can change\nNo Essential CPU ops or CPU side effects\n\n## PyTorch CUDAGraph Integration#\n\nPyTorch provides aconvenience wrapperaround CUDAGraphs that handles a couple of tricky interactions with PyTorch\u2019s caching allocator.\nThe CachingAllocator uses a separate memory pool for all the new allocations. During CUDAGraph recording, memory is accounted for, allocated, and freed exactly as during eager run. On replay, just the kernels are invoked, and there are no changes to the allocator. Subsequent to initial recording, the allocator does not know which memory is actively being used in user programs.\nUsing a separate memory pool between eager allocations and cudagraph allocations may increase the memory of your program if there is substantial memory allocated to both.\n\n## Make Graphed Callables#\n\nMake Graphed Callablesis a PyTorch Abstraction to share a single memory pool over a series of callables. Graphed Callables takes advantage of the fact that on CUDA Graph recording, memory is exactly accounted for by the caching allocator to safely share memory between separate CUDA Graph recordings. In each invocation, outputs are preserved as live memory, preventing one callable from overwriting the live memory of another. Graphed Callables can only be invoked in a single order; memory addresses from the first run are burned into the second, and so forth.\n\n## TorchDynamo Previous CUDA Graphs Integration#\n\nRunning withcudagraph_trees=Falsedoes not reuse memory across separate graph captures, which can lead to large memory regressions. Even for a model that has no graph breaks, this has issues. The forward and backward are separate graph captures, so the memory pools for forward and backward are not shared. In particular, memory for activations that are saved in the forward cannot be reclaimed in the backward.\ncudagraph_trees=False\n\n## CUDAGraph Trees Integration#\n\nLike Graph Callables, CUDA Graph Trees use a single memory pool across all graph captures. However, instead of requiring a single sequence of invocations, CUDA Graph Trees create separate trees of CUDA Graph captures. Let\u2019s take a look at an illustrative example:\n\n```python\n@torch.compile(mode=\"reduce-overhead\")\ndef foo(x):\n    # GRAPH 1\n    y = x * x * x\n    # graph break triggered here\n    if y.sum() > 0:\n        # GRAPH 2\n        z = y ** y\n    else:\n        # GRAPH 3\n        z = (y.abs() ** y.abs())\n    torch._dynamo.graph_break()\n    # GRAPH 4\n    return z * torch.rand_like(z)\n\n# the first run warms up each graph, which does things like CuBlas or Triton benchmarking\nfoo(torch.arange(0, 10, device=\"cuda\"))\n# The second run does a CUDA Graph recording, and replays it\nfoo(torch.arange(0, 10, device=\"cuda\"))\n# Finally we hit the optimized, CUDA Graph replay path\nfoo(torch.arange(0, 10, device=\"cuda\"))\n\n```\n\nIn this example, there are two separate paths that we make through the function: 1 -> 2 -> 4, or 1 -> 3 -> 4.\nWe share all of the memory in a single memory pool between separate recordings by building up a tape of CUDA Graph recordings, in this instance, 1 -> 2 -> 4. We add invariants to ensure that memory is always in the same location as it were recorded, and no live tensors exist in user programs that might be overwritten.\nSame constraints from CUDA Graphs apply: same kernels must be invoked with the same arguments (static sizes, addresses, etc)\nThe same pattern of memory must be observed between recording and replay: if a tensor output of one graph dies subsequent to another graph during recording, it must also do so during replay.\nLive memory in the CUDA pool forces a dependence between two recordings\nThese recordings can only be invoked in a single order 1 - > 2 -> 4\nAll of the memory is shared in a single memory pool, so there is no additional memory overhead compared to eager. Now, what happens if we were to hit a new path and run Graph 3?\nGraph 1 gets replayed, and then we hit Graph 3, which we have not yet recorded. On graph replays, the private memory pool is not updated, so y is not reflected in the allocator. Without care, we would overwrite it. To support reusing the same memory pool after replaying other graphs, we checkpoint the memory pool back to its state at the end of graph 1. Now that our live tensors are reflected in the caching allocator, we are safe to run a new graph.\nFirst, we would hit the optimized, CUDAGraph.replay() path that we have already recorded in graph 1. Then we would hit Graph 3. Just as before, we will need to warm up the graph once before recording. On the warmup run, the memory addresses are not fixed, so graph 4 will also fallback to the inductor, non-cudagraph invocation.\nThe second time we hit graph 3 we are warmed up and ready to record. We record graph 3 and then record graph 4 again since the input memory addresses have changed. This creates a tree of CUDA Graph recordings. A CUDA Graph Tree!\n\n```python\n  1\n / \\\\\n2   3\n \\\\   \\\\\n  4   4\n\n```\n\n\n## Input Mutation Support#\n\nInput mutation function refers to a function conducting in-place writes to an input tensor,\nas illustrated below:\n\n```python\ndef foo(x, y):\n    # mutates input x\n    x.add_(1)\n    return x + y\n\n```\n\nInput mutation functions generally lead to challenges for CUDAGraph Trees. Due to the static\nCUDA memory address requirement from CUDAGraph, for each input tensor x, CUDAGraph Trees may\nallocate a static memory address x\u2019. During execution, CUDAGraph Trees first copy the input\ntensor x to the static memory address x\u2019, and then replay the recorded CUDAGraph. For input\nmutation function, x\u2019 is in-place updated, which is not reflected on the input tensor x since\nx and x\u2019 reside on different CUDA memory addresses.\nA closer look at input mutation functions reveals that there are three types of inputs:\ninputs from eager: These tensors we assume will vary input tensor addresses from\nexecution to execution. Because cudagraphs freeze memory addresses, we need to copy these\ninputs to a static address tensor prior to graph recording and execution.\nParameters and buffers: These tensors we assume (and runtime-check) have the same tensor\naddresses on every execution. We do not need to copy over their contents because the recorded\nmemory address will be the same as the executed memory address.\nTensors which are prior outputs from CUDAGraph Trees: Because the output tensor addresses\nof a cudagraph are fixed, if we run CUDAGraph1, then run CUDAGraph2, the inputs which came from\nCUDAGraph1 into CUDAGraph2 will have a fixed memory address. These inputs, like parameters and\nbuffers, do not require copying over to a static address tensor. We check to make sure that\nthese inputs are stable at runtime, and if they\u2019re not we will re-record.\nCUDAGraph Trees support input mutation on parameters and buffers, and tensors which are prior\noutputs from CUDAGraph Trees. For mutation on inputs from eager, CUDAGraph Trees will run the\nfunction without CUDAGraph and emitskipping due to mutated inputslog. The following example\nshows CUDAGraph Trees\u2019 support for tensors which are prior outputs from CUDAGraph Trees.\n\n```python\nimport torch\n\n@torch.compile(mode=\"reduce-overhead\")\ndef foo(x):\n    return x + 1\n\n@torch.compile(mode=\"reduce-overhead\")\ndef mut(x):\n    return x.add_(2)\n\n# Enable input mutation support\ntorch._inductor.config.triton.cudagraph_support_input_mutation = True\n\nfor i in range(3):\n    torch.compiler.cudagraph_mark_step_begin()\n    inp = torch.rand([4], device=\"cuda\")\n\n    # CUDAGraph is applied since `foo` does not mutate `inp`\n    tmp = foo(inp)\n    # Although `mut` mutates `tmp`, which is an output of a CUDAGraph\n    # managed function. So CUDAGraph is still applied.\n    mut(tmp)\n\n\ntorch.compiler.cudagraph_mark_step_begin()\ninp = torch.rand([4], device=\"cuda\")\n\ntmp = foo(inp)\n# While `tmp` is a CUDAGraph Tree managed function's output, `tmp.clone()`\n# is not. So CUDAGraph is not applied to `mut` and there is a log\n# `skipping cudagraphs due to mutated inputs`\nmut(tmp.clone())\n\n```\n\nTo enable CUDAGraph Trees for a function mutating inputs from eager, please re-write\nthe function to avoid input mutation.\nNoteEnable input mutation support by settingtorch._inductor.config.cudagraph_support_input_mutation = Truefor \u201creduce-overhead\u201d mode.\n\n## Dynamic Shape Support#\n\nDynamic shapemeans that an input tensor has different shapes across function calls. Since CUDAGraph\nrequires fixed tensor addresses, CUDAGraph Trees re-record CUDAGraph for every unique\nshape of an input tensor. This leads to multiple CUDAGraphs for a single inductor graph.\nWhen there are limited shapes (e.g., batch sizes in inference), it is profitable to\nre-record CUDAGraphs. However, if input tensor shapes change frequently or even on\nevery invocation, re-recording CUDAGraph may not be profitable. Nvidia uses 64 KB of\ndevice memory per kernel launch in CUDAGraph, up until CUDA 12.4 and Driver Version 550+.\nThis memory cost can be significant with many CUDAGraph re-recordings.\nFor functions with frequently changing input tensor shapes, we suggest padding input\ntensors to a few fixed tensor shapes to still enjoy benefits from CUDAGraph. In addition,\nsettingtorch._inductor.config.triton.cudagraph_skip_dynamic_graphs=Trueallows to skip cudagraphing functions with dynamic shape inputs and only cudagraphing\nfunctions with static input tensor shapes.\n\n## NCCL Support#\n\nCUDAGraph Trees support functions with nccl operators. While CUDAGraph Trees perform per-device\nrecord for CUDAGraph, NCCL support allows cross-device communication.\n\n```python\n@torch.compile(mode=\"reduce-overhead\")\ndef func(x):\n    y = x * x\n    y = torch.distributed.all_reduce(y, op=torch.distributed.ReduceOp.SUM)\n    x = torch.nn.functional.silu(x)\n    return x * y\n\n```\n\n\n## Reasons for Skipping CUDAGraph#\n\nSince CUDAGraph has requirements such as static input tensor addresses and not supporting\nCPU operators, CUDAGraph Trees check whether a function satisfies these requirements and\nmay skip CUDAGraph when necessary. Here, we list common reasons for skipping CUDAGraph.\nInput mutation: CUDAGraph Trees skip functions that in-place mutates eager input.\nIn-place mutating parameters and buffers, or output tensors from CUDAGraph Tree managed\nfunctions are still supported. Please seeInput Mutation Supportsection for more details.\nCPU operators: Functions containing CPU operator are skipped. Please split the\nfunction into multiple functions and apply CUDAGraph Trees on functions with only GPU operators.\nMulti-device operators: A function is skipped if it contains operators on multiple\ndevices. Currently, CUDAGraph is applied on a per-device basis. Please use supported\nlibraries such as NCCL for cross-device communication. Please seeNCCL Supportsection for more details.\nFree unbacked symbols: Free unbacked symbols usually happen duringdynamic shapes.\nCUDAGraph Trees currently record a CUDAGraph for every unique input tensor shapes.\nPlease seeDynamic Shape Supportfor more details.\nCUDAGraph-unsafe custom ops: Some custom ops may include cudagraph unsafe ops, which causes cudagraph to be skipped. Please seeCUDAGraph Unsafe Custom Opsfor more details.\nIncompatible operators: CUDAGraph Trees skip a function if it contain incompatible\noperators. Please replace these operators in a function with supported operators. We\nshow an exhaustive list of incompatible operators:\n\n```python\naten._fused_moving_avg_obs_fq_helper.default\naten._fused_moving_avg_obs_fq_helper_functional.default\naten.multinomial.default\nfbgemm.dense_to_jagged.default\nfbgemm.jagged_to_padded_dense.default\nrun_and_save_rng_state\nrun_with_rng_state\naten._local_scalar_dense\naten._assert_scalar\n\n```\n\nThe following operators are incompatible whentorch.are_deterministic_algorithms_enabled().\n\n```python\naten._fused_moving_avg_obs_fq_helper.default\naten._fused_moving_avg_obs_fq_helper_functional.default\naten.multinomial.default\nfbgemm.dense_to_jagged.default\nfbgemm.jagged_to_padded_dense.default\nrun_and_save_rng_state\nrun_with_rng_state\naten._local_scalar_dense\naten._assert_scalar\n\n```\n\n\n## CUDAGraph Unsafe Custom Ops#\n\nCustom ops are assumed to be safe for CUDAGraph by default. However, some custom ops may include unsupported ops such as cpu ops. Since custom op are treated as black boxes by the compiler, users must explicitly mark these ops as unsafe for CUDAGraph by setting thetorch._C.Tag.cudagraph_unsafetag, as demonstrated in the example below. When a function contains cudagraph-unsafe custom ops, it will be skipped by CUDAGraph unlessCUDAGraph partitionis enabled.\ntorch._C.Tag.cudagraph_unsafe\n\n```python\n@torch.library.custom_op(\n    \"mylib::modify\",\n    mutates_args=(),\n    tags=(torch._C.Tag.cudagraph_unsafe,),\n)\ndef modify(pic: torch.Tensor) -> torch.Tensor:\n    pic1 = pic + 1\n    pic1_cpu = (pic1.cpu() + 1) * 2\n    return pic1_cpu.cuda() + pic\n\n@modify.register_fake\ndef _(pic):\n    return torch.empty_like(pic)\n\n```\n\n\n## CUDAGraph Partition#\n\nAs we discussed earlier, CUDAGraph does not support some ops (e.g., cpu ops) which may limit its adoption. CUDAGraph partition is a compiler solution that automatically splits off these ops, reorders ops to reduce the number of partitions, and applies CUDAGraph to each partition individually. Please settorch._inductor.config.graph_partition=Trueto enable CUDAGraph partition.\ntorch._inductor.config.graph_partition=True\nConsider the following example wherexandyare gpu inputs buty_cpuis a cpu tensor. Without graph partition, this function must be skipped due to cpu ops. With graph partition, the CPU ops are split off, and the remaining GPU ops are cudagraphified, resulting in two separate separate CUDAGraphs.\nx\ny\ny_cpu\n\n```python\ndef f(x, y):\n    x1 = x + 1\n    y1 = y + 1\n    y_cpu = y1.cpu() + 1\n    z = x @ y\n    return x1 + y1 + z + y_cpu.cuda()\n\n```\n\nCurrently, CUDAGraph partition supports splitting off the following types of ops:\nNon-GPU Ops: Popular examples include computation on cpu tensors.\nDevice Copy Ops: Data transfers between devices, such as they1.cpu()in the example above.\ny1.cpu()\nControl Flow Ops:Control flow opsare split off since they are not yet supported by CUDAGraph.\nCUDAGraph Unsafe Custom Ops: Custom ops tagged withtorch._C.Tag.cudagraph_unsafeare split off. SeeCUDAGraph Unsafe Custom Opssection for details.\ntorch._C.Tag.cudagraph_unsafe\nUnbacked Symints: Please refer toDynamic Shape Supportsection for more information.\n\n## Limitations#\n\nBecause CUDA Graph fixes memory addresses, CUDA Graphs do not have a great way of handling live tensors from a previous invocation.\nLet\u2019s say we are benchmarking running inference with the following code:\n\n```python\nimport torch\n\n@torch.compile(mode=\"reduce-overhead\")\ndef my_model(x):\n    y = torch.matmul(x, x)\n    return y\n\nx = torch.randn(10, 10, device=\"cuda\")\ny1 = my_model(x)\ny2 = my_model(x)\nprint(y1)\n# RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.\n\n```\n\nIn the Separate CUDA Graph implementation, the output from the first invocation will be overwritten by the second invocation. In CUDAGraph\nTrees, we don\u2019t want to add unintended dependencies between iterations that would cause us to not hit the hot path, nor do we want we want\nto prematurely free memory from a prior invocation. Our heuristics are in inference we start a new iteration on each invocation for\ntorch.compile, and in training we do the same so long as there is not a pending backward that has not been invoked. If those heuristics\nare wrong, you can mark the start of a new iteration withtorch.compiler.mark_step_begin(), or clone\ntensors of a prior iteration (outside of torch.compile) before you begin the next run.\n\n## Comparisons#\n\nFootguns\nSeparate CudaGraph\nCUDAGraph Trees\nMemory Can Increase\nOn each graph compilation (new sizes, etc.)\nIf you are also running non-cudagraph memory\nRecordings\nOn any new invocation of a graph\nWill re-record on any new, unique path you take through your program\nFootguns\nInvocation of one graph will overwrite prior invocation\nCannot persist memory between separate runs through your model - one training loop training, or one run of inference",
    "url": "https://pytorch.org/docs/stable/torch.compiler_cudagraph_trees.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f612dfd477bcd98de5f7d460af61680e",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.nested_graph_breaks.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "02b480afaa74c690035ae644d93ec87c",
    "source": "pytorch_docs",
    "title": "tlparse / TORCH_TRACE \u2014 PyTorch 2.9 documentation",
    "text": "\n## tlparse / TORCH_TRACE#\n\nCreated On: Jul 29, 2025 | Last Updated On: Jul 29, 2025\ntlparse /TORCH_TRACEare a pair of tools that produce compilation reports that looklike this.\nTORCH_TRACE\nTraces are fairly straightforward to collect. To collect a trace, run your model like so:\n\n```python\nTORCH_TRACE=\"/tmp/tracedir\" python foo.py\npip install tlparse\ntlparse /tmp/tracedir\n\n```\n\nThis approach works even if you are running a distributed job, providing a trace for each rank.\nIt will open your browser with HTML similar to what\u2019s generated above.\nIf you are making a bug report for a complicated problem that you don\u2019t have a standalone reproduction for,\nyou can still greatly assist PyTorch developers by attaching the trace log generated in/tmp/tracedir.\n/tmp/tracedir\nWarning\nThe trace log contains all of your model code.\nDo not share the trace log if the model you are working on is sensitive. The trace log does NOT contain weights.\nThe output oftlparseis primarily aimed for PyTorch developers,\nand the log format is easy to upload and share on GitHub.\nHowever,  as a non-PyTorch developer, you can still extract useful information from it.\nWe recommend starting with the inline help text in the report, which explains its contents.\nHere are some insights you can gain from atlparse:\ntlparse\ntlparse\nWhat model code was compiled by looking at the stack trie?\nThis is especially useful if you\u2019re not familiar with the codebase being compiled!\nHow many graph breaks / distinct compilation regions are there?\n(Each distinct compile is its own color coded block like[0/0]).\nFrames that are potentially graph-broken are light green[2/4].\nIf there are a lot of frames, that is suspicious, and suggests that you had some catastrophic graph breaks,\nor maybe your code isn\u2019t a good match fortorch.compile.\ntorch.compile\nHow many times did I recompile a particular frame? Something that recompiled a lot will look like:[10/0][10/1][10/2]- if something is being recompiled a lot, that is very suspicious and worth looking into, even if it isn\u2019t the root cause of your problem.\nWas there a compilation error? Frames that errored will look like[0/1].\nWhat intermediate compiler products did I generate for a given frame?\nFor example, you can look at the high-level generated FX graph or the generated Triton code.\nIs there relevant information for a particular frame? You can find these incompilation_metrics.\ncompilation_metrics\n\n## TORCH_LOGS#\n\nYou can use theTORCH_LOGSenvironment variable to selectively enable parts of thetorch.compilestack to log.TORCH_LOGSis in fact the source of logs fortlparse. The format of theTORCH_LOGSenvironment variable looks like this:\nTORCH_LOGS\ntorch.compile\nTORCH_LOGS\ntlparse\nTORCH_LOGS\n\n```python\nTORCH_LOGS=\"<option1>,<option2>,...\" python foo.py\n\n```\n\nYou can also programmatically set logging options usingtorch._logging.set_logs:\ntorch._logging.set_logs\n\n```python\nimport logging\ntorch._logging.set_logs(graph_breaks=True, dynamic=logging.DEBUG)\n\n```\n\nThe most useful options are:\ngraph_breaks: logs locations of graph breaks in user code and the reason for the graph break\ngraph_breaks\nguards: logs guards that are generated\nguards\nrecompiles: logs which function recompiled and the guards that failed, leading to the recompilation\nrecompiles\ndynamic: logs related to dynamic shapes\ndynamic\noutput_code: logs the code generated by Inductor\noutput_code\nSome more helpfulTORCH_LOGSoptions include:\nTORCH_LOGS\nOption\nDescription\n+all\nOutput debug logs from alltorch.compilecomponents\ntorch.compile\n+dynamo\nOutput debug logs from TorchDynamo\n+aot\nOutput debug logs from AOTAutograd\n+inductor\nOutput debug logs from TorchInductor\ndynamic\nOutput logs from dynamic shapes\ngraph_code\nOutput the Python code for the FX graph that Dynamo generated\ngraph_sizes\nOutput the tensor sizes of the FX graph that Dynamo generated\ntrace_bytecode\nOutput the bytecode instructions that Dynamo is tracing through and the symbolic interpreter stack Dynamo is keeping track of\ntrace_source\nOutput the line of code in the original source that Dynamo is currently tracing through\nbytecode\nOutput Dynamo-generated bytecode\nguards\nOutput generated guards\nrecompiles\nOutput recompilation reasons (only the first guard check that fails)\nrecompiles_verbose\nOutput all guard checks that fail when a recompilation occurs\naot_graphs\nOutput graph generated by AOTAutograd\naot_joint_graphs\nOutput the joint forward-backward graph generated by AOTAutograd\noutput_code\nOutput code generated by Inductor\nkernel_code\nOutput code generated by Inductor on a per-kernel basis\nschedule\nOutput Inductor scheduling logs\nperf_hints\nOutput Inductor perf hint logs\nfusion\nOutput Inductor fusion logs\nFor the full list of options, seetorch._loggingandtorch._logging.set_logs.\n\n## tlparse vs. TORCH_LOGS#\n\nGenerally, we suggest first usingtlparsewhen encountering issues.tlparseis ideal for debugging large models and gaining a high-level overview of how your model was compiled.\nOn the other hand,TORCH_LOGSis preferred for small examples and fine-grained debugging detail,\nwhen we already have an idea of whichtorch.compilecomponent is causing the problem.\ntlparse\ntlparse\nTORCH_LOGS\ntorch.compile",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.observability.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2fbe167fe3e3d94948b2d93419bd47a5",
    "source": "pytorch_docs",
    "title": "torch.hub \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.hub#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jun 13, 2025\nPytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.\n\n## Publishing models#\n\nPytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a GitHub repository by adding a simplehubconf.pyfile;\nhubconf.py\nhubconf.pycan have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish).\nhubconf.py\n\n```python\n  def entrypoint_name(*args, **kwargs):\n      # args & kwargs are optional, for models which take positional/keyword arguments.\n      ...\n\n```\n\n\n## How to implement an entrypoint?#\n\nHere is a code snippet specifies an entrypoint forresnet18model if we expand\nthe implementation inpytorch/vision/hubconf.py.\nIn most case importing the right function inhubconf.pyis sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script inpytorch/vision repo\nresnet18\npytorch/vision/hubconf.py\nhubconf.py\n\n```python\n  dependencies = ['torch']\n  from torchvision.models.resnet import resnet18 as _resnet18\n\n  # resnet18 is the name of entrypoint\n  def resnet18(pretrained=False, **kwargs):\n      \"\"\" # This docstring shows up in hub.help()\n      Resnet18 model\n      pretrained (bool): kwargs, load pretrained weights into the model\n      \"\"\"\n      # Call the model, load pretrained weights\n      model = _resnet18(pretrained=pretrained, **kwargs)\n      return model\n\n```\n\ndependenciesvariable is alistof package names required toloadthe model. Note this might\nbe slightly different from dependencies required for training a model.\ndependencies\nargsandkwargsare passed along to the real callable function.\nargs\nkwargs\nDocstring of the function works as a help message. It explains what does the model do and what\nare the allowed positional/keyword arguments. It\u2019s highly recommended to add a few examples here.\nEntrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.\nCallables prefixed with underscore are considered as helper functions which won\u2019t show up intorch.hub.list().\ntorch.hub.list()\nPretrained weights can either be stored locally in the GitHub repo, or loadable bytorch.hub.load_state_dict_from_url(). If less than 2GB, it\u2019s recommended to attach it to aproject releaseand use the url from the release.\nIn the example abovetorchvision.models.resnet.resnet18handlespretrained, alternatively you can put the following logic in the entrypoint definition.\ntorch.hub.load_state_dict_from_url()\ntorchvision.models.resnet.resnet18\npretrained\n\n```python\n  if pretrained:\n      # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n      dirname = os.path.dirname(__file__)\n      checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n      state_dict = torch.load(checkpoint)\n      model.load_state_dict(state_dict)\n\n      # For checkpoint saved elsewhere\n      checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n      model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n\n```\n\n\n## Important Notice#\n\nThe published models should be at least in a branch/tag. It can\u2019t be a random commit.\n\n## Loading models from Hub#\n\nPytorch Hub provides convenient APIs to explore all available models in hub\nthroughtorch.hub.list(), show docstring and examples throughtorch.hub.help()and load the pre-trained models usingtorch.hub.load().\ntorch.hub.list()\ntorch.hub.help()\ntorch.hub.load()\nList all callable entrypoints available in the repo specified bygithub.\ngithub\ngithub(str) \u2013 a string with format \u201crepo_owner/repo_name[:ref]\u201d with an optional\nref (tag or branch). Ifrefis not specified, the default branch is assumed to bemainif\nit exists, and otherwisemaster.\nExample: \u2018pytorch/vision:0.10\u2019\nref\nmain\nmaster\nforce_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault isFalse.\nFalse\nskip_validation(bool,optional) \u2013 ifFalse, torchhub will check that the branch or commit\nspecified by thegithubargument properly belongs to the repo owner. This will make\nrequests to the GitHub API; you can specify a non-default GitHub token by setting theGITHUB_TOKENenvironment variable. Default isFalse.\nFalse\ngithub\nGITHUB_TOKEN\nFalse\ntrust_repo(bool,strorNone) \u2013\"check\",True,FalseorNone.\nThis parameter was introduced in v1.12 and helps ensuring that users\nonly run code from repos that they trust.IfFalse, a prompt will ask the user whether the repo should\nbe trusted.IfTrue, the repo will be added to the trusted list and loaded\nwithout requiring explicit confirmation.If\"check\", the repo will be checked against the list of\ntrusted repos in the cache. If it is not present in that list, the\nbehaviour will fall back onto thetrust_repo=Falseoption.IfNone: this will raise a warning, inviting the user to settrust_repoto eitherFalse,Trueor\"check\". This\nis only present for backward compatibility and will be removed in\nv2.0.Default isNoneand will eventually change to\"check\"in v2.0.\n\"check\",True,FalseorNone.\nThis parameter was introduced in v1.12 and helps ensuring that users\nonly run code from repos that they trust.\n\"check\"\nTrue\nFalse\nNone\nIfFalse, a prompt will ask the user whether the repo should\nbe trusted.\nFalse\nIfTrue, the repo will be added to the trusted list and loaded\nwithout requiring explicit confirmation.\nTrue\nIf\"check\", the repo will be checked against the list of\ntrusted repos in the cache. If it is not present in that list, the\nbehaviour will fall back onto thetrust_repo=Falseoption.\n\"check\"\ntrust_repo=False\nIfNone: this will raise a warning, inviting the user to settrust_repoto eitherFalse,Trueor\"check\". This\nis only present for backward compatibility and will be removed in\nv2.0.\nNone\ntrust_repo\nFalse\nTrue\n\"check\"\nDefault isNoneand will eventually change to\"check\"in v2.0.\nNone\n\"check\"\nverbose(bool,optional) \u2013 IfFalse, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Default isTrue.\nFalse\nTrue\nThe available callables entrypoint\nlist\nExample\n\n```python\n>>> entrypoints = torch.hub.list(\"pytorch/vision\", force_reload=True)\n\n```\n\nShow the docstring of entrypointmodel.\nmodel\ngithub(str) \u2013 a string with format <repo_owner/repo_name[:ref]> with an optional\nref (a tag or a branch). Ifrefis not specified, the default branch is assumed\nto bemainif it exists, and otherwisemaster.\nExample: \u2018pytorch/vision:0.10\u2019\nref\nmain\nmaster\nmodel(str) \u2013 a string of entrypoint name defined in repo\u2019shubconf.py\nhubconf.py\nforce_reload(bool,optional) \u2013 whether to discard the existing cache and force a fresh download.\nDefault isFalse.\nFalse\nskip_validation(bool,optional) \u2013 ifFalse, torchhub will check that the ref\nspecified by thegithubargument properly belongs to the repo owner. This will make\nrequests to the GitHub API; you can specify a non-default GitHub token by setting theGITHUB_TOKENenvironment variable. Default isFalse.\nFalse\ngithub\nGITHUB_TOKEN\nFalse\ntrust_repo(bool,strorNone) \u2013\"check\",True,FalseorNone.\nThis parameter was introduced in v1.12 and helps ensuring that users\nonly run code from repos that they trust.IfFalse, a prompt will ask the user whether the repo should\nbe trusted.IfTrue, the repo will be added to the trusted list and loaded\nwithout requiring explicit confirmation.If\"check\", the repo will be checked against the list of\ntrusted repos in the cache. If it is not present in that list, the\nbehaviour will fall back onto thetrust_repo=Falseoption.IfNone: this will raise a warning, inviting the user to settrust_repoto eitherFalse,Trueor\"check\". This\nis only present for backward compatibility and will be removed in\nv2.0.Default isNoneand will eventually change to\"check\"in v2.0.\n\"check\",True,FalseorNone.\nThis parameter was introduced in v1.12 and helps ensuring that users\nonly run code from repos that they trust.\n\"check\"\nTrue\nFalse\nNone\nIfFalse, a prompt will ask the user whether the repo should\nbe trusted.\nFalse\nIfTrue, the repo will be added to the trusted list and loaded\nwithout requiring explicit confirmation.\nTrue\nIf\"check\", the repo will be checked against the list of\ntrusted repos in the cache. If it is not present in that list, the\nbehaviour will fall back onto thetrust_repo=Falseoption.\n\"check\"\ntrust_repo=False\nIfNone: this will raise a warning, inviting the user to settrust_repoto eitherFalse,Trueor\"check\". This\nis only present for backward compatibility and will be removed in\nv2.0.\nNone\ntrust_repo\nFalse\nTrue\n\"check\"\nDefault isNoneand will eventually change to\"check\"in v2.0.\nNone\n\"check\"\nExample\n\n```python\n>>> print(torch.hub.help(\"pytorch/vision\", \"resnet18\", force_reload=True))\n\n```\n\nLoad a model from a github repo or a local directory.\nNote: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.\nIfsourceis \u2018github\u2019,repo_or_diris expected to be\nof the formrepo_owner/repo_name[:ref]with an optional\nref (a tag or a branch).\nsource\nrepo_or_dir\nrepo_owner/repo_name[:ref]\nIfsourceis \u2018local\u2019,repo_or_diris expected to be a\npath to a local directory.\nsource\nrepo_or_dir\nrepo_or_dir(str) \u2013 Ifsourceis \u2018github\u2019,\nthis should correspond to a github repo with formatrepo_owner/repo_name[:ref]with\nan optional ref (tag or branch), for example \u2018pytorch/vision:0.10\u2019. Ifrefis not specified,\nthe default branch is assumed to bemainif it exists, and otherwisemaster.\nIfsourceis \u2018local\u2019  then it should be a path to a local directory.\nsource\nrepo_owner/repo_name[:ref]\nref\nmain\nmaster\nsource\nmodel(str) \u2013 the name of a callable (entrypoint) defined in the\nrepo/dir\u2019shubconf.py.\nhubconf.py\n*args(optional) \u2013 the corresponding args for callablemodel.\nmodel\nsource(str,optional) \u2013 \u2018github\u2019 or \u2018local\u2019. Specifies howrepo_or_diris to be interpreted. Default is \u2018github\u2019.\nrepo_or_dir\ntrust_repo(bool,strorNone) \u2013\"check\",True,FalseorNone.\nThis parameter was introduced in v1.12 and helps ensuring that users\nonly run code from repos that they trust.IfFalse, a prompt will ask the user whether the repo should\nbe trusted.IfTrue, the repo will be added to the trusted list and loaded\nwithout requiring explicit confirmation.If\"check\", the repo will be checked against the list of\ntrusted repos in the cache. If it is not present in that list, the\nbehaviour will fall back onto thetrust_repo=Falseoption.IfNone: this will raise a warning, inviting the user to settrust_repoto eitherFalse,Trueor\"check\". This\nis only present for backward compatibility and will be removed in\nv2.0.Default isNoneand will eventually change to\"check\"in v2.0.\n\"check\",True,FalseorNone.\nThis parameter was introduced in v1.12 and helps ensuring that users\nonly run code from repos that they trust.\n\"check\"\nTrue\nFalse\nNone\nIfFalse, a prompt will ask the user whether the repo should\nbe trusted.\nFalse\nIfTrue, the repo will be added to the trusted list and loaded\nwithout requiring explicit confirmation.\nTrue\nIf\"check\", the repo will be checked against the list of\ntrusted repos in the cache. If it is not present in that list, the\nbehaviour will fall back onto thetrust_repo=Falseoption.\n\"check\"\ntrust_repo=False\nIfNone: this will raise a warning, inviting the user to settrust_repoto eitherFalse,Trueor\"check\". This\nis only present for backward compatibility and will be removed in\nv2.0.\nNone\ntrust_repo\nFalse\nTrue\n\"check\"\nDefault isNoneand will eventually change to\"check\"in v2.0.\nNone\n\"check\"\nforce_reload(bool,optional) \u2013 whether to force a fresh download of\nthe github repo unconditionally. Does not have any effect ifsource='local'. Default isFalse.\nsource='local'\nFalse\nverbose(bool,optional) \u2013 IfFalse, mute messages about hitting\nlocal caches. Note that the message about first download cannot be\nmuted. Does not have any effect ifsource='local'.\nDefault isTrue.\nFalse\nsource='local'\nTrue\nskip_validation(bool,optional) \u2013 ifFalse, torchhub will check that the branch or commit\nspecified by thegithubargument properly belongs to the repo owner. This will make\nrequests to the GitHub API; you can specify a non-default GitHub token by setting theGITHUB_TOKENenvironment variable. Default isFalse.\nFalse\ngithub\nGITHUB_TOKEN\nFalse\n**kwargs(optional) \u2013 the corresponding kwargs for callablemodel.\nmodel\nThe output of themodelcallable when called with the given*argsand**kwargs.\nmodel\n*args\n**kwargs\nExample\n\n```python\n>>> # from a github repo\n>>> repo = \"pytorch/vision\"\n>>> model = torch.hub.load(\n...     repo, \"resnet50\", weights=\"ResNet50_Weights.IMAGENET1K_V1\"\n... )\n>>> # from a local directory\n>>> path = \"/some/local/path/pytorch/vision\"\n>>> model = torch.hub.load(path, \"resnet50\", weights=\"ResNet50_Weights.DEFAULT\")\n\n```\n\nDownload object at the given URL to a local path.\nurl(str) \u2013 URL of the object to download\ndst(str) \u2013 Full path where object will be saved, e.g./tmp/temporary_file\n/tmp/temporary_file\nhash_prefix(str,optional) \u2013 If not None, the SHA256 downloaded file should start withhash_prefix.\nDefault: None\nhash_prefix\nprogress(bool,optional) \u2013 whether or not to display a progress bar to stderr\nDefault: True\nExample\n\n```python\n>>> torch.hub.download_url_to_file(\n...     \"https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth\",\n...     \"/tmp/temporary_file\",\n... )\n\n```\n\nLoads the Torch serialized object at the given URL.\nIf downloaded file is a zip file, it will be automatically\ndecompressed.\nIf the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir().\nmodel_dir\n<hub_dir>/checkpoints\nhub_dir\nget_dir()\nurl(str) \u2013 URL of the object to download\nmodel_dir(str,optional) \u2013 directory in which to save the object\nmap_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load)\nprogress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True\ncheck_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False\nfilename-<sha256>.ext\n<sha256>\nfile_name(str,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set.\nurl\nweights_only(bool,optional) \u2013 If True, only weights will be loaded and no complex pickled objects.\nRecommended for untrusted sources. Seeload()for more details.\nload()\ndict[str,Any]\nExample\n\n```python\n>>> state_dict = torch.hub.load_state_dict_from_url(\n...     \"https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth\"\n... )\n\n```\n\n\n## Running a loaded model:#\n\nNote that*argsand**kwargsintorch.hub.load()are used toinstantiatea model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is\n*args\n**kwargs\ntorch.hub.load()\ndir(model)to see all available methods of the model.\ndir(model)\nhelp(model.foo)to check what argumentsmodel.footakes to run\nhelp(model.foo)\nmodel.foo\nTo help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It\u2019s also helpful\nto include a minimal working example.\n\n## Where are my downloaded models saved?#\n\nThe locations are used in the order of\nCallinghub.set_dir(<PATH_TO_HUB_DIR>)\nhub.set_dir(<PATH_TO_HUB_DIR>)\n$TORCH_HOME/hub, if environment variableTORCH_HOMEis set.\n$TORCH_HOME/hub\nTORCH_HOME\n$XDG_CACHE_HOME/torch/hub, if environment variableXDG_CACHE_HOMEis set.\n$XDG_CACHE_HOME/torch/hub\nXDG_CACHE_HOME\n~/.cache/torch/hub\n~/.cache/torch/hub\nGet the Torch Hub cache directory used for storing downloaded models & weights.\nIfset_dir()is not called, default path is$TORCH_HOME/hubwhere\nenvironment variable$TORCH_HOMEdefaults to$XDG_CACHE_HOME/torch.$XDG_CACHE_HOMEfollows the X Design Group specification of the Linux\nfilesystem layout, with a default value~/.cacheif the environment\nvariable is not set.\nset_dir()\n$TORCH_HOME/hub\n$TORCH_HOME\n$XDG_CACHE_HOME/torch\n$XDG_CACHE_HOME\n~/.cache\nstr\nOptionally set the Torch Hub directory used to save downloaded models & weights.\nd(str) \u2013 path to a local folder to save downloaded models & weights.\n\n## Caching logic#\n\nBy default, we don\u2019t clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned byget_dir().\nget_dir()\nUsers can force a reload by callinghub.load(...,force_reload=True). This will delete\nthe existing GitHub folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.\nhub.load(...,force_reload=True)\n\n## Known limitations:#\n\nTorch hub works by importing the package as if it was installed. There are some side effects\nintroduced by importing in Python. For example, you can see new items in Python cachessys.modulesandsys.path_importer_cachewhich is normal Python behavior.\nThis also means that you may have import errors when importing different models\nfrom different repos, if the repos have the same sub-package names (typically, amodelsubpackage). A workaround for these kinds of import errors is to\nremove the offending sub-package from thesys.modulesdict; more details can\nbe found inthis GitHub issue.\nsys.modules\nsys.path_importer_cache\nmodel\nsys.modules\nA known limitation that is worth mentioning here: usersCANNOTload two different branches of\nthe same repo in thesame python process. It\u2019s just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it\u2019s totally fine to load them in separate processes.",
    "url": "https://pytorch.org/docs/stable/hub.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "288d4577bf567d71f870a7e1ee1b7139",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/cpp_extension.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4b9cce14ac5f117fe036e99883295bb1",
    "source": "pytorch_docs",
    "title": "PyTorch Main Components \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch Main Components#\n\nCreated On: Aug 13, 2025 | Last Updated On: Aug 13, 2025\nPyTorch is a flexible and powerful library for deep learning that provides a comprehensive set of tools for building, training, and deploying machine learning models.\n\n## PyTorch Components for Basic Deep Learning#\n\nSome of the basic PyTorch components include:\nTensors- N-dimensional arrays that serve as PyTorch\u2019s fundamental\ndata structure. They support automatic differentiation, hardware acceleration, and provide a comprehensive API for mathematical operations.\nAutograd- PyTorch\u2019s automatic differentiation engine\nthat tracks operations performed on tensors and builds a computational\ngraph dynamically to be able to compute gradients.\nNeural Network API- A modular framework for building neural networks with pre-defined layers,\nactivation functions, and loss functions. Thenn.Modulebase class provides a clean interface\nfor creating custom network architectures with parameter management.\nnn.Module\nDataLoaders- Tools for efficient data handling that provide\nfeatures like batching, shuffling, and parallel data loading. They abstract away the complexities\nof data preprocessing and iteration, allowing for optimized training loops.\n\n## PyTorch Compiler#\n\nThe PyTorch compiler is a suite of tools that optimize model execution and\nreduce resource requirements. You can learn more about the PyTorch compilerhere.",
    "url": "https://pytorch.org/docs/stable/user_guide/pytorch_main_components.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8532364d8979e6ecd0e3e1e24de244d8",
    "source": "pytorch_docs",
    "title": "HIP (ROCm) semantics \u2014 PyTorch 2.9 documentation",
    "text": "\n## HIP (ROCm) semantics#\n\nCreated On: May 12, 2021 | Last Updated On: Aug 08, 2025\nROCm\u2122 is AMD\u2019s open source software platform for GPU-accelerated high\nperformance computing and machine learning. HIP is ROCm\u2019s C++ dialect designed\nto ease conversion of CUDA applications to portable C++ code. HIP is used when\nconverting existing CUDA applications like PyTorch to portable C++ and for new\nprojects that require portability between AMD and NVIDIA.\n\n## HIP Interfaces Reuse the CUDA Interfaces#\n\nPyTorch for HIP intentionally reuses the existingtorch.cudainterfaces.\nThis helps to accelerate the porting of existing PyTorch code and models because\nvery few code changes are necessary, if any.\ntorch.cuda\nThe example fromCUDA semanticswill work exactly the same for HIP:\n\n```python\ncuda = torch.device('cuda')     # Default HIP device\ncuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n\n```\n\n\n## Checking for HIP#\n\nWhether you are using PyTorch for CUDA or HIP, the result of callingis_available()will be the same. If you are using a PyTorch\nthat has been built with GPU support, it will returnTrue. If you must check\nwhich version of PyTorch you are using, refer to this example below:\nis_available()\n\n```python\nif torch.cuda.is_available() and torch.version.hip:\n    # do something specific for HIP\nelif torch.cuda.is_available() and torch.version.cuda:\n    # do something specific for CUDA\n\n```\n\n\n## TensorFloat-32(TF32) on ROCm#\n\nTF32 is not supported on ROCm.\n\n## Memory management#\n\nPyTorch uses a caching memory allocator to speed up memory allocations. This\nallows fast memory deallocation without device synchronizations. However, the\nunused memory managed by the allocator will still show as if used inrocm-smi. You can usememory_allocated()andmax_memory_allocated()to monitor memory occupied by\ntensors, and usememory_reserved()andmax_memory_reserved()to monitor the total amount of memory\nmanaged by the caching allocator. Callingempty_cache()releases allunusedcached memory from PyTorch so that those can be used\nby other GPU applications. However, the occupied GPU memory by tensors will not\nbe freed so it can not increase the amount of GPU memory available for PyTorch.\nrocm-smi\nmemory_allocated()\nmax_memory_allocated()\nmemory_reserved()\nmax_memory_reserved()\nempty_cache()\nFor more advanced users, we offer more comprehensive memory benchmarking viamemory_stats(). We also offer the capability to capture a\ncomplete snapshot of the memory allocator state viamemory_snapshot(), which can help you understand the\nunderlying allocation patterns produced by your code.\nmemory_stats()\nmemory_snapshot()\nTo debug memory errors, setPYTORCH_NO_HIP_MEMORY_CACHING=1in your environment to disable caching.PYTORCH_NO_CUDA_MEMORY_CACHING=1is also accepted for ease of porting.\nPYTORCH_NO_HIP_MEMORY_CACHING=1\nPYTORCH_NO_CUDA_MEMORY_CACHING=1\n\n## hipBLAS workspaces#\n\nFor each combination of hipBLAS handle and HIP stream, a hipBLAS workspace will be allocated if that\nhandle and stream combination executes a hipBLAS kernel that requires a workspace.  In order to\navoid repeatedly allocating workspaces, these workspaces are not deallocated unlesstorch._C._cuda_clearCublasWorkspaces()is called; note that it\u2019s the same function for CUDA or\nHIP. The workspace size per allocation can be specified via the environment variableHIPBLAS_WORKSPACE_CONFIGwith the format:[SIZE]:[COUNT].  As an example, the environment\nvariableHIPBLAS_WORKSPACE_CONFIG=:4096:2:16:8specifies a total size of2*4096+8*16KiBor 8 MIB. The default workspace size is 32 MiB; MI300 and newer defaults to 128 MiB. To force\nhipBLAS to avoid using workspaces, setHIPBLAS_WORKSPACE_CONFIG=:0:0. For convenience,CUBLAS_WORKSPACE_CONFIGis also accepted.\ntorch._C._cuda_clearCublasWorkspaces()\nHIPBLAS_WORKSPACE_CONFIG\n:[SIZE]:[COUNT]\nHIPBLAS_WORKSPACE_CONFIG=:4096:2:16:8\n2*4096+8*16KiB\nHIPBLAS_WORKSPACE_CONFIG=:0:0\nCUBLAS_WORKSPACE_CONFIG\n\n## hipFFT/rocFFT plan cache#\n\nSetting the size of the cache for hipFFT/rocFFT plans is not supported.\n\n## torch.distributed backends#\n\nCurrently, only the \u201cnccl\u201d and \u201cgloo\u201d backends for torch.distributed are supported on ROCm.\n\n## CUDA API to HIP API mappings in C++#\n\nPlease refer:https://rocm.docs.amd.com/projects/HIP/en/latest/reference/api_syntax.html\nNOTE: The CUDA_VERSION macro, cudaRuntimeGetVersion and cudaDriverGetVersion APIs do not\nsemantically map to the same values as HIP_VERSION macro, hipRuntimeGetVersion and\nhipDriverGetVersion APIs. Please do not use them interchangeably when doing version checks.\nFor example: Instead of using\n#ifdefined(CUDA_VERSION)&&CUDA_VERSION>=11000to implicitly exclude ROCm/HIP,\n#ifdefined(CUDA_VERSION)&&CUDA_VERSION>=11000\nuse the following to not take the code path for ROCm/HIP:\n#ifdefined(CUDA_VERSION)&&CUDA_VERSION>=11000&&!defined(USE_ROCM)\n#ifdefined(CUDA_VERSION)&&CUDA_VERSION>=11000&&!defined(USE_ROCM)\nAlternatively, if it is desired to take the code path for ROCm/HIP:\n#if(defined(CUDA_VERSION)&&CUDA_VERSION>=11000)||defined(USE_ROCM)\n#if(defined(CUDA_VERSION)&&CUDA_VERSION>=11000)||defined(USE_ROCM)\nOr if it is desired to take the code path for ROCm/HIP only for specific HIP versions:\n#if(defined(CUDA_VERSION)&&CUDA_VERSION>=11000)||(defined(USE_ROCM)&&ROCM_VERSION>=40300)\n#if(defined(CUDA_VERSION)&&CUDA_VERSION>=11000)||(defined(USE_ROCM)&&ROCM_VERSION>=40300)\n\n## Refer to CUDA Semantics doc#\n\nFor any sections not listed here, please refer to the CUDA semantics doc:CUDA semantics\n\n## Enabling kernel asserts#\n\nKernel asserts are supported on ROCm, but they are disabled due to performance overhead. It can be enabled\nby recompiling the PyTorch from source.\nPlease add below line as an argument to cmake command parameters:\n\n```python\n-DROCM_FORCE_ENABLE_GPU_ASSERTS:BOOL=ON\n\n```\n\n\n## Enabling/Disabling ROCm Composable Kernel#\n\nEnabling composable_kernel (CK) for both SDPA and GEMMs is a two-part process. First the user must have built\npytorch while setting the corresponding environment variable to \u20181\u2019\nSDPA:USE_ROCM_CK_SDPA=1\nUSE_ROCM_CK_SDPA=1\nGEMMs:USE_ROCM_CK_GEMM=1\nUSE_ROCM_CK_GEMM=1\nSecond, the user must explicitly request that CK be used as the backend library via the corresponding python\ncall\nSDPA:setROCmFAPreferredBackend('<choice>')\nsetROCmFAPreferredBackend('<choice>')\nGEMMs:setBlasPreferredBackend('<choice>')\nsetBlasPreferredBackend('<choice>')\nTo enable CK in either scenario, simply pass \u2018ck\u2019 to those functions.\nIn order to set the backend to CK, the user MUST have built with the correct environment variable. If not,\nPyTorch will print a warning and use the \u201cdefault\u201d backend. For GEMMs, this will route to hipblas and\nfor SDPA it routes to aotriton.",
    "url": "https://pytorch.org/docs/stable/notes/hip.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "063fdd35e76abeae908df4944dc70f6f",
    "source": "pytorch_docs",
    "title": "torch.onnx \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.onnx#\n\nCreated On: Jun 10, 2025 | Last Updated On: Aug 29, 2025\n\n## Overview#\n\nOpen Neural Network eXchange (ONNX)is an open standard\nformat for representing machine learning models. Thetorch.onnxmodule captures the computation graph from a\nnative PyTorchtorch.nn.Modulemodel and converts it into anONNX graph.\ntorch.onnx\ntorch.nn.Module\nThe exported model can be consumed by any of the manyruntimes that support ONNX, including\nMicrosoft\u2019sONNX Runtime.\nNext example shows how to export a simple model.\n\n```python\nimport torch\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.conv1 = torch.nn.Conv2d(1, 128, 5)\n\n    def forward(self, x):\n        return torch.relu(self.conv1(x))\n\ninput_tensor = torch.rand((1, 1, 128, 128), dtype=torch.float32)\n\nmodel = MyModel()\n\ntorch.onnx.export(\n    model,                  # model to export\n    (input_tensor,),        # inputs of the model,\n    \"my_model.onnx\",        # filename of the ONNX model\n    input_names=[\"input\"],  # Rename inputs for the ONNX model\n    dynamo=True             # True or False to select the exporter to use\n)\n\n```\n\n\n## torch.export-based ONNX Exporter#\n\nThe torch.export-based ONNX exporter is the newest exporter for PyTorch 2.6 and newer\ntorch.exportengine is leveraged to produce a traced graph representing only the Tensor computation of the function in an\nAhead-of-Time (AOT) fashion. The resulting traced graph (1) produces normalized operators in the functional\nATen operator set (as well as any user-specified custom operators), (2) has eliminated all Python control\nflow and data structures (with certain exceptions), and (3) records the set of shape constraints needed to\nshow that this normalization and control-flow elimination is sound for future inputs, before it is finally\ntranslated into an ONNX graph.\nLearn more about the torch.export-based ONNX Exporter\n\n## Frequently Asked Questions#\n\nQ: I have exported my LLM model, but its input size seems to be fixed?\nThe tracer records the shapes of the example inputs. If the model should accept\ninputs of dynamic shapes, setdynamic_shapeswhen callingtorch.onnx.export().\ndynamic_shapes\ntorch.onnx.export()\nQ: How to export models containing loops?\nSeetorch.cond.\n\n## Contributing / Developing#\n\nThe ONNX exporter is a community project and we welcome contributions. We follow thePyTorch guidelines for contributions, but you might\nalso be interested in reading ourdevelopment wiki.\n\n## torch.onnx APIs#\n\n\n## Functions#\n\nExports a model into ONNX format.\nSettingdynamo=Trueenables the new ONNX export logic\nwhich is based ontorch.export.ExportedProgramand a more modern\nset of translation logic. This is the recommended and default way to export models\nto ONNX.\ndynamo=True\ntorch.export.ExportedProgram\nWhendynamo=True:\ndynamo=True\nThe exporter tries the following strategies to get an ExportedProgram for conversion to ONNX.\nIf the model is already an ExportedProgram, it will be used as-is.\nUsetorch.export.export()and setstrict=False.\ntorch.export.export()\nstrict=False\nUsetorch.export.export()and setstrict=True.\ntorch.export.export()\nstrict=True\nmodel(torch.nn.Module|torch.export.ExportedProgram|torch.jit.ScriptModule|torch.jit.ScriptFunction) \u2013 The model to be exported.\nargs(tuple[Any,...]) \u2013 Example positional inputs. Any non-Tensor arguments will be hard-coded into the\nexported model; any Tensor arguments will become inputs of the exported model,\nin the order they occur in the tuple.\nf(str|os.PathLike|None) \u2013 Path to the output ONNX model file. E.g. \u201cmodel.onnx\u201d. This argument is kept for\nbackward compatibility. It is recommended to leave unspecified (None)\nand use the returnedtorch.onnx.ONNXProgramto serialize the model\nto a file instead.\ntorch.onnx.ONNXProgram\nkwargs(dict[str,Any]|None) \u2013 Optional example keyword inputs.\nverbose(bool|None) \u2013 Whether to enable verbose logging.\ninput_names(Sequence[str]|None) \u2013 names to assign to the input nodes of the graph, in order.\noutput_names(Sequence[str]|None) \u2013 names to assign to the output nodes of the graph, in order.\nopset_version(int|None) \u2013 The version of thedefault (ai.onnx) opsetto target. You should setopset_versionaccording to the supported opset versions\nof the runtime backend or compiler you want to run the exported model with.\nLeave as default (None) to use the recommended version, or refer to\nthe ONNX operators documentation for more information.\nopset_version\nNone\ndynamo(bool) \u2013 Whether to export the model withtorch.exportExportedProgram instead of TorchScript.\ntorch.export\nexternal_data(bool) \u2013 Whether to save the model weights as an external data file.\nThis is required for models with large weights that exceed the ONNX file size limit (2GB).\nWhen False, the weights are saved in the ONNX file with the model architecture.\ndynamic_shapes(dict[str,Any]|tuple[Any,...]|list[Any]|None) \u2013 A dictionary or a tuple of dynamic shapes for the model inputs. Refer totorch.export.export()for more details. This is only used (and preferred) when dynamo is True.\nNote that dynamic_shapes is designed to be used when the model is exported with dynamo=True, while\ndynamic_axes is used when dynamo=False.\ntorch.export.export()\ncustom_translation_table(dict[Callable,Callable|Sequence[Callable]]|None) \u2013 A dictionary of custom decompositions for operators in the model.\nThe dictionary should have the callable target in the fx Node as the key (e.g.torch.ops.aten.stft.default),\nand the value should be a function that builds that graph using ONNX Script. This option\nis only valid when dynamo is True.\ntorch.ops.aten.stft.default\nreport(bool) \u2013 Whether to generate a markdown report for the export process. This option\nis only valid when dynamo is True.\noptimize(bool) \u2013 Whether to optimize the exported model. This option\nis only valid when dynamo is True. Default is True.\nverify(bool) \u2013 Whether to verify the exported model using ONNX Runtime. This option\nis only valid when dynamo is True.\nprofile(bool) \u2013 Whether to profile the export process. This option\nis only valid when dynamo is True.\ndump_exported_program(bool) \u2013 Whether to dump thetorch.export.ExportedProgramto a file.\nThis is useful for debugging the exporter. This option is only valid when dynamo is True.\ntorch.export.ExportedProgram\nartifacts_dir(str|os.PathLike) \u2013 The directory to save the debugging artifacts like the report and the serialized\nexported program. This option is only valid when dynamo is True.\nfallback(bool) \u2013 Whether to fallback to the TorchScript exporter if the dynamo exporter fails.\nThis option is only valid when dynamo is True. When fallback is enabled, It is\nrecommended to set dynamic_axes even when dynamic_shapes is provided.\nexport_params(bool) \u2013When ``f`` is specified: If false, parameters (weights) will not be exported.You can also leave it unspecified and use the returnedtorch.onnx.ONNXProgramto control how initializers are treated when serializing the model.\nWhen ``f`` is specified: If false, parameters (weights) will not be exported.\nYou can also leave it unspecified and use the returnedtorch.onnx.ONNXProgramto control how initializers are treated when serializing the model.\ntorch.onnx.ONNXProgram\nkeep_initializers_as_inputs(bool) \u2013When ``f`` is specified: If True, all the\ninitializers (typically corresponding to model weights) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe user inputs are added as inputs.Set this to True if you intend to supply model weights at runtime.\nSet it to False if the weights are static to allow for better optimizations\n(e.g. constant folding) by backends/runtimes.You can also leave it unspecified and use the returnedtorch.onnx.ONNXProgramto control how initializers are treated when serializing the model.\nWhen ``f`` is specified: If True, all the\ninitializers (typically corresponding to model weights) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe user inputs are added as inputs.\nSet this to True if you intend to supply model weights at runtime.\nSet it to False if the weights are static to allow for better optimizations\n(e.g. constant folding) by backends/runtimes.\nYou can also leave it unspecified and use the returnedtorch.onnx.ONNXProgramto control how initializers are treated when serializing the model.\ntorch.onnx.ONNXProgram\ndynamic_axes(Mapping[str,Mapping[int,str]]|Mapping[str,Sequence[int]]|None) \u2013Prefer specifyingdynamic_shapeswhendynamo=Trueand whenfallbackis not enabled.By default the exported model will have the shapes of all input and output tensors\nset to exactly match those given inargs. To specify axes of tensors as\ndynamic (i.e. known only at run-time), setdynamic_axesto a dict with schema:KEY (str): an input or output name. Each name must also be provided ininput_namesoroutput_names.VALUE (dict or list): If a dict, keys are axis indices and values are axis names. If alist, each element is an axis index.For example:classSumModule(torch.nn.Module):defforward(self,x):returntorch.sum(x,dim=1)torch.onnx.export(SumModule(),(torch.ones(2,2),),\"onnx.pb\",input_names=[\"x\"],output_names=[\"sum\"],)Produces:input{name:\"x\"...shape{dim{dim_value:2# axis 0}dim{dim_value:2# axis 1...output{name:\"sum\"...shape{dim{dim_value:2# axis 0...While:torch.onnx.export(SumModule(),(torch.ones(2,2),),\"onnx.pb\",input_names=[\"x\"],output_names=[\"sum\"],dynamic_axes={# dict value: manually named axes\"x\":{0:\"my_custom_axis_name\"},# list value: automatic names\"sum\":[0],},)Produces:input{name:\"x\"...shape{dim{dim_param:\"my_custom_axis_name\"# axis 0}dim{dim_value:2# axis 1...output{name:\"sum\"...shape{dim{dim_param:\"sum_dynamic_axes_1\"# axis 0...\nPrefer specifyingdynamic_shapeswhendynamo=Trueand whenfallbackis not enabled.\ndynamic_shapes\ndynamo=True\nfallback\nBy default the exported model will have the shapes of all input and output tensors\nset to exactly match those given inargs. To specify axes of tensors as\ndynamic (i.e. known only at run-time), setdynamic_axesto a dict with schema:\nargs\ndynamic_axes\ninput_names\noutput_names.\noutput_names\nlist, each element is an axis index.\nFor example:\n\n```python\nclass SumModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.sum(x, dim=1)\n\n\ntorch.onnx.export(\n    SumModule(),\n    (torch.ones(2, 2),),\n    \"onnx.pb\",\n    input_names=[\"x\"],\n    output_names=[\"sum\"],\n)\n\n```\n\nProduces:\n\n```python\ninput {\n  name: \"x\"\n  ...\n      shape {\n        dim {\n          dim_value: 2  # axis 0\n        }\n        dim {\n          dim_value: 2  # axis 1\n...\noutput {\n  name: \"sum\"\n  ...\n      shape {\n        dim {\n          dim_value: 2  # axis 0\n...\n\n```\n\nWhile:\n\n```python\ntorch.onnx.export(\n    SumModule(),\n    (torch.ones(2, 2),),\n    \"onnx.pb\",\n    input_names=[\"x\"],\n    output_names=[\"sum\"],\n    dynamic_axes={\n        # dict value: manually named axes\n        \"x\": {0: \"my_custom_axis_name\"},\n        # list value: automatic names\n        \"sum\": [0],\n    },\n)\n\n```\n\nProduces:\n\n```python\ninput {\n  name: \"x\"\n  ...\n      shape {\n        dim {\n          dim_param: \"my_custom_axis_name\"  # axis 0\n        }\n        dim {\n          dim_value: 2  # axis 1\n...\noutput {\n  name: \"sum\"\n  ...\n      shape {\n        dim {\n          dim_param: \"sum_dynamic_axes_1\"  # axis 0\n...\n\n```\n\ntraining(_C_onnx.TrainingMode) \u2013 Deprecated option. Instead, set the training mode of the model before exporting.\noperator_export_type(_C_onnx.OperatorExportTypes) \u2013 Deprecated option. Only ONNX is supported.\ndo_constant_folding(bool) \u2013 Deprecated option.\ncustom_opsets(Mapping[str,int]|None) \u2013 Deprecated option.\nexport_modules_as_functions(bool|Collection[type[torch.nn.Module]]) \u2013 Deprecated option.\nautograd_inlining(bool) \u2013 Deprecated option.\ntorch.onnx.ONNXProgramif dynamo is True, otherwise None.\ntorch.onnx.ONNXProgram\nONNXProgram| None\nChanged in version 2.6:trainingis now deprecated. Instead, set the training mode of the model before exporting.operator_export_typeis now deprecated. Only ONNX is supported.do_constant_foldingis now deprecated. It is always enabled.export_modules_as_functionsis now deprecated.autograd_inliningis now deprecated.\nChanged in version 2.7:optimizeis now True by default.\nChanged in version 2.9:dynamois now True by default.\nReturns whether it is in the middle of ONNX export.\nbool\n\n## Classes#\n\nA class to represent an ONNX program that is callable with torch tensors.\nmodel\u2013 The ONNX model as an ONNX IR model object.\nexported_program\u2013 The exported program that produced the ONNX model.\nErrors raised by the ONNX exporter. This is the base class for all exporter errors.\n\n## Deprecated APIs#\n\nDeprecated since version 2.6:These functions are deprecated and will be removed in a future version.\nRegisters a symbolic function for a custom operator.\nWhen the user registers symbolic for custom/contrib ops,\nit is highly recommended to add shape inference for that operator via setType API,\notherwise the exported graph may have incorrect shape inference in some extreme cases.\nAn example of setType istest_aten_embedding_2intest_operators.py.\nSee \u201cCustom Operators\u201d in the module documentation for an example usage.\nsymbolic_name(str) \u2013 The name of the custom operator in \u201c<domain>::<op>\u201d\nformat.\nsymbolic_fn(Callable) \u2013 A function that takes in the ONNX graph and\nthe input arguments to the current operator, and returns new\noperator nodes to add to the graph.\nopset_version(int) \u2013 The ONNX opset version in which to register.\nUnregisterssymbolic_name.\nsymbolic_name\nSee \u201cCustom Operators\u201d in the module documentation for an example usage.\nsymbolic_name(str) \u2013 The name of the custom operator in \u201c<domain>::<op>\u201d\nformat.\nopset_version(int) \u2013 The ONNX opset version in which to unregister.\nA context manager to temporarily set the training mode ofmodeltomode, resetting it when we exit the with-block.\nmodel\nmode\nDeprecated since version 2.7:Please set training mode before exporting the model.\nmodel\u2013 Same type and meaning asmodelarg toexport().\nmodel\nexport()\nmode(TrainingMode) \u2013 Same type and meaning astrainingarg toexport().\ntraining\nexport()",
    "url": "https://pytorch.org/docs/stable/onnx.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3b4586edf12eafeecb11d07e7188aab2",
    "source": "pytorch_docs",
    "title": "torch.utils \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils#\n\nCreated On: Jul 21, 2023 | Last Updated On: Jun 06, 2025\nrename_privateuse1_backend\n\nrename_privateuse1_backend\nRename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.\ngenerate_methods_for_privateuse1_backend\n\ngenerate_methods_for_privateuse1_backend\nAutomatically generate attributes and methods for the custom backend after rename privateuse1 backend.\nget_cpp_backtrace\n\nget_cpp_backtrace\nReturn a string containing the C++ stack trace of the current thread.\nset_module\n\nset_module\nSet the module attribute on a python object for a given object for nicer printing\nswap_tensors\n\nswap_tensors\nThis function swaps the content of the two Tensor objects.",
    "url": "https://pytorch.org/docs/stable/utils.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6dcb872df927c32969bd676c88906218",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/broadcasting.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9df2da52fcd78b9c9067b99db1e34fb7",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/export/pt2_archive.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2bcd98ede1a74de3febb1c6cfdc3d5b5",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/package.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7d9c12ce2d39db300219a6835e727039",
    "source": "pytorch_docs",
    "title": "torch.config \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.config#\n\nCreated On: Apr 09, 2019 | Last Updated On: Jun 13, 2025\nReturn a human-readable string with descriptions of the\nconfiguration of PyTorch.\nstr\nReturns detailed string with parallelization settings\nstr",
    "url": "https://pytorch.org/docs/stable/config_mod.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "999a7020e607da6886be59e7171497c8",
    "source": "pytorch_docs",
    "title": "Getting Started on Intel GPU \u2014 PyTorch 2.9 documentation",
    "text": "\n## Getting Started on Intel GPU#\n\nCreated On: Jun 14, 2024 | Last Updated On: Sep 01, 2025\n\n## Hardware Prerequisite#\n\nFor Intel Data Center GPU\nDevice\nRed Hat* Enterprise Linux* 9.2\nSUSE Linux Enterprise Server* 15 SP5\nUbuntu* Server 22.04 (>= 5.15 LTS kernel)\nIntel\u00ae Data Center GPU Max Series (CodeName: Ponte Vecchio)\nyes\nyes\nyes\nFor Intel Client GPU\nSupported OS\nValidated Hardware\nIntel GPUs support (Prototype) is ready from PyTorch* 2.5 for Intel\u00ae Client GPUs and Intel\u00ae Data Center GPU Max Series on both Linux and Windows, which brings Intel GPUs and the SYCL* software stack into the official PyTorch stack with consistent user experience to embrace more AI application scenarios.\n\n## Software Prerequisite#\n\nTo use PyTorch on Intel GPUs, you need to install the Intel GPUs driver first. For installation guide, visitIntel GPUs Driver Installation.\nPlease skip the Intel\u00ae Deep Learning Essentials installation section if you install from binaries. For building from source, please refer toPyTorch Installation Prerequisites for Intel GPUsfor both Intel GPU Driver and Intel\u00ae Deep Learning Essentials Installation.\n\n## Installation#\n\n\n## Binaries#\n\nNow that we haveIntel GPU Driverinstalled, use the following commands to installpytorch,torchvision,torchaudio.\npytorch\ntorchvision\ntorchaudio\nFor release wheels\n\n```python\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu\n\n```\n\nFor nightly wheels\n\n```python\npip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu\n\n```\n\n\n## From Source#\n\nNow that we haveIntel GPU Driver and Intel\u00ae Deep Learning Essentialsinstalled. Follow guides to buildpytorch,torchvision,torchaudiofrom source.\npytorch\ntorchvision\ntorchaudio\nBuild from source fortorchrefer toPyTorch Installation Build from source.\ntorch\nBuild from source fortorchvisionrefer toTorchvision Installation Build from source.\ntorchvision\nBuild from source fortorchaudiorefer toTorchaudio Installation Build from source.\ntorchaudio\n\n## Check availability for Intel GPU#\n\nTo check if your Intel GPU is available, you would typically use the following code:\n\n```python\nimport torch\nprint(torch.xpu.is_available())  # torch.xpu is the API for Intel GPU support\n\n```\n\nIf the output isFalse, double check driver installation for Intel GPUs.\nFalse\n\n## Minimum Code Change#\n\nIf you are migrating code fromcuda, you would change references fromcudatoxpu. For example:\ncuda\ncuda\nxpu\n\n```python\n# CUDA CODE\ntensor = torch.tensor([1.0, 2.0]).to(\"cuda\")\n\n# CODE for Intel GPU\ntensor = torch.tensor([1.0, 2.0]).to(\"xpu\")\n\n```\n\nThe following points outline the support and limitations for PyTorch with Intel GPU:\nBoth training and inference workflows are supported.\nBoth eager mode andtorch.compileis supported. The featuretorch.compileis also supported on Windows from PyTorch* 2.7 with Intel GPU, refer toHow to use torch.compile on Windows CPU/XPU.\ntorch.compile\ntorch.compile\nData types such as FP32, BF16, FP16, and Automatic Mixed Precision (AMP) are all supported.\n\n## Examples#\n\nThis section contains usage examples for both inference and training workflows.\n\n## Inference Examples#\n\nHere is a few inference workflow examples.\n\n```python\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\n\nwith torch.no_grad():\n    model(data)\n\nprint(\"Execution finished\")\n\n```\n\n\n```python\nimport torch\nimport torchvision.models as models\n\nmodel = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\n\nwith torch.no_grad():\n    d = torch.rand(1, 3, 224, 224)\n    d = d.to(\"xpu\")\n    # set dtype=torch.bfloat16 for BF16\n    with torch.autocast(device_type=\"xpu\", dtype=torch.float16, enabled=True):\n        model(data)\n\nprint(\"Execution finished\")\n\n```\n\ntorch.compile\n\n```python\nimport torch\nimport torchvision.models as models\nimport time\n\nmodel = models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\nITERS = 10\n\nmodel = model.to(\"xpu\")\ndata = data.to(\"xpu\")\n\nfor i in range(ITERS):\n    start = time.time()\n    with torch.no_grad():\n        model(data)\n        torch.xpu.synchronize()\n    end = time.time()\n    print(f\"Inference time before torch.compile for iteration {i}: {(end-start)*1000} ms\")\n\nmodel = torch.compile(model)\nfor i in range(ITERS):\n    start = time.time()\n    with torch.no_grad():\n        model(data)\n        torch.xpu.synchronize()\n    end = time.time()\n    print(f\"Inference time after torch.compile for iteration {i}: {(end-start)*1000} ms\")\n\nprint(\"Execution finished\")\n\n```\n\n\n## Training Examples#\n\nHere is a few training workflow examples.\n\n```python\nimport torch\nimport torchvision\n\nLR = 0.001\nDOWNLOAD = True\nDATA = \"datasets/cifar10/\"\n\ntransform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize((224, 224)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\ntrain_dataset = torchvision.datasets.CIFAR10(\n    root=DATA,\n    train=True,\n    transform=transform,\n    download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)\ntrain_len = len(train_loader)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\nmodel.train()\nmodel = model.to(\"xpu\")\ncriterion = criterion.to(\"xpu\")\n\nprint(f\"Initiating training\")\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    if (batch_idx + 1) % 10 == 0:\n         iteration_loss = loss.item()\n         print(f\"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}\")\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    },\n    \"checkpoint.pth\",\n)\n\nprint(\"Execution finished\")\n\n```\n\nNote: Training withGradScalerrequires hardware support forFP64.FP64is not natively supported by the Intel\u00ae Arc\u2122 A-Series Graphics. If you run your workloads on Intel\u00ae Arc\u2122 A-Series Graphics, please disableGradScaler.\nGradScaler\nFP64\nFP64\nGradScaler\n\n```python\nimport torch\nimport torchvision\n\nLR = 0.001\nDOWNLOAD = True\nDATA = \"datasets/cifar10/\"\n\nuse_amp=True\n\ntransform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize((224, 224)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\ntrain_dataset = torchvision.datasets.CIFAR10(\n    root=DATA,\n    train=True,\n    transform=transform,\n    download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)\ntrain_len = len(train_loader)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\nscaler = torch.amp.GradScaler(device=\"xpu\", enabled=use_amp)\n\nmodel.train()\nmodel = model.to(\"xpu\")\ncriterion = criterion.to(\"xpu\")\n\nprint(f\"Initiating training\")\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    # set dtype=torch.bfloat16 for BF16\n    with torch.autocast(device_type=\"xpu\", dtype=torch.float16, enabled=use_amp):\n        output = model(data)\n        loss = criterion(output, target)\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad()\n    if (batch_idx + 1) % 10 == 0:\n         iteration_loss = loss.item()\n         print(f\"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}\")\n\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    },\n    \"checkpoint.pth\",\n)\n\nprint(\"Execution finished\")\n\n```\n\ntorch.compile\n\n```python\nimport torch\nimport torchvision\n\nLR = 0.001\nDOWNLOAD = True\nDATA = \"datasets/cifar10/\"\n\ntransform = torchvision.transforms.Compose(\n    [\n        torchvision.transforms.Resize((224, 224)),\n        torchvision.transforms.ToTensor(),\n        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ]\n)\ntrain_dataset = torchvision.datasets.CIFAR10(\n    root=DATA,\n    train=True,\n    transform=transform,\n    download=DOWNLOAD,\n)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128)\ntrain_len = len(train_loader)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.9)\nmodel.train()\nmodel = model.to(\"xpu\")\ncriterion = criterion.to(\"xpu\")\nmodel = torch.compile(model)\n\nprint(f\"Initiating training with torch compile\")\nfor batch_idx, (data, target) in enumerate(train_loader):\n    data = data.to(\"xpu\")\n    target = target.to(\"xpu\")\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    if (batch_idx + 1) % 10 == 0:\n         iteration_loss = loss.item()\n         print(f\"Iteration [{batch_idx+1}/{train_len}], Loss: {iteration_loss:.4f}\")\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n    },\n    \"checkpoint.pth\",\n)\n\nprint(\"Execution finished\")\n\n```\n",
    "url": "https://pytorch.org/docs/stable/notes/get_start_xpu.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f3cd91411db21fb4a2dec98ca784957f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/user_guide/index.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4141e5882a8a2676fa22ef39cd078e8a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/add_scalar.png",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "47f16377afa9e1121921c6bb6fa5ce58",
    "source": "pytorch_docs",
    "title": "Operator Registration \u2014 PyTorch 2.9 documentation",
    "text": "\n## Operator Registration#\n\nCreated On: Aug 27, 2025 | Last Updated On: Sep 02, 2025\nFor new accelerators, one of the most important and fundamental aspects of integration is supporting high-performance operators. To facilitate operator adaptation for users and accelerator developers, PyTorch provides multiple methods for developing and registering operators in bothPythonandC++. The following sections detail some of PyTorch\u2019s fundamental capabilities for operator registration.\nPython\nC++\nNote\nDispatchKeyis used to uniquely identify accelerator within PyTorch, such asCPU,CUDA,MPS, andPrivateUse1. In theory, all subsequent new accelerators will sharePrivateUse1, leveraging its built-in comprehensive scaffolding capabilities to complete the integration of new accelerators. Please refer toLet\u2019s talk about the PyTorch dispatcherif you are interested with dispatcher.\nDispatchKey\nCPU\nCUDA\nMPS\nPrivateUse1\nPrivateUse1\n\n## Operator Set#\n\nPyTorch currently has over 3500 built-in operators (including related operator variants). This represents a significant workload from any perspective, and supporting this massive number of operators in a short period of time is no easy task. Therefore, as the first step in developing new backend operators, our goal should be to focus on the essential operators. For other operators, we can first use the community\u2019s fallback mechanism to support the feature as the first priority. After that, we can gradually complete other operators to improve the performance of the new backend.\nThe required operator set is listed below, primarily consisting of low-level operators required by factory functions and fallback operators:\nOperator Name\nDispatch Key\nDescription\nempty.memory_format\nPrivateUse1\nCreate an uninitialized Tensor with the specified shape and memory layout (the stride is automatically calculated)\nempty_strided\nPrivateUse1\nCreate an uninitialized Tensor of the specified shape and stride (more degrees of freedom)\nas_strided\nPrivateUse1\nCreate a shared view of the input Tensor with new shape, stride, and offset (without allocating new memory)\nview\nPrivateUse1\nCreate a shared view of the input Tensor with new shape, but the original Tensor must be memory-contiguous\n_reshape_alias\nPrivateUse1\nCreates a shared view without safety checks(Internal version of reshape)\nresize_\nPrivateUse1\nModify the shape of the Tensor in place and reallocate memory if capacity is insufficient\n_copy_from\nPrivateUse1\nThe underlying core function of Tensor.copy_ is responsible for the actual cross-device data copying\n_copy_from_and_resize\nPrivateUse1\nCombineresize_and_copy_fromto resize first and then copy\nresize_\n_copy_from\n_local_scalar_dense\nPrivateUse1\nThe underlying implementation of.item(), extracting values from Tensor to CPU scalars\n.item()\nset_.source_Tensor\nPrivateUse1\nSet the current Tensor using the specified Tensor\nset_.source_Storage\nPrivateUse1\nSet the current Tensor using the specified Storage\nset_.source_Storage_storage_offset\nPrivateUse1\nSet the current Tensor using the specified Storage with the storage offset\nfallback\nPrivateUse1\nFallback to CPU\n\n## Basics#\n\nNow that we have defined the initial scope of operator support, we can begin developing operator adaptations. This section will explain these implementations inPythonandC++based on actual scenarios.\nPython\nC++\n\n## Step 1#\n\nThe operators mentioned aboveshare a common characteristic: They are built-in PyTorch operators with definednamespacesandSchemas, and these operators\u2019 built-in accelerators (CPU,CUDA, etc.) have been implemented. What we have to do next is to implement these operators for the new accelerators.\nnamespaces\nSchemas\nCPU\nCUDA\n\n```python\n 1at::Tensor empty_memory_format(\n 2    c10::IntArrayRef size,\n 3    std::optional<c10::ScalarType> dtype_opt,\n 4    std::optional<c10::Layout> layout_opt,\n 5    std::optional<c10::Device> device_opt,\n 6    std::optional<bool> pin_memory_opt,\n 7    std::optional<c10::MemoryFormat> memory_format_opt) {\n 8  const auto device = c10::device_or_default(device_opt);\n 9  const auto dtype = c10::dtype_or_default(dtype_opt);\n10  TORCH_CHECK(device.is_privateuseone());\n11  TORCH_CHECK(\n12      c10::layout_or_default(layout_opt) == c10::Layout::Strided,\n13      \"Non strided layout not supported\");\n14  TORCH_CHECK(\n15      !c10::pinned_memory_or_default(pin_memory_opt),\n16      \"Pin memory can only be on CPU\");\n17  const c10::DeviceGuard device_guard(device);\n18  constexpr c10::DispatchKeySet pu1_dks(c10::DispatchKey::PrivateUse1);\n19  auto allocator = at::GetAllocator(at::kPrivateUse1);\n20  return at::detail::empty_generic(\n21      size, allocator, pu1_dks, dtype, memory_format_opt);\n22}\n\n```\n\n\n```python\n 1at::Tensor wrapper_empty_memory_format(\n 2    c10::IntArrayRef size,\n 3    std::optional<c10::ScalarType> dtype_opt,\n 4    std::optional<c10::Layout> layout_opt,\n 5    std::optional<c10::Device> device_opt,\n 6    std::optional<bool> pin_memory_opt,\n 7    std::optional<c10::MemoryFormat> memory_format_opt) {\n 8  return at::native::openreg::empty_memory_format(\n 9      size,\n10      dtype_opt,\n11      layout_opt,\n12      device_opt,\n13      pin_memory_opt,\n14      memory_format_opt);\n15}\n\n```\n\nTaking theempty.memory_formatoperator as an example, we first need to query the operator\u2019sschemainformation innative_functions.yaml, which contains detailed signature information. Then, we can implement the operator based on the capabilities of the new accelerator.\nempty.memory_format\nschema\nnative_functions.yaml\n\n```python\n- func: empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor\ndispatch:\n    CPU: empty_cpu\n    CUDA: empty_cuda\n    ...\n\n```\n\n\n```python\n 1TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n 2  m.impl(\"empty.memory_format\", wrapper_empty_memory_format);\n 3  m.impl(\"empty_strided\", wrapper_empty_strided);\n 4  m.impl(\"as_strided\", wrapper_as_strided);\n 5  m.impl(\"resize_\", wrapper_resize_);\n 6  m.impl(\"_reshape_alias\", wrapper__reshape_alias);\n 7  m.impl(\"_copy_from\", wrapper__copy_from);\n 8  m.impl(\"_copy_from_and_resize\", wrapper__copy_from_and_resize);\n 9  m.impl(\"_local_scalar_dense\", wrapper__local_scalar_densor);\n10  m.impl(\"set_.source_Tensor\", wrapper_set_source_Tensor_);\n11  m.impl(\"set_.source_Storage\", wrapper_set_source_Storage_);\n12  m.impl(\n13      \"set_.source_Storage_storage_offset\",\n14      wrapper_set_source_Storage_storage_offsetset_);\n15  m.impl(\"view\", wrapper_view);\n16}\n\n```\n\nAfter completing thewrapper_empty_memory_format, we can registeraten::empty.memory_formatforPrivateUse1throughTORCH_LIBRARY_IMPL.\nwrapper_empty_memory_format\naten::empty.memory_format\nPrivateUse1\nTORCH_LIBRARY_IMPL\n\n## Step 2#\n\nBy followingStep 1, we can complete the development and registration of all operators exceptfallback. Next, to support operators related to operations (such as mathematical operations and convolution operations), we need to implement the registration of fallback semantics. This is a built-in capability provided by the PyTorch framework that can fallback some operations that are not supported by new accelerators to the CPU for execution. For new backends in development, this is an extremely effective way to ensure functionality at the expense of performance.\nfallback\n\n```python\n 1void cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {\n 2  static const std::unordered_set<c10::OperatorName> cpu_fallback_blocklist = {\n 3      c10::OperatorName(\"aten::abs\", \"\"),\n 4      c10::OperatorName(\"aten::abs\", \"out\"),\n 5  };\n 6\n 7  const auto& op_name = op.schema().operator_name();\n 8  if (cpu_fallback_blocklist.count(op_name)) {\n 9    TORCH_CHECK(\n10        false,\n11        \"Operator '\",\n12        op_name,\n13        \"' is not implemented for device openreg.\");\n14  } else {\n15    at::native::cpu_fallback(op, stack);\n16  }\n17}\n\n```\n\n\n```python\n1void wrapper_cpu_fallback(\n2    const c10::OperatorHandle& op,\n3    torch::jit::Stack* stack) {\n4  at::native::openreg::cpu_fallback(op, stack);\n5}\n\n```\n\n\n```python\n1TORCH_LIBRARY_IMPL(_, PrivateUse1, m) {\n2  m.fallback(\n3      torch::CppFunction::makeFromBoxedFunction<&wrapper_cpu_fallback>());\n4}\n\n```\n\nwrapper_cpu_fallbackwraps theat::native::cpu_fallbackmethod provided by PyTorch and is registered withPrivateUse1in PyTorch viaTORCH_LIBRARY_IMPL. Subsequent operations not supported by the new backend will automatically fall back to the CPU for execution, and the results will be passed back to the new backend after execution.\nwrapper_cpu_fallback\nat::native::cpu_fallback\nPrivateUse1\nTORCH_LIBRARY_IMPL\n\n## Advanced#\n\n\n## Selective Fallback#\n\nEnabling the fallback mechanism only for certain operators, while following PyTorch\u2019s default behavior for other operators (an error will be reported if the accelerator does not have a corresponding operator implementation), this is a very reasonable scenario as well.\n\n```python\n1void wrapper_cpu_fallback(\n2    const c10::OperatorHandle& op,\n3    torch::jit::Stack* stack) {\n4  at::native::openreg::cpu_fallback(op, stack);\n5}\n\n```\n\n\n```python\n1TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n2  m.impl(\n3      \"sub.Tensor\",\n4      torch::CppFunction::makeFromBoxedFunction<&wrapper_cpu_fallback>());\n5}\n\n```\n\nPer-operator fallbacks are very similar to global fallbacks, the only difference being the registration method: callingm.implregisters an implementation for a specific operator, whilem.fallbackregisters a default implementation for all operators.\nm.impl\nm.fallback\n\n```python\n 1void cpu_fallback(const c10::OperatorHandle& op, torch::jit::Stack* stack) {\n 2  static const std::unordered_set<c10::OperatorName> cpu_fallback_blocklist = {\n 3      c10::OperatorName(\"aten::abs\", \"\"),\n 4      c10::OperatorName(\"aten::abs\", \"out\"),\n 5  };\n 6\n 7  const auto& op_name = op.schema().operator_name();\n 8  if (cpu_fallback_blocklist.count(op_name)) {\n 9    TORCH_CHECK(\n10        false,\n11        \"Operator '\",\n12        op_name,\n13        \"' is not implemented for device openreg.\");\n14  } else {\n15    at::native::cpu_fallback(op, stack);\n16  }\n17}\n\n```\n\nOf course, global fallbacks can also be combined with a blacklist of fallbacks, which is a common approach, especially when only a few operators do not support fallbacks.\n\n## PyTorch STUB#\n\nPyTorch also provides another approach for built-in operators:STUB. This method is essentially based on theStep1<step-one>approach, but adds secondary scheduling capabilities (for example, scheduling based on CPU characteristics).\nSTUB\nStep1<step-one>\nNote\nTheSTUBmethod currently supports only a limited set of operators. For new accelerator devices, the advantage of theSTUBmethod is that it significantly reduces the cost of development at the cost of a small performance overhead. PyTorch currently does not clearly list the set of operators that can be registered throughSTUB. Due to the large number of related operators, only the query method for the supported operator list is provided here.\nSTUB\nSTUB\nSTUB\n\n```python\npushd ${TORCH_ROOT}\n\nfind aten -type f -a -name \"*.h\" | xargs -I {} grep -wl \"^DECLARE_DISPATCH\" {}\n\npopd\n\n```\n\nDECLARE_DISPATCHis a macro used to explicitly declareSTUB. It is currently distributed in theatendirectory. Based on this macro, you can find all operators that can be integrated using theSTUBmethod.\nDECLARE_DISPATCH\nSTUB\naten\nSTUB\n\n```python\n...\naten/src/ATen/native/Activation.h\naten/src/ATen/native/FusedSGD.h\naten/src/ATen/native/nested/NestedTensorBinaryOps.h\naten/src/ATen/native/TensorCompare.h\naten/src/ATen/native/Sorting.h\n...\n\n```\n\n\n```python\nusing unary_fn = void(*)(TensorIteratorBase&);\n\nDECLARE_DISPATCH(unary_fn, abs_stub)\n\n```\n\nThe above listing contains the file that declares theSTUBoperator, where you can clearly see the STUB name and the associated function signature. Next, we will takeabs_stubas an example to briefly introduce the path to support operators throughSTUB.\nSTUB\nabs_stub\nSTUB\n\n```python\n 1void abs_kernel(at::TensorIteratorBase& iter) {\n 2  TORCH_CHECK(iter.ntensors() == 2, \"Abs kernel expects 2 tensors\");\n 3  TORCH_CHECK(\n 4      iter.common_dtype() == at::ScalarType::Float,\n 5      \"Abs kernel only supports float type\");\n 6\n 7  auto& output_tensor = iter.tensor(0);\n 8  auto& input_tensor = iter.tensor(1);\n 9\n10  TORCH_CHECK(\n11      input_tensor.sizes() == output_tensor.sizes(),\n12      \"Input and output tensor sizes must match.\");\n13\n14  auto abs_loop = [](float* out_ptr, const float* in_ptr, int64_t n) {\n15    for (int64_t i = 0; i < n; ++i) {\n16      out_ptr[i] = std::abs(in_ptr[i]);\n17    }\n18  };\n19\n20  MemoryGuard guard(input_tensor, output_tensor);\n21\n22  if (iter.is_contiguous()) {\n23    abs_loop(\n24        static_cast<float*>(iter.data_ptr(0)),\n25        static_cast<float*>(iter.data_ptr(1)),\n26        iter.numel());\n27  } else {\n28    TORCH_CHECK(\n29        input_tensor.is_contiguous(), \"Input tensor must be contiguous.\")\n30\n31    auto output = at::empty(\n32        input_tensor.sizes(),\n33        input_tensor.options().memory_format(\n34            input_tensor.suggest_memory_format()));\n35\n36    MemoryGuard guard(output);\n37\n38    abs_loop(\n39        static_cast<float*>(output.data_ptr()),\n40        static_cast<float*>(iter.data_ptr(1)),\n41        iter.numel());\n42\n43    output_tensor.copy_(output);\n44  }\n45}\n\n```\n\n\n```python\n1REGISTER_PRIVATEUSE1_DISPATCH(abs_stub, &wrapper_abs_stub);\n2REGISTER_PRIVATEUSE1_DISPATCH(\n3    quantize_tensor_per_tensor_affine_stub,\n4    &wrapper_quantize_tensor_per_tensor_affine_stub);\n5REGISTER_PRIVATEUSE1_DISPATCH(\n6    _fused_sdp_choice_stub,\n7    &wrapper__fused_sdp_choice);\n\n```\n\nFrom the signature, we can see that the input ofabs_stubisTensorIteratorBase, a powerful helper class provided by PyTorch that contains all input and output operators, as well as some other auxiliary methods. Based on it, we can develop theabs_kerneloperator and then callREGISTER_PRIVATEUSE1_DISPATCHto specifyabs_stubto complete the registration.\nabs_stub\nTensorIteratorBase\nabs_kernel\nREGISTER_PRIVATEUSE1_DISPATCH\nabs_stub\n\n## Custom Operators#\n\nIn addition to PyTorch\u2019s built-in operators, custom accelerator operators are also very common to improve performance in specific scenarios. These can be categorized into three main approaches:\nForward-only\nForward and backward: Separate registration\nForward and backward: Implemented usingtorch.autograd.Function\ntorch.autograd.Function\nNote\nThere are more details in PyTorch tutorials, so refer toPyTorch Custom Operatorsif you are interested.\nHere, we\u2019ll briefly introduce the implementation process of custom operators, focusing on the forward-only approach. The implementation can be summarized into the following three points:\nDefine Schema:\n\n```python\n1TORCH_LIBRARY(openreg, m) {\n2  m.def(\"custom_abs(Tensor input)-> Tensor\");\n3}\n\n```\n\nNamespace Name:openreg\nopenreg\nFunction Name:custom_abs\ncustom_abs\nInput Parameters:\nType:Tensor\nTensor\nName:input\ninput\nOutput Type:Tensor\nTensor\nRegister Operator&Autograd Fallback:\n\n```python\n1TORCH_LIBRARY_IMPL(openreg, PrivateUse1, m) {\n2  m.impl(\"custom_abs\", &wrapper_custom_abs);\n3}\n\n```\n\n\n```python\n1TORCH_LIBRARY_IMPL(_, AutogradPrivateUse1, m) {\n2  m.fallback(torch::autograd::autogradNotImplementedFallback());\n3}\n\n```\n\nUseTORCH_LIBRARY_IMPLto register thewrapper_custom_absimplementation for thecustom_absoperator inPrivateUse1. However, becauseAutogradis always enabled in PyTorch, PyTorch defaults to finding and executing the corresponding backward implementation even if only forward computation is required(will fallthrough in backward implementation). Therefore, we also need to register the corresponding implementation forAutogradPrivateUse1of thecustom_absoperator. Fortunately, PyTorch also provides a generalAutogradFallbackmechanism namedtorch::autograd::autogradNotImplementedFallback, if only forward computation is involved, it is equivalent to a fallthrough operation, selecting the next DispatchKey for computation; if backward computation is involved, an error is thrown.\nTORCH_LIBRARY_IMPL\nwrapper_custom_abs\ncustom_abs\nPrivateUse1\nAutograd\nAutogradPrivateUse1\ncustom_abs\nAutogradFallback\ntorch::autograd::autogradNotImplementedFallback\nRegister Metadata(optional, but required by the graph mode, etc.):\n\n```python\n1lib = torch.library.Library(\"openreg\", \"IMPL\", \"Meta\")  # noqa: TOR901\n2\n3\n4@torch.library.impl(lib, \"custom_abs\")\n5def custom_abs(self):\n6    return torch.empty_like(self)\n7\n8\n\n```\n\nPyTorch supports registeringMetain both C++ and Python. Since Python registration is simpler, Python is used as an example here. Similar to theTORCH_LIBRARY_IMPLfunction in C++, Python provides the more user-friendlytorch.library.impldecorator.\nMeta\nTORCH_LIBRARY_IMPL\ntorch.library.impl\n\n## Tools#\n\nOperator registration in PyTorch is complex, with diverse registration methods and numerous scenarios. Therefore, the PyTorch community has provided a number of tools to help developers quickly understand the underlying principles and assist in troubleshooting. Here we briefly introduce several commonly used tools:\n\n## Commands#\n\nPyTorch provides a set of commands prefixed withtorch._C._dispatch_around its Dispatch feature. You can query all related interfaces using the following command:\ntorch._C._dispatch_\n\n```python\npython -c 'import torch; print(\"\\n\".join([x for x in dir(torch._C) if x.startswith(\"_dispatch_\")]))'\n\n...\n_dispatch_dump\n_dispatch_dump_table\n_dispatch_has_kernel\n_dispatch_has_kernel_for_any_dispatch_key\n_dispatch_has_kernel_for_dispatch_key\n_dispatch_isTensorSubclassLike\n_dispatch_is_alias_key\n_dispatch_is_included_in_alias\n_dispatch_is_main_interpreter\n_dispatch_kernel_for_dispatch_key_is_fallthrough\n_dispatch_key_for_device\n_dispatch_key_name\n_dispatch_key_parse\n_dispatch_key_set\n...\n\n```\n\nHere are explanations for several commonly used commands:\ntorch._C._dispatch_key_set:\ntorch._C._dispatch_key_set\nDisplays the DispatchKey of the current Tensor, with priority increasing from left to right.\n\n```python\n>>> import torch\n>>> a = torch.randn(3,3,device=\"cuda\")\n>>> torch._C._dispatch_key_set(a)\n'DispatchKeySet(CUDA, ADInplaceOrView, AutogradCUDA, AutocastCUDA)'\n\n```\n\ntorch._C._dispatch_dump_table:\ntorch._C._dispatch_dump_table\nQueries the support status of a given operator across different Dispatch Keys, making it easy to locate the corresponding implementation code.\n\n```python\n>>> import torch\n>>> print(torch._C._dispatch_dump_table(\"aten::add.Tensor\"))\n>>> ...\n    CPU: registered at ./build/aten/src/ATen/RegisterCPU_0.cpp:1309 [kernel]\n    CUDA: registered at ./build/aten/src/ATen/RegisterCUDA_0.cpp:2420 [kernel]\n    HIP: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    MPS: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    IPU: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    XPU: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    HPU: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    VE: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    MTIA: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    MAIA: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    PrivateUse1: registered at ./build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:1373 [default backend kernel]\n    ...\n\n```\n\nYou can easily query the corresponding implementation of theaten::add.Tensoroperator on other platforms, so that you can track the entire operator calling process from the source code level.\naten::add.Tensor\n\n## Environment Variables#\n\nPyTorch also provides some dispatcher-related environment variables that can help with learning and quickly locating issues.\nTORCH_SHOW_DISPATCH_TRACE\nDisplays detailed internal dispatch key scheduling during PyTorch execution.\n\n```python\nexport TORCH_SHOW_DISPATCH_TRACE=1\n\n```\n\n\n```python\n>>> import torch\n>>> a = torch.randn(3,3)\n [call] op=[aten::randn], key=[BackendSelect]\n   [redispatch] op=[aten::randn], key=[CPU]\n     [call] op=[aten::empty.memory_format], key=[BackendSelect]\n       [redispatch] op=[aten::empty.memory_format], key=[CPU]\n     [call] op=[aten::normal_], key=[CPU]\n\n```\n\nYou can clearly see all the underlying operators called by Python-level operators within PyTorch: including the operator name, calling hierarchy, and correspondingDispatchKey.\nDispatchKey",
    "url": "https://pytorch.org/docs/stable/accelerator/operators.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0b70e96f69a62d3bbc8663758eca0a72",
    "source": "pytorch_docs",
    "title": "torch.mutation \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.mutation#\n\n\n## user_input_mutation#\n\nNote\nTags:torch.mutation\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass UserInputMutation(torch.nn.Module):\n    \"\"\"\n    Directly mutate user input in forward\n    \"\"\"\n\n    def forward(self, x):\n        x.mul_(2)\n        return x.cos()\n\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.mutation\"}\nmodel = UserInputMutation()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 mul_: \"f32[3, 2]\" = torch.ops.aten.mul_.Tensor(x, 2);  x = None\n\n                 cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(mul_);  mul_ = None\n            return (cos,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    cos: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/torch.mutation.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d226b8a7284bc31c54f0a0bc78311f59",
    "source": "pytorch_docs",
    "title": "Generic Join Context Manager \u2014 PyTorch 2.9 documentation",
    "text": "\n## Generic Join Context Manager#\n\nCreated On: Jun 06, 2025 | Last Updated On: Jun 06, 2025\nThe generic join context manager facilitates distributed training on uneven\ninputs. This page outlines the API of the relevant classes:Join,Joinable, andJoinHook. For a tutorial, seeDistributed Training with Uneven Inputs Using the Join Context Manager.\nJoin\nJoinable\nJoinHook\nThis class defines the generic join context manager, which allows custom hooks to be called after a process joins.\nThese hooks should shadow the\ncollective communications of non-joined processes to prevent hanging and\nerroring and to ensure algorithmic correctness. Refer toJoinHookfor details about the hook definition.\nJoinHook\nWarning\nThe context manager requires each participatingJoinableto\ncall the methodnotify_join_context()before its own per-\niteration collective communications to ensure correctness.\nJoinable\nnotify_join_context()\nWarning\nThe context manager requires that allprocess_groupattributes in\ntheJoinHookobjects are the same. If there are multipleJoinHookobjects, then thedeviceof the first is used.\nThe process group and device information is used for checking for non-\njoined processes and for notifying processes to throw an exception ifthrow_on_early_terminationis enabled, both of which using an all-\nreduce.\nprocess_group\nJoinHook\nJoinHook\ndevice\nthrow_on_early_termination\njoinables(List[Joinable]) \u2013 a list of the participatingJoinables; their hooks are iterated over in the given\norder.\nJoinable\nenable(bool) \u2013 a flag enabling uneven input detection; setting toFalsedisables the context manager\u2019s functionality and should\nonly be set when the user knows the inputs will not be uneven\n(default:True).\nFalse\nTrue\nthrow_on_early_termination(bool) \u2013 a flag controlling whether to throw an\nexception upon detecting uneven inputs (default:False).\nFalse\nExample:\n\n```python\n>>> import os\n>>> import torch\n>>> import torch.distributed as dist\n>>> import torch.multiprocessing as mp\n>>> import torch.nn.parallel.DistributedDataParallel as DDP\n>>> import torch.distributed.optim.ZeroRedundancyOptimizer as ZeRO\n>>> from torch.distributed.algorithms.join import Join\n>>>\n>>> # On each spawned worker\n>>> def worker(rank):\n>>>     dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n>>>     model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n>>>     optim = ZeRO(model.parameters(), torch.optim.Adam, lr=0.01)\n>>>     # Rank 1 gets one more input than rank 0\n>>>     inputs = [torch.tensor([1.]).to(rank) for _ in range(10 + rank)]\n>>>     with Join([model, optim]):\n>>>         for input in inputs:\n>>>             loss = model(input).sum()\n>>>             loss.backward()\n>>>             optim.step()\n>>>     # All ranks reach here without hanging/erroring\n\n```\n\nNotifies the join context manager that the calling process has not yet joined.\nThen, ifthrow_on_early_termination=True, checks if uneven inputs have been detected\n(i.e. if one process has already joined) and throws an exception if so.\nthrow_on_early_termination=True\nThis method should be called from aJoinableobject before\nits per-iteration collective communications. For example, this should\nbe called at the beginning of the forward pass inDistributedDataParallel.\nJoinable\nDistributedDataParallel\nOnly the firstJoinableobject passed into the context\nmanager performs the collective communications in this method, and\nfor the others, this method is vacuous.\nJoinable\njoinable(Joinable) \u2013 theJoinableobject calling this\nmethod.\nJoinable\nAn async work handle for the all-reduce meant to notify the context\nmanager that the process has not yet joined ifjoinableis the\nfirst one passed into the context manager;Noneotherwise.\njoinable\nNone\nThis defines an abstract base class for joinable classes.\nA joinable class\n(inheriting fromJoinable) should implementjoin_hook(),\nwhich returns aJoinHookinstance, in addition tojoin_device()andjoin_process_group()that return device and\nprocess group information, respectively.\nJoinable\njoin_hook()\nJoinHook\njoin_device()\njoin_process_group()\nReturn the device from which to perform collective communications needed by the join context manager.\nReturn aJoinHookinstance for the givenJoinable.\nJoinHook\nJoinable\nkwargs(dict) \u2013 adictcontaining any keyword arguments\nto modify the behavior of the join hook at run time; allJoinableinstances sharing the same join context\nmanager are forwarded the same value forkwargs.\ndict\nJoinable\nkwargs\nJoinHook\nReturns the process group for the collective communications needed by the join context manager itself.\nThis defines a join hook, which provides two entry points in the join context manager.\nEntry points : a main hook, which is called repeatedly while there exists a non-joined\nprocess, and a post-hook, which is called once all processes have joined.\nTo implement a join hook for the generic join context manager, define a\nclass that inherits fromJoinHookand overridemain_hook()andpost_hook()as appropriate.\nJoinHook\nmain_hook()\npost_hook()\nCall this hook while there exists a non-joined process to shadow collective communications in a training iteration.\nTraining iteration i.e., in one forward pass, backward pass, and optimizer step.\nCall hook after all processes have joined.\nIt is passed an additionalboolargumentis_last_joiner, which indicates if the rank is one of the last to join.\nbool\nis_last_joiner\nis_last_joiner(bool) \u2013Trueif the rank is one of the last to\njoin;Falseotherwise.\nTrue\nFalse",
    "url": "https://pytorch.org/docs/stable/distributed.algorithms.join.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7c61c9efe41b7aa50c1b526e0579020c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch_nccl_environment_variables.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4657867218e0c10cebb02c095419e782",
    "source": "pytorch_docs",
    "title": "TorchInductor and AOTInductor Provenance Tracking \u2014 PyTorch 2.9 documentation",
    "text": "\n## TorchInductor and AOTInductor Provenance Tracking#\n\nCreated On: May 09, 2025 | Last Updated On: May 23, 2025\nWarning\nThis feature is a prototype under active development and there will be\nbreaking change in future releases.\nThe current compatibility of this tool is limited to the latest nightly build of PyTorch.\nThis section describes how to use the provenance tracking feature for TorchInductor and AOTInductor intlparse.\nProvenance tracking helps you visualize the relationships between the input GraphModule to (AOT)Inductor and the optimized code generated. This feature allows you to trace how your original operations are transformed during compilation.\ntlparse\nSome example screenshots of the provenance tracking tool are shown below.\nThe tool visualizes the mapping between nodes in the input graph (panel 1), the post grad graph (panel 2), and the Inductor generated code (panel 3).\nTheboldedlines represent nodes/kernels covered by the current provenance tracing functionality.\nWe currently cover triton kernels, cpp kernels, and combo kernels.\nThe yellow highlighting shows the provenance of the nodes/kernels.\n\n## Using the Provenance Tracking Highlighter#\n\nFollow these steps to enable and use provenance tracking in your PyTorch project:\nInstalltlparsebycargoinstalltlparse. If you don\u2019t havecargo, seeThe Cargo Bookfor instructions to install.\ntlparse\ncargoinstalltlparse\ncargo\nRun your program with required flags:\n\n```python\nTORCH_TRACE=~/my_trace_log_dir TORCH_LOGS=\"+inductor\" TORCH_COMPILE_DEBUG=1 python your_program.py\n\n```\n\nThis will generate a log file in~/my_trace_log_dir. The log file will be used by tlparse to generate the provenance tracking highlighter.\n~/my_trace_log_dir\nRuntlparseon the log with--inductor-provenanceflag. For example:\ntlparse\n--inductor-provenance\n\n```python\ntlparse log_file_name.log --inductor-provenance\n\n```\n\nEven if you don\u2019t add the--inductor-provenanceflag, you should be able to see the mapping in json format in theinductor_provenance_tracking_node_mappings_<number>.jsonfile in theindex.htmltlparse output.\n--inductor-provenance\ninductor_provenance_tracking_node_mappings_<number>.json\nindex.html\nRuntlparedirectly on the log file. It might not work if you run \u201ctlparse parse <folder_name>  \u2013inductor-provenance\u201d.\ntlpare\nThetlparseartifacts used by the provenance tracking highlighter are:\ntlparse\nbefore_pre_grad_graph.txt\nbefore_pre_grad_graph.txt\nafter_post_grad_graph.txt\nafter_post_grad_graph.txt\ninductor_aot_wrapper_code.txt\ninductor_aot_wrapper_code.txt\ninductor_output_code.txt\ninductor_output_code.txt\ninductor_provenance_tracking_node_mappings.json\ninductor_provenance_tracking_node_mappings.json\nAfter runningtlparse<file_name>--inductor-provenance, you should see an additional \u201cProvenance Tracking\u201d section in the tlparse output. Clicking into the link(s) to access the provenance tracking tool.\nFor a demo, see:pytorch/tlparse#93\ntlparse<file_name>--inductor-provenance\n\n## See Also#\n\ntlparseis a tool written in Rust.\ntlparse\nLink to the tlparse GitHub repo:pytorch/tlparse\nLearn more abouttlparseattorch.compile Troubleshooting\ntlparse",
    "url": "https://pytorch.org/docs/stable/torch.compiler_inductor_provenance.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f49b578ba813a87b3a656c274dd795b6",
    "source": "pytorch_docs",
    "title": "torch.cond \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.cond#\n\n\n## cond_branch_class_method#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass MySubModule(torch.nn.Module):\n    def foo(self, x):\n        return x.cos()\n\n    def forward(self, x):\n        return self.foo(x)\n\nclass CondBranchClassMethod(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n\n    This example demonstrates using class method in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.subm = MySubModule()\n\n    def bar(self, x):\n        return x.sin()\n\n    def forward(self, x):\n        return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchClassMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 sin: \"f32[3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nested_function#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNestedFunction(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n    This example demonstrates using nested function in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        def true_fn(x):\n            def inner_true_fn(y):\n                return x + y\n\n            return inner_true_fn(x)\n\n        def false_fn(x):\n            def inner_false_fn(y):\n                return x - y\n\n            return inner_false_fn(x)\n\n        return cond(x.shape[0] < 10, true_fn, false_fn, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNestedFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 add: \"f32[3]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nonlocal_variables#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNonlocalVariables(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n    - both branches must take the same args, which must also match the branch args passed to cond.\n    - both branches must return a single tensor\n    - returned tensor must have the same tensor metadata, e.g. shape and dtype\n    - branch function can be free function, nested function, lambda, class methods\n    - branch function can not have closure variables\n    - no inplace mutations on inputs or global variables\n\n    This example demonstrates how to rewrite code to avoid capturing closure variables in branch functions.\n\n    The code below will not work because capturing closure variables is not supported.\n    ```\n    my_tensor_var = x + 100\n    my_primitive_var = 3.14\n\n    def true_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y + my_tensor_var + my_primitive_var\n\n    def false_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y - my_tensor_var - my_primitive_var\n\n    return cond(x.shape[0] > 5, true_fn, false_fn, [x])\n    ```\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        my_tensor_var = x + 100\n        my_primitive_var = 3.14\n\n        def true_fn(x, y, z):\n            return x + y + z\n\n        def false_fn(x, y, z):\n            return x - y - z\n\n        return cond(\n            x.shape[0] > 5,\n            true_fn,\n            false_fn,\n            [x, my_tensor_var, torch.tensor(my_primitive_var)],\n        )\n\nexample_args = (torch.randn(6),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNonlocalVariables()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, c_lifted_tensor_0: \"f32[]\", x: \"f32[6]\"):\n                 add: \"f32[6]\" = torch.ops.aten.add.Tensor(x, 100)\n\n                 lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(c_lifted_tensor_0);  c_lifted_tensor_0 = None\n            detach_: \"f32[]\" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None\n\n                 add_1: \"f32[6]\" = torch.ops.aten.add.Tensor(x, add);  x = add = None\n            add_2: \"f32[6]\" = torch.ops.aten.add.Tensor(add_1, detach_);  add_1 = detach_ = None\n            return (add_2,)\n\nGraph signature:\n    # inputs\n    c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0'\n    x: USER_INPUT\n\n    # outputs\n    add_2: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_closed_over_variable#\n\nNote\nTags:python.closure,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondClosedOverVariable(torch.nn.Module):\n    \"\"\"\n    torch.cond() supports branches closed over arbitrary variables.\n    \"\"\"\n\n    def forward(self, pred, x):\n        def true_fn(val):\n            return x * 2\n\n        def false_fn(val):\n            return x - 2\n\n        return cond(pred, true_fn, false_fn, [x + 1])\n\nexample_args = (torch.tensor(True), torch.randn(3, 2))\ntags = {\"torch.cond\", \"python.closure\"}\nmodel = CondClosedOverVariable()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pred: \"b8[]\", x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1);  add = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, (x,));  pred = true_graph_0 = false_graph_0 = x = None\n            getitem: \"f32[3, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, 2);  x = None\n                return (mul,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(x, 2);  x = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    pred: USER_INPUT\n    x: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_operands#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.export import Dim\n\nx = torch.randn(3, 2)\ny = torch.randn(2)\ndim0_x = Dim(\"dim0_x\")\n\nclass CondOperands(torch.nn.Module):\n    \"\"\"\n    The operands passed to cond() must be:\n    - a list of tensors\n    - match arguments of `true_fn` and `false_fn`\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x, y):\n        def true_fn(x, y):\n            return x + y\n\n        def false_fn(x, y):\n            return x - y\n\n        return torch.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n\nexample_args = (x, y)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nextra_inputs = (torch.randn(2, 2), torch.randn(2))\ndynamic_shapes = {\"x\": {0: dim0_x}, \"y\": None}\nmodel = CondOperands()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n             #\n            sym_size_int_1: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0)\n\n                 gt: \"Sym(s77 > 2)\" = sym_size_int_1 > 2;  sym_size_int_1 = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x, y));  gt = true_graph_0 = false_graph_0 = x = y = None\n            getitem: \"f32[s77, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         add: \"f32[s77, 2]\" = torch.ops.aten.add.Tensor(x, y);  x = y = None\n                return (add,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         sub: \"f32[s77, 2]\" = torch.ops.aten.sub.Tensor(x, y);  x = y = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {s77: VR[0, int_oo]}\n\n```\n\n\n## cond_predicate#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondPredicate(torch.nn.Module):\n    \"\"\"\n    The conditional statement (aka predicate) passed to cond() must be one of the following:\n      - torch.Tensor with a single element\n      - boolean expression\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        pred = x.dim() > 2 and x.shape[2] > 10\n\n        return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x])\n\nexample_args = (torch.randn(6, 4, 3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondPredicate()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[6, 4, 3]\"):\n                 sin: \"f32[6, 4, 3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/torch.cond.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d34c7ed6df985f1be876811cfe2abb35",
    "source": "pytorch_docs",
    "title": "Distributed RPC Framework \u2014 PyTorch 2.9 documentation",
    "text": "\n## Distributed RPC Framework#\n\nCreated On: Nov 14, 2019 | Last Updated On: Jul 09, 2025\nThe distributed RPC framework provides mechanisms for multi-machine model\ntraining through a set of primitives to allow for remote communication, and a\nhigher-level API to automatically differentiate models split across several\nmachines.\nWarning\nAPIs in the RPC package are stable and in maintenance mode.\nWarning\nCUDA support is abetafeature.\nNot all features of the RPC package are yet compatible with CUDA support and\nthus their use is discouraged. These unsupported features include: RRefs,\nJIT compatibility, dist autograd and dist optimizer, and profiling.\nNote\nPlease refer toPyTorchDistributedOverview<https://pytorch.org/tutorials/beginner/dist_overview.html>__\nfor a brief introduction to all features related to distributed training.\nPyTorchDistributedOverview<https://pytorch.org/tutorials/beginner/dist_overview.html>\n\n## Basics#\n\nThe distributed RPC framework makes it easy to run functions remotely, supports\nreferencing remote objects without copying the real data around, and provides\nautograd and optimizer APIs to transparently run backward and update parameters\nacross RPC boundaries. These features can be categorized into four sets of APIs.\nRemote Procedure Call (RPC)supports running a function on the specified\ndestination worker with the given arguments and getting the return value back\nor creating a reference to the return value. There are three main RPC APIs:rpc_sync()(synchronous),rpc_async()(asynchronous), andremote()(asynchronous and returns a reference\nto the remote return value). Use the synchronous API if the user code cannot\nproceed without the return value. Otherwise, use the asynchronous API to get\na future, and wait on the future when the return value is needed on the\ncaller. Theremote()API is useful when the\nrequirement is to create something remotely but never need to fetch it to\nthe caller. Imagine the case that a driver process is setting up a parameter\nserver and a trainer. The driver can create an embedding table on the\nparameter server and then share the reference to the embedding table with the\ntrainer, but itself will never use the embedding table locally. In this case,rpc_sync()andrpc_async()are no longer appropriate, as they\nalways imply that the return value will be returned to the caller\nimmediately or in the future.\nrpc_sync()\nrpc_async()\nremote()\nremote()\nrpc_sync()\nrpc_async()\nRemote Reference (RRef)serves as a distributed shared pointer to a local\nor remote object. It can be shared with other workers and reference counting\nwill be handled transparently. Each RRef only has one owner and the object\nonly lives on that owner. Non-owner workers holding RRefs can get copies of\nthe object from the owner by explicitly requesting it. This is useful when\na worker needs to access some data object, but itself is neither the creator\n(the caller ofremote()) or the owner of the\nobject. The distributed optimizer, as we will discuss below, is one example\nof such use cases.\nremote()\nDistributed Autogradstitches together local autograd engines on all the\nworkers involved in the forward pass, and automatically reach out to them\nduring the backward pass to compute gradients. This is especially helpful if\nthe forward pass needs to span multiple machines when conducting, e.g.,\ndistributed model parallel training, parameter-server training, etc. With\nthis feature, user code no longer needs to worry about how to send gradients\nacross RPC boundaries and in which order should the local autograd engines\nbe launched, which can become quite complicated where there are nested and\ninter-dependent RPC calls in the forward pass.\nDistributed Optimizer\u2019s constructor takes aOptimizer()(e.g.,SGD(),Adagrad(), etc.) and a list of parameter RRefs, creates anOptimizer()instance on each distinct RRef owner, and\nupdates parameters accordingly when runningstep(). When you have\ndistributed forward and backward passes, parameters and gradients will be\nscattered across multiple workers, and hence it requires an optimizer on each\nof the involved workers. Distributed Optimizer wraps all those local\noptimizers into one, and provides a concise constructor andstep()API.\nOptimizer()\nSGD()\nAdagrad()\nOptimizer()\nstep()\nstep()\n\n## RPC#\n\nBefore using RPC and distributed autograd primitives, initialization must take\nplace. To initialize the RPC framework we need to useinit_rpc()which would initialize the RPC\nframework, RRef framework and distributed autograd.\ninit_rpc()\nInitializes RPC primitives such as the local RPC agent\nand distributed autograd, which immediately makes the current\nprocess ready to send and receive RPCs.\nname(str) \u2013 a globally unique name of this node. (e.g.,Trainer3,ParameterServer2,Master,Worker1)\nName can only contain number, alphabet, underscore, colon,\nand/or dash, and must be shorter than 128 characters.\nTrainer3\nParameterServer2\nMaster\nWorker1\nbackend(BackendType,optional) \u2013 The type of RPC backend\nimplementation. Supported values isBackendType.TENSORPIPE(the default).\nSeeBackendsfor more information.\nBackendType.TENSORPIPE\nrank(int) \u2013 a globally unique id/rank of this node.\nworld_size(int) \u2013 The number of workers in the group.\nrpc_backend_options(RpcBackendOptions,optional) \u2013 The options\npassed to the RpcAgent constructor. It must be an agent-specific\nsubclass ofRpcBackendOptionsand contains agent-specific initialization configurations. By\ndefault, for all agents, it sets the default timeout to 60\nseconds and performs the rendezvous with an underlying process\ngroup initialized usinginit_method=\"env://\",\nmeaning that environment variablesMASTER_ADDRandMASTER_PORTneed to be set properly. SeeBackendsfor more information and find which options\nare available.\nRpcBackendOptions\ninit_method=\"env://\"\nMASTER_ADDR\nMASTER_PORT\nThe following APIs allow users to remotely execute functions as well as create\nreferences (RRefs) to remote data objects. In these APIs, when passing aTensoras an argument or a return value, the destination worker will try to\ncreate aTensorwith the same meta (i.e., shape, stride, etc.). We\nintentionally disallow transmitting CUDA tensors because it might crash if the\ndevice lists on source and destination workers do not match. In such cases,\napplications can always explicitly move the input tensors to CPU on the caller\nand move it to the desired devices on the callee if necessary.\nTensor\nTensor\nMake a blocking RPC call to run functionfuncon workerto. RPC\nmessages are sent and received in parallel to execution of Python code. This\nmethod is thread-safe.\nfunc\nto\nto(strorWorkerInfoorint) \u2013 name/rank/WorkerInfoof the destination worker.\nWorkerInfo\nfunc(Callable) \u2013 a callable function, such as Python callables, builtin\noperators (e.g.add()) and annotated\nTorchScript functions.\nadd()\nargs(tuple) \u2013 the argument tuple for thefuncinvocation.\nfunc\nkwargs(dict) \u2013 is a dictionary of keyword arguments for thefuncinvocation.\nfunc\ntimeout(float,optional) \u2013 timeout in seconds to use for this RPC. If\nthe RPC does not complete in this amount of\ntime, an exception indicating it has\ntimed out will be raised. A value of 0\nindicates an infinite timeout, i.e. a timeout\nerror will never be raised. If not provided,\nthe default value set during initialization\nor with_set_rpc_timeoutis used.\n_set_rpc_timeout\nReturns the result of runningfuncwithargsandkwargs.\nfunc\nargs\nkwargs\nMake sure thatMASTER_ADDRandMASTER_PORTare set properly\non both workers. Refer toinit_process_group()API for more details. For example,\nMASTER_ADDR\nMASTER_PORT\ninit_process_group()\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\nThen run the following code in two different processes:\n\n```python\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nBelow is an example of running a TorchScript function using RPC.\n\n```python\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar)\n\n```\n\n\n```python\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nMake a non-blocking RPC call to run functionfuncon workerto. RPC\nmessages are sent and received in parallel to execution of Python code. This\nmethod is thread-safe. This method will immediately return aFuturethat can be awaited on.\nfunc\nto\nFuture\nto(strorWorkerInfoorint) \u2013 name/rank/WorkerInfoof the destination worker.\nWorkerInfo\nfunc(Callable) \u2013 a callable function, such as Python callables, builtin\noperators (e.g.add()) and annotated\nTorchScript functions.\nadd()\nargs(tuple) \u2013 the argument tuple for thefuncinvocation.\nfunc\nkwargs(dict) \u2013 is a dictionary of keyword arguments for thefuncinvocation.\nfunc\ntimeout(float,optional) \u2013 timeout in seconds to use for this RPC. If\nthe RPC does not complete in this amount of\ntime, an exception indicating it has\ntimed out will be raised. A value of 0\nindicates an infinite timeout, i.e. a timeout\nerror will never be raised. If not provided,\nthe default value set during initialization\nor with_set_rpc_timeoutis used.\n_set_rpc_timeout\nReturns aFutureobject that can be waited\non. When completed, the return value offunconargsandkwargscan be retrieved from theFutureobject.\nFuture\nfunc\nargs\nkwargs\nFuture\nWarning\nUsing GPU tensors as arguments or return values offuncis not\nsupported since we don\u2019t support sending GPU tensors over the wire. You\nneed to explicitly copy GPU tensors to CPU before using them as\narguments or return values offunc.\nfunc\nfunc\nWarning\nTherpc_asyncAPI does not copy storages of argument tensors until\nsending them over the wire, which could be done by a different thread\ndepending on the RPC backend type. The caller should make sure that the\ncontents of those tensors stay intact until the returnedFuturecompletes.\nrpc_async\nFuture\nMake sure thatMASTER_ADDRandMASTER_PORTare set properly\non both workers. Refer toinit_process_group()API for more details. For example,\nMASTER_ADDR\nMASTER_PORT\ninit_process_group()\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\nThen run the following code in two different processes:\n\n```python\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2))\n>>> result = fut1.wait() + fut2.wait()\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nBelow is an example of running a TorchScript function using RPC.\n\n```python\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar)\n\n```\n\n\n```python\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut = rpc.rpc_async(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> ret = fut.wait()\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nMake a remote call to runfuncon workertoand return anRRefto the result value immediately.\nWorkertowill be the owner of the returnedRRef, and the worker callingremoteis\na user. The owner manages the global reference count of itsRRef, and the ownerRRefis only destructed when globally there\nare no living references to it.\nfunc\nto\nRRef\nto\nRRef\nremote\nRRef\nRRef\nto(strorWorkerInfoorint) \u2013 name/rank/WorkerInfoof the destination worker.\nWorkerInfo\nfunc(Callable) \u2013 a callable function, such as Python callables, builtin\noperators (e.g.add()) and annotated\nTorchScript functions.\nadd()\nargs(tuple) \u2013 the argument tuple for thefuncinvocation.\nfunc\nkwargs(dict) \u2013 is a dictionary of keyword arguments for thefuncinvocation.\nfunc\ntimeout(float,optional) \u2013 timeout in seconds for this remote call. If the\ncreation of thisRRefon workertois not successfully processed on this\nworker within this timeout, then the next time\nthere is an attempt to use the RRef (such asto_here()), a timeout will be raised\nindicating this failure. A value of 0 indicates\nan infinite timeout, i.e. a timeout error will\nnever be raised. If not provided, the default\nvalue set during initialization or with_set_rpc_timeoutis used.\nRRef\nto\nto_here()\n_set_rpc_timeout\nA userRRefinstance to the result\nvalue. Use the blocking APItorch.distributed.rpc.RRef.to_here()to retrieve the result value locally.\nRRef\ntorch.distributed.rpc.RRef.to_here()\nWarning\nTheremoteAPI does not copy storages of argument tensors until\nsending them over the wire, which could be done by a different thread\ndepending on the RPC backend type. The caller should make sure that the\ncontents of those tensors stay intact until the returned RRef is\nconfirmed by the owner, which can be checked using thetorch.distributed.rpc.RRef.confirmed_by_owner()API.\nremote\ntorch.distributed.rpc.RRef.confirmed_by_owner()\nWarning\nErrors such as timeouts for theremoteAPI are handled on a\nbest-effort basis. This means that when remote calls initiated byremotefail, such as with a timeout error, we take a best-effort\napproach to error handling. This means that errors are handled and set\non the resulting RRef on an asynchronous basis. If the RRef has not been\nused by the application before this handling (such asto_hereor\nfork call), then future uses of theRRefwill appropriately raise\nerrors. However, it is possible that the user application will use theRRefbefore the errors are handled. In this case, errors may not be\nraised as they have not yet been handled.\nremote\nremote\nto_here\nRRef\nRRef\nExample:\n\n```python\nMake sure that ``MASTER_ADDR`` and ``MASTER_PORT`` are set properly\non both workers. Refer to :meth:`~torch.distributed.init_process_group`\nAPI for more details. For example,\n\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\n\nThen run the following code in two different processes:\n\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>> x = rref1.to_here() + rref2.to_here()\n>>> rpc.shutdown()\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\nBelow is an example of running a TorchScript function using RPC.\n\n>>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(tensor: torch.Tensor, scalar: int):\n>>>    return torch.add(tensor, scalar)\n\n>>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref = rpc.remote(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rref.to_here()\n>>> rpc.shutdown()\n\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nGetWorkerInfoof a given worker name.\nUse thisWorkerInfoto avoid passing an\nexpensive string on every invocation.\nWorkerInfo\nWorkerInfo\nworker_name(str) \u2013 the string name of a worker. IfNone, return the\nthe id of the current worker. (defaultNone)\nNone\nNone\nWorkerInfoinstance for the givenworker_nameorWorkerInfoof the\ncurrent worker ifworker_nameisNone.\nWorkerInfo\nworker_name\nWorkerInfo\nworker_name\nNone\nPerform a shutdown of the RPC agent, and then destroy the RPC agent. This\nstops the local agent from accepting outstanding requests, and shuts\ndown the RPC framework by terminating all RPC threads. Ifgraceful=True,\nthis will block until all local and remote RPC processes reach this method\nand wait for all outstanding work to complete. Otherwise, ifgraceful=False, this is a local shutdown, and it does not wait for other\nRPC processes to reach this method.\ngraceful=True\ngraceful=False\nWarning\nForFutureobjects returned byrpc_async(),future.wait()should not\nbe called aftershutdown().\nFuture\nrpc_async()\nfuture.wait()\nshutdown()\ngraceful(bool) \u2013 Whether to do a graceful shutdown or not. If True,\nthis will 1) wait until there is no pending system\nmessages forUserRRefsand delete them; 2) block\nuntil all local and remote RPC processes have reached\nthis method and wait for all outstanding work to\ncomplete.\nUserRRefs\nMake sure thatMASTER_ADDRandMASTER_PORTare set properly\non both workers. Refer toinit_process_group()API for more details. For example,\nMASTER_ADDR\nMASTER_PORT\ninit_process_group()\nexport MASTER_ADDR=localhost\nexport MASTER_PORT=5678\nThen run the following code in two different processes:\n\n```python\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> # do some work\n>>> result = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(1), 1))\n>>> # ready to shutdown\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> # wait for worker 0 to finish work, and then shutdown.\n>>> rpc.shutdown()\n\n```\n\nA structure that encapsulates information of a worker in the system.\nContains the name and ID of the worker. This class is not meant to\nbe constructed directly, rather, an instance can be retrieved\nthroughget_worker_info()and the\nresult can be passed in to functions such asrpc_sync(),rpc_async(),remote()to avoid copying a string on\nevery invocation.\nget_worker_info()\nrpc_sync()\nrpc_async()\nremote()\nGlobally unique id to identify the worker.\nThe name of the worker.\nThe RPC package also provides decorators which allow applications to specify\nhow a given function should be treated on the callee side.\nA decorator for a function indicating that the return value of the function\nis guaranteed to be aFutureobject and this\nfunction can run asynchronously on the RPC callee. More specifically, the\ncallee extracts theFuturereturned by the wrapped\nfunction and installs subsequent processing steps as a callback to thatFuture. The installed callback will read the value\nfrom theFuturewhen completed and send the\nvalue back as the RPC response. That also means the returnedFutureonly exists on the callee side and is never\nsent through RPC. This decorator is useful when the wrapped function\u2019s\n(fn) execution needs to pause and resume due to, e.g., containingrpc_async()or waiting for other signals.\nFuture\nFuture\nFuture\nFuture\nFuture\nfn\nrpc_async()\nNote\nTo enable asynchronous execution, applications must pass the\nfunction object returned by this decorator to RPC APIs. If RPC detected\nattributes installed by this decorator, it knows that this function\nreturns aFutureobject and will handle that accordingly.\nHowever, this does not mean this decorator has to be outmost one when\ndefining a function. For example, when combined with@staticmethodor@classmethod,@rpc.functions.async_executionneeds to be the\ninner decorator to allow the target function be recognized as a static\nor class function. This target function can still execute asynchronously\nbecause, when accessed, the static or class method preserves attributes\ninstalled by@rpc.functions.async_execution.\nFuture\n@staticmethod\n@classmethod\n@rpc.functions.async_execution\n@rpc.functions.async_execution\nThe returnedFutureobject can come fromrpc_async(),then(), orFutureconstructor. The example below shows directly using theFuturereturned bythen().\nFuture\nrpc_async()\nthen()\nFuture\nFuture\nthen()\n\n```python\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @rpc.functions.async_execution\n>>> def async_add_chained(to, x, y, z):\n>>>     # This function runs on \"worker1\" and returns immediately when\n>>>     # the callback is installed through the `then(cb)` API. In the\n>>>     # mean time, the `rpc_async` to \"worker2\" can run concurrently.\n>>>     # When the return value of that `rpc_async` arrives at\n>>>     # \"worker1\", \"worker1\" will run the lambda function accordingly\n>>>     # and set the value for the previously returned `Future`, which\n>>>     # will then trigger RPC to send the result back to \"worker0\".\n>>>     return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>         lambda fut: fut.wait() + z\n>>>     )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add_chained,\n>>>     args=(\"worker2\", torch.ones(2), 1, 1)\n>>> )\n>>> print(ret)  # prints tensor([3., 3.])\n\n```\n\nWhen combined with TorchScript decorators, this decorator must be the\noutmost one.\n\n```python\n>>> from torch import Tensor\n>>> from torch.futures import Future\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @torch.jit.script\n>>> def script_add(x: Tensor, y: Tensor) -> Tensor:\n>>>     return x + y\n>>>\n>>> @rpc.functions.async_execution\n>>> @torch.jit.script\n>>> def async_add(to: str, x: Tensor, y: Tensor) -> Future[Tensor]:\n>>>     return rpc.rpc_async(to, script_add, (x, y))\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1)\n>>> )\n>>> print(ret)  # prints tensor([2., 2.])\n\n```\n\nWhen combined with static or class method, this decorator must be the\ninner one.\n\n```python\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> class AsyncExecutionClass:\n>>>\n>>>     @staticmethod\n>>>     @rpc.functions.async_execution\n>>>     def static_async_add(to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>>     @classmethod\n>>>     @rpc.functions.async_execution\n>>>     def class_async_add(cls, to, x, y, z):\n>>>         ret_fut = torch.futures.Future()\n>>>         rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: ret_fut.set_result(fut.wait() + z)\n>>>         )\n>>>         return ret_fut\n>>>\n>>>     @rpc.functions.async_execution\n>>>     def bound_async_add(self, to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.static_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.class_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n\n```\n\nThis decorator also works with RRef helpers, i.e., .torch.distributed.rpc.RRef.rpc_sync(),torch.distributed.rpc.RRef.rpc_async(), andtorch.distributed.rpc.RRef.remote().\ntorch.distributed.rpc.RRef.rpc_sync()\ntorch.distributed.rpc.RRef.rpc_async()\ntorch.distributed.rpc.RRef.remote()\n\n```python\n>>> from torch.distributed import rpc\n>>>\n>>> # reuse the AsyncExecutionClass class above\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_sync().static_async_add(\"worker2\", torch.ones(2), 1, 2)\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_async().static_async_add(\"worker2\", torch.ones(2), 1, 2).wait()\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.remote().static_async_add(\"worker2\", torch.ones(2), 1, 2).to_here()\n>>> print(ret)  # prints tensor([4., 4.])\n\n```\n\n\n## Backends#\n\nThe RPC module can leverage different backends to perform the communication\nbetween the nodes. The backend to be used can be specified in theinit_rpc()function, by passing a certain value of\ntheBackendTypeenum. Regardless of what backend\nis used, the rest of the RPC API won\u2019t change. Each backend also defines its own\nsubclass of theRpcBackendOptionsclass, an\ninstance of which can also be passed toinit_rpc()to configure the backend\u2019s behavior.\ninit_rpc()\nBackendType\nRpcBackendOptions\ninit_rpc()\nAn enum class of available backends.\nPyTorch ships with a builtinBackendType.TENSORPIPEbackend.\nAdditional ones can be registered using theregister_backend()function.\nBackendType.TENSORPIPE\nregister_backend()\nAn abstract structure encapsulating the options passed into the RPC\nbackend. An instance of this class can be passed in toinit_rpc()in order to initialize RPC\nwith specific configurations, such as the RPC timeout andinit_methodto be used.\ninit_rpc()\ninit_method\nURL specifying how to initialize the process group.\nDefault isenv://\nenv://\nA float indicating the timeout to use for all\nRPCs. If an RPC does not complete in this timeframe, it will\ncomplete with an exception indicating that it has timed out.\nThe TensorPipe agent, which is the default, leveragesthe TensorPipe library, which provides a natively\npoint-to-point communication primitive specifically suited for machine learning\nthat fundamentally addresses some of the limitations of Gloo. Compared to Gloo,\nit has the advantage of being asynchronous, which allows a large number of\ntransfers to occur simultaneously, each at their own speed, without blocking\neach other. It will only open pipes between pairs of nodes when needed, on\ndemand, and when one node fails only its incident pipes will be closed, while\nall other ones will keep working as normal. In addition, it is able to support\nmultiple different transports (TCP, of course, but also shared memory, NVLink,\nInfiniBand, \u2026) and can automatically detect their availability and negotiate\nthe best transport to use for each pipe.\nThe TensorPipe backend comes with a TCP-based transport, just like Gloo. It is also able to\nautomatically chunk and multiplex large tensors over multiple sockets and\nthreads in order to achieve very high bandwidths. The agent will be able to pick\nthe best transport on its own, with no intervention required.\nExample:\n\n```python\nimport os\nfrom torch.distributed import rpc\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '29500'\n\nrpc.init_rpc(\n    \"worker1\",\n    rank=0,\n    world_size=2,\n    rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n        num_worker_threads=8,\n        rpc_timeout=20 # 20 second timeout\n    )\n)\n\n# omitting init_rpc invocation on worker2\n\n```\n\nThe backend options forTensorPipeAgent, derived fromRpcBackendOptions.\nTensorPipeAgent\nRpcBackendOptions\nnum_worker_threads(int,optional) \u2013 The number of threads in the\nthread-pool used byTensorPipeAgentto execute\nrequests (default: 16).\nTensorPipeAgent\nrpc_timeout(float,optional) \u2013 The default timeout, in seconds,\nfor RPC requests (default: 60 seconds). If the RPC has not\ncompleted in this timeframe, an exception indicating so will\nbe raised. Callers can override this timeout for individual\nRPCs inrpc_sync()andrpc_async()if necessary.\nrpc_sync()\nrpc_async()\ninit_method(str,optional) \u2013 The URL to initialize the distributed\nstore used for rendezvous. It takes any value accepted for the\nsame argument ofinit_process_group()(default:env://).\ninit_process_group()\nenv://\ndevice_maps(Dict[str,Dict],optional) \u2013 Device placement mappings from\nthis worker to the callee. Key is the callee worker name and value\nthe dictionary (Dictofint,str, ortorch.device)\nthat maps this worker\u2019s devices to the callee worker\u2019s devices.\n(default:None)\nDict\nint\nstr\ntorch.device\nNone\ndevices(List[int, str, ortorch.device], optional) \u2013 all local\nCUDA devices used by RPC agent. By Default, it will be initialized\nto all local devices from its owndevice_mapsand corresponding\ndevices from its peers\u2019device_maps. When processing CUDA RPC\nrequests, the agent will properly synchronize CUDA streams for\nall devices in thisList.\ntorch.device\ndevice_maps\ndevice_maps\nList\nThe device map locations.\nAll devices used by the local agent.\nURL specifying how to initialize the process group.\nDefault isenv://\nenv://\nThe number of threads in the thread-pool used byTensorPipeAgentto execute\nrequests.\nTensorPipeAgent\nA float indicating the timeout to use for all\nRPCs. If an RPC does not complete in this timeframe, it will\ncomplete with an exception indicating that it has timed out.\nSet device mapping between each RPC caller and callee pair. This\nfunction can be called multiple times to incrementally add\ndevice placement configurations.\nto(str) \u2013 Callee name.\ndevice_map(Dictofint,str, ortorch.device) \u2013 Device placement\nmappings from this worker to the callee. This map must be\ninvertible.\nExample\n\n```python\n>>> # both workers\n>>> def add(x, y):\n>>>     print(x)  # tensor([1., 1.], device='cuda:1')\n>>>     return x + y, (x + y).to(2)\n>>>\n>>> # on worker 0\n>>> options = TensorPipeRpcBackendOptions(\n>>>     num_worker_threads=8,\n>>>     device_maps={\"worker1\": {0: 1}}\n>>> # maps worker0's cuda:0 to worker1's cuda:1\n>>> )\n>>> options.set_device_map(\"worker1\", {1: 2})\n>>> # maps worker0's cuda:1 to worker1's cuda:2\n>>>\n>>> rpc.init_rpc(\n>>>     \"worker0\",\n>>>     rank=0,\n>>>     world_size=2,\n>>>     backend=rpc.BackendType.TENSORPIPE,\n>>>     rpc_backend_options=options\n>>> )\n>>>\n>>> x = torch.ones(2)\n>>> rets = rpc.rpc_sync(\"worker1\", add, args=(x.to(0), 1))\n>>> # The first argument will be moved to cuda:1 on worker1. When\n>>> # sending the return value back, it will follow the invert of\n>>> # the device map, and hence will be moved back to cuda:0 and\n>>> # cuda:1 on worker0\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:0')\n>>> print(rets[1])  # tensor([2., 2.], device='cuda:1')\n\n```\n\nSet local devices used by the TensorPipe RPC agent. When processing\nCUDA RPC requests, the TensorPipe RPC agent will properly synchronize\nCUDA streams for all devices in thisList.\nList\ndevices(Listofint,str, ortorch.device) \u2013 local devices used by\nthe TensorPipe RPC agent.\nNote\nThe RPC framework does not automatically retry anyrpc_sync(),rpc_async()andremote()calls. The reason being that there is\nno way the RPC framework can determine whether an operation is idempotent or\nnot and whether it is safe to retry. As a result, it is the application\u2019s\nresponsibility to deal with failures and retry if necessary. RPC communication\nis based on TCP and as a result failures could happen due to network failures\nor intermittent network connectivity issues. In such scenarios, the application\nneeds to retry appropriately with reasonable backoffs to ensure the network\nisn\u2019t overwhelmed by aggressive retries.\nrpc_sync()\nrpc_async()\nremote()\n\n## RRef#\n\nWarning\nRRefs are not currently supported when using CUDA tensors\nAnRRef(Remote REFerence) is a reference to a value of some typeT(e.g.Tensor) on a remote worker. This handle keeps the referenced remote\nvalue alive on the owner, but there is no implication that the value will be\ntransferred to the local worker in the future. RRefs can be used in\nmulti-machine training by holding references tonn.Modulesthat exist on\nother workers, and calling the appropriate functions to retrieve or modify their\nparameters during training. SeeRemote Reference Protocolfor more\ndetails.\nRRef\nT\nTensor\nA class encapsulating a reference to a value of some type on a remote\nworker. This handle will keep the referenced remote value alive on the\nworker. AUserRRefwill be deleted when 1) no references to it in\nboth the application code and in the local RRef context, or 2) the\napplication has called a graceful shutdown. Invoking methods on a\ndeleted RRef leads to undefined behaviors. RRef implementation only\noffers best-effort error detection, and applications should not useUserRRefsafterrpc.shutdown().\nUserRRef\nUserRRefs\nrpc.shutdown()\nWarning\nRRefs can only be serialized and deserialized by the RPC module.\nSerializing and deserializing RRefs without RPC (e.g., Python\npickle, torchsave()/load(),\nJITsave()/load(), etc.) will\nlead to errors.\nsave()\nload()\nsave()\nload()\nvalue(object) \u2013 The value to be wrapped by this RRef.\ntype_hint(Type,optional) \u2013 Python type that should be passed toTorchScriptcompiler as type hint forvalue.\nTorchScript\nvalue\nFollowing examples skip RPC initialization and shutdown code\nfor simplicity. Refer to RPC docs for those details.\nCreate an RRef using rpc.remote\n\n```python\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> # get a copy of value from the RRef\n>>> x = rref.to_here()\n\n```\n\nCreate an RRef from a local object\n\n```python\n>>> import torch\n>>> from torch.distributed.rpc import RRef\n>>> x = torch.zeros(2, 2)\n>>> rref = RRef(x)\n\n```\n\nShare an RRef with other workers\n\n```python\n>>> # On both worker0 and worker1:\n>>> def f(rref):\n>>>   return rref.to_here() + 1\n\n```\n\n\n```python\n>>> # On worker0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> from torch.distributed.rpc import RRef\n>>> rref = RRef(torch.zeros(2, 2))\n>>> # the following RPC shares the rref with worker1, reference\n>>> # count is automatically updated.\n>>> rpc.rpc_sync(\"worker1\", f, args=(rref,))\n\n```\n\nRuns the backward pass using the RRef as the root of the\nbackward pass. Ifdist_autograd_ctx_idis provided,\nwe perform a distributed backward pass using the provided\nctx_id starting from the owner of the RRef. In this case,get_gradients()should be\nused to retrieve the gradients. Ifdist_autograd_ctx_idisNone, it is assumed that this is a local autograd graph\nand we only perform a local backward pass. In the local case,\nthe node calling this API has to be the owner of the RRef.\nThe value of the RRef is expected to be a scalar Tensor.\ndist_autograd_ctx_id\nget_gradients()\ndist_autograd_ctx_id\nNone\ndist_autograd_ctx_id(int,optional) \u2013 The distributed\nautograd context id for which we should retrieve the\ngradients (default: -1).\nretain_graph(bool,optional) \u2013 IfFalse, the graph used to\ncompute the grad will be freed. Note that in nearly all\ncases setting this option toTrueis not needed and\noften can be worked around in a much more efficient way.\nUsually, you need to set this toTrueto run backward\nmultiple times (default: False).\nFalse\nTrue\nTrue\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n\n```\n\nReturns whether thisRRefhas been confirmed by the owner.OwnerRRefalways returns true, whileUserRRefonly\nreturns true when the owner knowns about thisUserRRef.\nRRef\nOwnerRRef\nUserRRef\nUserRRef\nReturns whether or not the current node is the owner of thisRRef.\nRRef\nIf the current node is the owner, returns a reference to the\nlocal value. Otherwise, throws an exception.\nReturns worker information of the node that owns thisRRef.\nRRef\nReturns worker name of the node that owns thisRRef.\nRRef\nCreate a helper proxy to easily launch aremoteusing\nthe owner of the RRef as the destination to run functions on\nthe object referenced by this RRef. More specifically,rref.remote().func_name(*args,**kwargs)is the same as\nthe following:\nremote\nrref.remote().func_name(*args,**kwargs)\n\n```python\n>>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n\n```\n\ntimeout(float,optional) \u2013 Timeout forrref.remote(). If\nthe creation of thisRRefis not successfully completed within the timeout, then the\nnext time there is an attempt to use the RRef\n(such asto_here), a timeout will be raised. If not\nprovided, the default RPC timeout will be used. Please seerpc.remote()for specific timeout semantics forRRef.\nrref.remote()\nRRef\nto_here\nrpc.remote()\nRRef\n\n```python\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n\n```\n\nCreate a helper proxy to easily launch anrpc_asyncusing\nthe owner of the RRef as the destination to run functions on\nthe object referenced by this RRef. More specifically,rref.rpc_async().func_name(*args,**kwargs)is the same as\nthe following:\nrpc_async\nrref.rpc_async().func_name(*args,**kwargs)\n\n```python\n>>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))\n\n```\n\ntimeout(float,optional) \u2013 Timeout forrref.rpc_async().\nIf the call does not complete within this timeframe, an\nexception indicating so will be raised. If this argument\nis not provided, the default RPC timeout will be used.\nrref.rpc_async()\n\n```python\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n\n```\n\nCreate a helper proxy to easily launch anrpc_syncusing\nthe owner of the RRef as the destination to run functions on\nthe object referenced by this RRef. More specifically,rref.rpc_sync().func_name(*args,**kwargs)is the same as\nthe following:\nrpc_sync\nrref.rpc_sync().func_name(*args,**kwargs)\n\n```python\n>>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))\n\n```\n\ntimeout(float,optional) \u2013 Timeout forrref.rpc_sync().\nIf the call does not complete within this timeframe, an\nexception indicating so will be raised. If this argument\nis not provided, the default RPC timeout will be used.\nrref.rpc_sync()\n\n```python\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n\n```\n\nBlocking call that copies the value of the RRef from the owner\nto the local node and returns it. If the current node is the\nowner, returns a reference to the local value.\ntimeout(float,optional) \u2013 Timeout forto_here. If\nthe call does not complete within this timeframe, an\nexception indicating so will be raised. If this\nargument is not provided, the default RPC timeout\n(60s) will be used.\nto_here\nMore Information about RRef\n\n## RemoteModule#\n\nWarning\nRemoteModule is not currently supported when using CUDA tensors\nRemoteModuleis an easy way to create an nn.Module remotely on a different\nprocess. The actual module resides on a remote host, but the local host has a\nhandle to this module and invoke this module similar to a regular nn.Module.\nThe invocation however incurs RPC calls to the remote end and can be performed\nasynchronously if needed via additional APIs supported by RemoteModule.\nRemoteModule\nA RemoteModule instance can only be created after RPC initialization.\nIt creates a user-specified module on a specified remote node.\nIt behaves like a regularnn.Moduleexcept that theforwardmethod is\nexecuted on the remote node.\nIt takes care of autograd recording to ensure the backward pass propagates\ngradients back to the corresponding remote module.\nnn.Module\nforward\nIt generates two methodsforward_asyncandforwardbased on the\nsignature of theforwardmethod ofmodule_cls.forward_asyncruns asynchronously and returns a Future. The arguments offorward_asyncandforwardare the same as theforwardmethod of the module\nreturned by themodule_cls.\nforward_async\nforward\nforward\nmodule_cls\nforward_async\nforward_async\nforward\nforward\nmodule_cls\nFor example, ifmodule_clsreturns an instance ofnn.Linear,\nthat hasforwardmethod signature:defforward(input:Tensor)->Tensor:,\nthe generatedRemoteModulewill have 2 methods with the signatures:\nmodule_cls\nnn.Linear\nforward\ndefforward(input:Tensor)->Tensor:\nRemoteModule\ndefforward(input:Tensor)->Tensor:\ndefforward_async(input:Tensor)->Future[Tensor]:\nremote_device(str) \u2013 Device on the destination worker where we\u2019d like to place this module.\nThe format should be \u201c<workername>/<device>\u201d, where the device field can be parsed as torch.device type.\nE.g., \u201ctrainer0/cpu\u201d, \u201ctrainer0\u201d, \u201cps0/cuda:0\u201d.\nIn addition, the device field can be optional and the default value is \u201ccpu\u201d.\nmodule_cls(nn.Module) \u2013Class for the module to be created remotely. For example,>>>classMyModule(nn.Module):>>>defforward(input):>>>returninput+1>>>>>>module_cls=MyModule\nClass for the module to be created remotely. For example,\n\n```python\n>>> class MyModule(nn.Module):\n>>>     def forward(input):\n>>>         return input + 1\n>>>\n>>> module_cls = MyModule\n\n```\n\nargs(Sequence,optional) \u2013 args to be passed tomodule_cls.\nmodule_cls\nkwargs(Dict,optional) \u2013 kwargs to be passed tomodule_cls.\nmodule_cls\nA remote module instance which wraps theModulecreated by the\nuser-providedmodule_cls, it has a blockingforwardmethod and an\nasynchronousforward_asyncmethod that returns a future of theforwardcall\non the user-provided module on the remote side.\nModule\nmodule_cls\nforward\nforward_async\nforward\nRun the following code in two different processes:\n\n```python\n>>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> from torch import nn, Tensor\n>>> from torch.distributed.nn.api.remote_module import RemoteModule\n>>>\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> remote_linear_module = RemoteModule(\n>>>     \"worker1/cpu\", nn.Linear, args=(20, 30),\n>>> )\n>>> input = torch.randn(128, 20)\n>>> ret_fut = remote_linear_module.forward_async(input)\n>>> ret = ret_fut.wait()\n>>> rpc.shutdown()\n\n```\n\n\n```python\n>>> # On worker 1:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>>\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n\n```\n\nFurthermore, a more practical example that is combined withDistributedDataParallel(DDP)\ncan be found in thistutorial.\nReturn anRRef(RRef[nn.Module]) pointing to the remote module.\nRRef\nRRef[nn.Module]\nRRef[Module]\nReturn a list ofRRefpointing to the remote module\u2019s parameters.\nRRef\nThis can typically be used in conjunction\nwithDistributedOptimizer.\nDistributedOptimizer\nrecurse(bool) \u2013 if True, then returns parameters of the remote\nmodule and all submodules of the remote module. Otherwise,\nreturns only parameters that are direct members of the\nremote module.\nA list ofRRef(List[RRef[nn.Parameter]])\nto remote module\u2019s parameters.\nRRef\nList[RRef[nn.Parameter]]\nlist[torch.distributed.rpc.api.RRef[torch.nn.parameter.Parameter]]\n\n## Distributed Autograd Framework#\n\nWarning\nDistributed autograd is not currently supported when using CUDA tensors\nThis module provides an RPC-based distributed autograd framework that can be\nused for applications such as model parallel training. In short, applications\nmay send and receive gradient recording tensors over RPC. In the forward pass,\nwe record when gradient recording tensors are sent over RPC and during the\nbackward pass we use this information to perform a distributed backward pass\nusing RPC. For more details seeDistributed Autograd Design.\nKicks off the distributed backward pass using the provided roots. This\ncurrently implements theFAST mode algorithmwhich\nassumes all RPC messages sent in the same distributed autograd context\nacross workers would be part of the autograd graph during the backward pass.\nWe use the provided roots to discover the autograd graph and compute\nappropriate dependencies. This method blocks until the entire\nautograd computation is done.\nWe accumulate the gradients in the appropriatetorch.distributed.autograd.contexton each of the nodes. The autograd\ncontext to be used is looked up given thecontext_idthat is passed in whentorch.distributed.autograd.backward()is called. If there is no valid\nautograd context corresponding to the given ID, we throw an error. You can\nretrieve the accumulated gradients using theget_gradients()API.\ntorch.distributed.autograd.context\ncontext_id\ntorch.distributed.autograd.backward()\nget_gradients()\ncontext_id(int) \u2013 The autograd context id for which we should retrieve the gradients.\nroots(list) \u2013 Tensors which represent the roots of the autograd\ncomputation. All the tensors should be scalars.\nretain_graph(bool,optional) \u2013 If False, the graph used to compute the grad\nwill be freed. Note that in nearly all cases setting this\noption to True is not needed and often can be worked around\nin a much more efficient way. Usually, you need to set this\nto True to run backward multiple times.\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     pred = model.forward()\n>>>     loss = loss_func(pred, loss)\n>>>     dist_autograd.backward(context_id, loss)\n\n```\n\nContext object to wrap forward and backward passes when using\ndistributed autograd. Thecontext_idgenerated in thewithstatement  is required to uniquely identify a distributed backward pass\non all workers. Each worker stores metadata associated with thiscontext_id, which is required to correctly execute a distributed\nautograd pass.\ncontext_id\nwith\ncontext_id\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = rpc.rpc_sync(\"worker1\", torch.add, args=(t1, t2)).sum()\n>>>     dist_autograd.backward(context_id, [loss])\n\n```\n\nRetrieves a map from Tensor to the appropriate gradient for that Tensor\naccumulated in the provided context corresponding to the givencontext_idas part of the distributed autograd backward pass.\ncontext_id\ncontext_id(int) \u2013 The autograd context id for which we should retrieve the\ngradients.\nA map where the key is the Tensor and the value is the associated gradient\nfor that Tensor.\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = t1 + t2\n>>>     dist_autograd.backward(context_id, [loss.sum()])\n>>>     grads = dist_autograd.get_gradients(context_id)\n>>>     print(grads[t1])\n>>>     print(grads[t2])\n\n```\n\nMore Information about RPC Autograd\n\n## Distributed Optimizer#\n\nSee thetorch.distributed.optimpage for documentation on distributed optimizers.\n\n## Design Notes#\n\nThe distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.\nDistributed Autograd Design\nThe RRef design note covers the design of theRRef(Remote REFerence) protocol used to refer to values on remote workers by the framework.\nRemote Reference Protocol\n\n## Tutorials#\n\nThe RPC tutorials introduce users to the RPC framework, provide several example applications\nusingtorch.distributed.rpcAPIs, and demonstrate how\nto usethe profilerto profile RPC-based workloads.\nGetting started with Distributed RPC Framework\nImplementing a Parameter Server using Distributed RPC Framework\nCombining Distributed DataParallel with Distributed RPC Framework(coversRemoteModuleas well)\nImplementing batch RPC processing",
    "url": "https://pytorch.org/docs/stable/rpc.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1cf9af912783ad06aef0795aee5fdb75",
    "source": "pytorch_docs",
    "title": "torch.func \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.func#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\ntorch.func, previously known as \u201cfunctorch\u201d, isJAX-likecomposable function transforms for PyTorch.\nNote\nThis library is currently inbeta.\nWhat this means is that the features generally work (unless otherwise documented)\nand we (the PyTorch team) are committed to bringing this library forward. However, the APIs\nmay change under user feedback and we don\u2019t have full coverage over PyTorch operations.\nIf you have suggestions on the API or use-cases you\u2019d like to be covered, please\nopen a GitHub issue or reach out. We\u2019d love to hear about how you\u2019re using the library.\n\n## What are composable function transforms?#\n\nA \u201cfunction transform\u201d is a higher-order function that accepts a numerical function\nand returns a new function that computes a different quantity.\ntorch.funchas auto-differentiation transforms (grad(f)returns a function that\ncomputes the gradient off), a vectorization/batching transform (vmap(f)returns a function that computesfover batches of inputs), and others.\ntorch.func\ngrad(f)\nf\nvmap(f)\nf\nThese function transforms can compose with each other arbitrarily. For example,\ncomposingvmap(grad(f))computes a quantity called per-sample-gradients that\nstock PyTorch cannot efficiently compute today.\nvmap(grad(f))\n\n## Why composable function transforms?#\n\nThere are a number of use cases that are tricky to do in PyTorch today:\ncomputing per-sample-gradients (or other per-sample quantities)\nrunning ensembles of models on a single machine\nefficiently batching together tasks in the inner-loop of MAML\nefficiently computing Jacobians and Hessians\nefficiently computing batched Jacobians and Hessians\nComposingvmap(),grad(), andvjp()transforms allows us to express the above without designing a separate subsystem for each.\nThis idea of composable function transforms comes from theJAX framework.\nvmap()\ngrad()\nvjp()\n\n## Read More#\n",
    "url": "https://pytorch.org/docs/stable/func.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5f4609001cd092d8ad8e91929cd16e1a",
    "source": "pytorch_docs",
    "title": "Python Module Index \u2014 PyTorch 2.9 documentation",
    "text": "\n## Python Module Index\n\ntorch\ntorch.__config__\ntorch.__future__\ntorch._functorch._aot_autograd.fx_utils\ntorch._logging\ntorch.accelerator\ntorch.accelerator.memory\ntorch.amp\ntorch.amp.autocast_mode\ntorch.amp.grad_scaler\ntorch.ao\ntorch.ao.nn\ntorch.ao.nn.intrinsic\ntorch.ao.nn.intrinsic.modules\ntorch.ao.nn.intrinsic.modules.fused\ntorch.ao.nn.intrinsic.qat\ntorch.ao.nn.intrinsic.qat.modules\ntorch.ao.nn.intrinsic.qat.modules.conv_fused\ntorch.ao.nn.intrinsic.qat.modules.linear_fused\ntorch.ao.nn.intrinsic.qat.modules.linear_relu\ntorch.ao.nn.intrinsic.quantized\ntorch.ao.nn.intrinsic.quantized.dynamic\ntorch.ao.nn.intrinsic.quantized.dynamic.modules\ntorch.ao.nn.intrinsic.quantized.dynamic.modules.linear_relu\ntorch.ao.nn.intrinsic.quantized.modules\ntorch.ao.nn.intrinsic.quantized.modules.bn_relu\ntorch.ao.nn.intrinsic.quantized.modules.conv_add\ntorch.ao.nn.intrinsic.quantized.modules.conv_relu\ntorch.ao.nn.intrinsic.quantized.modules.linear_relu\ntorch.ao.nn.qat\ntorch.ao.nn.qat.dynamic\ntorch.ao.nn.qat.dynamic.modules\ntorch.ao.nn.qat.dynamic.modules.linear\ntorch.ao.nn.qat.modules\ntorch.ao.nn.qat.modules.conv\ntorch.ao.nn.qat.modules.embedding_ops\ntorch.ao.nn.qat.modules.linear\ntorch.ao.nn.quantizable\ntorch.ao.nn.quantizable.modules\ntorch.ao.nn.quantizable.modules.activation\ntorch.ao.nn.quantizable.modules.rnn\ntorch.ao.nn.quantized\ntorch.ao.nn.quantized.dynamic\ntorch.ao.nn.quantized.dynamic.modules\ntorch.ao.nn.quantized.dynamic.modules.conv\ntorch.ao.nn.quantized.dynamic.modules.linear\ntorch.ao.nn.quantized.dynamic.modules.rnn\ntorch.ao.nn.quantized.functional\ntorch.ao.nn.quantized.modules\ntorch.ao.nn.quantized.modules.activation\ntorch.ao.nn.quantized.modules.batchnorm\ntorch.ao.nn.quantized.modules.conv\ntorch.ao.nn.quantized.modules.dropout\ntorch.ao.nn.quantized.modules.embedding_ops\ntorch.ao.nn.quantized.modules.functional_modules\ntorch.ao.nn.quantized.modules.linear\ntorch.ao.nn.quantized.modules.normalization\ntorch.ao.nn.quantized.modules.rnn\ntorch.ao.nn.quantized.modules.utils\ntorch.ao.nn.quantized.reference\ntorch.ao.nn.quantized.reference.modules\ntorch.ao.nn.quantized.reference.modules.conv\ntorch.ao.nn.quantized.reference.modules.linear\ntorch.ao.nn.quantized.reference.modules.rnn\ntorch.ao.nn.quantized.reference.modules.sparse\ntorch.ao.nn.quantized.reference.modules.utils\ntorch.ao.nn.sparse\ntorch.ao.nn.sparse.quantized\ntorch.ao.nn.sparse.quantized.dynamic\ntorch.ao.nn.sparse.quantized.dynamic.linear\ntorch.ao.nn.sparse.quantized.linear\ntorch.ao.nn.sparse.quantized.utils\ntorch.ao.ns\ntorch.ao.ns.fx\ntorch.ao.ns.fx.graph_matcher\ntorch.ao.ns.fx.graph_passes\ntorch.ao.ns.fx.mappings\ntorch.ao.ns.fx.n_shadows_utils\ntorch.ao.ns.fx.ns_types\ntorch.ao.ns.fx.pattern_utils\ntorch.ao.ns.fx.qconfig_multi_mapping\ntorch.ao.ns.fx.utils\ntorch.ao.ns.fx.weight_utils\ntorch.ao.pruning\ntorch.ao.pruning.scheduler\ntorch.ao.pruning.scheduler.base_scheduler\ntorch.ao.pruning.scheduler.cubic_scheduler\ntorch.ao.pruning.scheduler.lambda_scheduler\ntorch.ao.pruning.sparsifier\ntorch.ao.pruning.sparsifier.base_sparsifier\ntorch.ao.pruning.sparsifier.nearly_diagonal_sparsifier\ntorch.ao.pruning.sparsifier.utils\ntorch.ao.pruning.sparsifier.weight_norm_sparsifier\ntorch.ao.quantization\ntorch.ao.quantization.backend_config\ntorch.ao.quantization.backend_config.backend_config\ntorch.ao.quantization.backend_config.executorch\ntorch.ao.quantization.backend_config.fbgemm\ntorch.ao.quantization.backend_config.native\ntorch.ao.quantization.backend_config.onednn\ntorch.ao.quantization.backend_config.qnnpack\ntorch.ao.quantization.backend_config.tensorrt\ntorch.ao.quantization.backend_config.utils\ntorch.ao.quantization.backend_config.x86\ntorch.ao.quantization.fake_quantize\ntorch.ao.quantization.fuse_modules\ntorch.ao.quantization.fuser_method_mappings\ntorch.ao.quantization.fx\ntorch.ao.quantization.fx.convert\ntorch.ao.quantization.fx.custom_config\ntorch.ao.quantization.fx.fuse\ntorch.ao.quantization.fx.fuse_handler\ntorch.ao.quantization.fx.graph_module\ntorch.ao.quantization.fx.lower_to_fbgemm\ntorch.ao.quantization.fx.lower_to_qnnpack\ntorch.ao.quantization.fx.lstm_utils\ntorch.ao.quantization.fx.match_utils\ntorch.ao.quantization.fx.pattern_utils\ntorch.ao.quantization.fx.prepare\ntorch.ao.quantization.fx.qconfig_mapping_utils\ntorch.ao.quantization.fx.quantize_handler\ntorch.ao.quantization.fx.tracer\ntorch.ao.quantization.fx.utils\ntorch.ao.quantization.observer\ntorch.ao.quantization.pt2e\ntorch.ao.quantization.pt2e.duplicate_dq_pass\ntorch.ao.quantization.pt2e.export_utils\ntorch.ao.quantization.pt2e.graph_utils\ntorch.ao.quantization.pt2e.lowering\ntorch.ao.quantization.pt2e.port_metadata_pass\ntorch.ao.quantization.pt2e.prepare\ntorch.ao.quantization.pt2e.qat_utils\ntorch.ao.quantization.pt2e.representation\ntorch.ao.quantization.pt2e.representation.rewrite\ntorch.ao.quantization.pt2e.utils\ntorch.ao.quantization.qconfig\ntorch.ao.quantization.qconfig_mapping\ntorch.ao.quantization.quant_type\ntorch.ao.quantization.quantization_mappings\ntorch.ao.quantization.quantize_fx\ntorch.ao.quantization.quantize_jit\ntorch.ao.quantization.quantize_pt2e\ntorch.ao.quantization.quantizer\ntorch.ao.quantization.quantizer.composable_quantizer\ntorch.ao.quantization.quantizer.embedding_quantizer\ntorch.ao.quantization.quantizer.quantizer\ntorch.ao.quantization.quantizer.utils\ntorch.ao.quantization.quantizer.x86_inductor_quantizer\ntorch.ao.quantization.quantizer.xnnpack_quantizer\ntorch.ao.quantization.quantizer.xnnpack_quantizer_utils\ntorch.ao.quantization.quantizer.xpu_inductor_quantizer\ntorch.ao.quantization.stubs\ntorch.ao.quantization.utils\ntorch.autograd\ntorch.autograd.anomaly_mode\ntorch.autograd.forward_ad\ntorch.autograd.function\ntorch.autograd.functional\ntorch.autograd.grad_mode\ntorch.autograd.gradcheck\ntorch.autograd.graph\ntorch.autograd.profiler\ntorch.autograd.profiler_legacy\ntorch.autograd.profiler_util\ntorch.autograd.variable\ntorch.backends\ntorch.backends.cpu\ntorch.backends.cuda\ntorch.backends.cudnn\ntorch.backends.cudnn.rnn\ntorch.backends.cusparselt\ntorch.backends.kleidiai\ntorch.backends.mha\ntorch.backends.miopen\ntorch.backends.mkl\ntorch.backends.mkldnn\ntorch.backends.mps\ntorch.backends.nnpack\ntorch.backends.openmp\ntorch.backends.opt_einsum\ntorch.backends.quantized\ntorch.backends.xeon\ntorch.backends.xeon.run_cpu\ntorch.backends.xnnpack\ntorch.compiler\ntorch.compiler.config\ntorch.contrib\ntorch.cpu\ntorch.cpu.amp\ntorch.cpu.amp.autocast_mode\ntorch.cpu.amp.grad_scaler\ntorch.cuda\ntorch.cuda._sanitizer\ntorch.cuda.amp\ntorch.cuda.amp.autocast_mode\ntorch.cuda.amp.common\ntorch.cuda.amp.grad_scaler\ntorch.cuda.comm\ntorch.cuda.gds\ntorch.cuda.graphs\ntorch.cuda.jiterator\ntorch.cuda.memory\ntorch.cuda.nccl\ntorch.cuda.nvtx\ntorch.cuda.profiler\ntorch.cuda.random\ntorch.cuda.sparse\ntorch.cuda.streams\ntorch.cuda.tunable\ntorch.distributed\ntorch.distributed._dist2\ntorch.distributed.algorithms\ntorch.distributed.algorithms.ddp_comm_hooks\ntorch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook\ntorch.distributed.algorithms.ddp_comm_hooks.debugging_hooks\ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks\ntorch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks\ntorch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks\ntorch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook\ntorch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook\ntorch.distributed.algorithms.ddp_comm_hooks.quantization_hooks\ntorch.distributed.algorithms.join\ntorch.distributed.algorithms.model_averaging\ntorch.distributed.algorithms.model_averaging.averagers\ntorch.distributed.algorithms.model_averaging.hierarchical_model_averager\ntorch.distributed.algorithms.model_averaging.utils\ntorch.distributed.argparse_util\ntorch.distributed.autograd\ntorch.distributed.c10d_logger\ntorch.distributed.checkpoint\ntorch.distributed.checkpoint.api\ntorch.distributed.checkpoint.default_planner\ntorch.distributed.checkpoint.filesystem\ntorch.distributed.checkpoint.format_utils\ntorch.distributed.checkpoint.hf_storage\ntorch.distributed.checkpoint.logger\ntorch.distributed.checkpoint.logging_handlers\ntorch.distributed.checkpoint.metadata\ntorch.distributed.checkpoint.optimizer\ntorch.distributed.checkpoint.planner\ntorch.distributed.checkpoint.planner_helpers\ntorch.distributed.checkpoint.quantized_hf_storage\ntorch.distributed.checkpoint.resharding\ntorch.distributed.checkpoint.staging\ntorch.distributed.checkpoint.state_dict\ntorch.distributed.checkpoint.state_dict_loader\ntorch.distributed.checkpoint.state_dict_saver\ntorch.distributed.checkpoint.stateful\ntorch.distributed.checkpoint.storage\ntorch.distributed.checkpoint.utils\ntorch.distributed.collective_utils\ntorch.distributed.constants\ntorch.distributed.device_mesh\ntorch.distributed.distributed_c10d\ntorch.distributed.elastic\ntorch.distributed.elastic.agent\ntorch.distributed.elastic.agent.server\ntorch.distributed.elastic.agent.server.api\ntorch.distributed.elastic.agent.server.health_check_server\ntorch.distributed.elastic.agent.server.local_elastic_agent\ntorch.distributed.elastic.control_plane\ntorch.distributed.elastic.events\ntorch.distributed.elastic.events.api\ntorch.distributed.elastic.events.handlers\ntorch.distributed.elastic.metrics\ntorch.distributed.elastic.metrics.api\ntorch.distributed.elastic.multiprocessing\ntorch.distributed.elastic.multiprocessing.api\ntorch.distributed.elastic.multiprocessing.errors\ntorch.distributed.elastic.multiprocessing.errors.error_handler\ntorch.distributed.elastic.multiprocessing.errors.handlers\ntorch.distributed.elastic.multiprocessing.redirects\ntorch.distributed.elastic.multiprocessing.subprocess_handler\ntorch.distributed.elastic.multiprocessing.subprocess_handler.handlers\ntorch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler\ntorch.distributed.elastic.multiprocessing.tail_log\ntorch.distributed.elastic.rendezvous\ntorch.distributed.elastic.rendezvous.api\ntorch.distributed.elastic.rendezvous.c10d_rendezvous_backend\ntorch.distributed.elastic.rendezvous.dynamic_rendezvous\ntorch.distributed.elastic.rendezvous.etcd_rendezvous\ntorch.distributed.elastic.rendezvous.etcd_rendezvous_backend\ntorch.distributed.elastic.rendezvous.etcd_server\ntorch.distributed.elastic.rendezvous.etcd_store\ntorch.distributed.elastic.rendezvous.registry\ntorch.distributed.elastic.rendezvous.static_tcp_rendezvous\ntorch.distributed.elastic.rendezvous.utils\ntorch.distributed.elastic.timer\ntorch.distributed.elastic.timer.api\ntorch.distributed.elastic.timer.debug_info_logging\ntorch.distributed.elastic.timer.file_based_local_timer\ntorch.distributed.elastic.timer.local_timer\ntorch.distributed.elastic.utils\ntorch.distributed.elastic.utils.api\ntorch.distributed.elastic.utils.data\ntorch.distributed.elastic.utils.data.cycling_iterator\ntorch.distributed.elastic.utils.data.elastic_distributed_sampler\ntorch.distributed.elastic.utils.distributed\ntorch.distributed.elastic.utils.log_level\ntorch.distributed.elastic.utils.logging\ntorch.distributed.elastic.utils.store\ntorch.distributed.fsdp\ntorch.distributed.fsdp.api\ntorch.distributed.fsdp.fully_sharded_data_parallel\ntorch.distributed.fsdp.sharded_grad_scaler\ntorch.distributed.fsdp.wrap\ntorch.distributed.launch\ntorch.distributed.launcher\ntorch.distributed.launcher.api\ntorch.distributed.logging_handlers\ntorch.distributed.nn\ntorch.distributed.nn.api\ntorch.distributed.nn.api.remote_module\ntorch.distributed.nn.functional\ntorch.distributed.nn.jit\ntorch.distributed.nn.jit.instantiator\ntorch.distributed.nn.jit.templates\ntorch.distributed.nn.jit.templates.remote_module_template\ntorch.distributed.optim\ntorch.distributed.optim.apply_optimizer_in_backward\ntorch.distributed.optim.functional_adadelta\ntorch.distributed.optim.functional_adagrad\ntorch.distributed.optim.functional_adam\ntorch.distributed.optim.functional_adamax\ntorch.distributed.optim.functional_adamw\ntorch.distributed.optim.functional_rmsprop\ntorch.distributed.optim.functional_rprop\ntorch.distributed.optim.functional_sgd\ntorch.distributed.optim.named_optimizer\ntorch.distributed.optim.optimizer\ntorch.distributed.optim.post_localSGD_optimizer\ntorch.distributed.optim.utils\ntorch.distributed.optim.zero_redundancy_optimizer\ntorch.distributed.pipelining\ntorch.distributed.pipelining.microbatch\ntorch.distributed.pipelining.schedules\ntorch.distributed.pipelining.stage\ntorch.distributed.remote_device\ntorch.distributed.rendezvous\ntorch.distributed.rpc\ntorch.distributed.rpc.api\ntorch.distributed.rpc.backend_registry\ntorch.distributed.rpc.constants\ntorch.distributed.rpc.functions\ntorch.distributed.rpc.internal\ntorch.distributed.rpc.options\ntorch.distributed.rpc.rref_proxy\ntorch.distributed.rpc.server_process_global_profiler\ntorch.distributed.run\ntorch.distributed.tensor\ntorch.distributed.tensor.debug\ntorch.distributed.tensor.device_mesh\ntorch.distributed.tensor.experimental\ntorch.distributed.tensor.parallel\ntorch.distributed.tensor.parallel.api\ntorch.distributed.tensor.parallel.ddp\ntorch.distributed.tensor.parallel.fsdp\ntorch.distributed.tensor.parallel.input_reshard\ntorch.distributed.tensor.parallel.loss\ntorch.distributed.tensor.parallel.style\ntorch.distributed.tensor.placement_types\ntorch.distributed.utils\ntorch.distributions\ntorch.distributions.bernoulli\ntorch.distributions.beta\ntorch.distributions.binomial\ntorch.distributions.categorical\ntorch.distributions.cauchy\ntorch.distributions.chi2\ntorch.distributions.constraint_registry\ntorch.distributions.constraints\ntorch.distributions.continuous_bernoulli\ntorch.distributions.dirichlet\ntorch.distributions.distribution\ntorch.distributions.exp_family\ntorch.distributions.exponential\ntorch.distributions.fishersnedecor\ntorch.distributions.gamma\ntorch.distributions.generalized_pareto\ntorch.distributions.geometric\ntorch.distributions.gumbel\ntorch.distributions.half_cauchy\ntorch.distributions.half_normal\ntorch.distributions.independent\ntorch.distributions.inverse_gamma\ntorch.distributions.kl\ntorch.distributions.kumaraswamy\ntorch.distributions.laplace\ntorch.distributions.lkj_cholesky\ntorch.distributions.log_normal\ntorch.distributions.logistic_normal\ntorch.distributions.lowrank_multivariate_normal\ntorch.distributions.mixture_same_family\ntorch.distributions.multinomial\ntorch.distributions.multivariate_normal\ntorch.distributions.negative_binomial\ntorch.distributions.normal\ntorch.distributions.one_hot_categorical\ntorch.distributions.pareto\ntorch.distributions.poisson\ntorch.distributions.relaxed_bernoulli\ntorch.distributions.relaxed_categorical\ntorch.distributions.studentT\ntorch.distributions.transformed_distribution\ntorch.distributions.transforms\ntorch.distributions.uniform\ntorch.distributions.utils\ntorch.distributions.von_mises\ntorch.distributions.weibull\ntorch.distributions.wishart\ntorch.export\ntorch.export.custom_obj\ntorch.export.custom_ops\ntorch.export.decomp_utils\ntorch.export.dynamic_shapes\ntorch.export.experimental\ntorch.export.exported_program\ntorch.export.graph_signature\ntorch.export.passes\ntorch.export.pt2_archive\ntorch.export.pt2_archive.constants\ntorch.export.unflatten\ntorch.fft\ntorch.func\ntorch.functional\ntorch.futures\ntorch.fx\ntorch.fx.annotate\ntorch.fx.config\ntorch.fx.experimental\ntorch.fx.experimental.accelerator_partitioner\ntorch.fx.experimental.const_fold\ntorch.fx.experimental.debug\ntorch.fx.experimental.graph_gradual_typechecker\ntorch.fx.experimental.merge_matmul\ntorch.fx.experimental.meta_tracer\ntorch.fx.experimental.migrate_gradual_types\ntorch.fx.experimental.migrate_gradual_types.constraint\ntorch.fx.experimental.migrate_gradual_types.constraint_generator\ntorch.fx.experimental.migrate_gradual_types.constraint_transformation\ntorch.fx.experimental.migrate_gradual_types.operation\ntorch.fx.experimental.migrate_gradual_types.transform_to_z3\ntorch.fx.experimental.migrate_gradual_types.util\ntorch.fx.experimental.migrate_gradual_types.z3_types\ntorch.fx.experimental.normalize\ntorch.fx.experimental.optimization\ntorch.fx.experimental.partitioner_utils\ntorch.fx.experimental.proxy_tensor\ntorch.fx.experimental.recording\ntorch.fx.experimental.refinement_types\ntorch.fx.experimental.rewriter\ntorch.fx.experimental.schema_type_annotation\ntorch.fx.experimental.sym_node\ntorch.fx.experimental.symbolic_shapes\ntorch.fx.experimental.unification\ntorch.fx.experimental.unification.core\ntorch.fx.experimental.unification.dispatch\ntorch.fx.experimental.unification.match\ntorch.fx.experimental.unification.more\ntorch.fx.experimental.unification.multipledispatch\ntorch.fx.experimental.unification.multipledispatch.conflict\ntorch.fx.experimental.unification.multipledispatch.core\ntorch.fx.experimental.unification.multipledispatch.dispatcher\ntorch.fx.experimental.unification.multipledispatch.utils\ntorch.fx.experimental.unification.multipledispatch.variadic\ntorch.fx.experimental.unification.unification_tools\ntorch.fx.experimental.unification.utils\ntorch.fx.experimental.unification.variable\ntorch.fx.experimental.unify_refinements\ntorch.fx.experimental.validator\ntorch.fx.graph\ntorch.fx.graph_module\ntorch.fx.immutable_collections\ntorch.fx.interpreter\ntorch.fx.node\ntorch.fx.operator_schemas\ntorch.fx.passes\ntorch.fx.passes.annotate_getitem_nodes\ntorch.fx.passes.backends\ntorch.fx.passes.backends.cudagraphs\ntorch.fx.passes.dialect\ntorch.fx.passes.dialect.common\ntorch.fx.passes.dialect.common.cse_pass\ntorch.fx.passes.fake_tensor_prop\ntorch.fx.passes.graph_drawer\ntorch.fx.passes.graph_manipulation\ntorch.fx.passes.graph_transform_observer\ntorch.fx.passes.infra\ntorch.fx.passes.infra.partitioner\ntorch.fx.passes.infra.pass_base\ntorch.fx.passes.infra.pass_manager\ntorch.fx.passes.net_min_base\ntorch.fx.passes.operator_support\ntorch.fx.passes.param_fetch\ntorch.fx.passes.pass_manager\ntorch.fx.passes.reinplace\ntorch.fx.passes.runtime_assert\ntorch.fx.passes.shape_prop\ntorch.fx.passes.split_module\ntorch.fx.passes.split_utils\ntorch.fx.passes.splitter_base\ntorch.fx.passes.tests\ntorch.fx.passes.tests.test_pass_manager\ntorch.fx.passes.tools_common\ntorch.fx.passes.utils\ntorch.fx.passes.utils.common\ntorch.fx.passes.utils.fuser_utils\ntorch.fx.passes.utils.matcher_utils\ntorch.fx.passes.utils.matcher_with_name_node_map_utils\ntorch.fx.passes.utils.source_matcher_utils\ntorch.fx.proxy\ntorch.fx.subgraph_rewriter\ntorch.fx.tensor_type\ntorch.fx.traceback\ntorch.hub\ntorch.jit\ntorch.jit.annotations\ntorch.jit.frontend\ntorch.jit.generate_bytecode\ntorch.jit.mobile\ntorch.jit.quantized\ntorch.jit.supported_ops\ntorch.jit.unsupported_tensor_ops\ntorch.library\ntorch.linalg\ntorch.masked\ntorch.masked.maskedtensor\ntorch.masked.maskedtensor.binary\ntorch.masked.maskedtensor.core\ntorch.masked.maskedtensor.creation\ntorch.masked.maskedtensor.passthrough\ntorch.masked.maskedtensor.reductions\ntorch.masked.maskedtensor.unary\ntorch.monitor\ntorch.mps\ntorch.mps.event\ntorch.mps.profiler\ntorch.mtia\ntorch.mtia.memory\ntorch.multiprocessing\ntorch.multiprocessing.pool\ntorch.multiprocessing.queue\ntorch.multiprocessing.reductions\ntorch.multiprocessing.spawn\ntorch.nested\ntorch.nn\ntorch.nn.attention\ntorch.nn.attention.bias\ntorch.nn.attention.experimental\ntorch.nn.attention.flex_attention\ntorch.nn.backends\ntorch.nn.backends.thnn\ntorch.nn.common_types\ntorch.nn.cpp\ntorch.nn.functional\ntorch.nn.grad\ntorch.nn.init\ntorch.nn.intrinsic\ntorch.nn.intrinsic.modules\ntorch.nn.intrinsic.modules.fused\ntorch.nn.intrinsic.qat\ntorch.nn.intrinsic.qat.modules\ntorch.nn.intrinsic.qat.modules.conv_fused\ntorch.nn.intrinsic.qat.modules.linear_fused\ntorch.nn.intrinsic.qat.modules.linear_relu\ntorch.nn.intrinsic.quantized\ntorch.nn.intrinsic.quantized.dynamic\ntorch.nn.intrinsic.quantized.dynamic.modules\ntorch.nn.intrinsic.quantized.dynamic.modules.linear_relu\ntorch.nn.intrinsic.quantized.modules\ntorch.nn.intrinsic.quantized.modules.bn_relu\ntorch.nn.intrinsic.quantized.modules.conv_relu\ntorch.nn.intrinsic.quantized.modules.linear_relu\ntorch.nn.modules\ntorch.nn.modules.activation\ntorch.nn.modules.adaptive\ntorch.nn.modules.batchnorm\ntorch.nn.modules.channelshuffle\ntorch.nn.modules.container\ntorch.nn.modules.conv\ntorch.nn.modules.distance\ntorch.nn.modules.dropout\ntorch.nn.modules.flatten\ntorch.nn.modules.fold\ntorch.nn.modules.instancenorm\ntorch.nn.modules.lazy\ntorch.nn.modules.linear\ntorch.nn.modules.loss\ntorch.nn.modules.module\ntorch.nn.modules.normalization\ntorch.nn.modules.padding\ntorch.nn.modules.pixelshuffle\ntorch.nn.modules.pooling\ntorch.nn.modules.rnn\ntorch.nn.modules.sparse\ntorch.nn.modules.transformer\ntorch.nn.modules.upsampling\ntorch.nn.modules.utils\ntorch.nn.parallel\ntorch.nn.parallel.comm\ntorch.nn.parallel.distributed\ntorch.nn.parallel.parallel_apply\ntorch.nn.parallel.replicate\ntorch.nn.parallel.scatter_gather\ntorch.nn.parameter\ntorch.nn.qat\ntorch.nn.qat.dynamic\ntorch.nn.qat.dynamic.modules\ntorch.nn.qat.dynamic.modules.linear\ntorch.nn.qat.modules\ntorch.nn.qat.modules.conv\ntorch.nn.qat.modules.embedding_ops\ntorch.nn.qat.modules.linear\ntorch.nn.quantizable\ntorch.nn.quantizable.modules\ntorch.nn.quantizable.modules.activation\ntorch.nn.quantizable.modules.rnn\ntorch.nn.quantized\ntorch.nn.quantized.dynamic\ntorch.nn.quantized.dynamic.modules\ntorch.nn.quantized.dynamic.modules.conv\ntorch.nn.quantized.dynamic.modules.linear\ntorch.nn.quantized.dynamic.modules.rnn\ntorch.nn.quantized.functional\ntorch.nn.quantized.modules\ntorch.nn.quantized.modules.activation\ntorch.nn.quantized.modules.batchnorm\ntorch.nn.quantized.modules.conv\ntorch.nn.quantized.modules.dropout\ntorch.nn.quantized.modules.embedding_ops\ntorch.nn.quantized.modules.functional_modules\ntorch.nn.quantized.modules.linear\ntorch.nn.quantized.modules.normalization\ntorch.nn.quantized.modules.rnn\ntorch.nn.quantized.modules.utils\ntorch.nn.utils\ntorch.nn.utils.clip_grad\ntorch.nn.utils.convert_parameters\ntorch.nn.utils.fusion\ntorch.nn.utils.init\ntorch.nn.utils.memory_format\ntorch.nn.utils.parametrizations\ntorch.nn.utils.parametrize\ntorch.nn.utils.prune\ntorch.nn.utils.rnn\ntorch.nn.utils.stateless\ntorch.numa\ntorch.numa.binding\ntorch.onnx\ntorch.onnx.errors\ntorch.onnx.operators\ntorch.onnx.ops\ntorch.onnx.symbolic_helper\ntorch.onnx.symbolic_opset10\ntorch.onnx.symbolic_opset11\ntorch.onnx.symbolic_opset12\ntorch.onnx.symbolic_opset13\ntorch.onnx.symbolic_opset14\ntorch.onnx.symbolic_opset15\ntorch.onnx.symbolic_opset16\ntorch.onnx.symbolic_opset17\ntorch.onnx.symbolic_opset18\ntorch.onnx.symbolic_opset19\ntorch.onnx.symbolic_opset20\ntorch.onnx.symbolic_opset7\ntorch.onnx.symbolic_opset8\ntorch.onnx.symbolic_opset9\ntorch.onnx.utils\ntorch.onnx.verification\ntorch.optim\ntorch.optim.adadelta\ntorch.optim.adagrad\ntorch.optim.adam\ntorch.optim.adamax\ntorch.optim.adamw\ntorch.optim.asgd\ntorch.optim.lbfgs\ntorch.optim.lr_scheduler\ntorch.optim.nadam\ntorch.optim.optimizer\ntorch.optim.radam\ntorch.optim.rmsprop\ntorch.optim.rprop\ntorch.optim.sgd\ntorch.optim.sparse_adam\ntorch.optim.swa_utils\ntorch.overrides\ntorch.package\ntorch.package.analyze\ntorch.package.analyze.find_first_use_of_broken_modules\ntorch.package.analyze.is_from_package\ntorch.package.analyze.trace_dependencies\ntorch.package.file_structure_representation\ntorch.package.find_file_dependencies\ntorch.package.glob_group\ntorch.package.importer\ntorch.package.package_exporter\ntorch.package.package_importer\ntorch.profiler\ntorch.profiler.itt\ntorch.profiler.profiler\ntorch.profiler.python_tracer\ntorch.quantization\ntorch.quantization.fake_quantize\ntorch.quantization.fuse_modules\ntorch.quantization.fuser_method_mappings\ntorch.quantization.fx\ntorch.quantization.fx.convert\ntorch.quantization.fx.fuse\ntorch.quantization.fx.fusion_patterns\ntorch.quantization.fx.graph_module\ntorch.quantization.fx.match_utils\ntorch.quantization.fx.pattern_utils\ntorch.quantization.fx.prepare\ntorch.quantization.fx.quantization_patterns\ntorch.quantization.fx.quantization_types\ntorch.quantization.fx.utils\ntorch.quantization.observer\ntorch.quantization.qconfig\ntorch.quantization.quant_type\ntorch.quantization.quantization_mappings\ntorch.quantization.quantize\ntorch.quantization.quantize_fx\ntorch.quantization.quantize_jit\ntorch.quantization.stubs\ntorch.quantization.utils\ntorch.quasirandom\ntorch.random\ntorch.return_types\ntorch.serialization\ntorch.signal\ntorch.signal.windows\ntorch.signal.windows.windows\ntorch.sparse\ntorch.sparse.semi_structured\ntorch.special\ntorch.storage\ntorch.testing\ntorch.torch_version\ntorch.types\ntorch.utils\ntorch.utils.backcompat\ntorch.utils.backend_registration\ntorch.utils.benchmark\ntorch.utils.benchmark.examples\ntorch.utils.benchmark.examples.compare\ntorch.utils.benchmark.examples.fuzzer\ntorch.utils.benchmark.examples.op_benchmark\ntorch.utils.benchmark.examples.simple_timeit\ntorch.utils.benchmark.examples.spectral_ops_fuzz_test\ntorch.utils.benchmark.op_fuzzers\ntorch.utils.benchmark.op_fuzzers.binary\ntorch.utils.benchmark.op_fuzzers.sparse_binary\ntorch.utils.benchmark.op_fuzzers.sparse_unary\ntorch.utils.benchmark.op_fuzzers.spectral\ntorch.utils.benchmark.op_fuzzers.unary\ntorch.utils.benchmark.utils\ntorch.utils.benchmark.utils.common\ntorch.utils.benchmark.utils.compare\ntorch.utils.benchmark.utils.compile\ntorch.utils.benchmark.utils.cpp_jit\ntorch.utils.benchmark.utils.fuzzer\ntorch.utils.benchmark.utils.sparse_fuzzer\ntorch.utils.benchmark.utils.timer\ntorch.utils.benchmark.utils.valgrind_wrapper\ntorch.utils.benchmark.utils.valgrind_wrapper.timer_interface\ntorch.utils.bottleneck\ntorch.utils.bundled_inputs\ntorch.utils.checkpoint\ntorch.utils.collect_env\ntorch.utils.cpp_backtrace\ntorch.utils.cpp_extension\ntorch.utils.data\ntorch.utils.data.backward_compatibility\ntorch.utils.data.dataloader\ntorch.utils.data.datapipes\ntorch.utils.data.datapipes.dataframe\ntorch.utils.data.datapipes.dataframe.dataframe_wrapper\ntorch.utils.data.datapipes.dataframe.dataframes\ntorch.utils.data.datapipes.dataframe.datapipes\ntorch.utils.data.datapipes.dataframe.structures\ntorch.utils.data.datapipes.datapipe\ntorch.utils.data.datapipes.gen_pyi\ntorch.utils.data.datapipes.iter\ntorch.utils.data.datapipes.iter.callable\ntorch.utils.data.datapipes.iter.combinatorics\ntorch.utils.data.datapipes.iter.combining\ntorch.utils.data.datapipes.iter.filelister\ntorch.utils.data.datapipes.iter.fileopener\ntorch.utils.data.datapipes.iter.grouping\ntorch.utils.data.datapipes.iter.routeddecoder\ntorch.utils.data.datapipes.iter.selecting\ntorch.utils.data.datapipes.iter.sharding\ntorch.utils.data.datapipes.iter.streamreader\ntorch.utils.data.datapipes.iter.utils\ntorch.utils.data.datapipes.map\ntorch.utils.data.datapipes.map.callable\ntorch.utils.data.datapipes.map.combinatorics\ntorch.utils.data.datapipes.map.combining\ntorch.utils.data.datapipes.map.grouping\ntorch.utils.data.datapipes.map.utils\ntorch.utils.data.datapipes.utils\ntorch.utils.data.datapipes.utils.common\ntorch.utils.data.datapipes.utils.decoder\ntorch.utils.data.datapipes.utils.snapshot\ntorch.utils.data.dataset\ntorch.utils.data.distributed\ntorch.utils.data.graph\ntorch.utils.data.graph_settings\ntorch.utils.data.sampler\ntorch.utils.deterministic\ntorch.utils.dlpack\ntorch.utils.file_baton\ntorch.utils.flop_counter\ntorch.utils.hipify\ntorch.utils.hipify.constants\ntorch.utils.hipify.cuda_to_hip_mappings\ntorch.utils.hipify.hipify_python\ntorch.utils.hipify.version\ntorch.utils.hooks\ntorch.utils.jit\ntorch.utils.jit.log_extract\ntorch.utils.mkldnn\ntorch.utils.mobile_optimizer\ntorch.utils.model_dump\ntorch.utils.model_zoo\ntorch.utils.module_tracker\ntorch.utils.serialization\ntorch.utils.serialization.config\ntorch.utils.show_pickle\ntorch.utils.tensorboard\ntorch.utils.tensorboard.summary\ntorch.utils.tensorboard.writer\ntorch.utils.throughput_benchmark\ntorch.utils.viz\ntorch.utils.weak\ntorch.version\ntorch.xpu\ntorch.xpu.memory\ntorch.xpu.random\ntorch.xpu.streams",
    "url": "https://pytorch.org/docs/stable/py-modindex.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "44c530c881b96ec25e2a8c843d5a48e7",
    "source": "pytorch_docs",
    "title": "Disabling and Suppressing Errors \u2014 PyTorch 2.9 documentation",
    "text": "\n## Disabling and Suppressing Errors#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nFor some model architectures, there are portions of the model which are particularly difficult to compile -\neither there are many graph breaks, or there are crashes.\nYou may want to explicitly disable these portions of the model which are problematic so that you can applytorch.compileto the parts that work. You can do this by using the@torch.compiler.disabledecorator.\nWhentorch.compileattempts to call a disabled function, it breaks the graph and skips tracing the disabled function,\nresuming tracing after the call. By default, all recursive calls made from a disabled function are also disabled.\nUse therecursive=Falseoption to allow compilation for recursive calls.\ntorch.compile\n@torch.compiler.disable\ntorch.compile\nrecursive=False\n\n```python\ndef inner1(x):\n    torch._dynamo.graph_break()  # not traced\n    return x + 1  # not traced\n\n@torch.compiler.disable\ndef outer1(x):\n    x = x + 2  # not traced\n    torch._dynamo.graph_break()  # not traced\n    return inner1(x)\n\n@torch.compile\ndef f(x):\n    x = outer1(x)\n    return x + 4  # traced\n\nprint(f(torch.ones(3)))\n\n```\n\n\n```python\ntensor([8., 8., 8.])\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_183/1421264493.py:13\nGraph Break Reason: Skip calling `torch.compiler.disable()`d function\n  Explanation: Skip calling function `<function outer1 at 0x7f597c0792d0>` since it was wrapped with `torch.compiler.disable` (reason: None)\n  Hint: Remove the `torch.compiler.disable` call\n\n  Developer debug context: <function outer1 at 0x7f597c0792d0>\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0098.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_183/1421264493.py\", line 16, in <module>\n    print(f(torch.ones(3)))\n  File \"/tmp/ipykernel_183/1421264493.py\", line 13, in f\n    x = outer1(x)\n\nTRACED GRAPH\n ===== __compiled_fn_4_d64fea0f_2ed0_41fa_b031_95a5e2bf9415 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_stack0_: \"f32[3][1]cpu\"):\n        l_stack0_ = L_stack0_\n        \n         # File: /tmp/ipykernel_183/1421264493.py:14 in torch_dynamo_resume_in_f_at_13, code: return x + 4  # traced\n        add: \"f32[3][1]cpu\" = l_stack0_ + 4;  l_stack0_ = None\n        return (add,)\n        \n\n```\n\n\n```python\ndef inner2(x):\n    torch._dynamo.graph_break()  # traced\n    return x + 1  # traced\n\n@torch.compiler.disable(recursive=False)\ndef outer2(x):\n    x = x + 2  # not traced\n    torch._dynamo.graph_break()  # not traced\n    return inner2(x)\n\n@torch.compile\ndef g(x):\n    x = outer2(x)\n    return x + 4  # traced\n\nprint(g(torch.ones(3)))\n\n```\n\n\n```python\ntensor([8., 8., 8.])\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_183/881423632.py:13\nGraph Break Reason: Skip inlining `torch.compiler.disable()`d function\n  Explanation: Skip inlining function <function outer2 at 0x7f5918bc3e20> since it was wrapped with `torch.compiler.disable` (reason: None)\n  Hint: Remove the `torch.compiler.disable` call\n\n  Developer debug context: <function outer2 at 0x7f5918bc3e20>\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0099.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_183/881423632.py\", line 16, in <module>\n    print(g(torch.ones(3)))\n  File \"/tmp/ipykernel_183/881423632.py\", line 13, in g\n    x = outer2(x)\n\nGraph break in user code at /tmp/ipykernel_183/881423632.py:2\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_183/881423632.py\", line 16, in <module>\n    print(g(torch.ones(3)))\n  File \"/tmp/ipykernel_183/881423632.py\", line 13, in g\n    x = outer2(x)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 196, in nonrecursive_disable_wrapper\n    return fn(*args, **kwargs)\n  File \"/tmp/ipykernel_183/881423632.py\", line 9, in outer2\n    return inner2(x)\n  File \"/tmp/ipykernel_183/881423632.py\", line 2, in inner2\n    torch._dynamo.graph_break()  # traced\n\nTRACED GRAPH\n ===== __compiled_fn_12_26020017_c915_42ce_9ca0_f38bb8979845 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_183/881423632.py:3 in torch_dynamo_resume_in_inner2_at_2, code: return x + 1  # traced\n        add: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        return (add,)\n        \n\nTRACED GRAPH\n ===== __compiled_fn_14_51c0ecfe_ceb5_435c_9b11_d2460775d0e0 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_stack0_: \"f32[3][1]cpu\"):\n        l_stack0_ = L_stack0_\n        \n         # File: /tmp/ipykernel_183/881423632.py:14 in torch_dynamo_resume_in_g_at_13, code: return x + 4  # traced\n        add: \"f32[3][1]cpu\" = l_stack0_ + 4;  l_stack0_ = None\n        return (add,)\n        \n\n```\n\nFor example, one can usetorch.compiler.disableto disabletorch.compileon sparse architecture in\nrecommendation models, as the sparse arch is difficult to compile.\nPreprocessing and logging functions are other examples of functions that typically cause\na lot of graph breaks and do not get value from being compiled.\ntorch.compiler.disable\ntorch.compile\nIf you are experiencing compiler crashes and you want to continue regardless,\nyou can settorch._dynamo.config.suppress_errors=True.\nWhen the compiler crashes, we will just skip tracing the function and try again later.This is not best practice- it is better to eventually manually adddisableannotations as necessary.\ntorch._dynamo.config.suppress_errors=True\ndisable",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.compiler_disable.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9e7902a2ada27e4bdf4b51879c761594",
    "source": "pytorch_docs",
    "title": "Tensor Views \u2014 PyTorch 2.9 documentation",
    "text": "\n## Tensor Views#\n\nCreated On: Feb 28, 2020 | Last Updated On: Feb 26, 2025\nPyTorch allows a tensor to be aViewof an existing tensor. View tensor shares the same underlying data\nwith its base tensor. SupportingViewavoids explicit data copy, thus allows us to do fast and memory efficient\nreshaping, slicing and element-wise operations.\nView\nView\nFor example, to get a view of an existing tensort, you can callt.view(...).\nt\nt.view(...)\n\n```python\n>>> t = torch.rand(4, 4)\n>>> b = t.view(2, 8)\n>>> t.storage().data_ptr() == b.storage().data_ptr()  # `t` and `b` share the same underlying data.\nTrue\n# Modifying view tensor changes base tensor as well.\n>>> b[0][0] = 3.14\n>>> t[0][0]\ntensor(3.14)\n\n```\n\nSince views share underlying data with its base tensor, if you edit the data\nin the view, it will be reflected in the base tensor as well.\nTypically a PyTorch op returns a new tensor as output, e.g.add().\nBut in case of view ops, outputs are views of input tensors to avoid unnecessary data copy.\nNo data movement occurs when creating a view, view tensor just changes the way\nit interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor.\nUsers should pay additional attention as contiguity might have implicit performance impact.transpose()is a common example.\nadd()\ntranspose()\n\n```python\n>>> base = torch.tensor([[0, 1],[2, 3]])\n>>> base.is_contiguous()\nTrue\n>>> t = base.transpose(0, 1)  # `t` is a view of `base`. No data movement happened here.\n# View tensors might be non-contiguous.\n>>> t.is_contiguous()\nFalse\n# To get a contiguous tensor, call `.contiguous()` to enforce\n# copying data when `t` is not contiguous.\n>>> c = t.contiguous()\n\n```\n\nFor reference, here\u2019s a full list of view ops in PyTorch:\nBasic slicing and indexing op, e.g.tensor[0,2:,1:7:2]returns a view of basetensor, see note below.\ntensor[0,2:,1:7:2]\ntensor\nadjoint()\nadjoint()\nas_strided()\nas_strided()\ndetach()\ndetach()\ndiagonal()\ndiagonal()\nexpand()\nexpand()\nexpand_as()\nexpand_as()\nmovedim()\nmovedim()\nnarrow()\nnarrow()\npermute()\npermute()\nselect()\nselect()\nsqueeze()\nsqueeze()\ntranspose()\ntranspose()\nt()\nt()\nT\nT\nH\nH\nmT\nmT\nmH\nmH\nreal\n\nreal\nimag\n\nimag\nview_as_real()\nview_as_real()\nunflatten()\nunflatten()\nunfold()\nunfold()\nunsqueeze()\nunsqueeze()\nview()\nview()\nview_as()\nview_as()\nunbind()\nunbind()\nsplit()\nsplit()\nhsplit()\nhsplit()\nvsplit()\nvsplit()\ntensor_split()\ntensor_split()\nsplit_with_sizes()\nsplit_with_sizes()\nswapaxes()\nswapaxes()\nswapdims()\nswapdims()\nchunk()\nchunk()\nindices()(sparse tensor only)\nindices()\nvalues()(sparse tensor only)\nvalues()\nNote\nWhen accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors\nthat basic indexing returns views, while advanced indexing returns a copy.\nAssignment via either basic or advanced indexing is in-place. See more examples inNumpy indexing documentation.\nIt\u2019s also worth mentioning a few ops with special behaviors:\nreshape(),reshape_as()andflatten()can return either a view or new tensor, user code shouldn\u2019t rely on whether it\u2019s view or not.\nreshape()\nreshape_as()\nflatten()\ncontiguous()returnsitselfif input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data.\ncontiguous()\nFor a more detailed walk-through of PyTorch internal implementation,\nplease refer toezyang\u2019s blogpost about PyTorch Internals.",
    "url": "https://pytorch.org/docs/stable/tensor_view.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "758dc621b17bdbc55eb93245662261c1",
    "source": "pytorch_docs",
    "title": "Tensor Attributes \u2014 PyTorch 2.9 documentation",
    "text": "\n## Tensor Attributes#\n\nCreated On: Apr 21, 2018 | Last Updated On: Jun 27, 2025\nEachtorch.Tensorhas atorch.dtype,torch.device, andtorch.layout.\ntorch.Tensor\ntorch.dtype\ntorch.device\ntorch.layout\n\n## torch.dtype#\n\nAtorch.dtypeis an object that represents the data type of atorch.Tensor. PyTorch has several different data types:\ntorch.dtype\ntorch.Tensor\nFloating point dtypes\ndtype\ndescription\ntorch.float32ortorch.float\ntorch.float32\ntorch.float\n32-bit floating point, as defined inhttps://en.wikipedia.org/wiki/IEEE_754\ntorch.float64ortorch.double\ntorch.float64\ntorch.double\n64-bit floating point, as defined inhttps://en.wikipedia.org/wiki/IEEE_754\ntorch.float16ortorch.half\ntorch.float16\ntorch.half\n16-bit floating point, as defined inhttps://en.wikipedia.org/wiki/IEEE_754, S-E-M 1-5-10\ntorch.bfloat16\ntorch.bfloat16\n16-bit floating point, sometimes referred to as Brain floating point, S-E-M 1-8-7\ntorch.complex32ortorch.chalf\ntorch.complex32\ntorch.chalf\n32-bit complex with twofloat16components\ntorch.complex64ortorch.cfloat\ntorch.complex64\ntorch.cfloat\n64-bit complex with twofloat32components\ntorch.complex128ortorch.cdouble\ntorch.complex128\ntorch.cdouble\n128-bit complex with twofloat64components\ntorch.float8_e4m3fn[shell],1\ntorch.float8_e4m3fn\n8-bit floating point, S-E-M 1-4-3, fromhttps://arxiv.org/abs/2209.05433\ntorch.float8_e5m2[shell]\ntorch.float8_e5m2\n8-bit floating point, S-E-M 1-5-2, fromhttps://arxiv.org/abs/2209.05433\ntorch.float8_e4m3fnuz[shell],1\ntorch.float8_e4m3fnuz\n8-bit floating point, S-E-M 1-4-3, fromhttps://arxiv.org/pdf/2206.02915\ntorch.float8_e5m2fnuz[shell],1\ntorch.float8_e5m2fnuz\n8-bit floating point, S-E-M 1-5-2, fromhttps://arxiv.org/pdf/2206.02915\ntorch.float8_e8m0fnu[shell],1\ntorch.float8_e8m0fnu\n8-bit floating point, S-E-M 0-8-0, fromhttps://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf\ntorch.float4_e2m1fn_x2[shell],1\ntorch.float4_e2m1fn_x2\npacked 4-bit floating point, S-E-M 1-2-1, fromhttps://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf\nInteger dtypes\ndtype\ndescription\ntorch.uint8\ntorch.uint8\n8-bit integer (unsigned)\ntorch.int8\ntorch.int8\n8-bit integer (signed)\ntorch.uint16[shell],2\ntorch.uint16\n16-bit integer (unsigned)\ntorch.int16ortorch.short\ntorch.int16\ntorch.short\n16-bit integer (signed)\ntorch.uint32[shell],2\ntorch.uint32\n32-bit integer (unsigned)\ntorch.int32ortorch.int\ntorch.int32\ntorch.int\n32-bit integer (signed)\ntorch.uint64[shell],2\ntorch.uint64\n64-bit integer (unsigned)\ntorch.int64ortorch.long\ntorch.int64\ntorch.long\n64-bit integer (signed)\ntorch.bool\ntorch.bool\nBoolean\na shell dtype a specialized dtype with limited op and backend support.\nSpecifically, ops that support tensor creation (torch.empty,torch.fill,torch.zeros)\nand operations which do not peek inside the data elements (torch.cat,torch.view,torch.reshape)\nare supported.  Ops that peek inside the data elements such as casting,\nmatrix multiplication, nan/inf checks are supported only on a case by\ncase basis, depending on maturity and presence of hardware accelerated kernels\nand established use cases.\ntorch.empty\ntorch.fill\ntorch.zeros\ntorch.cat\ntorch.view\ntorch.reshape\nThe \u201cfn\u201d, \u201cfnu\u201d and \u201cfnuz\u201d dtype suffixes mean:\n\u201cf\u201d - finite value encodings only, no infinity;\n\u201cn\u201d - nan value encodings differ from the IEEE spec;\n\u201cuz\u201d - \u201cunsigned zero\u201d only, i.e. no negative zero encoding\nUnsigned types asides fromuint8are currently planned to only have\nlimited support in eager mode (they primarily exist to assist usage with\ntorch.compile); if you need eager support and the extra range is not needed,\nwe recommend using their signed variants instead.  Seepytorch/pytorch#58734for more details.\nuint8\nNote: legacy constructors such astorch.*.FloatTensor,torch.*.DoubleTensor,torch.*.HalfTensor,torch.*.BFloat16Tensor,torch.*.ByteTensor,torch.*.CharTensor,torch.*.ShortTensor,torch.*.IntTensor,torch.*.LongTensor,torch.*.BoolTensoronly remain for backwards compatibility and should no longer be used.\ntorch.*.FloatTensor\ntorch.*.DoubleTensor\ntorch.*.HalfTensor\ntorch.*.BFloat16Tensor\ntorch.*.ByteTensor\ntorch.*.CharTensor\ntorch.*.ShortTensor\ntorch.*.IntTensor\ntorch.*.LongTensor\ntorch.*.BoolTensor\nTo find out if atorch.dtypeis a floating point data type, the propertyis_floating_pointcan be used, which returnsTrueif the data type is a floating point data type.\ntorch.dtype\nis_floating_point\nTrue\nTo find out if atorch.dtypeis a complex data type, the propertyis_complexcan be used, which returnsTrueif the data type is a complex data type.\ntorch.dtype\nis_complex\nTrue\nWhen the dtypes of inputs to an arithmetic operation (add,sub,div,mul) differ, we promote\nby finding the minimum dtype that satisfies the following rules:\nIf the type of a scalar operand is of a higher category than tensor operands\n(where complex > floating > integral > boolean), we promote to a type with sufficient size to hold\nall scalar operands of that category.\nIf a zero-dimension tensor operand has a higher category than dimensioned operands,\nwe promote to a type with sufficient size and category to hold all zero-dim tensor operands of\nthat category.\nIf there are no higher-category zero-dim operands, we promote to a type with sufficient size\nand category to hold all dimensioned operands.\nA floating point scalar operand has dtypetorch.get_default_dtype()and an integral\nnon-boolean scalar operand has dtypetorch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimumdtypesof an operand.  Complex types\nare not yet supported. Promotion for shell dtypes is not defined.\nPromotion Examples:\n\n```python\n>>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32\n\n```\n\nAn integral output tensor cannot accept a floating point tensor.\nA boolean output tensor cannot accept a non-boolean tensor.\nA non-complex output tensor cannot accept a complex tensor\nCasting Examples:\n\n```python\n# allowed:\n>>> float_tensor *= float_tensor\n>>> float_tensor *= int_tensor\n>>> float_tensor *= uint_tensor\n>>> float_tensor *= bool_tensor\n>>> float_tensor *= double_tensor\n>>> int_tensor *= long_tensor\n>>> int_tensor *= uint_tensor\n>>> uint_tensor *= int_tensor\n\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\n>>> int_tensor *= float_tensor\n>>> bool_tensor *= int_tensor\n>>> bool_tensor *= uint_tensor\n>>> float_tensor *= complex_float_tensor\n\n```\n\n\n## torch.device#\n\nAtorch.deviceis an object representing the device on which atorch.Tensoris\nor will be allocated.\ntorch.device\ntorch.Tensor\nThetorch.devicecontains a device type (most commonly \u201ccpu\u201d or\n\u201ccuda\u201d, but also potentially\u201cmps\u201d,\u201cxpu\u201d,\u201cxla\u201dor\u201cmeta\u201d) and optional\ndevice ordinal for the device type. If the device ordinal is not present, this object will always represent\nthe current device for the device type, even aftertorch.cuda.set_device()is called; e.g.,\natorch.Tensorconstructed with device'cuda'is equivalent to'cuda:X'where X is\nthe result oftorch.cuda.current_device().\ntorch.device\ntorch.cuda.set_device()\ntorch.Tensor\n'cuda'\n'cuda:X'\ntorch.cuda.current_device()\nAtorch.Tensor\u2019s device can be accessed via theTensor.deviceproperty.\ntorch.Tensor\nTensor.device\nAtorch.devicecan be constructed using:\ntorch.device\nA device string, which is a string representation of the device type and optionally the device ordinal.\nA device type and a device ordinal.\nA device ordinal, where the currentacceleratortype will be used.\nVia a device string:\n\n```python\n>>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('mps')\ndevice(type='mps')\n\n>>> torch.device('cuda')  # implicit index is the \"current device index\"\ndevice(type='cuda')\n\n```\n\nVia a device type and a device ordinal:\n\n```python\n>>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('mps', 0)\ndevice(type='mps', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n\n```\n\nVia a device ordinal:\nNote\nThis method will raise a RuntimeError if no accelerator is currently detected.\n\n```python\n>>> torch.device(0)  # the current accelerator is cuda\ndevice(type='cuda', index=0)\n\n>>> torch.device(1)  # the current accelerator is xpu\ndevice(type='xpu', index=1)\n\n>>> torch.device(0)  # no current accelerator detected\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: Cannot access accelerator device when none is available.\n\n```\n\nThe device object can also be used as a context manager to change the default\ndevice tensors are allocated on:\n\n```python\n>>> with torch.device('cuda:1'):\n...     r = torch.randn(2, 3)\n>>> r.device\ndevice(type='cuda', index=1)\n\n```\n\nThis context manager has no effect if a factory function is passed an explicit,\nnon-None device argument.  To globally change the default device, see alsotorch.set_default_device().\ntorch.set_default_device()\nWarning\nThis function imposes a slight performance cost on every Python\ncall to the torch API (not just factory functions).  If this\nis causing problems for you, please comment onpytorch/pytorch#92701\nNote\nThetorch.deviceargument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code.\ntorch.device\n\n```python\n>>> # Example of a function that takes in a torch.device\n>>> cuda1 = torch.device('cuda:1')\n>>> torch.randn((2,3), device=cuda1)\n\n```\n\n\n```python\n>>> # You can substitute the torch.device with a string\n>>> torch.randn((2,3), device='cuda:1')\n\n```\n\nNote\nMethods which take a device will generally accept a (properly formatted) string\nor an integer device ordinal, i.e. the following are all equivalent:\n\n```python\n>>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # equivalent to 'cuda:1' if the current accelerator is cuda\n\n```\n\nNote\nTensors are never moved automatically between devices and require an explicit call from the user. Scalar Tensors (with tensor.dim()==0) are the only exception to this rule and they are automatically transferred from CPU to GPU when needed as this operation can be done \u201cfor free\u201d.\nExample:\n\n```python\n>>> # two scalars\n>>> torch.ones(()) + torch.ones(()).cuda()  # OK, scalar auto-transferred from CPU to GPU\n>>> torch.ones(()).cuda() + torch.ones(())  # OK, scalar auto-transferred from CPU to GPU\n\n```\n\n\n```python\n>>> # one scalar (CPU), one vector (GPU)\n>>> torch.ones(()) + torch.ones(1).cuda()  # OK, scalar auto-transferred from CPU to GPU\n>>> torch.ones(1).cuda() + torch.ones(())  # OK, scalar auto-transferred from CPU to GPU\n\n```\n\n\n```python\n>>> # one scalar (GPU), one vector (CPU)\n>>> torch.ones(()).cuda() + torch.ones(1)  # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU\n>>> torch.ones(1) + torch.ones(()).cuda()  # Fail, scalar not auto-transferred from GPU to CPU and non-scalar not auto-transferred from CPU to GPU\n\n```\n\n\n## torch.layout#\n\nWarning\nThetorch.layoutclass is in beta and subject to change.\ntorch.layout\nAtorch.layoutis an object that represents the memory layout of atorch.Tensor. Currently, we supporttorch.strided(dense Tensors)\nand have beta support fortorch.sparse_coo(sparse COO Tensors).\ntorch.layout\ntorch.Tensor\ntorch.strided\ntorch.sparse_coo\ntorch.stridedrepresents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associatedtorch.Storage, which holds its data. These tensors provide\nmulti-dimensional,stridedview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently.\ntorch.strided\ntorch.Storage\nExample:\n\n```python\n>>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)\n\n```\n\nFor more information ontorch.sparse_cootensors, seetorch.sparse.\ntorch.sparse_coo\n\n## torch.memory_format#\n\nAtorch.memory_formatis an object representing the memory format on which atorch.Tensoris\nor will be allocated.\ntorch.memory_format\ntorch.Tensor\nPossible values are:\ntorch.contiguous_format:\nTensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.\ntorch.contiguous_format\ntorch.channels_last:\nTensor is or will be allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[1]==1aka NHWC order.\ntorch.channels_last\nstrides[0]>strides[2]>strides[3]>strides[1]==1\ntorch.channels_last_3d:\nTensor is or will be allocated in dense non-overlapping memory. Strides represented by values instrides[0]>strides[2]>strides[3]>strides[4]>strides[1]==1aka NDHWC order.\ntorch.channels_last_3d\nstrides[0]>strides[2]>strides[3]>strides[4]>strides[1]==1\ntorch.preserve_format:\nUsed in functions likecloneto preserve the memory format of the input tensor. If input tensor is\nallocated in dense non-overlapping memory, the output tensor strides will be copied from the input.\nOtherwise output strides will followtorch.contiguous_format\ntorch.preserve_format\ntorch.contiguous_format",
    "url": "https://pytorch.org/docs/stable/tensor_attributes.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "970ab7a031bbc57591633eee09f3c515",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/size.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8818ef4ff5cbe18901ed26f8fb7a3c5f",
    "source": "pytorch_docs",
    "title": "torch.compile Troubleshooting \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.compile Troubleshooting#\n\nCreated On: Nov 28, 2022 | Last Updated On: Aug 14, 2025\nYou\u2019re trying to usetorch.compileon your PyTorch model to enhance its performance\nbut it\u2019s not working as expected. Perhaps performance isn\u2019t improving, crashes are happening, or compilation time is too long. This article provides tips, workarounds, and debugging tools to help you overcome these challenges.\ntorch.compile\nContents\nSetting Expectations\nCompile times\nTerminology\nGraph break\nGuards\nRecompilation\nDynamic Shapes\nLogging Tools\ntlparse / TORCH_TRACE\nTORCH_LOGS\ntlparse vs. TORCH_LOGS\nSimple Workarounds\nWhere to apply torch.compile?\nDisabling and Suppressing Errors\nResolving graph breaks\nData-dependent operations\nCustom ops\nPrinting\nIncorrect code\nDealing with recompilations\nIs dynamic shapes enabled?\nChanging the cache size limit\nWrapping constants with tensors\nReporting Issues\nAblation\nBisecting\nCreating a reproducer\nMinifier\nDebugging Deeper\nTorchDynamo\nLogging what Dynamo is tracing\nBreakpointing Dynamo tracing\nBytecode generation errors\nAOTAutograd\nSummary of TORCH_LOGS options\nRelated Articles\n\n## Setting Expectations#\n\ntorch.compileis designed as a general-purpose PyTorch compiler.\nUnlike the previous compiler solution, TorchScript,torch.compilerequires fewer code changes, meaning models typically don\u2019t need to be rewritten from scratch.\nIt also manages unsupported code more gracefully - unsupported code results in a lost optimization opportunity rather than a crash.\ntorch.compile\ntorch.compile\nIn the ideal world, one can simply applytorch.compileto any PyTorch model and enjoy automatic speedups.\nHowever, in reality, code complexities can lead to one of three scenarios:\ntorch.compile\ntorch.compileworks seamlessly, providing speedups.\ntorch.compile\nSome code modifications are necessary.torch.compiledoesn\u2019t crash or take too long,\nbut you might not be seeing significant performance gains.\ntorch.compile\nExtensive changes to your code are required.\nWe anticipate most code will fall under scenarios (1) and (2).\nThis document provides tips, arranged by level of involvement, to help address code issues in scenario (2).\n\n## Compile times#\n\ntorch.compilefunctions as a just-in-time compiler, so the initial one or two runs\nof the compiled function are expected to be significantly slower. Recompilations, which can occur under certain conditions (detailed below),\nwill also make runs slower. Varioustorch.compilecomponents cache results to\nreduce compilation time for future invocations, even in different processes.\nCold-start (uncached) compilation time typically ranges from seconds to minutes for common or benchmarked models.\nLarger models may take upwards of 30 minutes to a few hours.\ntorch.compile\ntorch.compile\n\n## Terminology#\n\nThe following terms are relevant to troubleshootingtorch.compileproblems.\ntorch.compile\n\n## Graph break#\n\ntorch.compiletraces your code and attempts to capture your PyTorch code into a\nsingle computation graph of PyTorch operators (FX graph). However, this is not always possible.\nWhen encountering code that can\u2019t be traced, a \u201cgraph break\u201d occurs.\nA graph break involves compiling the FX graph has been determined so far, running the unsupported code,\nthen resuming tracing after the unsupported code with a new FX graph.\nBecause the computation graph is broken up, we lose optimization opportunities,\nso model code should avoid graph breaks whenever possible.\nGraph breaks occur on things like:\ntorch.compile\nData-dependent if-statements\nMany Python built-in functions\nC functions\nBelow is an example of a graph break due to the functioncopy.deepcopyfrom a Python builtin library\n(exact output may differ).\ncopy.deepcopy\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x):\n    x = x + 1\n    with open(\"test.txt\", \"r\") as f:\n        return x + len(f.read())\n\nfn(torch.ones(3, 3))\n\n```\n\n\n```python\n$TORCH_LOGS=\"graph_breaks\" python playground.py\nGraph break in user code at /data/users/williamwen/pytorch/playground.py:7\nReason: Unsupported: builtin: open [<class 'torch._dynamo.variables.constant.ConstantVariable'>, <class 'torch._dynamo.variables.constant.ConstantVariable'>] False\nUser code traceback:\nFile \"/data/users/williamwen/pytorch/playground.py\", line 7, in fn\n    with open(\"test.txt\", \"r\") as f:\nTraceback (most recent call last):\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 635, in wrapper\n    return inner_fn(self, inst)\n        ^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2414, in CALL\n    self._call(inst)\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2408, in _call\n    self.call_function(fn, args, kwargs)\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 962, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/builtin.py\", line 997, in call_function\n    return handler(tx, args, kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/builtin.py\", line 831, in <lambda>\n    return lambda *args: unimplemented(error_msg)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/exc.py\", line 313, in unimplemented\n    raise Unsupported(msg, case_name=case_name)\ntorch._dynamo.exc.Unsupported: builtin: open [<class 'torch._dynamo.variables.constant.ConstantVariable'>, <class 'torch._dynamo.variables.constant.ConstantVariable'>] False\n\n```\n\n\n## Guards#\n\ntorch.compilemakes some assumptions about runtime values as we trace through code.\nDuring tracing, we generate \u201cguards\u201d, which are runtime checks for these assumptions.\nGuards are run in future calls to the compiled function to determine if we can reuse previously compiled code.\nExamples of runtime checks are constant values, types, and object IDs.\ntorch.compile\nBelow is an example of generated guards. TheTENSOR_MATCHguard checks for the input\u2019s type, device, dtype, shape, etc.\nTENSOR_MATCH\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x):\n    return x + 1\n\nfn(torch.ones(3, 3))\n\n```\n\n\n```python\n$ TORCH_LOGS=\"guards\" python playground.py\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3, 3], stride=[3, 1])  # return x + 1  # playground.py:6 in fn\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x + 1  # playground.py:6 in fn\n\n```\n\n\n## Recompilation#\n\nIf the guards fail for every instance of previously compiled code,\nthentorch.compilemust \u201crecompile\u201d the function, requiring the original code to be traced again.\ntorch.compile\nIn the example below, recompilation is necessary because the guard checking the tensor argument\u2019s shape failed.\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x):\n    return x + 1\n\nfn(torch.ones(3, 3))\nfn(torch.ones(4, 4))\n\n```\n\n\n```python\n$ TORCH_LOGS=\"recompiles\" python playground.py\nRecompiling function fn in /data/users/williamwen/pytorch/playground.py:3\n    triggered by the following guard failure(s):\n    - 0/0: tensor 'L['x']' size mismatch at index 0. expected 3, actual 4\n\n```\n\n\n## Dynamic Shapes#\n\ntorch.compileinitially assumes tensor shapes are static/constant and guards based on these assumptions.\nBy using \u201cdynamic shapes,\u201d we can gettorch.compileto produce compiled code that can accept\ntensor inputs with different shapes - we avoid recompiling every time shapes differ.\nBy default, automatic dynamic shapes are enabledtorch.compile(dynamic=None)-\nif compilation fails due to shape mismatch, recompilation is attempted with dynamic shapes.\nDynamic shapes can also be fully enableddynamic=Trueor disableddynamic=False.\ntorch.compile\ntorch.compile\ntorch.compile(dynamic=None)\ndynamic=True\ndynamic=False\nBelow, we enable dynamic shapes and note that we no longer need to recompile.\n\n```python\nimport torch\n\n@torch.compile(dynamic=True)\ndef fn(x):\n    return x + 1\n\nfn(torch.ones(3, 3))\nfn(torch.ones(4, 4))\n\n```\n\n\n```python\n$ TORCH_LOGS=\"dynamic,recompiles\" python playground.py\ncreate_symbol s0 = 3 for L['x'].size()[0] [2, int_oo] at playground.py:5 in fn (_dynamo/variables/builder.py:2718 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s0\"\nproduce_guards\nproduce_guards\n\n```\n\nFor more information on dynamic shapes, seeThe dynamic shapes manual.\n\n## Logging Tools#\n\n\n## tlparse / TORCH_TRACE#\n\ntlparse/TORCH_TRACEare a pair of tools that produce compilation reports that look like this:https://web.mit.edu/~ezyang/Public/bhack-20240609-tlparse/index.html.\ntlparse\nTORCH_TRACE\nTraces are very easy to collect. To collect a trace, run your reproduction command with\n\n```python\nTORCH_TRACE=\"/tmp/tracedir\" python foo.py\npip install tlparse\ntlparse /tmp/tracedir\n\n```\n\nThis approach works even if you are running a distributed job, providing a trace for each rank.\nIt will open your browser with HTML similar to what\u2019s generated above.\nIf you are making a bug report for a complicated problem that you don\u2019t have a standalone reproduction for,\nyou can still greatly assist PyTorch developers by attaching the trace log generated in/tmp/tracedir.\n/tmp/tracedir\nWarning\nThe trace log contains all of your model code.\nDo not share the trace log if the model you are working on is sensitive. The trace log does NOT contain weights.\nThe output oftlparseis primarily aimed for PyTorch developers,\nand the log format is easy to upload and share on GitHub.\nHowever,  as a non-PyTorch developer, you can still extract useful information from it.\nWe recommend starting with the inline help text in the report, which explains its contents.\nHere are some insights you can gain from atlparse:\ntlparse\ntlparse\nWhat model code was compiled by looking at the stack trie?\nThis is especially useful if you\u2019re not familiar with the codebase being compiled!\nHow many graph breaks / distinct compilation regions are there?\n(Each distinct compile is its own color coded block like[0/0]).\nFrames that are potentially graph-broken are light green[2/4].\nIf there are a lot of frames, that is suspicious, and suggests that you had some catastrophic graph breaks,\nor maybe your code isn\u2019t a good match fortorch.compile.\ntorch.compile\nHow many times did I recompile a particular frame? Something that recompiled a lot will look like:[10/0][10/1][10/2]- if something is being recompiled a lot, that is very suspicious and worth looking into, even if it isn\u2019t the root cause of your problem.\nWas there a compilation error? Frames that errored will look like[0/1].\nWhat intermediate compiler products did I generate for a given frame?\nFor example, you can look at the high-level generated FX graph or the generated Triton code.\nIs there relevant information for a particular frame? You can find these incompilation_metrics.\ncompilation_metrics\n\n## TORCH_LOGS#\n\nYou can use theTORCH_LOGSenvironment variable to selectively enable parts of thetorch.compilestack to log.TORCH_LOGSis in fact the source of logs fortlparse. The format of theTORCH_LOGSenvironment variable looks like this:\nTORCH_LOGS\ntorch.compile\nTORCH_LOGS\ntlparse\nTORCH_LOGS\n\n```python\nTORCH_LOGS=\"<option1>,<option2>,...\" python foo.py\n\n```\n\nUseful high-level options include:\ngraph_breaks: logs locations of graph breaks in user code and the reason for the graph break\ngraph_breaks\nguards: logs guards that are generated\nguards\nrecompiles: logs which function recompiled and the guards that failed, leading to the recompilation\nrecompiles\ndynamic: logs related to dynamic shapes\ndynamic\nAlso, you can programmatically set logging options usingtorch._logging.set_logs:\ntorch._logging.set_logs\n\n```python\nimport logging\ntorch._logging.set_logs(graph_breaks=True)\n...\n\n```\n\nMoreTORCH_LOGSoptions areSummary of TORCH_LOGS options.\nFor the full list of options, seetorch._loggingandtorch._logging.set_logs.\nTORCH_LOGS\n\n## tlparse vs. TORCH_LOGS#\n\nGenerally, we suggest first usingtlparsewhen encountering issues.tlparseis ideal for debugging large models and gaining a high-level overview of how your model was compiled.\nOn the other hand,TORCH_LOGSis preferred for small examples and fine-grained debugging detail,\nwhen we already have an idea of whichtorch.compilecomponent is causing the problem.\ntlparse\ntlparse\nTORCH_LOGS\ntorch.compile\n\n## Simple Workarounds#\n\nHere, we describe some workarounds totorch.compileissues involving small code modifications\nor changing sometorch.compilesettings.\ntorch.compile\ntorch.compile\n\n## Where to apply torch.compile?#\n\nWe recommend applyingtorch.compileto the highest-level function that doesn\u2019t cause excessive problems.\nTypically, it is your train or eval step with the optimizer but without the loop, your top-levelnn.Module,\nor some sub-nn.Module``s.``torch.compilespecifically doesn\u2019t handle distributed wrapper modules like\nDDP or FSDP very well, so consider applyingtorch.compileto the inner module passed to the wrapper.\ntorch.compile\nnn.Module\nnn.Module``s.``torch.compile\ntorch.compile\n\n```python\n# inference\nmodel = ...\nopt_model = torch.compile(model)\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = opt_model(inp)\n\n```\n\n\n```python\n# training\nmodel = ...\nopt = torch.optim.Adam(model.parameters())\n\n@torch.compile\ndef train(mod, data):\n    opt.zero_grad(True)\n    pred = mod(data[0])\n    loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n    loss.backward()\n    opt.step()\n\nfor _ in range(N_ITERS):\n    inp = ...\n    train(model, inp)\n\n```\n\n\n```python\n# DistributedDataParallel\nmodel = ...\nopt_model = torch.compile(model)\nmodel_ddp = DistributedDataParallel(opt_model, ...)\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = model_ddp(inp)\n\n```\n\n\n## Disabling and Suppressing Errors#\n\nFor some model architectures, there are portions of the model which are particularly difficult to compile\n- either there are many graph breaks, or there are crashes. You may want to explicitly disable these\nportions of the model which are problematic so that you can applytorch.compileto the parts that work.\nYou can do this by using the@torch.compiler.disabledecorator. Whentorch.compileattempts to call a\ndisabled function, it breaks the graph and skips tracing the disabled function, resuming tracing after the call.\nBy default, all recursive calls made from a disabled function are also disabled. Use therecursive=Falseoption to allow compilation for recursive calls.\ntorch.compile\n@torch.compiler.disable\ntorch.compile\nrecursive=False\n\n```python\ndef bad1_inner(...):\n    # skipped\n\n@torch.compiler.disable\ndef bad1_outer(...):\n    # skipped\n    bad1_inner(...)\n\ndef bad2_inner(...)\n    # traced\n\n@torch.compiler.disable(recursive=False)\ndef bad2_outer(...):\n    # skipped\n    bad2_inner(...)\n\n@torch.compile\ndef fn(...):\n    # graph break\n    bad1_outer(...)\n        ...\n    # graph break\n    bad2_outer(...)\n\n```\n\nFor example, we usetorch.compiler.disableto disabletorch.compileon sparse architecture in\nrecommendation models, as the sparse arch is difficult to compile. Preprocessing and logging functions\nare other examples of functions that typically cause a lot of graph breaks and do not get value from being compiled.\ntorch.compiler.disable\ntorch.compile\nIf you are experiencing compiler crashes and you want to continue regardless, you can settorch._dynamo.config.suppress_errors=True. When the compiler crashes, we will just skip tracing\nthe function and try again later. This is not best practice - it is better to eventually manually add\ndisable annotations as necessary.\ntorch._dynamo.config.suppress_errors=True\n\n## Resolving graph breaks#\n\nTo maximize optimization opportunities, it\u2019s important to reduce the number of graph breaks.\nRecall that you can see what graph breaks are happening usingtlparseorTORCH_LOGS=\"graph_breaks\".\nIn general, graph breaks are caused by one of the following:\ntlparse\nTORCH_LOGS=\"graph_breaks\"\nYou\u2019re trying to do something that fundamentally cannot be traced, such as data-dependent control flow.\nYou\u2019re trying to do something not yet supported. .\nFor example, we currently have limited support for tracing code that uses the built-in Pythoninspectmodule.\ninspect\nYour code has an error in it. For example, you may have tried calling a function with an incorrect number of arguments.\nGraph break logs will tell you the user code location and reason for the graph break.\nUnfortunately, many graph breaks are not actionable without a deeper understanding of Dynamo.\nIt can even be challenging to determine which of the three causes was the true cause of your graph break.\nWe are working on making graph break messages more actionable.\nAdditionally, the impact of lost optimization opportunities differs between graph breaks.\nFor example, graph breaks that happen in the middle of your model\u2019sforwardare likely to have a more negatie impact than\ngraph breaks in a preprocessing part at the beginning of theforward. So it is not crucial to preventevery singlebreak, but rather to prevent the ones that cause significant performance hits.\nforward\nforward\nIf a graph break message doesn\u2019t suggest any action, you suspect that the cause of your graph break is (2),\nand you believe that the graph break is causing performance hits,\nthen please report the graph break as an issue. If a function has many graph breaks,\nconsider disabling compilation on that function, as the overhead cost for the graph breaks may become prohibitive.\nBelow are some common graph breaks and some workarounds.\ntorch.compilegraph breaks on data-dependent operations such as data-dependent control flow\n(if-statements, loops with tensors) and direct tensor data accesses (.item,.data_ptr).\ntorch.compile\n.item\n.data_ptr\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x):\n    y = x.sum()\n    if y > 0:\n        return x + y.item()\n    return x - y.item()\n\nfn(torch.ones(3, 3))\n\n```\n\n\n```python\n$ TORCH_LOGS=\"graph_breaks\" python playground.py\nGraph break in user code at /data/users/williamwen/pytorch/playground.py:6\nReason: Data-dependent jump\nUser code traceback:\nFile \"/data/users/williamwen/pytorch/playground.py\", line 6, in fn\n    if y > 0:\n\nGraph break in user code at /data/users/williamwen/pytorch/playground.py:7\nReason: Unsupported: Tensor.item\nUser code traceback:\nFile \"/data/users/williamwen/pytorch/playground.py\", line 7, in torch_dynamo_resume_in_fn_at_6\n    return x + y.item()\nTraceback (most recent call last):\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 616, in wrapper\n    return inner_fn(self, inst)\n        ^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2288, in CALL\n    self._call(inst)\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2282, in _call\n    self.call_function(fn, args, kwargs)\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 838, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/misc.py\", line 1038, in call_function\n    return self.obj.call_method(tx, self.name, args, kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/tensor.py\", line 527, in call_method\n    result = handler_method(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/tensor.py\", line 773, in method_item\n    unimplemented(\"Tensor.item\")\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/exc.py\", line 304, in unimplemented\n    raise Unsupported(msg, case_name=case_name)\ntorch._dynamo.exc.Unsupported: Tensor.item\n\n```\n\nThe general workaround for these graph breaks is to avoid doing data-dependent operations. Some specific workarounds are:\nIf your control flow doesn\u2019t actually depend on data values, consider modifying your code to perform control flow on constants.\n\n```python\n# old\nx = torch.randn(3, 3)\n@torch.compile\ndef fn(y):\n    if x.sum() > 0:\n        return y + x\n    else:\n        return y - x\n\n# new\nx = torch.randn(3, 3)\ncond = (x.sum() > 0).item()\n@torch.compile\ndef fn(y):\n    if cond:\n        return y + x\n    else:\n        return y - x\n\n```\n\nUse higher-order ops liketorch.cond(https://pytorch.org/docs/main/cond.html) in place of data-dependent control flow\ntorch.cond\n\n```python\n# old\n@torch.compile\ndef fn(x):\n    if x.sum() > 0:\n        return x + 1\n    return x - 1\n\n# new\n@torch.compile\ndef fn(x):\n    return torch.cond(\n        x.sum() > 0,\n        lambda x: x + 1,\n        lambda x: x - 1,\n        (x,),\n    )\n\n```\n\nIf you have a.item()call, trytorch._dynamo.config.capture_scalar_outputs=TrueorTORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n.item()\ntorch._dynamo.config.capture_scalar_outputs=True\nTORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\nWrap problematic parts of the function in a custom op\nIf you have code thattorch.compilehas trouble tracing through, either due to missing support or fundamental incompatibility,\nyou can consider wrapping the problematic code in a custom op.\ntorch.compile\nCustom ops require a little bit of additional work to get them to be compatible withtorch.compile.\nSeehttps://pytorch.org/tutorials/advanced/custom_ops_landing_page.htmlfor more details.\ntorch.compile\nPrinting/logging/issuing warnings will result in a graph break. If you have a function that makes many logging calls,\nfor example, a function that logs data about a training iteration, consider applyingtorch.compiler.disableon it.\ntorch.compiler.disable\nAlternatively, you can try usingtorch._dynamo.config.reorderable_logging_functions.\nThis config is used to reorder logging functions so that they are called at the end of the traced function,\nthus avoiding a graph break. However, the logged contents may differ if, for example, a mutation occurs.\ntorch._dynamo.config.reorderable_logging_functions\n\n```python\nimport torch\n\ntorch._dynamo.config.reorderable_logging_functions.add(print)\n\n@torch.compile\ndef fn(x):\n    x += 1\n    print(\"log!\")\n    return torch.sin(x)\n\nfn(torch.ones(3, 3))\n\n```\n\n\n```python\n$ TORCH_LOGS=\"graph_breaks\" python playground.py\nlog!\n\n```\n\nYour code may be wrong, or is otherwise encountering an error from outsidetorch.compile.\nIn the code below, we made a typo in thetorch.sincall by providing an extra argument.\ntorch.compile\ntorch.sin\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x):\n    y = torch.sin(x, x)\n    return y\n\nfn(torch.ones(3, 3))\n\n```\n\n\n```python\n$ TORCH_LOGS=\"graph_breaks\" python playground.py\nGraph break in user code at /data/users/williamwen/pytorch/playground.py:5\nReason: Unsupported: TypeError <built-in method sin of type object at 0x7fd6fd764600>: sin() takes 1 positional argument but 2 were given\nUser code traceback:\nFile \"/data/users/williamwen/pytorch/playground.py\", line 5, in fn\n    y = torch.sin(x, x)\n...\n\n```\n\nIt can be difficult to tell from the logs if the error is caused by your code or because of atorch.compilebug.\nIn order to differentiate, we recommend trying to run your code withouttorch.compileto see if you still get the error.\ntorch.compile\ntorch.compile\n\n## Dealing with recompilations#\n\nYou can view recompilations and their reasons usingtlparseorTORCH_LOGS=recompiles.\ntlparse\nTORCH_LOGS=recompiles\nRecompilations due to mismatched shapes are in the form:\n\n```python\ntensor 'L['x']' size mismatch at index 0. expected 3, actual 4\n\n```\n\nMake sure that thedynamicoption oftorch.compileis not set toFalse.\nThe default option,dynamic=None, will only attempt dynamic shapes after the first compilation.\nYou can setdynamic=Trueto upfront compile as dynamic as possible.\ndynamic\ntorch.compile\nFalse\ndynamic=None\ndynamic=True\nFor more information on dynamic shapes, seeThe dynamic shapes manual.\nThere is a limit to how many times a function can be recompiled, determined bytorch._dynamo.config.recompile_limitandtorch._dynamo.config.accumulated_recompile_limit.\nIf either limit is exceeded, then we will not attempt to compile the function again and instead will run the function eagerly.torch.compilewill also issue a warning containing the affected function and which limit was hit.\nIn the example below, each function call results in a recompile attempt.\nWhen we hit the cache size limit (8), we stop attempting to recompile.\ntorch._dynamo.config.recompile_limit\ntorch._dynamo.config.accumulated_recompile_limit\ntorch.compile\n\n```python\nimport torch\n\n@torch.compile(dynamic=False)\ndef fn(x):\n    return x + 1\n\nfor i in range(1, 10):\n    fn(torch.ones(i))\n\n```\n\n\n```python\n$ python playground.py\ntorch._dynamo hit config.recompile_limit (8)\n    function: 'fn' (/data/users/williamwen/pytorch/playground.py:5)\n    last reason: 0/0: tensor 'L['x']' size mismatch at index 0. expected 1, actual 9\n\n```\n\nIf you know that the number of recompilations has a reasonable constant upper bound, you can raise the cache size limit.\nIf the cost of recompilation outweighs the benefit of compilation, then you can consider lowering the cache size limit.\nBy default,int/floatvariables are treated as constants and are guarded as such.\nIn the below example, we have a recompilation for each function call.\nint\nfloat\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, c):\n    return x + c\n\nfor i in range(1, 10):\n    fn(torch.ones(i), 0.5 + i)\n\n```\n\n\n```python\n$ TORCH_LOGS=\"recompiles\" python playground.py\nRecompiling function fn in /data/users/williamwen/pytorch/playground.py:3\n    triggered by the following guard failure(s):\n    - 0/7: L['c'] == 8.5\n    - 0/6: L['c'] == 7.5\n    - 0/5: L['c'] == 6.5\n    - 0/4: L['c'] == 5.5\n    - 0/3: L['c'] == 4.5\n    - 0/2: L['c'] == 3.5\n    - 0/1: L['c'] == 2.5\n    - 0/0: L['c'] == 1.5\ntorch._dynamo hit config.recompile_limit (8)\n    function: 'fn' (/data/users/williamwen/pytorch/playground.py:3)\n    last reason: 0/0: L['c'] == 1.5\n\n```\n\nIn particular, for LR schedulers, initializing with a constant can lead to recompilations:\n\n```python\nimport torch\n\nmod = torch.nn.Linear(3, 3)\nopt = torch.optim.Adam(mod.parameters(), lr=0.01)\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, 0.9)\n\n@torch.compile\ndef fn(inp):\n    opt.zero_grad(True)\n    out = mod(inp).sum()\n    out.backward()\n    opt.step()\n    sched.step()\n\nfor i in range(1, 10):\n    fn(torch.ones(3, 3))\n\n```\n\n\n```python\n$ TORCH_LOGS=\"recompiles\" python playground.py\nRecompiling function step in /data/users/williamwen/pytorch/torch/optim/adam.py:189\n    triggered by the following guard failure(s):\n    - 3/7: L['self'].param_groups[0]['lr'] == 0.004782969000000002\n    - 3/6: L['self'].param_groups[0]['lr'] == 0.005314410000000002\n    - 3/5: L['self'].param_groups[0]['lr'] == 0.005904900000000002\n    - 3/4: L['self'].param_groups[0]['lr'] == 0.006561000000000002\n    - 3/3: L['self'].param_groups[0]['lr'] == 0.007290000000000001\n    - 3/2: L['self'].param_groups[0]['lr'] == 0.008100000000000001\n    - 3/1: L['self'].param_groups[0]['lr'] == 0.009000000000000001\n    - 3/0: L['self'].param_groups[0]['lr'] == 0.01\ntorch._dynamo hit config.recompile_limit (8)\n    function: 'step' (/data/users/williamwen/pytorch/torch/optim/adam.py:189)\n    last reason: 3/0: L['self'].param_groups[0]['lr'] == 0.01\n\n```\n\nIn both examples, we can wrap float variables in tensors in order to prevent recompilations.\n\n```python\n# first example\nfor i in range(1, 10):\n    fn(torch.ones(i), torch.tensor(0.5 + i))\n\n# second example\nopt = torch.optim.Adam(mod.parameters(), lr=torch.tensor(0.01))\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, torch.tensor(0.9))\n\n```\n\n\n## Reporting Issues#\n\nIf the workarounds provided above were not enough to gettorch.compileworking,\nthen you should consider reporting the issue to PyTorch.\nBut there are a few things that you can do to make our lives significantly easier.\ntorch.compile\n\n## Ablation#\n\nCheck which component of thetorch.compilestack is the one causing the issue using thebackend=option fortorch.compile.\nIn particular, try:\ntorch.compile\nbackend=\ntorch.compile\ntorch.compile(fn,backend=\"eager\"), which only runs TorchDynamo, the graph capture component oftorch.compile.\ntorch.compile(fn,backend=\"eager\")\ntorch.compile\ntorch.compile(fn,backend=\"aot_eager\"), which runs TorchDynamo and AOTAutograd, which additionally generates the backward graph during compilation.\ntorch.compile(fn,backend=\"aot_eager\")\ntorch.compile(fn,backend=\"aot_eager_decomp_partition\"), which runs TorchDynamo and AOTAutograd with operator decompositions/partitions.\ntorch.compile(fn,backend=\"aot_eager_decomp_partition\")\ntorch.compile(fn,backend=\"inductor\"), which runs TorchDynamo, AOTAutograd, and TorchInductor, the backend ML compiler that generates compiled kernels.\ntorch.compile(fn,backend=\"inductor\")\nIf you only fail with the Inductor backend, you can additionally test various Inductor modes:\ntorch.compile(fn,backend=\"inductor\",mode=\"default\")\ntorch.compile(fn,backend=\"inductor\",mode=\"default\")\ntorch.compile(fn,backend=\"inductor\",mode=\"reduce-overhead\")\ntorch.compile(fn,backend=\"inductor\",mode=\"reduce-overhead\")\ntorch.compile(fn,backend=\"inductor\",mode=\"max-autotune\")\ntorch.compile(fn,backend=\"inductor\",mode=\"max-autotune\")\nYou can also check if dynamic shapes is causing issues with any backend:\ntorch.compile(fn,dynamic=True)(always use dynamic shapes)\ntorch.compile(fn,dynamic=True)\ntorch.compile(fn,dynamic=False)(never use dynamic shapes)\ntorch.compile(fn,dynamic=False)\ntorch.compile(fn,dynamic=None)(automatic dynamic shapes)\ntorch.compile(fn,dynamic=None)\n\n## Bisecting#\n\nDid you try on the latest nightly? Did something work in the past but now no longer works?\nCan you bisect to determine the first nightly where your issue occurs?\nBisecting is especially helpful for performance, accuracy, or compile time regressions,\nwhere it is not immediately obvious where the problem originates from.\n\n## Creating a reproducer#\n\nCreating reproducers is a lot of work, and it is perfectly fine if you do not have the time to do it.\nHowever, if you are a motivated user unfamiliar with the internals oftorch.compile,\ncreating a standalone reproducer can have a huge impact on our ability to fix the bug.\nWithout a reproducer, your bug report must contain enough information for us to identify the root cause of the problem and write a reproducer from scratch.\ntorch.compile\nHere\u2019s a list of useful reproducers, ranked from most to least preferred:\nSelf-contained, small reproducer:A script with no external dependencies, under 100 lines of code, that reproduces the problem when run.\nSelf-contained, large reproducer:Even if it\u2019s large, being self-contained is a huge advantage!\nNon-self-contained reproducer with manageable dependencies:For example, if you can reproduce the problem by running a script afterpipinstalltransformers,\nthat\u2019s manageable. We can likely run it and investigate.\npipinstalltransformers\nNon-self-contained reproducer requiring substantial setup:This might involve downloading datasets,\nmultiple environment setup steps, or specific system library versions requiring a Docker image.\nThe more complex the setup, the harder it is for us to recreate the environment.\nNote\n\n```python\nDocker simplifies setup but complicates changes to the environment, so it's not a perfect solution, though we'll use it if necessary.\n\n```\n\nSomewhat orthogonally, a reproducer that can be run in a single process is better than a reproducer\nthat requires multiprocess training (but once again, if you only have a multiprocess reproducer, we\u2019ll take it!).\nAdditionally, below is a non-exhaustive list of aspects to check in your\nissue that you can attempt to replicate in your reproducer:\nAutograd. Did you have tensor inputs withrequires_grad=True? Did you callbackward()on the output?\nrequires_grad=True\nbackward()\nDynamic shapes. Did you setdynamic=True? Or did you run the test code multiple times with varying shapes?\ndynamic=True\nCustom operators. Is there a custom operator involved in the real workflow?\nCan you replicate some of its important characteristics using the Python custom operator API?\nConfiguration. Did you set all the same configuration?\nThis includestorch._dynamo.configandtorch._inductor.configsettings,\nas well as arguments totorch.compilelikebackend/mode.\ntorch._dynamo.config\ntorch._inductor.config\ntorch.compile\nbackend\nmode\nContext managers. Did you replicate any active context managers?\nThis could betorch.no_grad, automatic mixed precision,TorchFunctionMode/TorchDispatchMode,\nactivation checkpointing, compiled autograd etc.\ntorch.no_grad\nTorchFunctionMode\nTorchDispatchMode\nTensor subclasses. Is there a tensor subclass involved?\n\n## Minifier#\n\nThe minifier is an earlytorch.compiletool that, given an FX graph that crashes when we attempt to run or compile it,\nfinds a subgraph that also crashes and outputs the code that performs that subgraph\u2019s operations.\nEssentially, the minifier finds a minimal repro for a certain class oftorch.compile-related crashes.\nThis assumes that we were able to successfully trace through code.\ntorch.compile\ntorch.compile\nUnfortunately, most of the time nowadays, the minifier doesn\u2019t work as expected, and alternative methods may be necessary.\nThis is likely because bugs that can be automatically reproduced in this manner are generally easier to fix\nand have already been addressed, leaving more complex issues that do not reproduce easily.\nHowever, it is straightforward to attempt using the minifier, so it is worth trying even if it may not succeed.\nInstructions for operating the minifier can be foundhere.\nIf the compiler is crashing, you can setTORCHDYNAMO_REPRO_AFTER=\"dynamo\"orTORCHDYNAMO_REPRO_AFTER=\"aot\"Theaotoption is more likely to succeed, although it may not identify theAOTAutogradissues. This will generate therepro.pyfile which may help to diagnose the problem.\nFor accuracy-related issues, consider settingTORCHDYNAMO_REPRO_LEVEL=4. Please note that this may not always successfully identify the problematic subgraph.\nTORCHDYNAMO_REPRO_AFTER=\"dynamo\"\nTORCHDYNAMO_REPRO_AFTER=\"aot\"\naot\nAOTAutograd\nrepro.py\nTORCHDYNAMO_REPRO_LEVEL=4\n\n## Debugging Deeper#\n\nThis section provides tools and techniques for independently debuggingtorch.compileissues\nor for gaining a deeper understanding of thetorch.compilestack.\nThese methods are more involved than those presented above and are used by PyTorch developers regularly\nto debug realtorch.compileissues.\ntorch.compile\ntorch.compile\ntorch.compile\nBelow is a high-level overview of the stack:\n\nThe stack comprises three main components: TorchDynamo, AOTAutograd, and Inductor.\nOur debugging strategy involves first identifying the component in which the error occurs\nand then individually debugging the component. To determine the component responsible for the issue,\nsee theAblationsection underReportingIssuesabove. For guidance on debugging a specific component, consult the sections below.\nAblation\nReportingIssues\n\n## TorchDynamo#\n\nTheTORCH_LOGS=trace_bytecodeoption enables you to view the precise bytecode instructions that Dynamo is tracing,\nas well as a symbolic representation of the Python interpreter stack. When encountering a graph break or crash,\nit is advisable to inspect the last few bytecode instructions traced.\nTORCH_LOGS=trace_bytecode\nYou can also useTORCH_LOGS=trace_sourceto see which lines of source code Dynamo is tracing through.\nThis is useful in combination withtrace_bytecodeto see the line of source code each traced bytecode instruction corresponds to.\nTORCH_LOGS=trace_source\ntrace_bytecode\nFinally, you can useTORCH_LOGS=graph_codeto see the Python code representing the FX graph that Dynamo traced.\nYou can view this code to double check that the correct ops are being traced.\nTORCH_LOGS=graph_code\n\n```python\nimport torch\n\ndef g(x, y):\n    return x + y\n\n@torch.compile(backend=\"eager\")\ndef f(x):\n    x = torch.sin(x)\n    x = g(x, x)\n    return x\n\nf(torch.ones(3, 3))\n\n```\n\n\n```python\n$ TORCH_LOGS=\"trace_bytecode,trace_source,graph_code\" python playground.py\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:6 in f ()\n    @torch.compile(backend=\"eager\")\nTRACE RESUME 0 []\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:8 in f (f)\n        x = torch.sin(x)\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR sin [NullVariable(), PythonModuleVariable(<module 'torch' from '/data/users/williamwen/pytorch/torch/__init__.py'>)]\nTRACE LOAD_FAST x [NullVariable(), TorchInGraphFunctionVariable(<built-in method sin of type object at 0x7f00f6964600>)]\nTRACE CALL 1 [NullVariable(), TorchInGraphFunctionVariable(<built-in method sin of type object at 0x7f00f6964600>), LazyVariableTracker()]\nTRACE STORE_FAST x [TensorVariable()]\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:9 in f (f)\n        x = g(x, x)\nTRACE LOAD_GLOBAL g []\nTRACE LOAD_FAST x [NullVariable(), UserFunctionVariable()]\nTRACE LOAD_FAST x [NullVariable(), UserFunctionVariable(), TensorVariable()]\nTRACE CALL 2 [NullVariable(), UserFunctionVariable(), TensorVariable(), TensorVariable()]\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:3 in g (g) (inline depth: 1)\n    def g(x, y):\nTRACE RESUME 0 []\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:4 in g (g) (inline depth: 1)\n        return x + y\nTRACE LOAD_FAST x []\nTRACE LOAD_FAST y [TensorVariable()]\nTRACE BINARY_OP 0 [TensorVariable(), TensorVariable()]\nTRACE RETURN_VALUE None [TensorVariable()]\nTRACE STORE_FAST x [TensorVariable()]\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:10 in f (f)\n        return x\nTRACE LOAD_FAST x []\nTRACE RETURN_VALUE None [TensorVariable()]\nTRACED GRAPH\n===== __compiled_fn_1 =====\n/data/users/williamwen/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3, 3][3, 1]cpu\"):\n        l_x_ = L_x_\n\n        # File: /data/users/williamwen/pytorch/playground.py:8 in f, code: x = torch.sin(x)\n        x: \"f32[3, 3][3, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n\n        # File: /data/users/williamwen/pytorch/playground.py:4 in g, code: return x + y\n        x_1: \"f32[3, 3][3, 1]cpu\" = x + x;  x = None\n        return (x_1,)\n\n```\n\nInserting a breakpoint in Dynamo/user code is helpful at times to see what the state of Dynamo is when tracing through user code.\nUnfortunately, inserting a breakpoint in the normal Python fashion will result in a graph break in TorchDynamo,\nso we will not be able to view the state of Dynamo at the point where we intended to breakpoint.\nThe first method for setting a breakpoint is to insert it within the Dynamo source code. Three recommended locations to place a breakpoint are:\nIntorch/_dynamo/symbolic_convert.py, breakpoint at functions that are named after the problematic bytecode instruction,\nsuch asdefCALL_FUNCTIONanddefSTORE_ATTR. You can conditionally breakpoint depending on inputs,\nfor example, theargvalof the instruction, or the name of the object at the top of the stack since some bytecode opcodes are frequently used.\ntorch/_dynamo/symbolic_convert.py\ndefCALL_FUNCTION\ndefSTORE_ATTR\nargval\nBreakpoint where the graph break or error originates from. Typically, graph breaks are emitted from a call tounimplemented(...).\nunimplemented(...)\nBreakpoint intorch/_dynamo/variables/builder.py,function:_wrap. You will likely have to conditionally breakpoint on the input.\nThis function determines how to symbolically represent a given value. Consider breakpointing here if you suspect that a value is represented incorrectly.\ntorch/_dynamo/variables/builder.py,function:_wrap\nThe second way to insert a breakpoint is to usetorch._dynamo.comptime.comptime.breakpoint:\ntorch._dynamo.comptime.comptime.breakpoint\n\n```python\nfrom torch._dynamo.comptime import comptime\n\n@torch.compile\ndef f(...):\n    ...\n    comptime.breakpoint()\n    ...\n\n```\n\nA comptime breakpoint is convenient as it enables you to inspect the Dynamo state at a specific location within the user code being traced.\nIt does not require you to insert a breakpoint in the Dynamo source or to conditionally breakpoint based on variables.\nWhen a comptime breakpoint is triggered, you can do the following:\nctx.print_bt()to print the user stack trace\nctx.print_bt()\nctx.print_locals()to print all current locals\nctx.print_locals()\nctx.print_graph()to print the currently traced graph\nctx.print_graph()\nctx.disas()to print the currently traced function\u2019s bytecode\nctx.disas()\nUse standardpdbcommands, such asbt/u/d/n/s/r, - you can go up thepdbstack to inspect more Dynamo internals\npdb\nbt/u/d/n/s/r\npdb\n\n```python\nimport torch\nfrom torch._dynamo.comptime import comptime\n\n@torch.compile(backend=\"eager\")\ndef f(x):\n    y = x + 1\n    comptime.breakpoint()\n    y = y + 1\n    return y\n\nf(torch.ones(3, 3))\n\n```\n\n\n```python\n$ python playground.py\n--Return--\n> /data/users/williamwen/pytorch/torch/_dynamo/comptime.py(392)inner()->None\n-> builtins.breakpoint()\n(Pdb) ctx.print_bt()\nFile \"/data/users/williamwen/pytorch/playground.py\", line 7, in f\n    comptime.breakpoint()\n\n(Pdb) ctx.print_locals()\nx = FakeTensor(..., size=(3, 3))\ny = FakeTensor(..., size=(3, 3))\n(Pdb) bt\n...\n/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py(826)call_function()\n-> self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n/data/users/williamwen/pytorch/torch/_dynamo/variables/misc.py(331)call_function()\n-> func(ComptimeContext(tx))\n> /data/users/williamwen/pytorch/torch/_dynamo/comptime.py(392)inner()->None\n-> builtins.breakpoint()\n(Pdb) ctx.print_graph()\n\n\n\ndef forward(self, L_x_: \"f32[3, 3]\"):\n    l_x_ = L_x_\n\n    # File: /data/users/williamwen/pytorch/playground.py:6 in f, code: y = x + 1\n    y: \"f32[3, 3]\" = l_x_ + 1;  l_x_ = y = None\n\n```\n\nAlthough uncommon, Dynamo may generate incorrect bytecode. This may occur if you determine the following:\nAblation reveals the error is happening at the TorchDynamo level\nThe error is not being emitted from TorchDynamo stack frames\nThe error looks more like a user error rather than a Dynamo error, or is a segmentation fault\nThe error does not occur withouttorch.compile\ntorch.compile\nBytecode generation bugs are generally tricky to fix and we recommend submitting an issue instead of trying to fix those yourself.\nIf you are interested in seeing the bytecode that Dynamo generates, you can useTORCH_LOGS=bytecode.\nYou can see a high-level overview on what bytecode Dynamo generateshere.\nTORCH_LOGS=bytecode\n\n## AOTAutograd#\n\nAOTAutograd errors are typically difficult to debug - we recommend just submitting an issue.\nAOTAutograd logging output is primarily helpful to see what the input to Inductor is.\n\n## Summary of TORCH_LOGS options#\n\nA summary of helpfulTORCH_LOGSoptions is:\nTORCH_LOGS\nOption\nDescription\n+all\nOutput debug logs from alltorch.compilecomponents\ntorch.compile\n+dynamo\nOutput debug logs from TorchDynamo\n+aot\nOutput debug logs from AOTAutograd\n+inductor\nOutput debug logs from TorchInductor\ndynamic\nOutput logs from dynamic shapes\ngraph_code\nOutput the Python code for the FX graph that Dynamo generated\ngraph_sizes\nOutput the tensor sizes of the FX graph that Dynamo generated\ntrace_bytecode\nOutput the bytecode instructions that Dynamo is tracing through and the symbolic interpreter stack Dynamo is keeping track of\ntrace_source\nOutput the line of code in the original source that Dynamo is currently tracing through\nbytecode\nOutput Dynamo-generated bytecode\nguards\nOutput generated guards\nrecompiles\nOutput recompilation reasons (only the first guard check that fails)\nrecompiles_verbose\nOutput all guard checks that fail when a recompilation occurs\naot_graphs\nOutput graph generated by AOTAutograd\naot_joint_graphs\nOutput the joint forward-backward graph generated by AOTAutograd\noutput_code\nOutput code generated by Inductor\nkernel_code\nOutput code generated by Inductor on a per-kernel basis\nschedule\nOutput Inductor scheduling logs\nperf_hints\nOutput Inductor perf hint logs\nfusion\nOutput Inductor fusion logs\nFor the full list of options, seetorch._loggingandtorch._logging.set_logs.\n\n## Related Articles#\n\ntorch.compile tutorial\ntorch.compile fine-grained APIs\ntorch.compile FAQ\ntorch.compiler namespace overview\ntorch.compiler API reference\nProfiling torch.compile\ntorch.compile missing manual\nThe dynamic shapes manual\nTorchInductor caching tutorial",
    "url": "https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f1374f26faaab451938d63faecbacd77",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/cpu.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "57b4de594ef1dd27b6b6bde06838358c",
    "source": "pytorch_docs",
    "title": "Dynamo Overview \u2014 PyTorch 2.9 documentation",
    "text": "\n## Dynamo Overview#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jun 13, 2025\nBefore you read this section, readtorch.compiler.\nTorchDynamo (or simply Dynamo) is a Python-level Just-In-Time (JIT) compiler designed to make\nunmodified PyTorch programs faster. Dynamo hooks into the frame evaluation\nAPI in CPython (PEP 523) to\ndynamically modify Python bytecode right before it is executed. It\nrewrites Python bytecode to extract sequences of PyTorch\noperations into anFX Graphwhich is then compiled with a customizable backend.\nIt creates this FX Graph through bytecode analysis and is designed to\nmix Python execution with compiled backends to get the best of both\nworlds \u2014 usability and performance.\nDynamo makes it easy to experiment with different compiler\nbackends to make PyTorch code faster with a single line decoratortorch._dynamo.optimize()which is wrapped for convenience bytorch.compile()\ntorch._dynamo.optimize()\ntorch.compile()\nThe following diagram demonstrates how PyTorch works withtorch.compileand without it:\ntorch.compile\nTorchInductoris one of the backends\nsupported byDynamo GraphintoTritonfor GPUs orC++/OpenMPfor CPUs. We have atraining performance dashboardthat provides performance comparison for different training backends. You can read\nmore in theTorchInductor post on PyTorch\ndev-discuss.\nTorchInductor\nFor an in-depth overview, read the sections below, watch the deep-dive video,\nand check out the dev-discuss topics.\nDynamo deep-dive video\ndev-discuss topics\n\n## Dynamo Internals#\n\nAuthor:Jason AnselandKaichao You\nThis section will go over some of the Dynamo internals and will\ndemonstrate how Dynamo works under the hood.\n\n## What is a guard?#\n\nDynamo operates just-in-time and specializes graphs based on\ndynamic properties. Below is a basic example of how to use Dynamo.\nOne can decorate a function or a method usingtorchdynamo.optimizeto enable\nDynamo optimization:\ntorchdynamo.optimize\n\n```python\nfrom typing import List\nimport torch\nfrom torch import _dynamo as torchdynamo\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n\n@torchdynamo.optimize(my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n\n```\n\nFor example, the first graph above has the following\nguards:\n\n```python\nGUARDS:\nhasattr(L['a'], '_dynamo_dynamic_indices') == False\nhasattr(L['b'], '_dynamo_dynamic_indices') == False\nutils_device.CURRENT_DEVICE == None\n___skip_backend_check() or ___current_backend() == ___lookup_backend(140355900538256)\ncheck_tensor(L['a'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1])\ncheck_tensor(L['b'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1])\n\n```\n\nIf any of those guards fail, the graph will be recaptured and\nrecompiled. The interesting guard there ischeck_tensor, which\nchecks the followingtorch.Tensorproperties:\ncheck_tensor\ntorch.Tensor\nPython class of the tensor (tensor subclassing, etc)\ndtype\ndevice\nrequires_grad\ndispatch_key (with thread-local includes/excludes applied)\nndim\nsizes*\nstrides*\nThe full specialization mode allows the backend compiler to assume an\nentirely static graph. Unfortunately, most backends require this.\nOperators which return dynamic shapes will trigger a graph break when\nnot in dynamic shape mode.\n\n## What is Dynamo doing?#\n\nIf you want to understand better what Dynamo is doing, you can run your code with:\n\n```python\nTORCH_LOGS=\"+dynamo,guards,bytecode\"\n\n```\n\nIf you are not familiar with Python bytecode, you can add a decompiler hook\nto decompile the bytecode into human-readable source code. One available\ntool isdepyf. If you don\u2019t havedepyfalready installed, runpipinstalldepyf. Then, add the\nfollowing code to install decompilation hooks before you run any code.\ndepyf\npipinstalldepyf\n\n```python\nimport depyf\ndepyf.install()\n\n```\n\nThis code triggers useful (but spammy) printouts.\nFor example, the printouts for the first graph in thetoy_exampleare:\ntoy_example\n\n```python\n__compiled_fn_0 <eval_with_key>.1\nopcode         name     target                                                  args              kwargs\n-------------  -------  ------------------------------------------------------  ----------------  --------\nplaceholder    a        a                                                       ()                {}\nplaceholder    b        b                                                       ()                {}\ncall_function  abs_1    <built-in method abs of type object at 0x7f9ca082f8a0>  (a,)              {}\ncall_function  add      <built-in function add>                                 (abs_1, 1)        {}\ncall_function  truediv  <built-in function truediv>                             (a, add)          {}\ncall_method    sum_1    sum                                                     (b,)              {}\ncall_function  lt       <built-in function lt>                                  (sum_1, 0)        {}\noutput         output   output                                                  ((truediv, lt),)  {}\nORIGINAL BYTECODE toy_example example.py line 12\n 14           0 LOAD_FAST                0 (a)\n              2 LOAD_GLOBAL              0 (torch)\n              4 LOAD_METHOD              1 (abs)\n              6 LOAD_FAST                0 (a)\n              8 CALL_METHOD              1\n             10 LOAD_CONST               1 (1)\n             12 BINARY_ADD\n             14 BINARY_TRUE_DIVIDE\n             16 STORE_FAST               2 (x)\n 15          18 LOAD_FAST                1 (b)\n             20 LOAD_METHOD              2 (sum)\n             22 CALL_METHOD              0\n             24 LOAD_CONST               2 (0)\n             26 COMPARE_OP               0 (<)\n             28 POP_JUMP_IF_FALSE       19 (to 38)\n 16          30 LOAD_FAST                1 (b)\n             32 LOAD_CONST               3 (-1)\n             34 BINARY_MULTIPLY\n             36 STORE_FAST               1 (b)\n 17     >>   38 LOAD_FAST                2 (x)\n             40 LOAD_FAST                1 (b)\n             42 BINARY_MULTIPLY\n             44 RETURN_VALUE\nMODIFIED BYTECODE toy_example example.py line 12\n 12           0 LOAD_GLOBAL              3 (__compiled_fn_0)\n              2 LOAD_FAST                0 (a)\n              4 LOAD_FAST                1 (b)\n              6 CALL_FUNCTION            2\n              8 UNPACK_SEQUENCE          2\n             10 STORE_FAST               2 (x)\n             12 POP_JUMP_IF_FALSE       12 (to 24)\n             14 LOAD_GLOBAL              4 (__resume_at_30_1)\n             16 LOAD_FAST                1 (b)\n             18 LOAD_FAST                2 (x)\n             20 CALL_FUNCTION            2\n             22 RETURN_VALUE\n        >>   24 LOAD_GLOBAL              5 (__resume_at_38_2)\n             26 LOAD_FAST                1 (b)\n             28 LOAD_FAST                2 (x)\n             30 CALL_FUNCTION            2\n             32 RETURN_VALUE\npossible source code:\ndef toy_example(a, b):\n    __temp_1 = __compiled_fn_0(a, b)\n    x = __temp_1[0]\n    if __temp_1[1]:\n        return __resume_at_30_1(b, x)\n    return __resume_at_38_2(b, x)\nIf you find the decompiled code is wrong,please submit an issue at https://github.com/youkaichao/depyf/issues.\n\n```\n\nAt the top you can see the FX graph.\nNext, you see the original bytecode of the function, followed by the\nmodified bytecode generated by Dynamo, and the decompiled source\ncode for reference. Finally, you see the guards which we covered above.\nIn the modified bytecode,__compiled_fn_0is the return value ofmy_compiler()(the compiled graph).__resume_at_30_1and__resume_at_38_2are both generated continuation functions that pick\nup execution after a graph break (at bytecode offsets 30 and 38). Each\nof these functions take the form:\n__compiled_fn_0\nmy_compiler()\n__resume_at_30_1\n__resume_at_38_2\n\n```python\n__resume_at_<offset>:\n    ... restore stack state if needed ...\n    JUMP_ABSOLUTE <offset> into toy_example\n    ... original bytecode of toy_example ...\n\n```\n\nBy generating thisresume_atfunction, we force the remainder of the\nfunction to be executed in a new Python frame which recursively\ntriggers Dynamo to restart its capture once execution reaches that\npoint for the first time.\nresume_at\n\n## How to inspect artifacts generated by Dynamo?#\n\nTo inspect the artifacts generated by Dynamo, there is an APItorch._dynamo.eval_frame._debug_get_cache_entry_listthat retrieves compiled code and guards out of a function\u2019s__code__object. A compiled function can have several cache entries, and each cache entry consists a generated function to check guards, and atypes.CodeTypeobject to keep the code to be executed if the guarding conditions are satisfied.\ntorch._dynamo.eval_frame._debug_get_cache_entry_list\n__code__\ntypes.CodeType\n\n```python\nfrom torch._dynamo.eval_frame import _debug_get_cache_entry_list, innermost_fn\ncache_entries = _debug_get_cache_entry_list(innermost_fn(toy_example))\ncache_entry = cache_entries[0]\nguard, code = cache_entry.check_fn, cache_entry.code\n# the guard takes the local variables of an input frame, and tells whether a re-compilation should be triggered.\nimport dis\ndis.dis(guard)\ndis.dis(code)\n\n```\n\nIf you know Python bytecode, you can understand the above output.\nFor the guard function, there is no need to inspect the bytecode. We can directly access its guarding conditions:\n\n```python\nfor code_part in guard.code_parts:\n    print(code_part)\n\n```\n\nThe output is:\n\n```python\n___guarded_code.valid\n___check_global_state()\nhasattr(L['a'], '_dynamo_dynamic_indices') == False\nhasattr(L['b'], '_dynamo_dynamic_indices') == False\nutils_device.CURRENT_DEVICE == None\n___skip_backend_check() or ___current_backend() == ___lookup_backend(140215810860528)\n___check_tensors(L['a'], L['b'], tensor_check_names=tensor_check_names)\n\n```\n\nOnly when all the conditions are satisfied, the guard function returns true, and the compiled code is executed.\nFor the compiled code, we cannot directly access its source but have to decompile it.\n\n```python\nfrom depyf import decompile\nprint(decompile(code))\n\n```\n\nThe output is:\n\n```python\ndef toy_example(a, b):\n    __temp_1 = __compiled_fn_0(a, b)\n    x = __temp_1[0]\n    if __temp_1[1]:\n        return __resume_at_30_1(b, x)\n    return __resume_at_38_2(b, x)\n\n```\n\nSome names referenced in the code are:\nCompiled functions, stored in the global namespace of the module containing the original functiontoy_example. These include names like__compiled_fn_0/__resume_at_30_1/__resume_at_38_2.\ntoy_example\n__compiled_fn_0\n__resume_at_30_1\n__resume_at_38_2\nClosure variables used for checking guards. The names can be accessed fromguard.__code__.co_freevars, and the values are stored inguard.__closure__. These include names like___guarded_code/___is_grad_enabled/___are_deterministic_algorithms_enabled/___is_torch_function_enabled/utils_device/___check_tensors/tensor_check_names.\nguard.__code__.co_freevars\nguard.__closure__\n___guarded_code\n___is_grad_enabled\n___are_deterministic_algorithms_enabled\n___is_torch_function_enabled\nutils_device\n___check_tensors\ntensor_check_names\nArgumentLof theguardfunction. This is a dict mapping the name of arguments oftoy_exampleto its values. This is only available when the function is called, where the frame evaluation API comes into play. In short,Lis adictwith structure of{'a':value_a,'b':value_b}. Therefore, you can see the code usesL['a']to refer to the input variablea.\nL\nguard\ntoy_example\nL\ndict\n{'a':value_a,'b':value_b}\nL['a']\na\nThe graph break is shown in the code of compiledtoy_example, where we have to use Python interpreter to select the following graph to execute.\ntoy_example\nNote that we pass a simplemy_compilerfunction as the backend compiler, therefore the subgraph code__resume_at_38_2,__resume_at_30_1, and__compiled_fn_0remain Python code. This can also be inspected (please ignore the function name, and only use the function signature and function body code):\nmy_compiler\n__resume_at_38_2\n__resume_at_30_1\n__compiled_fn_0\n\n```python\nprint(\"source code of __compiled_fn_0:\")\nprint(innermost_fn(__compiled_fn_0).__self__.code)\nprint(\"=\" * 60)\nprint(\"source code of __resume_at_30_1:\")\nprint(decompile(__resume_at_30_1))\nprint(\"=\" * 60)\nprint(\"source code of __resume_at_38_2:\")\nprint(decompile(__resume_at_38_2))\n\n```\n\n\n```python\nsource code of __compiled_fn_0:\ndef forward(self, L_a_ : torch.Tensor, L_b_ : torch.Tensor):\n    l_a_ = L_a_\n    l_b_ = L_b_\n    abs_1 = torch.abs(l_a_)\n    add = abs_1 + 1;  abs_1 = None\n    truediv = l_a_ / add;  l_a_ = add = None\n    sum_1 = l_b_.sum();  l_b_ = None\n    lt = sum_1 < 0;  sum_1 = None\n    return (truediv, lt)\n# To see more debug info, please use ``graph_module.print_readable()``\n============================================================\nsource code of __resume_at_30_1:\ndef <resume in toy_example>(b, x):\n    b = b * -1\n    return x * b\n============================================================\nsource code of __resume_at_38_2:\ndef <resume in toy_example>(b, x):\n    return x * b\n\n```\n\nHowever, if we use other backends like the built-ininductor, the subgraph code will be compiled CUDA kernels for GPU or C++ code for CPU.\ninductor\nTo summarize, the compiled code is conceptually equivalent to the code below:\n\n```python\ndef compiled_example(a, b):\n    L = {'a': a, 'b': b}\n    for guard, code in get_cache_entries():\n        if guard(L):\n            return code(a, b)\n    recompile_and_add_another_cache_entry()\n\n```\n\nThe following diagram demonstrates howtorch.compiletransforms and optimizes user-written code: it first extracts computation graphs from the user-written function, and compiles these graphs into optimized functions, then assembles them into a new function, which is functionally equivalent to the user-written code but optimized to have a good computation speed.\ntorch.compile\nTo learn more about how all this is implemented internally, seeDynamo Deep-Dive.",
    "url": "https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "93a3dfb7effc369dad36bffbea01c479",
    "source": "pytorch_docs",
    "title": "Autograd mechanics \u2014 PyTorch 2.9 documentation",
    "text": "\n## Autograd mechanics#\n\nCreated On: Jan 16, 2017 | Last Updated On: Jun 16, 2025\nThis note will present an overview of how autograd works and records the\noperations. It\u2019s not strictly necessary to understand all this, but we recommend\ngetting familiar with it, as it will help you write more efficient, cleaner\nprograms, and can aid you in debugging.\n\n## How autograd encodes the history#\n\nAutograd is a reverse automatic differentiation system.  Conceptually,\nautograd records a graph recording all of the operations that created\nthe data as you execute operations, giving you a directed acyclic graph\nwhose leaves are the input tensors and roots are the output tensors.\nBy tracing this graph from roots to leaves, you can automatically\ncompute the gradients using the chain rule.\nInternally, autograd represents this graph as a graph ofFunctionobjects (really expressions), which can beapply()ed to compute the result of\nevaluating the graph.  When computing the forward pass, autograd\nsimultaneously performs the requested computations and builds up a graph\nrepresenting the function that computes the gradient (the.grad_fnattribute of eachtorch.Tensoris an entry point into this graph).\nWhen the forward pass is completed, we evaluate this graph in the\nbackwards pass to compute the gradients.\nFunction\napply()\n.grad_fn\ntorch.Tensor\nAn important thing to note is that the graph is recreated from scratch at every\niteration, and this is exactly what allows for using arbitrary Python control\nflow statements, that can change the overall shape and size of the graph at\nevery iteration. You don\u2019t have to encode all possible paths before you\nlaunch the training - what you run is what you differentiate.\n\n## Saved tensors#\n\nSome operations need intermediary results to be saved during the forward pass\nin order to execute the backward pass. For example, the functionx\u21a6x2x\\mapsto x^2x\u21a6x2saves the inputxxxto compute the gradient.\nWhen defining a custom PythonFunction, you can usesave_for_backward()to save\ntensors during the forward pass andsaved_tensorsto retrieve them\nduring the backward pass. SeeExtending PyTorchfor more information.\nFunction\nsave_for_backward()\nsaved_tensors\nFor operations that PyTorch defines (e.g.torch.pow()), tensors are\nautomatically saved as needed. You can explore (for educational or debugging\npurposes) which tensors are saved by a certaingrad_fnby looking for its\nattributes starting with the prefix_saved.\ntorch.pow()\ngrad_fn\n_saved\n\n```python\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\nprint(x.equal(y.grad_fn._saved_self))  # True\nprint(x is y.grad_fn._saved_self)  # True\n\n```\n\nIn the previous code,y.grad_fn._saved_selfrefers to the same Tensor object asx.\nBut that may not always be the case. For instance:\ny.grad_fn._saved_self\n\n```python\nx = torch.randn(5, requires_grad=True)\ny = x.exp()\nprint(y.equal(y.grad_fn._saved_result))  # True\nprint(y is y.grad_fn._saved_result)  # False\n\n```\n\nUnder the hood, to prevent reference cycles, PyTorch haspackedthe tensor\nupon saving andunpackedit into a different tensor for reading. Here, the\ntensor you get from accessingy.grad_fn._saved_resultis a different tensor\nobject thany(but they still share the same storage).\ny.grad_fn._saved_result\ny\nWhether a tensor will be packed into a different tensor object depends on\nwhether it is an output of its owngrad_fn, which is an implementation detail\nsubject to change and that users should not rely on.\nYou can control how PyTorch does packing / unpacking withHooks for saved tensors.\n\n## Gradients for non-differentiable functions#\n\nThe gradient computation using Automatic Differentiation is only valid when each elementary function being used is differentiable.\nUnfortunately many of the functions we use in practice do not have this property (reluorsqrtat0, for example).\nTo try and reduce the impact of functions that are non-differentiable, we define the gradients of the elementary operations by applying the following rules in order:\nrelu\nsqrt\n0\nIf the function is differentiable and thus a gradient exists at the current point, use it.\nIf the function is convex (at least locally), use the sub-gradient of minimum norm.\nIf the function is concave (at least locally), use the super-gradient of minimum norm (consider-f(x)and apply the previous point).\nIf the function is defined, define the gradient at the current point by continuity (note thatinfis possible here, for example forsqrt(0)). If multiple values are possible, pick one arbitrarily.\ninf\nsqrt(0)\nIf the function is not defined (sqrt(-1),log(-1)or most functions when the input isNaN, for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will useNaNas the gradient, but for performance reasons, some functions will use other values (log(-1), for example).\nsqrt(-1)\nlog(-1)\nNaN\nNaN\nlog(-1)\nIf the function is not a deterministic mapping (i.e. it is not amathematical function), it will be marked as non-differentiable. This will make it error out in the backward if used on tensors that require grad outside of ano_gradenvironment.\nno_grad\n\n## Division by Zero in Autograd#\n\nWhen performing division by zero in PyTorch (e.g.,x/0), the forward pass will produceinfvalues following IEEE-754 floating point arithmetic. While theseinfvalues can be masked out before computing the final loss (e.g., via indexing or masking), the autograd system still tracks and differentiates through the full computation graph, including the division by zero operation.\nx/0\ninf\ninf\nDuring backpropagation, this can lead to problematic gradient expressions. For example:\n\n```python\nx = torch.tensor([1., 1.], requires_grad=True)\ndiv = torch.tensor([0., 1.])\n\ny = x / div          # Results in [inf, 1]\nmask = div != 0      # [False, True]\nloss = y[mask].sum()\nloss.backward()\nprint(x.grad)        # [nan, 1], not [0, 1]\n\n```\n\nIn this example, even though we only use the masked output (which excludes the division by zero), autograd still computes gradients through the full computation graph, including the division by zero operation. This results innangradients for the masked elements, which can cause training instability.\nnan\nTo avoid this issue, there are several recommended approaches:\nMask before division:\n\n```python\nx = torch.tensor([1., 1.], requires_grad=True)\ndiv = torch.tensor([0., 1.])\n\nmask = div != 0\nsafe = torch.zeros_like(x)\nsafe[mask] = x[mask] / div[mask]\nloss = safe.sum()\nloss.backward()      # Produces safe gradients [0, 1]\n\n```\n\nUse MaskedTensor (experimental API):\n\n```python\nfrom torch.masked import as_masked_tensor\n\nx = torch.tensor([1., 1.], requires_grad=True)\ndiv = torch.tensor([0., 1.])\n\ny = x / div\nmask = div != 0\nloss = as_masked_tensor(y, mask).sum()\nloss.backward()      # Cleanly handles \"undefined\" vs \"zero\" gradients\n\n```\n\nThe key principle is to prevent the division by zero operation from being recorded in the computation graph, rather than masking its results after the fact. This ensures that autograd only computes gradients through valid operations.\nThis behavior is important to keep in mind when working with operations that might produceinfornanvalues, as masking the outputs does not prevent the problematic gradients from being computed.\ninf\nnan\n\n## Locally disabling gradient computation#\n\nThere are several mechanisms available from Python to locally disable gradient\ncomputation:\nTo disable gradients across entire blocks of code, there are context managers\nlike no-grad mode and inference mode.\nFor more fine-grained exclusion of subgraphs from gradient computation,\nthere is setting therequires_gradfield of a tensor.\nrequires_grad\nBelow, in addition to discussing the mechanisms above, we also describe\nevaluation mode (nn.Module.eval()), a method that is not used\nto disable gradient computation but, because of its name, is often mixed up with the three.\nnn.Module.eval()\n\n## Settingrequires_grad#\n\nrequires_grad\nrequires_gradis a flag, defaulting to falseunless wrapped\nin ann.Parameter, that allows for fine-grained exclusion of\nsubgraphs from gradient computation. It takes effect in both the\nforward and backward passes:\nrequires_grad\nnn.Parameter\nDuring the forward pass, an operation is only recorded in the backward graph if\nat least one of its input tensors require grad.\nDuring the backward pass (.backward()), only leaf tensors withrequires_grad=Truewill have gradients accumulated into their.gradfields.\n.backward()\nrequires_grad=True\n.grad\nIt is important to note that even though every tensor has this flag,settingit only makes sense for leaf tensors (tensors that do not have agrad_fn, e.g., ann.Module\u2019s parameters).\nNon-leaf tensors (tensors that do havegrad_fn) are tensors that have a\nbackward graph associated with them. Thus their gradients will be needed\nas an intermediary result to compute the gradient for a leaf tensor that\nrequires grad. From this definition, it is clear that all non-leaf tensors\nwill automatically haverequire_grad=True.\ngrad_fn\nnn.Module\ngrad_fn\nrequire_grad=True\nSettingrequires_gradshould be the main way you control which parts\nof the model are part of the gradient computation, for example, if you need to\nfreeze parts of your pretrained model during model fine-tuning.\nrequires_grad\nTo freeze parts of your model, simply apply.requires_grad_(False)to\nthe parameters that you don\u2019t want updated. And as described above,\nsince computations that use these parameters as inputs would not be recorded in\nthe forward pass, they won\u2019t have their.gradfields updated in the backward\npass because they won\u2019t be part of the backward graph in the first place, as\ndesired.\n.requires_grad_(False)\n.grad\nBecause this is such a common pattern,requires_gradcan also be set at\nthe module level withnn.Module.requires_grad_().\nWhen applied to a module,.requires_grad_()takes effect on all\nof the module\u2019s parameters (which haverequires_grad=Trueby default).\nrequires_grad\nnn.Module.requires_grad_()\n.requires_grad_()\nrequires_grad=True\n\n## Grad Modes#\n\nApart from settingrequires_gradthere are also three grad modes that can\nbe selected from Python that can affect how computations in PyTorch are\nprocessed by autograd internally: default mode (grad mode), no-grad mode,\nand inference mode, all of which can be togglable via context managers and\ndecorators.\nrequires_grad\nMode\nExcludes operations from being recorded in backward graph\nSkips additional autograd tracking overhead\nTensors created while the mode is enabled can be used in grad-mode later\nExamples\ndefault\n\u2713\nForward pass\nno-grad\n\u2713\n\u2713\nOptimizer updates\ninference\n\u2713\n\u2713\nData processing, model evaluation\n\n## Default Mode (Grad Mode)#\n\nThe \u201cdefault mode\u201d is the mode we are implicitly in when no other modes like\nno-grad and inference mode are enabled. To be contrasted with\n\u201cno-grad mode\u201d the default mode is also sometimes called \u201cgrad mode\u201d.\nThe most important thing to know about the default mode is that it is the only\nmode in whichrequires_gradtakes effect.requires_gradis always overridden\nto beFalsein both the two other modes.\nrequires_grad\nrequires_grad\nFalse\n\n## No-grad Mode#\n\nComputations in no-grad mode behave as if none of the inputs require grad.\nIn other words, computations in no-grad mode are never recorded in the backward graph\neven if there are inputs that haverequire_grad=True.\nrequire_grad=True\nEnable no-grad mode when you need to perform operations that should not be\nrecorded by autograd, but you\u2019d still like to use the outputs of these\ncomputations in grad mode later. This context manager makes it convenient to\ndisable gradients for a block of code or function without\nhaving to temporarily set tensors to haverequires_grad=False, and then\nback toTrue.\nrequires_grad=False\nTrue\nFor example, no-grad mode might be useful when writing an optimizer: when\nperforming the training update you\u2019d like to update parameters\nin-place without the update being recorded by autograd.\nYou also intend to use the updated parameters for computations in\ngrad mode in the next forward pass.\nThe implementations intorch.nn.initalso\nrely on no-grad mode when initializing the parameters as to avoid\nautograd tracking when updating the initialized parameters in-place.\n\n## Inference Mode#\n\nInference mode is the extreme version of no-grad mode. Just like in no-grad\nmode, computations in inference mode are not recorded in the backward graph, but\nenabling inference mode will allow PyTorch to speed up your model even more.\nThis better runtime comes with a drawback: tensors created in inference mode\nwill not be able to be used in computations to be recorded by autograd after\nexiting inference mode.\nEnable inference mode when you are performing computations that do not have\ninteractions with autograd, AND you don\u2019t plan on using the tensors created\nin inference mode in any computation that is to be recorded by autograd later.\nIt is recommended that you try out inference mode in the parts of your code\nthat do not require autograd tracking (e.g., data processing and model evaluation).\nIf it works out of the box\nfor your use case it\u2019s a free performance win. If you run into errors after\nenabling inference mode, check that you are not using tensors created in\ninference mode in computations that are recorded by autograd after exiting inference\nmode. If you cannot avoid such use in your case, you can always switch back\nto no-grad mode.\nFor details on inference mode please seeInference Mode.\nFor implementation details of inference mode seeRFC-0011-InferenceMode.\n\n## Evaluation Mode (nn.Module.eval())#\n\nnn.Module.eval()\nEvaluation mode is not a mechanism to locally disable gradient computation.\nIt is included here anyway because it is sometimes confused to be such a mechanism.\nFunctionally,module.eval()(or equivalentlymodule.train(False)) are completely\northogonal to no-grad mode and inference mode. Howmodel.eval()affects\nyour model depends entirely on the specific modules used in your model and\nwhether they define any training-mode specific behavior.\nmodule.eval()\nmodule.train(False)\nmodel.eval()\nYou are responsible for callingmodel.eval()andmodel.train()if your\nmodel relies on modules such astorch.nn.Dropoutandtorch.nn.BatchNorm2dthat may behave\ndifferently depending on training mode, for example, to avoid updating your\nBatchNorm running statistics on validation data.\nmodel.eval()\nmodel.train()\ntorch.nn.Dropout\ntorch.nn.BatchNorm2d\nIt is recommended that you always usemodel.train()when\ntraining andmodel.eval()when evaluating your model (validation/testing) even\nif you aren\u2019t sure your model has training-mode specific behavior, because a\nmodule you are using might be updated to behave differently in training and\neval modes.\nmodel.train()\nmodel.eval()\n\n## In-place operations with autograd#\n\nSupporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nlower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.\nThere are two main reasons that limit the applicability of in-place operations:\nIn-place operations can potentially overwrite values required to compute\ngradients.\nEvery in-place operation requires the implementation to rewrite the\ncomputational graph. Out-of-place versions simply allocate new objects and\nkeep references to the old graph, while in-place operations, require\nchanging the creator of all inputs to theFunctionrepresenting\nthis operation. This can be tricky, especially if there are many Tensors\nthat reference the same storage (e.g. created by indexing or transposing),\nand in-place functions will raise an error if the storage of\nmodified inputs is referenced by any otherTensor.\nFunction\nTensor\n\n## In-place correctness checks#\n\nEvery tensor keeps a version counter, that is incremented every time it is\nmarked dirty in any operation. When a Function saves any tensors for backward,\na version counter of their containing Tensor is saved as well. Once you accessself.saved_tensorsit is checked, and if it is greater than the saved value\nan error is raised. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.\nself.saved_tensors\n\n## Multithreaded Autograd#\n\nThe autograd engine is responsible for running all the backward operations\nnecessary to compute the backward pass. This section will describe all the details\nthat can help you make the best use of it in a multithreaded environment. (This is\nrelevant only for PyTorch 1.6+ as the behavior in previous version was different.)\nUser could train their model with multithreading code (e.g. Hogwild training), and\ndoes not block on the concurrent backward computations, example code could be:\n\n```python\n# Define a train function to be used in different threads\ndef train_fn():\n    x = torch.ones(5, 5, requires_grad=True)\n    # forward\n    y = (x + 3) * (x + 4) * 0.5\n    # backward\n    y.sum().backward()\n    # potential optimizer update\n\n\n# User write their own threading code to drive the train_fn\nthreads = []\nfor _ in range(10):\n    p = threading.Thread(target=train_fn, args=())\n    p.start()\n    threads.append(p)\n\nfor p in threads:\n    p.join()\n\n```\n\nNote that some behaviors that user should be aware of:\n\n## Concurrency on CPU#\n\nWhen you runbackward()orgrad()via python or C++ API in multiple\nthreads on CPU, you are expecting to see extra concurrency instead of\nserializing all the backward calls in a specific order during execution\n(behavior before PyTorch 1.6).\nbackward()\ngrad()\n\n## Non-determinism#\n\nIf you are callingbackward()from multiple threads concurrently and have\nshared inputs (i.e. Hogwild CPU training), then non-determinism should be expected.\nThis can occur because parameters are automatically shared across threads,\nas such, multiple threads may access and try to accumulate the same.gradattribute during gradient accumulation. This is technically not safe, and\nit might result in race condition and the result might be invalid to use.\nbackward()\n.grad\nUsers developing multithreaded models featuring shared parameters should have the\nthreading model in mind and should understand the issues described above.\nThe functional APItorch.autograd.grad()may be used to calculate the\ngradients instead ofbackward()to avoid non-determinism.\ntorch.autograd.grad()\nbackward()\n\n## Graph retaining#\n\nIf part of the autograd graph is shared between threads, i.e. run first\npart of forward single thread, then run second part in multiple threads,\nthen the first part of graph is shared. In this case different threads\nexecutegrad()orbackward()on the same graph might have issue of\ndestroying the graph on the fly of one thread, and the other thread will\ncrash in this case. Autograd will error out to the user similar to what callbackward()twice with outretain_graph=True, and let the user know\nthey should useretain_graph=True.\ngrad()\nbackward()\nbackward()\nretain_graph=True\nretain_graph=True\n\n## Thread Safety on Autograd Node#\n\nSince Autograd allows the caller thread to drive its backward execution for\npotential parallelism, it\u2019s important that we ensure thread safety on CPU with\nparallelbackward()calls that share part/whole of the GraphTask.\nbackward()\nCustom Pythonautograd.Functions are automatically thread safe because of GIL.\nFor built-in C++ Autograd Nodes (e.g. AccumulateGrad, CopySlices) and customautograd::Functions, the Autograd Engine uses thread mutex locking to ensure\nthread safety on autograd Nodes that might have state write/read.\nautograd.Function\nautograd::Function\n\n## No thread safety on C++ hooks#\n\nAutograd relies on the user to write thread safe C++ hooks. If you want the hook\nto be correctly applied in multithreading environment, you will need to write\nproper thread locking code to ensure the hooks are thread safe.\n\n## Autograd for Complex Numbers#\n\nThe short version:\nWhen you use PyTorch to differentiate any functionf(z)f(z)f(z)with complex domain and/or codomain,\nthe gradients are computed under the assumption that the function is a part of a larger real-valued\nloss functiong(input)=Lg(input)=Lg(input)=L. The gradient computed is\u2202L\u2202z\u2217\\frac{\\partial L}{\\partial z^*}\u2202z\u2217\u2202L\u200b(note the conjugation of z), the negative of which is precisely the direction of steepest descent\nused in Gradient Descent algorithm. Thus, there is a viable path in making the existing optimizers\nwork out of the box with complex parameters.\nThis convention matches TensorFlow\u2019s convention for complex\ndifferentiation, but is different from JAX (which computes\u2202L\u2202z\\frac{\\partial L}{\\partial z}\u2202z\u2202L\u200b).\nIf you have a real-to-real function which internally uses complex\noperations, the convention here doesn\u2019t matter: you will always get\nthe same result that you would have gotten if it had been implemented\nwith only real operations.\nIf you are curious about the mathematical details, or want to know how\nto define complex derivatives in PyTorch, read on.\n\n## What are complex derivatives?#\n\nThe mathematical definition of complex-differentiability takes the\nlimit definition of a derivative and generalizes it to operate on\ncomplex numbers. Consider a functionf:C\u2192Cf: \u2102 \u2192 \u2102f:C\u2192C,\nwhereuuuandvvvare two variable real valued functions\nandjjjis the imaginary unit.\nUsing the derivative definition, we can write:\nIn order for this limit to exist, not only mustuuuandvvvmust be\nreal differentiable, butfffmust also satisfy the Cauchy-Riemannequations.  In\nother words: the limit computed for real and imaginary steps (hhh)\nmust be equal. This is a more restrictive condition.\nThe complex differentiable functions are commonly known as holomorphic\nfunctions. They are well behaved, have all the nice properties that\nyou\u2019ve seen from real differentiable functions, but are practically of no\nuse in the optimization world. For optimization problems, only real valued objective\nfunctions are used in the research community since complex numbers are not part of any\nordered field and so having complex valued loss does not make much sense.\nIt also turns out that no interesting real-valued objective fulfill the\nCauchy-Riemann equations. So the theory with holomorphic function cannot be\nused for optimization and most people therefore use the Wirtinger calculus.\n\n## Wirtinger Calculus comes into the picture \u2026#\n\nSo, we have this great theory of complex differentiability and\nholomorphic functions, and we can\u2019t use any of it at all, because many\nof the commonly used functions are not holomorphic. What\u2019s a poor\nmathematician to do? Well, Wirtinger observed that even iff(z)f(z)f(z)isn\u2019t holomorphic, one could rewrite it as a two variable functionf(z,z\u2217)f(z, z*)f(z,z\u2217)which is always holomorphic. This is because real and\nimaginary of the components ofzzzcan be expressed in terms ofzzzandz\u2217z^*z\u2217as:\nWirtinger calculus suggests to studyf(z,z\u2217)f(z, z^*)f(z,z\u2217)instead, which is\nguaranteed to be holomorphic iffffwas real differentiable (another\nway to think of it is as a change of coordinate system, fromf(x,y)f(x, y)f(x,y)tof(z,z\u2217)f(z, z^*)f(z,z\u2217).)  This function has partial derivatives\u2202\u2202z\\frac{\\partial }{\\partial z}\u2202z\u2202\u200band\u2202\u2202z\u2217\\frac{\\partial}{\\partial z^{*}}\u2202z\u2217\u2202\u200b.\nWe can use the chain rule to establish a\nrelationship between these partial derivatives and the partial\nderivatives w.r.t., the real and imaginary components ofzzz.\nFrom the above equations, we get:\nwhich is the classic definition of Wirtinger calculus that you would find onWikipedia.\nThere are a lot of beautiful consequences of this change.\nFor one, the Cauchy-Riemann equations translate into simply saying that\u2202f\u2202z\u2217=0\\frac{\\partial f}{\\partial z^*} = 0\u2202z\u2217\u2202f\u200b=0(that is to say, the functionfffcan be written\nentirely in terms ofzzz, without making reference toz\u2217z^*z\u2217).\nAnother important (and somewhat counterintuitive) result, as we\u2019ll see later, is that when we do optimization on a real-valued loss, the step we should\ntake while making variable update is given by\u2202Loss\u2202z\u2217\\frac{\\partial Loss}{\\partial z^*}\u2202z\u2217\u2202Loss\u200b(not\u2202Loss\u2202z\\frac{\\partial Loss}{\\partial z}\u2202z\u2202Loss\u200b).\nFor more reading, check out:https://arxiv.org/pdf/0906.4835.pdf\n\n## How is Wirtinger Calculus useful in optimization?#\n\nResearchers in audio and other fields, more commonly, use gradient\ndescent to optimize real valued loss functions with complex variables.\nTypically, these people treat the real and imaginary values as separate\nchannels that can be updated. For a step size\u03b1/2\\alpha/2\u03b1/2and lossLLL, we can write the following equations inR2\u211d^2R2:\nHow do these equations translate into complex spaceC\u2102C?\nSomething very interesting has happened: Wirtinger calculus tells us\nthat we can simplify the complex variable update formula above to only\nrefer to the conjugate Wirtinger derivative\u2202L\u2202z\u2217\\frac{\\partial L}{\\partial z^*}\u2202z\u2217\u2202L\u200b, giving us exactly the step we take in optimization.\nBecause the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative\nwhen you differentiate a function with a real valued loss.\n\n## How does PyTorch compute the conjugate Wirtinger derivative?#\n\nTypically, our derivative formulas take ingrad_outputas an input,\nrepresenting the incoming Vector-Jacobian product that we\u2019ve already\ncomputed, aka,\u2202L\u2202s\u2217\\frac{\\partial L}{\\partial s^*}\u2202s\u2217\u2202L\u200b, whereLLLis the loss of the entire computation (producing a real loss) andsssis the output of our function. The goal here is to compute\u2202L\u2202z\u2217\\frac{\\partial L}{\\partial z^*}\u2202z\u2217\u2202L\u200b, wherezzzis the input of\nthe function.  It turns out that in the case of real loss, we can\nget away withonlycalculating\u2202L\u2202s\u2217\\frac{\\partial L}{\\partial s^*}\u2202s\u2217\u2202L\u200b,\neven though the chain rule implies that we also need to\nhave access to\u2202L\u2202s\\frac{\\partial L}{\\partial s}\u2202s\u2202L\u200b.  If you want\nto skip this derivation, look at the last equation in this section\nand then skip to the next section.\nLet\u2019s continue working withf:C\u2192Cf: \u2102 \u2192 \u2102f:C\u2192Cdefined asf(z)=f(x+yj)=u(x,y)+v(x,y)jf(z) = f(x+yj) = u(x, y) + v(x, y)jf(z)=f(x+yj)=u(x,y)+v(x,y)j. As discussed above,\nautograd\u2019s gradient convention is centered around optimization for real\nvalued loss functions, so let\u2019s assumefffis a part of larger\nreal valued loss functionggg. Using chain rule, we can write:\nNow using Wirtinger derivative definition, we can write:\nIt should be noted here that sinceuuuandvvvare real\nfunctions, andLLLis real by our assumption thatfffis a\npart of a real valued function, we have:\ni.e.,\u2202L\u2202s\\frac{\\partial L}{\\partial s}\u2202s\u2202L\u200bequals tograd_output\u2217grad\\_output^*grad_output\u2217.\nSolving the above equations for\u2202L\u2202u\\frac{\\partial L}{\\partial u}\u2202u\u2202L\u200band\u2202L\u2202v\\frac{\\partial L}{\\partial v}\u2202v\u2202L\u200b, we get:\nSubstituting(3)in(1), we get:\nUsing(2), we get:\nThis last equation is the important one for writing your own gradients,\nas it decomposes our derivative formula into a simpler one that is easy\nto compute by hand.\n\n## How can I write my own derivative formula for a complex function?#\n\nThe above boxed equation gives us the general formula for all\nderivatives on complex functions.  However, we still need to\ncompute\u2202s\u2202z\\frac{\\partial s}{\\partial z}\u2202z\u2202s\u200band\u2202s\u2202z\u2217\\frac{\\partial s}{\\partial z^*}\u2202z\u2217\u2202s\u200b.\nThere are two ways you could do this:\nThe first way is to just use the definition of Wirtinger derivatives directly and calculate\u2202s\u2202z\\frac{\\partial s}{\\partial z}\u2202z\u2202s\u200band\u2202s\u2202z\u2217\\frac{\\partial s}{\\partial z^*}\u2202z\u2217\u2202s\u200bby\nusing\u2202s\u2202x\\frac{\\partial s}{\\partial x}\u2202x\u2202s\u200band\u2202s\u2202y\\frac{\\partial s}{\\partial y}\u2202y\u2202s\u200b(which you can compute in the normal way).\nThe second way is to use the change of variables trick and rewritef(z)f(z)f(z)as a two variable functionf(z,z\u2217)f(z, z^*)f(z,z\u2217), and compute\nthe conjugate Wirtinger derivatives by treatingzzzandz\u2217z^*z\u2217as independent variables. This is often easier; for example, if the function in question is holomorphic, onlyzzzwill be used (and\u2202s\u2202z\u2217\\frac{\\partial s}{\\partial z^*}\u2202z\u2217\u2202s\u200bwill be zero).\nLet\u2019s consider the functionf(z=x+yj)=c\u2217z=c\u2217(x+yj)f(z = x + yj) = c * z = c * (x+yj)f(z=x+yj)=c\u2217z=c\u2217(x+yj)as an example, wherec\u2208Rc \\in \u211dc\u2208R.\nUsing the first way to compute the Wirtinger derivatives, we have.\nUsing(4), andgrad_output = 1.0(which is the default grad output value used whenbackward()is called on a scalar output in PyTorch), we get:\nbackward()\nUsing the second way to compute Wirtinger derivatives, we directly get:\nAnd using(4)again, we get\u2202L\u2202z\u2217=c\\frac{\\partial L}{\\partial z^*} = c\u2202z\u2217\u2202L\u200b=c. As you can see, the second way involves lesser calculations, and comes\nin more handy for faster calculations.\n\n## What about cross-domain functions?#\n\nSome functions map from complex inputs to real outputs, or vice versa.\nThese functions form a special case of(4), which we can derive using the\nchain rule:\nForf:C\u2192Rf: \u2102 \u2192 \u211df:C\u2192R, we get:\nForf:R\u2192Cf: \u211d \u2192 \u2102f:R\u2192C, we get:\n\n## Hooks for saved tensors#\n\nYou can controlhow saved tensors are packed / unpackedby defining a pair ofpack_hook/unpack_hookhooks.  Thepack_hookfunction should take a tensor as its single argument\nbut can return any python object (e.g. another tensor, a tuple, or even a\nstring containing a filename). Theunpack_hookfunction takes as its single\nargument the output ofpack_hookand should return a tensor to be used in\nthe backward pass. The tensor returned byunpack_hookonly needs to have\nthe same content as the tensor passed as input topack_hook. In particular,\nany autograd-related metadata can be ignored as they will be overwritten during\nunpacking.\npack_hook\nunpack_hook\npack_hook\nunpack_hook\npack_hook\nunpack_hook\npack_hook\nAn example of such pair is:\n\n```python\nclass SelfDeletingTempFile():\n    def __init__(self):\n        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))\n\n    def __del__(self):\n        os.remove(self.name)\n\ndef pack_hook(tensor):\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(temp_file):\n    return torch.load(temp_file.name)\n\n```\n\nNotice that theunpack_hookshould not delete the temporary file because it\nmight be called multiple times: the temporary file should be alive for as long\nas the returnedSelfDeletingTempFileobject is alive.  In the above example,\nwe prevent leaking the temporary file by closing it when it is no longer needed\n(on deletion of theSelfDeletingTempFileobject).\nunpack_hook\nNote\nWe guarantee thatpack_hookwill only be called once butunpack_hookcan\nbe called as many times as the backward pass requires it and we expect it to\nreturn the same data each time.\npack_hook\nunpack_hook\nWarning\nPerforming inplace operations on the input of any of the functions is forbidden\nas they may lead to unexpected side-effects. PyTorch will throw an error if the\ninput to a pack hook is modified inplace but does not catch the case where the\ninput to an unpack hook is modified inplace.\n\n## Registering hooks for a saved tensor#\n\nYou can register a pair of hooks on a saved tensor by calling theregister_hooks()method on aSavedTensorobject. Those objects are exposed as attributes of agrad_fnand start with the_raw_saved_prefix.\nregister_hooks()\nSavedTensor\ngrad_fn\n_raw_saved_\n\n```python\nx = torch.randn(5, requires_grad=True)\ny = x.pow(2)\ny.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)\n\n```\n\nThepack_hookmethod is called as soon as the pair is registered.\nTheunpack_hookmethod is called each time the saved tensor needs to be\naccessed, either by means ofy.grad_fn._saved_selfor during the backward\npass.\npack_hook\nunpack_hook\ny.grad_fn._saved_self\nWarning\nIf you maintain a reference to aSavedTensorafter the saved\ntensors have been released (i.e. after backward has been called), calling\nitsregister_hooks()is forbidden.\nPyTorch will throw an error most of the time but it may fail\nto do so in some cases and undefined behavior may arise.\nSavedTensor\nregister_hooks()\n\n## Registering default hooks for saved tensors#\n\nAlternatively, you can use the context-managersaved_tensors_hooksto register a pair of\nhooks which will be applied toallsaved tensors that are created in\nthat context.\nsaved_tensors_hooks\nExample:\n\n```python\n# Only save on disk tensors that have size >= 1000\nSAVE_ON_DISK_THRESHOLD = 1000\n\ndef pack_hook(x):\n    if x.numel() < SAVE_ON_DISK_THRESHOLD:\n        return x.detach()\n    temp_file = SelfDeletingTempFile()\n    torch.save(tensor, temp_file.name)\n    return temp_file\n\ndef unpack_hook(tensor_or_sctf):\n    if isinstance(tensor_or_sctf, torch.Tensor):\n        return tensor_or_sctf\n    return torch.load(tensor_or_sctf.name)\n\nclass Model(nn.Module):\n    def forward(self, x):\n        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n          # ... compute output\n          output = x\n        return output\n\nmodel = Model()\nnet = nn.DataParallel(model)\n\n```\n\nThe hooks defined with this context manager are thread-local.\nHence, the following code will not produce the desired effects because the hooks do not go\nthroughDataParallel.\n\n```python\n# Example what NOT to do\n\nnet = nn.DataParallel(model)\nwith torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n    output = net(input)\n\n```\n\nNote that using those hooks disables all the optimization in place to reduce\nTensor object creation. For example:\n\n```python\nwith torch.autograd.graph.saved_tensors_hooks(lambda x: x.detach(), lambda x: x):\n    x = torch.randn(5, requires_grad=True)\n    y = x * x\n\n```\n\nWithout the hooks,x,y.grad_fn._saved_selfandy.grad_fn._saved_otherall refer to the same tensor object.\nWith the hooks, PyTorch will pack and unpackxinto two new tensor objects\nthat share the same storage with the originalx(no copy performed).\nx\ny.grad_fn._saved_self\ny.grad_fn._saved_other\n\n## Backward Hooks execution#\n\nThis section will discuss when different hooks fire or don\u2019t fire.\nThen it will discuss the order in which they are fired.\nThe hooks that will be covered are: backward hooks registered to Tensor viatorch.Tensor.register_hook(), post-accumulate-grad hooks registered to\nTensor viatorch.Tensor.register_post_accumulate_grad_hook(), post-hooks\nregistered to Node viatorch.autograd.graph.Node.register_hook(), and\npre-hooks registered to Node viatorch.autograd.graph.Node.register_prehook().\ntorch.Tensor.register_hook()\ntorch.Tensor.register_post_accumulate_grad_hook()\ntorch.autograd.graph.Node.register_hook()\ntorch.autograd.graph.Node.register_prehook()\n\n## Whether a particular hook will be fired#\n\nHooks registered to a Tensor viatorch.Tensor.register_hook()are executed when gradients are being computed for that Tensor. (Note that this does not require\nthe Tensor\u2019s grad_fn to be executed. For example, if the Tensor is passed\nas part of theinputsargument totorch.autograd.grad(),\nthe Tensor\u2019s grad_fn may not be executed, but the hook register to that Tensor will always be executed.)\ntorch.Tensor.register_hook()\ninputs\ntorch.autograd.grad()\nHooks registered to a Tensor viatorch.Tensor.register_post_accumulate_grad_hook()are executed after the gradients have been accumulated for that Tensor, meaning the\nTensor\u2019s grad field has been set. Whereas hooks registered viatorch.Tensor.register_hook()are run as gradients are being computed, hooks registered viatorch.Tensor.register_post_accumulate_grad_hook()are only triggered once the Tensor\u2019s grad field is updated by autograd at the end of\nthe backward pass. Thus, post-accumulate-grad hooks can only be registered for leaf\nTensors. Registering a hook viatorch.Tensor.register_post_accumulate_grad_hook()on a non-leaf Tensor will error, even if you callbackward(retain_graph=True).\ntorch.Tensor.register_post_accumulate_grad_hook()\ntorch.Tensor.register_hook()\ntorch.Tensor.register_post_accumulate_grad_hook()\ntorch.Tensor.register_post_accumulate_grad_hook()\nHooks registered totorch.autograd.graph.Nodeusingtorch.autograd.graph.Node.register_hook()ortorch.autograd.graph.Node.register_prehook()are only fired if\nthe Node it was registered to is executed.\ntorch.autograd.graph.Node\ntorch.autograd.graph.Node.register_hook()\ntorch.autograd.graph.Node.register_prehook()\nWhether a particular Node is executed may depend on whether the backward pass was called withtorch.autograd.grad()ortorch.autograd.backward().\nSpecifically, you should be aware of these differences when you register a hook on a\nNode corresponding to a Tensor that you are passing totorch.autograd.grad()ortorch.autograd.backward()as part of theinputsargument.\ntorch.autograd.grad()\ntorch.autograd.backward()\ntorch.autograd.grad()\ntorch.autograd.backward()\ninputs\nIf you are usingtorch.autograd.backward(), all of the above mentioned hooks will be executed,\nwhether or not you specified theinputsargument. This is because.backward()executes all\nNodes, even if they correspond to a Tensor specified as an input.\n(Note that the execution of this additional Node corresponding to Tensors passed asinputsis usually unnecessary, but done anyway. This behavior is subject to change;\nyou should not depend on it.)\ntorch.autograd.backward()\ninputs\ninputs\nOn the other hand, if you are usingtorch.autograd.grad(), the backward hooks registered\nto Nodes that correspond to the Tensors passed toinputmay not be executed, because\nthose Nodes will not be executed unless there is another input that depends on the gradient\nresult of this Node.\ntorch.autograd.grad()\ninput\n\n## The order in which the different hooks are fired#\n\nThe order in which things happen are:\nhooks registered to Tensor are executed\npre-hooks registered to Node are executed (if Node is executed).\nthe.gradfield is updated for Tensors that retain_grad\n.grad\nNode is executed (subject to rules above)\nfor leaf Tensors that have.gradaccumulated, post-accumulate-grad hooks are executed\n.grad\npost-hooks registered to Node are executed (if Node is executed)\nIf multiple hooks of the same type are registered on the same Tensor or Node\nthey are executed in the order in which they are registered.\nHooks that are executed later can observe the modifications to the gradient made by\nearlier hooks.\n\n## Special hooks#\n\ntorch.autograd.graph.register_multi_grad_hook()is implemented using hooks registered\nto Tensors. Each individual Tensor hook is fired following the Tensor hook ordering\ndefined above and the registered multi-grad hook is called when the last Tensor gradient\nis computed.\ntorch.autograd.graph.register_multi_grad_hook()\ntorch.nn.modules.module.register_module_full_backward_hook()is implemented using hooks\nregistered to Node. As the forward is computed, hooks are registered to grad_fn corresponding\nto the inputs and outputs of the module. Because a module may take multiple inputs and return\nmultiple outputs, a dummy custom autograd Function is first applied to the inputs of the module\nbefore forward and the outputs of the module before the output of forward is returned to ensure\nthat those Tensors share a single grad_fn, which we can then attach our hooks to.\ntorch.nn.modules.module.register_module_full_backward_hook()\n\n## Behavior of Tensor hooks when Tensor is modified in-place#\n\nUsually hooks registered to a Tensor receive the gradient of the outputs with respect to that\nTensor, where the value of the Tensor is taken to be its value at the time backward is computed.\nHowever, if you register hooks to a Tensor, and then modify that Tensor in-place, hooks\nregistered before in-place modification similarly receive gradients of the outputs with\nrespect to the Tensor, but the value of the Tensor is taken to be its value before\nin-place modification.\nIf you prefer the behavior in the former case,\nyou should register them to the Tensor after all in-place modifications to it have been made.\nFor example:\n\n```python\nt = torch.tensor(1., requires_grad=True).sin()\nt.cos_()\nt.register_hook(fn)\nt.backward()\n\n```\n\nFurthermore, it can be helpful to know that under the hood,\nwhen hooks are registered to a Tensor, they actually become permanently bound to the grad_fn\nof that Tensor, so if that Tensor is then modified in-place,\neven though the Tensor now has a new grad_fn, hooks registered before it was\nmodified in-place will continue to be associated with the old grad_fn, e.g. they will\nfire when that Tensor\u2019s old grad_fn is reached in the graph by the autograd engine.",
    "url": "https://pytorch.org/docs/stable/notes/autograd.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "03c09f8827c1771967c320ccc1223aa6",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/accelerator/index.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3231da8536d879231bf70fbbbf889707",
    "source": "pytorch_docs",
    "title": "Tensor Parallelism - torch.distributed.tensor.parallel \u2014 PyTorch 2.9 documentation",
    "text": "\n## Tensor Parallelism - torch.distributed.tensor.parallel#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jun 13, 2025\nTensor Parallelism(TP) is built on top of the PyTorch DistributedTensor\n(DTensor)[https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md]\nand provides different parallelism styles: Colwise, Rowwise, and Sequence Parallelism.\nWarning\nTensor Parallelism APIs are experimental and subject to change.\nThe entrypoint to parallelize yournn.Moduleusing Tensor Parallelism is:\nnn.Module\nApply Tensor Parallelism in PyTorch by parallelizing modules or sub-modules based on a user-specified plan.\nWe parallelize module or sub_modules based on a parallelize_plan. The parallelize_plan containsParallelStyle, which indicates how user wants the module or sub_module\nto be parallelized.\nParallelStyle\nUser can also specify different parallel style per module fully qualified name (FQN).\nNote thatparallelize_moduleonly accepts a 1-DDeviceMesh, if you have a 2-D or N-DDeviceMesh,\nslice the DeviceMesh to a 1-D sub DeviceMesh first then pass to this API(i.e.device_mesh[\"tp\"])\nparallelize_module\nDeviceMesh\nDeviceMesh\ndevice_mesh[\"tp\"]\nmodule(nn.Module) \u2013 Module to be parallelized.\nnn.Module\ndevice_mesh(DeviceMesh, optional) \u2013 Object which describes the mesh topology of devices for the DTensor.\nIf not specified, the call must be under a DeviceMesh context.\nDeviceMesh\nparallelize_plan(Union[ParallelStyle, Dict[str,ParallelStyle]], optional) \u2013 The plan used to parallelize the module. It can be either aParallelStyleobject which contains how we prepare\ninput/output for Tensor Parallelism or it can be a dict of module\nFQN and its correspondingParallelStyleobject. If not\nspecified, the call will do nothing at the moment.\nParallelStyle\nParallelStyle\nParallelStyle\nParallelStyle\nsrc_data_rank(int,optional) \u2013 the rank of the source data for the logical/global tensor, it is used bydistribute_tensor()to scatter/broadcast the shards/replicas to other ranks. By default,\nwe usegroup_rank=0on each DeviceMesh dimension as the source data to preserve the single-device\nsemantic. If passingNoneexplicitly,parallelize_module()simply uses its local data instead\nof trying to preserve the single-device semantic via scatter/broadcast. Default: 0\ndistribute_tensor()\ngroup_rank=0\nNone\nparallelize_module()\nAnn.Moduleobject parallelized.\nnn.Module\nModule\n\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>>\n>>> # Define the module.\n>>> m = Model(...)\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>> m = parallelize_module(m, tp_mesh, {\"w1\": ColwiseParallel(), \"w2\": RowwiseParallel()})\n>>>\n\n```\n\nNote\nFor complex module architecture like Attention, MLP layers, we recommend composing\ndifferent ParallelStyles together (i.e.ColwiseParallelandRowwiseParallel) and pass\nas a parallelize_plan, to achieves the desired sharding computation.\nColwiseParallel\nRowwiseParallel\nTensor Parallelism supports the following parallel styles:\nPartition a compatible nn.Module in a column-wise fashion. Currently supports nn.Linear and nn.Embedding.\nUsers can compose it together with RowwiseParallel to achieve the sharding of more complicated modules.\n(i.e. MLP, Attention)\ninput_layouts(Placement,optional) \u2013 The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to\nbecome a DTensor. If not specified, we assume the input tensor to be replicated.\noutput_layouts(Placement,optional) \u2013 The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module\nwith the user desired layout. If not specified, the output tensor is sharded on the last dimension.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module output, default: True.\ntorch.Tensor\nDTensor\nAParallelStyleobject that represents Colwise sharding of the nn.Module.\nParallelStyle\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> m = Model(...)  # m is a nn.Module that contains a \"w1\" nn.Linear submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # By default, the input of the \"w1\" Linear will be converted to Replicated DTensor\n>>> # and the output of \"w1\" will return :class:`torch.Tensor` that shards on the last dim.\n>>>\n>>> sharded_mod = parallelize_module(m, tp_mesh, {\"w1\": ColwiseParallel()})\n>>> ...\n\n```\n\nNote\nBy defaultColwiseParalleloutput is sharded on the last dimension if theoutput_layoutsnot\nspecified, if there\u2019re operators that require specific tensor shape (i.e. before the pairedRowwiseParallel),\nkeep in mind that if the output is sharded the operator might need to be adjusted to the sharded size.\nColwiseParallel\noutput_layouts\nRowwiseParallel\nPartition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding.\nUsers can compose it with ColwiseParallel to achieve the sharding of more complicated modules.\n(i.e. MLP, Attention)\ninput_layouts(Placement,optional) \u2013 The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to\nbecome a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.\noutput_layouts(Placement,optional) \u2013 The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module\nwith the user desired layout. If not specified, the output tensor is replicated.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module output, default: True.\ntorch.Tensor\nDTensor\nAParallelStyleobject that represents Rowwise sharding of the nn.Module.\nParallelStyle\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, RowwiseParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> m = Model(...)  # m is a nn.Module that contains a \"w2\" nn.Linear submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # By default, the input of the \"w2\" Linear will be converted to DTensor that shards on the last dim\n>>> # and the output of \"w2\" will return a replicated :class:`torch.Tensor`.\n>>>\n>>> sharded_mod = parallelize_module(m, tp_mesh, {\"w2\": RowwiseParallel()}),\n>>> ...\n\n```\n\nSequenceParallel replicates a compatiblenn.Moduleparameters and runs the sharded computation with\ninput sharded on the sequence dimension. This currently supportsnn.LayerNorm,nn.Dropout, and theRMSNorm python implementation\nnn.Module\nnn.LayerNorm\nnn.Dropout\nThis style implements the operation that is described in the paperReducing Activation Recomputation in Large Transformer Models\nIf the input passed in to thisnn.Moduleis atorch.Tensor, it assumes that the input is already sharded\non the sequence dimension and converts the input to aDTensorsharded on the sequence dimension. If the input\npassed in to thisnn.Moduleis already aDTensorbut is not sharded on the sequence dimension, it would\nredistribute the input to be sharded on the sequence dimension.\nnn.Module\ntorch.Tensor\nDTensor\nnn.Module\nDTensor\nThe output of thenn.Modulewill be sharded on the sequence dimension.\nnn.Module\nsequence_dim(int,optional) \u2013 The sequence dimension of the input tensor for thenn.Module, this is used to annotate the input tensor to\nbecome a DTensor that is sharded on the sequence dimension, default: 1.\nnn.Module\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module output, default: False.\ntorch.Tensor\nDTensor\nAParallelStyleobject that represents Sequence Parallel of thenn.Module.\nParallelStyle\nnn.Module\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, SequenceParallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> m = Model(...)  # m is a nn.Module that contains a \"norm\" nn.LayerNorm submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # By default, the input of the \"norm\" will be converted to DTensor that shards on the sequence dim\n>>> # and the output of \"norm\" will return a sharded on sequence dimension :class:`DTensor`.\n>>>\n>>> sharded_mod = parallelize_module(m, tp_mesh, {\"norm\": SequenceParallel()}),\n>>> ...\n\n```\n\nNote\nSequenceParallel style assumes ones initialization if there are weights in the nn.Module (i.e.nn.LayerNormorRMSNorm, and they by default have ones initialization). If you have custom\ninits for the weights on those modules, you need to broadcast the weights before/after parallelizing\nto ensure that they are replicated.\nnn.LayerNorm\nRMSNorm\nTo simply configure the nn.Module\u2019s inputs and outputs with DTensor layouts\nand perform necessary layout redistributions, without distribute the module\nparameters to DTensors, the followingParallelStyles can be used in\ntheparallelize_planwhen callingparallelize_module:\nParallelStyle\nparallelize_plan\nparallelize_module\nConfigure the nn.Module\u2019s inputs to convert the input tensors of the nn.Module to DTensors at runtime according toinput_layouts, and perform layout redistribution according to thedesired_input_layouts.\ninput_layouts\ndesired_input_layouts\ninput_layouts(Union[Placement,Tuple[Optional[Placement]]]) \u2013 The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to\nDTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,Noneneed to be specified\nas a placeholder. default: None.\nNone\ndesired_input_layouts(Union[Placement,Tuple[Optional[Placement]]]) \u2013 The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module\nhave the desired DTensor layouts. This argument needs to have the same length withinput_layouts. default: None.\ninput_layouts\ninput_kwarg_layouts(Dict[str,Placement]) \u2013 The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.\ndefault: None\ndesired_input_kwarg_layouts\u2013 (Dict[str, Placement]):\nThe desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module\nhave the desired DTensor layouts. default: None.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module inputs, default: False.\ntorch.Tensor\nDTensor\nAParallelStyleobject that prepares the sharding layouts of the nn.Module\u2019s inputs.\nParallelStyle\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInput\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> block = TransformerBlock(...)  # block is a nn.Module that contains an \"attn\" Attention submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # According to the style specified below, the first input of attn will be annotated to Sharded DTensor\n>>> # and then redistributed to Replicated DTensor.\n>>> parallelize_module(\n>>>     block, # this can be a submodule or module\n>>>     tp_mesh,\n>>>     parallelize_plan={\n>>>         \"attn\": PrepareModuleInput(\n>>>             input_layouts=(Shard(0), None, None, ...),\n>>>             desired_input_layouts=(Replicate(), None, None, ...)\n>>>         ),\n>>>     }\n>>> )\n\n```\n\nConfigure the nn.Module\u2019s outputs to convert the output tensors of the nn.Module to DTensors at runtime according tooutput_layouts, and perform layout redistribution according to thedesired_output_layouts.\noutput_layouts\ndesired_output_layouts\noutput_layouts(Union[Placement,Tuple[Placement]]) \u2013 The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to\nDTensors if they aretorch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,Noneneed to be specified as a placeholder.\ntorch.Tensor\nNone\ndesired_output_layouts(Union[Placement,Tuple[Placement]]) \u2013 The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module\nhave the desired DTensor layouts.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module outputs, default: True.\ntorch.Tensor\nDTensor\nA ParallelStyle object that prepares the sharding layouts of the nn.Module\u2019s outputs.\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleOutput\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> block = TransformerBlock(...)  # block is a nn.Module that contains an \"attn\" Attention submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # According to the style specified below, the output of the TransformerBlock will be converted to Replicated DTensor\n>>> # and then redistributed to Sharded DTensor.\n>>> parallelize_module(\n>>>     block, # this can be a submodule or module\n>>>     tp_mesh,\n>>>     parallelize_plan = PrepareModuleOutput(\n>>>         output_layouts=Replicate(),\n>>>         desired_output_layouts=Shard(0)\n>>>     )\n>>> )\n\n```\n\nConfigure the nn.Module\u2019s inputs (and outputs) to convert the input tensors (and output tensors, respectively) of the nn.Module\nto DTensors at runtime according toinput_layouts(and output_layouts, respectively), and perform layout redistribution\naccording to thedesired_input_layouts(anddesired_output_layouts, respectively). This is a combination ofPrepareModuleInputandPrepareModuleOutput.\ninput_layouts\ndesired_input_layouts\ndesired_output_layouts\nPrepareModuleInput\nPrepareModuleOutput\ninput_layouts(Union[Placement,Tuple[Optional[Placement]]]) \u2013 The DTensor layouts of input tensors for the nn.Module, this is used to convert the input tensors to\nDTensors. If some inputs are not torch.Tensor or no need to convert to DTensors,Noneneed to be specified\nas a placeholder. default: None.\nNone\ndesired_input_layouts(Union[Placement,Tuple[Optional[Placement]]]) \u2013 The desired DTensor layout of input tensors for the nn.Module, this is used to ensure the inputs of the nn.Module\nhave the desired DTensor layouts. This argument needs to have the same length withinput_layouts. default: None.\ninput_layouts\ninput_kwarg_layouts(Dict[str,Placement]) \u2013 The DTensor layouts of input kwargs for the nn.Module, this is used to convert the input kwarg tensors to DTensors.\ndefault: None\ndesired_input_kwarg_layouts\u2013 (Dict[str, Placement]):\nThe desired DTensor layout of input kwargs for the nn.Module, this is used to ensure the inputs of the nn.Module\nhave the desired DTensor layouts. default: None.\nuse_local_input(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module inputs, default: False.\ntorch.Tensor\nDTensor\noutput_layouts(Union[Placement,Tuple[Placement]]) \u2013 The DTensor layouts of output tensors for the nn.Module, this is used to convert the output tensors to\nDTensors if they aretorch.Tensor. If some outputs are not torch.Tensor or no need to convert to DTensors,Noneneed to be specified as a placeholder.\ntorch.Tensor\nNone\ndesired_output_layouts(Union[Placement,Tuple[Placement]]) \u2013 The desired DTensor layouts of output tensors for the nn.Module, this is used to ensure the outputs of the nn.Module\nhave the desired DTensor layouts.\nuse_local_output(bool,optional) \u2013 Whether to use localtorch.Tensorinstead ofDTensorfor the module outputs, default: True.\ntorch.Tensor\nDTensor\nAParallelStyleobject that prepares the sharding layouts of the nn.Module\u2019s inputs and outputs.\nParallelStyle\n\n```python\n>>> from torch.distributed.tensor.parallel import parallelize_module, PrepareModuleInputOutput\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> block = TransformerBlock(...)  # block is a nn.Module that contains an \"attn\" Attention submodule\n>>> tp_mesh = init_device_mesh(\"cuda\", (8,))\n>>>\n>>> # According to the style specified below, the first input of attn will be annotated as Sharded DTensor\n>>> # and then redistributed to Replicated DTensor, and the output of the TransformerBlock will be annotated\n>>> # as Replicated DTensor and then redistributed to Sharded DTensor.\n>>> parallelize_module(\n>>>     block, # this can be a submodule or module\n>>>     tp_mesh,\n>>>     parallelize_plan={\n>>>         \"attn\": PrepareModuleInputOutput(\n>>>             input_layouts=(Shard(0), None, None, ...),\n>>>             desired_input_layouts=(Replicate(), None, None, ...),\n>>>             output_layouts=Replicate(),\n>>>             desired_output_layouts=Shard(0),\n>>>         ),\n>>>     }\n>>> )\n\n```\n\nNote\nwhen using theShard(dim)as the input/output layouts for the aboveParallelStyles, we assume the input/output activation tensors are evenly sharded on\nthe tensor dimensiondimon theDeviceMeshthat TP operates on. For instance,\nsinceRowwiseParallelaccepts input that is sharded on the last dimension, it assumes\nthe input tensor has already been evenly sharded on the last dimension. For the case of uneven sharded activation tensors, one could pass in DTensor directly to the partitioned modules, and useuse_local_output=Falseto return DTensor after eachParallelStyle, where DTensor could track the uneven sharding information.\nShard(dim)\nParallelStyle\ndim\nDeviceMesh\nRowwiseParallel\nuse_local_output=False\nParallelStyle\nFor models like Transformer, we recommend users to useColwiseParallelandRowwiseParalleltogether in the parallelize_plan for achieve the desired\nsharding for the entire model (i.e. Attention and MLP).\nColwiseParallel\nRowwiseParallel\nParallelized cross-entropy loss computation (loss parallelism), is supported via the following context manager:\nA context manager that enables loss parallelism, where efficient parallelized loss computation\ncan be performed when the input is sharded on the class dimension. Currently only the cross-entropy\nloss is supported.\nWithin this context manager, one can usecross_entropy()orCrossEntropyLossas usual, with the following assumptions on the input parameters.\nThe correspondingbackward()call, if any, also needs to happen under this context manager.\ncross_entropy()\nCrossEntropyLoss\nbackward()\ninput(DTensor) \u2013 Input logits. Assumed to be sharded on the class dimension.\nDTensor\ntarget(Union[torch.Tensor,DTensor]) \u2013 Must be ground truth class indices (class probabilities currently not supported).\nAssumed to be replicated across theDeviceMesh.\ntorch.Tensor\nDTensor\nDeviceMesh\nweight(Union[torch.Tensor,DTensor], optional) \u2013 If given, assumed to be replicated across theDeviceMesh.\ntorch.Tensor\nDTensor\nDeviceMesh\nlabel_smoothing\u2013 Currently not supported.\nA replicatedDTensor.\nDTensor\nExample\nA sharded DTensor is manually created here to showcase the usage.\nIn practice, it is usually the output of a TP module.\n\n```python\n>>> from torch.distributed.tensor.parallel import loss_parallel\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>> ...\n>>> device_mesh = init_device_mesh(\"cuda\", (8,))\n>>> input = torch.randn(4, 16, device=\"cuda\", requires_grad=True)\n>>> dist_input = distribute_tensor(input, device_mesh, placements=[Shard(1)])\n>>> target = torch.randint(16, (4,), device=\"cuda\")\n>>> with loss_parallel():\n>>>     loss = F.cross_entropy(dist_input, target, reduction=\"mean\")\n>>>     loss.backward()\n>>> ...\n\n```\n\nWarning\n\n```python\nThe loss_parallel API is experimental and subject to change.\n\n```\n",
    "url": "https://pytorch.org/docs/stable/distributed.tensor.parallel.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b56488594646bc436df2955a43322b64",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/testing.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2f5ce38a0e1c621073b56ce1ff0b8f57",
    "source": "pytorch_docs",
    "title": "Migrating from functorch to torch.func \u2014 PyTorch 2.9 documentation",
    "text": "\n## Migrating from functorch to torch.func#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\ntorch.func, previously known as \u201cfunctorch\u201d, isJAX-likecomposable function transforms for PyTorch.\nfunctorch started as an out-of-tree library over at\nthepytorch/functorchrepository.\nOur goal has always been to upstream functorch directly into PyTorch and provide\nit as a core PyTorch library.\nAs the final step of the upstream, we\u2019ve decided to migrate from being a top level package\n(functorch) to being a part of PyTorch to reflect how the function transforms are\nintegrated directly into PyTorch core. As of PyTorch 2.0, we are deprecatingimportfunctorchand ask that users migrate to the newest APIs, which we\nwill maintain going forward.importfunctorchwill be kept around to maintain\nbackwards compatibility for a couple of releases.\nfunctorch\nimportfunctorch\nimportfunctorch\n\n## function transforms#\n\nThe following APIs are a drop-in replacement for the followingfunctorch APIs.\nThey are fully backwards compatible.\nfunctorch API\nPyTorch API (as of PyTorch 2.0)\nfunctorch.vmap\ntorch.vmap()ortorch.func.vmap()\ntorch.vmap()\ntorch.func.vmap()\nfunctorch.grad\ntorch.func.grad()\ntorch.func.grad()\nfunctorch.vjp\ntorch.func.vjp()\ntorch.func.vjp()\nfunctorch.jvp\ntorch.func.jvp()\ntorch.func.jvp()\nfunctorch.jacrev\ntorch.func.jacrev()\ntorch.func.jacrev()\nfunctorch.jacfwd\ntorch.func.jacfwd()\ntorch.func.jacfwd()\nfunctorch.hessian\ntorch.func.hessian()\ntorch.func.hessian()\nfunctorch.functionalize\ntorch.func.functionalize()\ntorch.func.functionalize()\nFurthermore, if you are using torch.autograd.functional APIs, please try out\nthetorch.funcequivalents instead.torch.funcfunction\ntransforms are more composable and more performant in many cases.\ntorch.func\ntorch.func\ntorch.autograd.functional API\ntorch.func API (as of PyTorch 2.0)\ntorch.autograd.functional.vjp()\ntorch.autograd.functional.vjp()\ntorch.func.grad()ortorch.func.vjp()\ntorch.func.grad()\ntorch.func.vjp()\ntorch.autograd.functional.jvp()\ntorch.autograd.functional.jvp()\ntorch.func.jvp()\ntorch.func.jvp()\ntorch.autograd.functional.jacobian()\ntorch.autograd.functional.jacobian()\ntorch.func.jacrev()ortorch.func.jacfwd()\ntorch.func.jacrev()\ntorch.func.jacfwd()\ntorch.autograd.functional.hessian()\ntorch.autograd.functional.hessian()\ntorch.func.hessian()\ntorch.func.hessian()\n\n## NN module utilities#\n\nWe\u2019ve changed the APIs to apply function transforms over NN modules to make them\nfit better into the PyTorch design philosophy. The new API is different, so\nplease read this section carefully.\n\n## functorch.make_functional#\n\ntorch.func.functional_call()is the replacement forfunctorch.make_functionalandfunctorch.make_functional_with_buffers.\nHowever, it is not a drop-in replacement.\ntorch.func.functional_call()\nIf you\u2019re in a hurry, you can usehelper functions in this gistthat emulate the behavior of functorch.make_functional and functorch.make_functional_with_buffers.\nWe recommend usingtorch.func.functional_call()directly because it is a more explicit\nand flexible API.\ntorch.func.functional_call()\nConcretely, functorch.make_functional returns a functional module and parameters.\nThe functional module accepts parameters and inputs to the model as arguments.torch.func.functional_call()allows one to call the forward pass of an existing\nmodule using new parameters and buffers and inputs.\ntorch.func.functional_call()\nHere\u2019s an example of how to compute gradients of parameters of a model using functorch\nvstorch.func:\ntorch.func\n\n```python\n# ---------------\n# using functorch\n# ---------------\nimport torch\nimport functorch\ninputs = torch.randn(64, 3)\ntargets = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nfmodel, params = functorch.make_functional(model)\n\ndef compute_loss(params, inputs, targets):\n    prediction = fmodel(params, inputs)\n    return torch.nn.functional.mse_loss(prediction, targets)\n\ngrads = functorch.grad(compute_loss)(params, inputs, targets)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport torch\ninputs = torch.randn(64, 3)\ntargets = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nparams = dict(model.named_parameters())\n\ndef compute_loss(params, inputs, targets):\n    prediction = torch.func.functional_call(model, params, (inputs,))\n    return torch.nn.functional.mse_loss(prediction, targets)\n\ngrads = torch.func.grad(compute_loss)(params, inputs, targets)\n\n```\n\nAnd here\u2019s an example of how to compute jacobians of model parameters:\n\n```python\n# ---------------\n# using functorch\n# ---------------\nimport torch\nimport functorch\ninputs = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nfmodel, params = functorch.make_functional(model)\njacobians = functorch.jacrev(fmodel)(params, inputs)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport torch\nfrom torch.func import jacrev, functional_call\ninputs = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nparams = dict(model.named_parameters())\n# jacrev computes jacobians of argnums=0 by default.\n# We set it to 1 to compute jacobians of params\njacobians = jacrev(functional_call, argnums=1)(model, params, (inputs,))\n\n```\n\nNote that it is important for memory consumption that you should only carry\naround a single copy of your parameters.model.named_parameters()does not copy\nthe parameters. If in your model training you update the parameters of the model\nin-place, then thenn.Modulethat is your model has the single copy of the\nparameters and everything is OK.\nmodel.named_parameters()\nnn.Module\nHowever, if you want to carry your parameters around in a dictionary and update\nthem out-of-place, then there are two copies of parameters: the one in the\ndictionary and the one in themodel. In this case, you should changemodelto not hold memory by converting it to the meta device viamodel.to('meta').\nmodel\nmodel\nmodel.to('meta')\n\n## functorch.combine_state_for_ensemble#\n\nPlease usetorch.func.stack_module_state()instead offunctorch.combine_state_for_ensembletorch.func.stack_module_state()returns two dictionaries, one of stacked parameters, and\none of stacked buffers, that can then be used withtorch.vmap()andtorch.func.functional_call()for ensembling.\ntorch.func.stack_module_state()\ntorch.func.stack_module_state()\ntorch.vmap()\ntorch.func.functional_call()\nFor example, here is an example of how to ensemble over a very simple model:\n\n```python\nimport torch\nnum_models = 5\nbatch_size = 64\nin_features, out_features = 3, 3\nmodels = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]\ndata = torch.randn(batch_size, 3)\n\n# ---------------\n# using functorch\n# ---------------\nimport functorch\nfmodel, params, buffers = functorch.combine_state_for_ensemble(models)\noutput = functorch.vmap(fmodel, (0, 0, None))(params, buffers, data)\nassert output.shape == (num_models, batch_size, out_features)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport copy\n\n# Construct a version of the model with no memory by putting the Tensors on\n# the meta device.\nbase_model = copy.deepcopy(models[0])\nbase_model.to('meta')\n\nparams, buffers = torch.func.stack_module_state(models)\n\n# It is possible to vmap directly over torch.func.functional_call,\n# but wrapping it in a function makes it clearer what is going on.\ndef call_single_model(params, buffers, data):\n    return torch.func.functional_call(base_model, (params, buffers), (data,))\n\noutput = torch.vmap(call_single_model, (0, 0, None))(params, buffers, data)\nassert output.shape == (num_models, batch_size, out_features)\n\n```\n\n\n## functorch.compile#\n\nWe are no longer supporting functorch.compile (also known as AOTAutograd)\nas a frontend for compilation in PyTorch; we have integrated AOTAutograd\ninto PyTorch\u2019s compilation story. If you are a user, please usetorch.compile()instead.\ntorch.compile()",
    "url": "https://pytorch.org/docs/stable/func.migrating.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6c6124f6634ee120b92bd658d77a3131",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/out.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ba7ddcf0dd281b6663fb996f2093787b",
    "source": "pytorch_docs",
    "title": "Train script \u2014 PyTorch 2.9 documentation",
    "text": "\n## Train script#\n\nCreated On: May 04, 2021 | Last Updated On: Feb 09, 2023\nIf your train script works withtorch.distributed.launchit will continue\nworking withtorchrunwith these differences:\ntorch.distributed.launch\ntorchrun\nNo need to manually passRANK,WORLD_SIZE,MASTER_ADDR, andMASTER_PORT.\nRANK\nWORLD_SIZE\nMASTER_ADDR\nMASTER_PORT\nrdzv_backendandrdzv_endpointcan be provided. For most users\nthis will be set toc10d(seerendezvous). The defaultrdzv_backendcreates a non-elastic rendezvous whererdzv_endpointholds\nthe master address.\nrdzv_backend\nrdzv_endpoint\nc10d\nrdzv_backend\nrdzv_endpoint\nMake sure you have aload_checkpoint(path)andsave_checkpoint(path)logic in your script. When any number of\nworkers fail we restart all the workers with the same program\narguments so you will lose progress up to the most recent checkpoint\n(seeelastic launch).\nload_checkpoint(path)\nsave_checkpoint(path)\nuse_envflag has been removed. If you were parsing local rank by parsing\nthe--local-rankoption, you need to get the local rank from the\nenvironment variableLOCAL_RANK(e.g.int(os.environ[\"LOCAL_RANK\"])).\nuse_env\n--local-rank\nLOCAL_RANK\nint(os.environ[\"LOCAL_RANK\"])\nBelow is an expository example of a training script that checkpoints on each\nepoch, hence the worst-case progress lost on failure is one full epoch worth\nof training.\n\n```python\ndef main():\n     args = parse_args(sys.argv[1:])\n     state = load_checkpoint(args.checkpoint_path)\n     initialize(state)\n\n     # torch.distributed.run ensures that this will work\n     # by exporting all the env vars needed to initialize the process group\n     torch.distributed.init_process_group(backend=args.backend)\n\n     for i in range(state.epoch, state.total_num_epochs)\n          for batch in iter(state.dataset)\n              train(batch, state.model)\n\n          state.epoch += 1\n          save_checkpoint(state)\n\n```\n\nFor concrete examples of torchelastic-compliant train scripts, visit\nourexamplespage.",
    "url": "https://pytorch.org/docs/stable/elastic/train_script.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ddec7183fda88250bfc28eab2f649345",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/libtorch_stable_abi.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7d8ccc946e7eb5d34e812f4f0470122d",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/community/governance.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ee2f9688be9496478039bd774d5e8012",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_aot_inductor.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1d9d89ac304e7c878377d88e332931ce",
    "source": "pytorch_docs",
    "title": "Meta device \u2014 PyTorch 2.9 documentation",
    "text": "\n## Meta device#\n\nCreated On: Jun 17, 2025 | Last Updated On: Jun 17, 2025\nThe \u201cmeta\u201d device is an abstract device which denotes a tensor which records\nonly metadata, but no actual data.  Meta tensors have two primary use cases:\nModels can be loaded on the meta device, allowing you to load a\nrepresentation of the model without actually loading the actual parameters\ninto memory.  This can be helpful if you need to make transformations on\nthe model before you load the actual data.\nMost operations can be performed on meta tensors, producing new meta\ntensors that describe what the result would have been if you performed\nthe operation on a real tensor.  You can use this to perform abstract\nanalysis without needing to spend time on compute or space to represent\nthe actual tensors.  Because meta tensors do not have real data, you cannot\nperform data-dependent operations liketorch.nonzero()oritem().  In some cases, not all device types (e.g., CPU\nand CUDA) have exactly the same output metadata for an operation; we\ntypically prefer representing the CUDA behavior faithfully in this\nsituation.\ntorch.nonzero()\nitem()\nWarning\nAlthough in principle meta tensor computation should always be faster than\nan equivalent CPU/CUDA computation, many meta tensor implementations are\nimplemented in Python and have not been ported to C++ for speed, so you\nmay find that you get lower absolute framework latency with small CPU tensors.\n\n## Idioms for working with meta tensors#\n\nAn object can be loaded withtorch.load()onto meta device by specifyingmap_location='meta':\ntorch.load()\nmap_location='meta'\n\n```python\n>>> torch.save(torch.randn(2), 'foo.pt')\n>>> torch.load('foo.pt', map_location='meta')\ntensor(..., device='meta', size=(2,))\n\n```\n\nIf you have some arbitrary code which performs some tensor construction without\nexplicitly specifying a device, you can override it to instead construct on meta device by using\nthetorch.device()context manager:\ntorch.device()\n\n```python\n>>> with torch.device('meta'):\n...     print(torch.randn(30, 30))\n...\ntensor(..., device='meta', size=(30, 30))\n\n```\n\nThis is especially helpful NN module construction, where you often are not\nable to explicitly pass in a device for initialization:\n\n```python\n>>> from torch.nn.modules import Linear\n>>> with torch.device('meta'):\n...     print(Linear(20, 30))\n...\nLinear(in_features=20, out_features=30, bias=True)\n\n```\n\nYou cannot convert a meta tensor directly to a CPU/CUDA tensor, because the\nmeta tensor stores no data and we do not know what the correct data values for\nyour new tensor are:\n\n```python\n>>> torch.ones(5, device='meta').to(\"cpu\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNotImplementedError: Cannot copy out of meta tensor; no data!\n\n```\n\nUse a factory function liketorch.empty_like()to explicitly specify how\nyou would like the missing data to be filled in.\ntorch.empty_like()\nNN modules have a convenience methodtorch.nn.Module.to_empty()that\nallows you to move the module to another device, leaving all parameters\nuninitialized.  You are expected to explicitly reinitialize the parameters\nmanually:\ntorch.nn.Module.to_empty()\n\n```python\n>>> from torch.nn.modules import Linear\n>>> with torch.device('meta'):\n...     m = Linear(20, 30)\n>>> m.to_empty(device=\"cpu\")\nLinear(in_features=20, out_features=30, bias=True)\n\n```\n\ntorch._subclasses.meta_utilscontains undocumented utilities for taking\nan arbitrary Tensor and constructing an equivalent meta Tensor with high\nfidelity.  These APIs are experimental and may be changed in a BC breaking way\nat any time.\ntorch._subclasses.meta_utils",
    "url": "https://pytorch.org/docs/stable/meta.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e7998298f60ba0980131a2fad3f48bd7",
    "source": "pytorch_docs",
    "title": "torch.backends \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.backends#\n\nCreated On: Sep 16, 2020 | Last Updated On: Aug 26, 2025\ntorch.backendscontrols the behavior of various backends that PyTorch supports.\ntorch.backends\nThese backends include:\ntorch.backends.cpu\ntorch.backends.cpu\ntorch.backends.cuda\ntorch.backends.cuda\ntorch.backends.cudnn\ntorch.backends.cudnn\ntorch.backends.cusparselt\ntorch.backends.cusparselt\ntorch.backends.mha\ntorch.backends.mha\ntorch.backends.mps\ntorch.backends.mps\ntorch.backends.mkl\ntorch.backends.mkl\ntorch.backends.mkldnn\ntorch.backends.mkldnn\ntorch.backends.nnpack\ntorch.backends.nnpack\ntorch.backends.openmp\ntorch.backends.openmp\ntorch.backends.opt_einsum\ntorch.backends.opt_einsum\ntorch.backends.xeon\ntorch.backends.xeon\n\n## torch.backends.cpu#\n\nReturn cpu capability as a string value.\nPossible values:\n- \u201cDEFAULT\u201d\n- \u201cVSX\u201d\n- \u201cZ VECTOR\u201d\n- \u201cNO AVX\u201d\n- \u201cAVX2\u201d\n- \u201cAVX512\u201d\n- \u201cSVE256\u201d\nstr\n\n## torch.backends.cuda#\n\nReturn whether PyTorch is built with CUDA support.\nNote that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch\nbinary were run on a machine with working CUDA drivers and devices, we would be able to use it.\nAboolthat controls whether TensorFloat-32 tensor cores may be used in matrix\nmultiplications on Ampere or newer GPUs. allow_tf32 is going to be deprecated. SeeTensorFloat-32 (TF32) on Ampere (and later) devices.\nbool\nAboolthat controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.\nbool\nAboolthat controls whether reduced precision reductions are allowed with bf16 GEMMs.\nbool\ncufft_plan_cachecontains the cuFFT plan caches for each CUDA device.\nQuery a specific devicei\u2019s cache viatorch.backends.cuda.cufft_plan_cache[i].\ncufft_plan_cache\nA readonlyintthat shows the number of plans currently in a cuFFT plan cache.\nint\nAintthat controls the capacity of a cuFFT plan cache.\nint\nClears a cuFFT plan cache.\nOverride the library PyTorch uses for BLAS operations. Choose between cuBLAS, cuBLASLt, and CK [ROCm-only].\nWarning\nThis flag is experimental and subject to change.\nWhen PyTorch runs a CUDA BLAS operation it defaults to cuBLAS even if both cuBLAS and cuBLASLt are available.\nFor PyTorch built for ROCm, hipBLAS, hipBLASLt, and CK may offer different performance.\nThis flag (astr) allows overriding which BLAS library to use.\nstr\nIf\u201ccublas\u201dis set then cuBLAS will be used wherever possible.\nIf\u201ccublaslt\u201dis set then cuBLASLt will be used wherever possible.\nIf\u201cck\u201dis set then CK will be used wherever possible.\nIf\u201cdefault\u201d(the default) is set then heuristics will be used to pick between the other options.\nWhen no input is given, this function returns the currently preferred library.\nUser may use the environment variable TORCH_BLAS_PREFER_CUBLASLT=1 to set the preferred library to cuBLASLt\nglobally.\nThis flag only sets the initial value of the preferred library and the preferred library\nmay still be overridden by this function call later in your script.\nNote: When a library is preferred other libraries may still be used if the preferred library\ndoesn\u2019t implement the operation(s) called.\nThis flag may achieve better performance if PyTorch\u2019s library selection is incorrect\nfor your application\u2019s inputs.\n_BlasBackend\n[ROCm-only]\nOverride the backend PyTorch uses in ROCm environments for Flash Attention. Choose between AOTriton and CK\nWarning\nThis flag is experimental and subject to change.\nWhen Flash Attention is enabled and desired, PyTorch defaults to using AOTriton as the backend.\nThis flag (astr) allows users to override this backend to use composable_kernel\nstr\nIf\u201cdefault\u201dis set then the default backend will be used wherever possible. Currently AOTriton.\nIf\u201caotriton\u201dis set then AOTriton will be used wherever possible.\nIf\u201cck\u201dis set then CK will be used wherever possible.\nWhen no input is given, this function returns the currently preferred library.\nUser may use the environment variable TORCH_ROCM_FA_PREFER_CK=1 to set the preferred library to CK\nglobally.\nNote: When a library is preferred other libraries may still be used if the preferred library\ndoesn\u2019t implement the operation(s) called.\nThis flag may achieve better performance if PyTorch\u2019s library selection is incorrect\nfor your application\u2019s inputs.\n_ROCmFABackend\nOverride the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for CUDA linear algebra operations.\nWarning\nThis flag is experimental and subject to change.\nWhen PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries,\nand if both are available it decides which to use with a heuristic.\nThis flag (astr) allows overriding those heuristics.\nstr\nIf\u201ccusolver\u201dis set then cuSOLVER will be used wherever possible.\nIf\u201cmagma\u201dis set then MAGMA will be used wherever possible.\nIf\u201cdefault\u201d(the default) is set then heuristics will be used to pick between\ncuSOLVER and MAGMA if both are available.\nWhen no input is given, this function returns the currently preferred library.\nUser may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER\nglobally.\nThis flag only sets the initial value of the preferred library and the preferred library\nmay still be overridden by this function call later in your script.\nNote: When a library is preferred other libraries may still be used if the preferred library\ndoesn\u2019t implement the operation(s) called.\nThis flag may achieve better performance if PyTorch\u2019s heuristic library selection is incorrect\nfor your application\u2019s inputs.\nCurrently supported linalg operators:\ntorch.linalg.inv()\ntorch.linalg.inv()\ntorch.linalg.inv_ex()\ntorch.linalg.inv_ex()\ntorch.linalg.cholesky()\ntorch.linalg.cholesky()\ntorch.linalg.cholesky_ex()\ntorch.linalg.cholesky_ex()\ntorch.cholesky_solve()\ntorch.cholesky_solve()\ntorch.cholesky_inverse()\ntorch.cholesky_inverse()\ntorch.linalg.lu_factor()\ntorch.linalg.lu_factor()\ntorch.linalg.lu()\ntorch.linalg.lu()\ntorch.linalg.lu_solve()\ntorch.linalg.lu_solve()\ntorch.linalg.qr()\ntorch.linalg.qr()\ntorch.linalg.eigh()\ntorch.linalg.eigh()\ntorch.linalg.eighvals()\ntorch.linalg.eighvals()\ntorch.linalg.svd()\ntorch.linalg.svd()\ntorch.linalg.svdvals()\ntorch.linalg.svdvals()\n_LinalgBackend\nWarning\nThis flag is beta and subject to change.\nReturns whether flash scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables memory efficient scaled dot product attention.\nWarning\nThis flag is beta and subject to change.\nReturns whether memory efficient scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables flash scaled dot product attention.\nWarning\nThis flag is beta and subject to change.\nReturns whether math scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables math scaled dot product attention.\nWarning\nThis flag is beta and subject to change.\nReturns whether fp16/bf16 reduction in math scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables fp16/bf16 reduction in math scaled dot product attention.\nWarning\nThis flag is beta and subject to change.\nReturns whether cuDNN scaled dot product attention is enabled or not.\nWarning\nThis flag is beta and subject to change.\nEnables or disables cuDNN scaled dot product attention.\nCheck if PyTorch was built with FlashAttention for scaled_dot_product_attention.\nTrue if FlashAttention is built and available; otherwise, False.\nbool\nNote\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False\nin non-CUDA environments.\nCheck if FlashAttention can be utilized in scaled_dot_product_attention.\nparams(_SDPAParams) \u2013 An instance of SDPAParams containing the tensors for query,\nkey, value, an optional attention mask, dropout rate, and\na flag indicating if the attention is causal.\ndebug(bool) \u2013 Whether to logging.warn debug information as to why FlashAttention could not be run.\nDefaults to False.\nTrue if FlashAttention can be used with the given parameters; otherwise, False.\nbool\nNote\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False\nin non-CUDA environments.\nCheck if efficient_attention can be utilized in scaled_dot_product_attention.\nparams(_SDPAParams) \u2013 An instance of SDPAParams containing the tensors for query,\nkey, value, an optional attention mask, dropout rate, and\na flag indicating if the attention is causal.\ndebug(bool) \u2013 Whether to logging.warn with information as to why efficient_attention could not be run.\nDefaults to False.\nTrue if efficient_attention can be used with the given parameters; otherwise, False.\nbool\nNote\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False\nin non-CUDA environments.\nCheck if cudnn_attention can be utilized in scaled_dot_product_attention.\nparams(_SDPAParams) \u2013 An instance of SDPAParams containing the tensors for query,\nkey, value, an optional attention mask, dropout rate, and\na flag indicating if the attention is causal.\ndebug(bool) \u2013 Whether to logging.warn with information as to why cuDNN attention could not be run.\nDefaults to False.\nTrue if cuDNN can be used with the given parameters; otherwise, False.\nbool\nNote\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False\nin non-CUDA environments.\nWarning\nThis flag is beta and subject to change.\nThis context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention.\nUpon exiting the context manager, the previous state of the flags will be restored.\n\n## torch.backends.cudnn#\n\nReturn the version of cuDNN.\nReturn a bool indicating if CUDNN is currently available.\nAboolthat controls whether cuDNN is enabled.\nbool\nAboolthat controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. allow_tf32 is going to be deprecated. SeeTensorFloat-32 (TF32) on Ampere (and later) devices.\nbool\nAboolthat, if True, causes cuDNN to only use deterministic convolution algorithms.\nSee alsotorch.are_deterministic_algorithms_enabled()andtorch.use_deterministic_algorithms().\nbool\ntorch.are_deterministic_algorithms_enabled()\ntorch.use_deterministic_algorithms()\nAboolthat, if True, causes cuDNN to benchmark multiple convolution algorithms\nand select the fastest.\nbool\nAintthat specifies the maximum number of cuDNN convolution algorithms to try whentorch.backends.cudnn.benchmarkis True. Setbenchmark_limitto zero to try every\navailable algorithm. Note that this setting only affects convolutions dispatched via the\ncuDNN v8 API.\nint\n\n## torch.backends.cusparselt#\n\nReturn the version of cuSPARSELt\nOptional[int]\nReturn a bool indicating if cuSPARSELt is currently available.\nbool\n\n## torch.backends.mha#\n\nReturns whether fast path for TransformerEncoder and MultiHeadAttention\nis enabled, orTrueif jit is scripting.\nTrue\nNote\nThe fastpath might not be run even ifget_fastpath_enabledreturnsTrueunless all conditions on inputs are met.\nget_fastpath_enabled\nTrue\nbool\nSets whether fast path is enabled\n\n## torch.backends.miopen#\n\nAboolthat, if True, causes MIOpen to use Immediate Mode\n(https://rocm.docs.amd.com/projects/MIOpen/en/latest/how-to/find-and-immediate.html).\nbool\n\n## torch.backends.mps#\n\nReturn a bool indicating if MPS is currently available.\nbool\nReturn whether PyTorch is built with MPS support.\nNote that this doesn\u2019t necessarily mean MPS is available; just that\nif this PyTorch binary were run a machine with working MPS drivers\nand devices, we would be able to use it.\nbool\n\n## torch.backends.mkl#\n\nReturn whether PyTorch is built with MKL support.\nOn-demand oneMKL verbosing functionality.\nTo make it easier to debug performance issues, oneMKL can dump verbose\nmessages containing execution information like duration while executing\nthe kernel. The verbosing functionality can be invoked via an environment\nvariable namedMKL_VERBOSE. However, this methodology dumps messages in\nall steps. Those are a large amount of verbose messages. Moreover, for\ninvestigating the performance issues, generally taking verbose messages\nfor one single iteration is enough. This on-demand verbosing functionality\nmakes it possible to control scope for verbose message dumping. In the\nfollowing example, verbose messages will be dumped out for the second\ninference only.\n\n```python\nimport torch\n\nmodel(data)\nwith torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):\n    model(data)\n\n```\n\nlevel\u2013 Verbose level\n-VERBOSE_OFF: Disable verbosing\n-VERBOSE_ON:  Enable verbosing\nVERBOSE_OFF\nVERBOSE_ON\n\n## torch.backends.mkldnn#\n\nOn-demand oneDNN (former MKL-DNN) verbosing functionality.\nTo make it easier to debug performance issues, oneDNN can dump verbose\nmessages containing information like kernel size, input data size and\nexecution duration while executing the kernel. The verbosing functionality\ncan be invoked via an environment variable namedDNNL_VERBOSE. However,\nthis methodology dumps messages in all steps. Those are a large amount of\nverbose messages. Moreover, for investigating the performance issues,\ngenerally taking verbose messages for one single iteration is enough.\nThis on-demand verbosing functionality makes it possible to control scope\nfor verbose message dumping. In the following example, verbose messages\nwill be dumped out for the second inference only.\n\n```python\nimport torch\n\nmodel(data)\nwith torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):\n    model(data)\n\n```\n\nlevel\u2013 Verbose level\n-VERBOSE_OFF: Disable verbosing\n-VERBOSE_ON:  Enable verbosing\n-VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation\nVERBOSE_OFF\nVERBOSE_ON\nVERBOSE_ON_CREATION\n\n## torch.backends.nnpack#\n\nReturn whether PyTorch is built with NNPACK support.\nContext manager for setting if nnpack is enabled globally\nSet if nnpack is enabled globally\n\n## torch.backends.openmp#\n\nReturn whether PyTorch is built with OpenMP support.\n\n## torch.backends.opt_einsum#\n\nReturn a bool indicating if opt_einsum is currently available.\nYou must install opt-einsum in order for torch to automatically optimize einsum. To\nmake opt-einsum available, you can install it along with torch:pipinstalltorch[opt-einsum]or by itself:pipinstallopt-einsum. If the package is installed, torch will import\nit automatically and use it accordingly. Use this function to check whether opt-einsum\nwas installed and properly imported by torch.\npipinstalltorch[opt-einsum]\npipinstallopt-einsum\nbool\nReturn the opt_einsum package if opt_einsum is currently available, else None.\nAny\nAboolthat controls whether opt_einsum is enabled (Trueby default). If so,\ntorch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)\nif available to calculate an optimal path of contraction for faster performance.\nbool\nTrue\nIf opt_einsum is not available, torch.einsum will fall back to the default contraction path\nof left to right.\nAstrthat specifies which strategies to try whentorch.backends.opt_einsum.enabledisTrue. By default, torch.einsum will try the \u201cauto\u201d strategy, but the \u201cgreedy\u201d and \u201coptimal\u201d\nstrategies are also supported. Note that the \u201coptimal\u201d strategy is factorial on the number of\ninputs as it tries all possible paths. See more details in opt_einsum\u2019s docs\n(https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).\nstr\ntorch.backends.opt_einsum.enabled\nTrue\n\n## torch.backends.xeon#\n",
    "url": "https://pytorch.org/docs/stable/backends.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8740dcbf582bda7401caf2a2e009e180",
    "source": "pytorch_docs",
    "title": "torch.export API Reference \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.export API Reference#\n\nCreated On: Jul 17, 2025 | Last Updated On: Jul 17, 2025\nexport()takes any nn.Module along with example inputs, and produces a traced graph representing\nonly the Tensor computation of the function in an Ahead-of-Time (AOT) fashion,\nwhich can subsequently be executed with different inputs or serialized.  The\ntraced graph (1) produces normalized operators in the functional ATen operator set\n(as well as any user-specified custom operators), (2) has eliminated all Python control\nflow and data structures (with certain exceptions), and (3) records the set of\nshape constraints needed to show that this normalization and control-flow elimination\nis sound for future inputs.\nexport()\nSoundness Guarantee\nWhile tracing,export()takes note of shape-related assumptions\nmade by the user program and the underlying PyTorch operator kernels.\nThe outputExportedProgramis considered valid only when these\nassumptions hold true.\nexport()\nExportedProgram\nTracing makes assumptions on the shapes (not values) of input tensors.\nSuch assumptions must be validated at graph capture time forexport()to succeed. Specifically:\nexport()\nAssumptions on static shapes of input tensors are automatically validated without additional effort.\nAssumptions on dynamic shape of input tensors require explicit specification\nby using theDim()API to construct dynamic dimensions and by associating\nthem with example inputs through thedynamic_shapesargument.\nDim()\ndynamic_shapes\nIf any assumption can not be validated, a fatal error will be raised. When that happens,\nthe error message will include suggested fixes to the specification that are needed\nto validate the assumptions. For exampleexport()might suggest the\nfollowing fix to the definition of a dynamic dimensiondim0_x, say appearing in the\nshape associated with inputx, that was previously defined asDim(\"dim0_x\"):\nexport()\ndim0_x\nx\nDim(\"dim0_x\")\n\n```python\ndim = Dim(\"dim0_x\", max=5)\n\n```\n\nThis example means the generated code requires dimension 0 of inputxto be less\nthan or equal to 5 to be valid. You can inspect the suggested fixes to dynamic dimension\ndefinitions and then copy them verbatim into your code without needing to change thedynamic_shapesargument to yourexport()call.\nx\ndynamic_shapes\nexport()\nmod(Module) \u2013 We will trace the forward method of this module.\nargs(tuple[Any,...]) \u2013 Example positional inputs.\nkwargs(Optional[Mapping[str,Any]]) \u2013 Optional example keyword inputs.\ndynamic_shapes(Optional[Union[dict[str,Any],tuple[Any,...],list[Any]]]) \u2013An optional argument where the type should either be:\n1) a dict from argument names offto their dynamic shape specifications,\n2) a tuple that specifies dynamic shape specifications for each input in original order.\nIf you are specifying dynamism on keyword args, you will need to pass them in the order that\nis defined in the original function signature.The dynamic shape of a tensor argument can be specified as either\n(1) a dict from dynamic dimension indices toDim()types, where it is\nnot required to include static dimension indices in this dict, but when they are,\nthey should be mapped to None; or (2) a tuple / list ofDim()types or None,\nwhere theDim()types correspond to dynamic dimensions, and static dimensions\nare denoted by None. Arguments that are dicts or tuples / lists of tensors are\nrecursively specified by using mappings or sequences of contained specifications.\nAn optional argument where the type should either be:\n1) a dict from argument names offto their dynamic shape specifications,\n2) a tuple that specifies dynamic shape specifications for each input in original order.\nIf you are specifying dynamism on keyword args, you will need to pass them in the order that\nis defined in the original function signature.\nf\nThe dynamic shape of a tensor argument can be specified as either\n(1) a dict from dynamic dimension indices toDim()types, where it is\nnot required to include static dimension indices in this dict, but when they are,\nthey should be mapped to None; or (2) a tuple / list ofDim()types or None,\nwhere theDim()types correspond to dynamic dimensions, and static dimensions\nare denoted by None. Arguments that are dicts or tuples / lists of tensors are\nrecursively specified by using mappings or sequences of contained specifications.\nDim()\nDim()\nDim()\nstrict(bool) \u2013 When disabled (default), the export function will trace the program through\nPython runtime, which by itself will not validate some of the implicit assumptions\nbaked into the graph. It will still validate most critical assumptions like shape\nsafety. When enabled (by settingstrict=True), the export function will trace\nthe program through TorchDynamo which will ensure the soundness of the resulting\ngraph. TorchDynamo has limited Python feature coverage, thus you may experience more\nerrors. Note that toggling this argument does not affect the resulting IR spec to be\ndifferent and the model will be serialized in the same way regardless of what value\nis passed here.\nstrict=True\npreserve_module_call_signature(tuple[str,...]) \u2013 A list of submodule paths for which the original\ncalling conventions are preserved as metadata. The metadata will be used when calling\ntorch.export.unflatten to preserve the original calling conventions of modules.\nAnExportedProgramcontaining the traced callable.\nExportedProgram\nExportedProgram\nAcceptable input/output types\nAcceptable types of inputs (forargsandkwargs) and outputs include:\nargs\nkwargs\nPrimitive types, i.e.torch.Tensor,int,float,boolandstr.\ntorch.Tensor\nint\nfloat\nbool\nstr\nDataclasses, but they must be registered by callingregister_dataclass()first.\nregister_dataclass()\n(Nested) Data structures comprising ofdict,list,tuple,namedtupleandOrderedDictcontaining all above types.\ndict\nlist\ntuple\nnamedtuple\nOrderedDict\nPackage of a program fromexport(). It contains\nantorch.fx.Graphthat represents Tensor computation, a state_dict containing\ntensor values of all lifted parameters and buffers, and various metadata.\nexport()\ntorch.fx.Graph\nYou can call an ExportedProgram like the original callable traced byexport()with the same calling convention.\nexport()\nTo perform transformations on the graph, use.moduleproperty to access\nantorch.fx.GraphModule. You can then useFX transformationto rewrite the graph. Afterwards, you can simply useexport()again to construct a correct ExportedProgram.\n.module\ntorch.fx.GraphModule\nexport()\nReturns an iterator over original module buffers.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nIterator[Tensor]\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nReturns a self contained GraphModule with all the parameters/buffers inlined.\nWhencheck_guards=True(default), a_guards_fnsubmodule is generated\nand a call to a_guards_fnsubmodule is inserted right after placeholders\nin the graph. This module checks guards on inputs.\nWhencheck_guards=False, a subset of these checks are performed by a\nforward pre-hook on the graph module. No_guards_fnsubmodule is generated.\nGraphModule\nWarning\nThis API is experimental and isNOTbackward-compatible.\nReturns an iterator over original module buffers, yielding\nboth the name of the buffer as well as the buffer itself.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nIterator[tuple[str,torch.Tensor]]\nReturns an iterator over original module parameters, yielding\nboth the name of the parameter as well as the parameter itself.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nIterator[tuple[str,torch.nn.parameter.Parameter]]\nReturns an iterator over original module\u2019s parameters.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nIterator[Parameter]\nWarning\nThis API is experimental and isNOTbackward-compatible.\nRun a set of decompositions on the exported program and returns a new\nexported program. By default we will run the Core ATen decompositions to\nget operators in theCore ATen Operator Set.\nFor now, we do not decompose joint graphs.\ndecomp_table(Optional[dict[torch._ops.OperatorBase,Callable]]) \u2013 An optional argument that specifies decomp behaviour for Aten ops\n(1) If None, we decompose to core aten decompositions\n(2) If empty, we don\u2019t decompose any operator\nExportedProgram\nSome examples:\nIf you don\u2019t want to decompose anything\n\n```python\nep = torch.export.export(model, ...)\nep = ep.run_decompositions(decomp_table={})\n\n```\n\nIf you want to get a core aten operator set except for certain operator, you can do following:\n\n```python\nep = torch.export.export(model, ...)\ndecomp_table = torch.export.default_decompositions()\ndecomp_table[your_op] = your_custom_decomp\nep = ep.run_decompositions(decomp_table=decomp_table)\n\n```\n\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInfers dynamic_shapes based on additional inputs.\nThis is useful particularly for deployment engineers who, on the one hand, may\nhave access to ample testing or profiling data that can provide a fair sense of\nrepresentative inputs for a model, but on the other hand, may not know enough\nabout the model to guess which input shapes should be dynamic.\nInput shapes that are different than the original are considered dynamic; conversely,\nthose that are the same as the original are considered static. Moreover, we verify\nthat the additional inputs are valid for the exported program. This guarantees that\ntracing with them instead of the original would have generated the same graph.\nExample:\n\n```python\nargs0, kwargs0 = ...  # example inputs for export\n\n# other representative inputs that the exported program will run on\ndynamic_shapes = torch.export.AdditionalInputs()\ndynamic_shapes.add(args1, kwargs1)\n...\ndynamic_shapes.add(argsN, kwargsN)\n\ntorch.export(..., args0, kwargs0, dynamic_shapes=dynamic_shapes)\n\n```\n\nAdditional inputargs()andkwargs().\nargs()\nkwargs()\nInfers adynamic_shapes()pytree structure by merging shapes of the\noriginal inputargs()andkwargs()and of each additional input\nargs and kwargs.\ndynamic_shapes()\nargs()\nkwargs()\nVerifies that an exported program is valid for each additional input.\nTheDimclass allows users to specify dynamism in their exported\nprograms. By marking a dimension with aDim, the compiler associates the\ndimension with a symbolic integer containing a dynamic range.\nDim\nDim\nThe API can be used in 2 ways: Dim hints (i.e. automatic dynamic shapes:Dim.AUTO,Dim.DYNAMIC,Dim.STATIC), or named Dims (i.e.Dim(\"name\",min=1,max=2)).\nDim.AUTO\nDim.DYNAMIC\nDim.STATIC\nDim(\"name\",min=1,max=2)\nDim hints provide the lowest barrier to exportability, with the user only\nneeding to specify if a dimension if dynamic, static, or left for the\ncompiler to decide (Dim.AUTO). The export process will automatically\ninfer the remaining constraints on min/max ranges and relationships between\ndimensions.\nDim.AUTO\nExample:\n\n```python\nclass Foo(nn.Module):\n    def forward(self, x, y):\n        assert x.shape[0] == 4\n        assert y.shape[0] >= 16\n        return x @ y\n\n\nx = torch.randn(4, 8)\ny = torch.randn(8, 16)\ndynamic_shapes = {\n    \"x\": {0: Dim.AUTO, 1: Dim.AUTO},\n    \"y\": {0: Dim.AUTO, 1: Dim.AUTO},\n}\nep = torch.export(Foo(), (x, y), dynamic_shapes=dynamic_shapes)\n\n```\n\nHere, export would raise an exception if we replaced all uses ofDim.AUTOwithDim.DYNAMIC,\nasx.shape[0]is constrained to be static by the model.\nDim.AUTO\nDim.DYNAMIC\nx.shape[0]\nMore complex relations between dimensions may also be codegened as runtime assertion nodes by the compiler,\ne.g.(x.shape[0]+y.shape[1])%4==0, to be raised if runtime inputs do not satisfy such constraints.\n(x.shape[0]+y.shape[1])%4==0\nYou may also specify min-max bounds for Dim hints, e.g.Dim.AUTO(min=16,max=32),Dim.DYNAMIC(max=64),\nwith the compiler inferring the remaining constraints within the ranges. An exception will be raised if\nthe valid range is entirely outside the user-specified range.\nDim.AUTO(min=16,max=32)\nDim.DYNAMIC(max=64)\nNamed Dims provide a stricter way of specifying dynamism, where exceptions are raised if the compiler\ninfers constraints that do not match the user specification. For example, exporting the previous\nmodel, the user would need the followingdynamic_shapesargument:\ndynamic_shapes\n\n```python\ns0 = Dim(\"s0\")\ns1 = Dim(\"s1\", min=16)\ndynamic_shapes = {\n    \"x\": {0: 4, 1: s0},\n    \"y\": {0: s0, 1: s1},\n}\nep = torch.export(Foo(), (x, y), dynamic_shapes=dynamic_shapes)\n\n```\n\nNamed Dims also allow specification of relationships between dimensions, up\nto univariate linear relations.  For example, the following indicates one\ndimension is a multiple of another plus 4:\n\n```python\ns0 = Dim(\"s0\")\ns1 = 3 * s0 + 4\n\n```\n\nBuilder for dynamic_shapes.\nUsed to assign dynamic shape specifications to tensors that appear in inputs.\nThis is useful particularly whenargs()is a nested input structure, and it\u2019s\neasier to index the input tensors, than to replicate the structure ofargs()in\nthedynamic_shapes()specification.\nargs()\nargs()\ndynamic_shapes()\nExample:\n\n```python\nargs = {\"x\": tensor_x, \"others\": [tensor_y, tensor_z]}\n\ndim = torch.export.Dim(...)\ndynamic_shapes = torch.export.ShapesCollection()\ndynamic_shapes[tensor_x] = (dim, dim + 1, 8)\ndynamic_shapes[tensor_y] = {0: dim * 2}\n# This is equivalent to the following (now auto-generated):\n# dynamic_shapes = {\"x\": (dim, dim + 1, 8), \"others\": [{0: dim * 2}, None]}\n\ntorch.export(..., args, dynamic_shapes=dynamic_shapes)\n\n```\n\nTo specify dynamism for integers, we need to first wrap the integers using\n_IntWrapper so that we have a \u201cunique identification tag\u201d for each integer.\nExample:\n\n```python\nargs = {\"x\": tensor_x, \"others\": [int_x, int_y]}\n# Wrap all ints with _IntWrapper\nmapped_args = pytree.tree_map_only(int, lambda a: _IntWrapper(a), args)\n\ndynamic_shapes = torch.export.ShapesCollection()\ndynamic_shapes[tensor_x] = (dim, dim + 1, 8)\ndynamic_shapes[mapped_args[\"others\"][0]] = Dim.DYNAMIC\n\n# This is equivalent to the following (now auto-generated):\n# dynamic_shapes = {\"x\": (dim, dim + 1, 8), \"others\": [Dim.DYNAMIC, None]}\n\ntorch.export(..., args, dynamic_shapes=dynamic_shapes)\n\n```\n\nGenerates thedynamic_shapes()pytree structure according toargs()andkwargs().\ndynamic_shapes()\nargs()\nkwargs()\nWhen exporting withdynamic_shapes(), export may fail with a ConstraintViolation error if the specification\ndoesn\u2019t match the constraints inferred from tracing the model. The error message may provide suggested fixes -\nchanges that can be made todynamic_shapes()to export successfully.\ndynamic_shapes()\ndynamic_shapes()\nExample ConstraintViolation error message:\n\n```python\nSuggested fixes:\n\n    dim = Dim('dim', min=3, max=6)  # this just refines the dim's range\n    dim = 4  # this specializes to a constant\n    dy = dx + 1  # dy was specified as an independent dim, but is actually tied to dx with this relation\n\n```\n\nThis is a helper function that takes the ConstraintViolation error message and the originaldynamic_shapes()spec,\nand returns a newdynamic_shapes()spec that incorporates the suggested fixes.\ndynamic_shapes()\ndynamic_shapes()\nExample usage:\n\n```python\ntry:\n    ep = export(mod, args, dynamic_shapes=dynamic_shapes)\nexcept torch._dynamo.exc.UserError as exc:\n    new_shapes = refine_dynamic_shapes_from_suggested_fixes(\n        exc.msg, dynamic_shapes\n    )\n    ep = export(mod, args, dynamic_shapes=new_shapes)\n\n```\n\nUnion[dict[str,Any],tuple[Any],list[Any]]\nWarning\nUnder active development, saved files may not be usable in newer versions\nof PyTorch.\nSaves anExportedProgramto a file-like object. It can then be\nloaded using the Python APItorch.export.load.\nExportedProgram\ntorch.export.load\nep(ExportedProgram) \u2013 The exported program to save.\nf(str|os.PathLike[str]|IO[bytes]) \u2013 implement write and flush) or a string containing a file name.\nextra_files(Optional[Dict[str,Any]]) \u2013 Map from filename to contents\nwhich will be stored as part of f.\nopset_version(Optional[Dict[str,int]]) \u2013 A map of opset names\nto the version of this opset\npickle_protocol(int) \u2013 can be specified to override the default protocol\nExample:\n\n```python\nimport torch\nimport io\n\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\n\nep = torch.export.export(MyModule(), (torch.randn(5),))\n\n# Save to file\ntorch.export.save(ep, \"exported_program.pt2\")\n\n# Save to io.BytesIO buffer\nbuffer = io.BytesIO()\ntorch.export.save(ep, buffer)\n\n# Save with extra files\nextra_files = {\"foo.txt\": b\"bar\".decode(\"utf-8\")}\ntorch.export.save(ep, \"exported_program.pt2\", extra_files=extra_files)\n\n```\n\nWarning\nUnder active development, saved files may not be usable in newer versions\nof PyTorch.\nLoads anExportedProgrampreviously saved withtorch.export.save.\nExportedProgram\ntorch.export.save\nf(str|os.PathLike[str]|IO[bytes]) \u2013 A file-like object (has to\nimplement write and flush) or a string containing a file name.\nextra_files(Optional[Dict[str,Any]]) \u2013 The extra filenames given in\nthis map would be loaded and their content would be stored in the\nprovided map.\nexpected_opset_version(Optional[Dict[str,int]]) \u2013 A map of opset names\nto expected opset versions\nAnExportedProgramobject\nExportedProgram\nExportedProgram\nExample:\n\n```python\nimport torch\nimport io\n\n# Load ExportedProgram from file\nep = torch.export.load(\"exported_program.pt2\")\n\n# Load ExportedProgram from io.BytesIO object\nwith open(\"exported_program.pt2\", \"rb\") as f:\n    buffer = io.BytesIO(f.read())\nbuffer.seek(0)\nep = torch.export.load(buffer)\n\n# Load with extra files.\nextra_files = {\"foo.txt\": \"\"}  # values will be replaced with data\nep = torch.export.load(\"exported_program.pt2\", extra_files=extra_files)\nprint(extra_files[\"foo.txt\"])\nprint(ep(torch.randn(5)))\n\n```\n\nSaves the artifacts to a PT2Archive format. The artifact can then be loaded\nusingload_pt2.\nload_pt2\nf(str|os.PathLike[str]|IO[bytes]) \u2013 A file-like object (has to\nimplement write and flush) or a string containing a file name.\nexported_programs(Union[ExportedProgram,dict[str,ExportedProgram]]) \u2013 The exported program to save, or a dictionary mapping model name to an\nexported program to save. The exported program will be saved under\nmodels/*.json. If only one ExportedProgram is specified, this will\nautomatically be named \u201cmodel\u201d.\naoti_files(Union[list[str],dict[str,list[str]]]) \u2013 A list of files\ngenerated by AOTInductor viatorch._inductor.aot_compile(...,{\"aot_inductor.package\":True}),\nor a dictionary mapping model name to its AOTInductor generated files.\nIf only one set of files is specified, this will automatically be named\n\u201cmodel\u201d.\ntorch._inductor.aot_compile(...,{\"aot_inductor.package\":True})\nextra_files(Optional[Dict[str,Any]]) \u2013 Map from filename to contents\nwhich will be stored as part of the pt2.\nopset_version(Optional[Dict[str,int]]) \u2013 A map of opset names\nto the version of this opset\npickle_protocol(int) \u2013 can be specified to override the default protocol\nUnion[str,PathLike[str],IO[bytes]]\nLoads all the artifacts previously saved withpackage_pt2.\npackage_pt2\nf(str|os.PathLike[str]|IO[bytes]) \u2013 A file-like object (has to\nimplement write and flush) or a string containing a file name.\nexpected_opset_version(Optional[Dict[str,int]]) \u2013 A map of opset names\nto expected opset versions\nnum_runners(int) \u2013 Number of runners to load AOTInductor artifacts\nrun_single_threaded(bool) \u2013 Whether the model should be run without\nthread synchronization logic. This is useful to avoid conflicts with\nCUDAGraphs.\ndevice_index(int) \u2013 The index of the device to which the PT2 package is\nto be loaded. By default,device_index=-1is used, which corresponds\nto the devicecudawhen using CUDA. Passingdevice_index=1would\nload the package tocuda:1, for example.\nAPT2ArchiveContentsobject which contains all the objects in the PT2.\nPT2ArchiveContents\nPT2ArchiveContents\nA version of torch.export.export which is designed to consistently produce\nan ExportedProgram, even if there are potential soundness issues, and to\ngenerate a report listing the issues found.\nExportedProgram\nAdapts input arguments withinput_specto aligntarget_spec.\ninput_spec\ntarget_spec\nNOTE: This adapter may mutate giveninput_args_with_path.\ninput_args_with_path\nlist[Any]\nReturns a list of paths that are used to access the flat args.\nlist[str]\nA module that uses torch.fx.Interpreter to execute instead of the usual\ncodegen that GraphModule uses. This provides better stack trace information\nand makes it easier to debug execution.\nA module that carries a sequence of InterpreterModules corresponding to\na sequence of calls of that module. Each call to the module dispatches\nto the next InterpreterModule, and wraps back around after the last.\nUnflatten an ExportedProgram, producing a module with the same module\nhierarchy as the original eager module. This can be useful if you are trying\nto usetorch.exportwith another system that expects a module\nhierarchy instead of the flat graph thattorch.exportusually produces.\ntorch.export\ntorch.export\nNote\nThe args/kwargs of unflattened modules will not necessarily match\nthe eager module, so doing a module swap (e.g.self.submod=new_mod) will not necessarily work. If you need to swap a module out, you\nneed to set thepreserve_module_call_signatureparameter oftorch.export.export().\nself.submod=new_mod\npreserve_module_call_signature\ntorch.export.export()\nmodule(ExportedProgram) \u2013 The ExportedProgram to unflatten.\nflat_args_adapter(Optional[FlatArgsAdapter]) \u2013 Adapt flat args if input TreeSpec does not match with exported module\u2019s.\nAn instance ofUnflattenedModule, which has the same module\nhierarchy as the original eager module pre-export.\nUnflattenedModule\nUnflattenedModule\nRegisters a dataclass as a valid input/output type fortorch.export.export().\ntorch.export.export()\ncls(type[Any]) \u2013 the dataclass type to register\nserialized_type_name(Optional[str]) \u2013 The serialized name for the dataclass. This is\nthis(required if you want to serialize the pytree TreeSpec containing) \u2013\ndataclass.\u2013\nExample:\n\n```python\nimport torch\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass InputDataClass:\n    feature: torch.Tensor\n    bias: int\n\n\n@dataclass\nclass OutputDataClass:\n    res: torch.Tensor\n\n\ntorch.export.register_dataclass(InputDataClass)\ntorch.export.register_dataclass(OutputDataClass)\n\n\nclass Mod(torch.nn.Module):\n    def forward(self, x: InputDataClass) -> OutputDataClass:\n        res = x.feature + x.bias\n        return OutputDataClass(res=res)\n\n\nep = torch.export.export(Mod(), (InputDataClass(torch.ones(2, 2), 1),))\nprint(ep)\n\n```\n\nThis is a custom dictionary that is specifically used for handling decomp_table in export.\nThe reason we need this is because in the new world, you can onlydeletean op from decomp\ntable to preserve it. This is problematic for custom ops because we don\u2019t know when the custom\nop will actually be loaded to the dispatcher. As a result, we need to record the custom ops operations\nuntil we really need to materialize it (which is when we run decomposition pass.)\nAll aten decomp is loaded at the init time\nWe materialize ALL ops when user ever reads from the table to make it more likely\nthat dispatcher picks up the custom op.\nIf it is write operation, we don\u2019t necessarily materialize\nWe load the final time during export, right before calling run_decompositions()\nCustomDecompTable\ndict[torch._ops.OperatorBase,Callable]\nMove the exported program to the given device.\nep(ExportedProgram) \u2013 The exported program to move.\nlocation(Union[torch.device,str,Dict[str,str]]) \u2013 The device to move the exported program to.\nIf a string, it is interpreted as a device name.\nIf a dict, it is interpreted as a mapping from\nthe existing device to the intended one\nThe moved exported program.\nExportedProgram\nContext manager for reading a PT2 archive.\nGet the archive version.\nint\nGet the file names in the archive.\nlist[str]\nRead a bytes object from the archive.\nname: The source file inside the archive.\nbytes\nRead a string object from the archive.\nname: The source file inside the archive.\nstr\nContext manager for writing a PT2 archive.\nClose the archive.\nCount the number of records that start with a given prefix.\nint\nCheck if a record exists in the archive.\nbool\nWrite a bytes object to the archive.\nname: The destination file inside the archive.\ndata: The bytes object to write.\nCopy a file into the archive.\nname: The destination file inside the archive.\nfile_path: The source file on disk.\nCopy a folder into the archive.\narchive_dir: The destination folder inside the archive.\nfolder_dir: The source folder on disk.\nWrite a string object to the archive.\nname: The destination file inside the archive.\ndata: The string object to write.\nCheck if the serialized model is a PT2 Archive package.\nbool\nThis is the default decomposition table which contains decomposition of\nall ATEN operators to core aten opset. Use this API together withrun_decompositions()\nrun_decompositions()\nCustomDecompTable\nMetadata which is stored on nodes representing ScriptObjects.\nExportGraphSignaturemodels the input/output signature of Export Graph,\nwhich is a fx.Graph with stronger invariants guarantees.\nExportGraphSignature\nExport Graph is functional and does not access \u201cstates\u201d like parameters\nor buffers within the graph viagetattrnodes. Instead,export()guarantees that parameters, buffers, and constant tensors are lifted out of\nthe graph as inputs.  Similarly, any mutations to buffers are not included\nin the graph either, instead the updated values of mutated buffers are\nmodeled as additional outputs of Export Graph.\ngetattr\nexport()\nThe ordering of all inputs and outputs are:\n\n```python\nInputs = [*parameters_buffers_constant_tensors, *flattened_user_inputs]\nOutputs = [*mutated_inputs, *flattened_user_outputs]\n\n```\n\ne.g. If following module is exported:\n\n```python\nclass CustomModule(nn.Module):\n    def __init__(self) -> None:\n        super(CustomModule, self).__init__()\n\n        # Define a parameter\n        self.my_parameter = nn.Parameter(torch.tensor(2.0))\n\n        # Define two buffers\n        self.register_buffer(\"my_buffer1\", torch.tensor(3.0))\n        self.register_buffer(\"my_buffer2\", torch.tensor(4.0))\n\n    def forward(self, x1, x2):\n        # Use the parameter, buffers, and both inputs in the forward method\n        output = (\n            x1 + self.my_parameter\n        ) * self.my_buffer1 + x2 * self.my_buffer2\n\n        # Mutate one of the buffers (e.g., increment it by 1)\n        self.my_buffer2.add_(1.0)  # In-place addition\n\n        return output\n\n\nmod = CustomModule()\nep = torch.export.export(mod, (torch.tensor(1.0), torch.tensor(2.0)))\n\n```\n\nResulting Graph is non-functional:\n\n```python\ngraph():\n    %p_my_parameter : [num_users=1] = placeholder[target=p_my_parameter]\n    %b_my_buffer1 : [num_users=1] = placeholder[target=b_my_buffer1]\n    %b_my_buffer2 : [num_users=2] = placeholder[target=b_my_buffer2]\n    %x1 : [num_users=1] = placeholder[target=x1]\n    %x2 : [num_users=1] = placeholder[target=x2]\n    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x1, %p_my_parameter), kwargs = {})\n    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, %b_my_buffer1), kwargs = {})\n    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%x2, %b_my_buffer2), kwargs = {})\n    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, %mul_1), kwargs = {})\n    %add_ : [num_users=0] = call_function[target=torch.ops.aten.add_.Tensor](args = (%b_my_buffer2, 1.0), kwargs = {})\n    return (add_1,)\n\n```\n\nResulting ExportGraphSignature of the non-functional Graph would be:\n\n```python\n# inputs\np_my_parameter: PARAMETER target='my_parameter'\nb_my_buffer1: BUFFER target='my_buffer1' persistent=True\nb_my_buffer2: BUFFER target='my_buffer2' persistent=True\nx1: USER_INPUT\nx2: USER_INPUT\n\n# outputs\nadd_1: USER_OUTPUT\n\n```\n\nTo get a functional Graph, you can userun_decompositions():\nrun_decompositions()\n\n```python\nmod = CustomModule()\nep = torch.export.export(mod, (torch.tensor(1.0), torch.tensor(2.0)))\nep = ep.run_decompositions()\n\n```\n\nResulting Graph is functional:\n\n```python\ngraph():\n    %p_my_parameter : [num_users=1] = placeholder[target=p_my_parameter]\n    %b_my_buffer1 : [num_users=1] = placeholder[target=b_my_buffer1]\n    %b_my_buffer2 : [num_users=2] = placeholder[target=b_my_buffer2]\n    %x1 : [num_users=1] = placeholder[target=x1]\n    %x2 : [num_users=1] = placeholder[target=x2]\n    %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x1, %p_my_parameter), kwargs = {})\n    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%add, %b_my_buffer1), kwargs = {})\n    %mul_1 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%x2, %b_my_buffer2), kwargs = {})\n    %add_1 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%mul, %mul_1), kwargs = {})\n    %add_2 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%b_my_buffer2, 1.0), kwargs = {})\n    return (add_2, add_1)\n\n```\n\nResulting ExportGraphSignature of the functional Graph would be:\n\n```python\n# inputs\np_my_parameter: PARAMETER target='my_parameter'\nb_my_buffer1: BUFFER target='my_buffer1' persistent=True\nb_my_buffer2: BUFFER target='my_buffer2' persistent=True\nx1: USER_INPUT\nx2: USER_INPUT\n\n# outputs\nadd_2: BUFFER_MUTATION target='my_buffer2'\nadd_1: USER_OUTPUT\n\n```\n\nReplace all uses of the old name with new name in the signature.\nAn enumeration.\nAn enumeration.",
    "url": "https://pytorch.org/docs/stable/export/api_reference.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1803a7ad588bc3753500782a491e0f2f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/events.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "44afbafd1af8b1faf1bb222ca0bb8bf5",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a401b7059899bbd849a2c183df9c7876",
    "source": "pytorch_docs",
    "title": "torch.utils.data \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.data#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jun 13, 2025\nAt the heart of PyTorch data loading utility is thetorch.utils.data.DataLoaderclass. It represents a Python iterable over a dataset, with support for\ntorch.utils.data.DataLoader\nmap-style and iterable-style datasets,\ncustomizing data loading order,\nautomatic batching,\nsingle- and multi-process data loading,\nautomatic memory pinning.\nThese options are configured by the constructor arguments of aDataLoader, which has signature:\nDataLoader\n\n```python\nDataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)\n\n```\n\nThe sections below describe in details the effects and usages of these options.\n\n## Dataset Types#\n\nThe most important argument ofDataLoaderconstructor isdataset, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets:\nDataLoader\ndataset\nMap-style datasets,\nIterable-style datasets.\n\n## Map-style datasets#\n\nA map-style dataset is one that implements the__getitem__()and__len__()protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples.\n__getitem__()\n__len__()\nFor example, such a dataset, when accessed withdataset[idx], could read\ntheidx-th image and its corresponding label from a folder on the disk.\ndataset[idx]\nidx\nSeeDatasetfor more details.\nDataset\n\n## Iterable-style datasets#\n\nAn iterable-style dataset is an instance of a subclass ofIterableDatasetthat implements the__iter__()protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data.\nIterableDataset\n__iter__()\nFor example, such a dataset, when callediter(dataset), could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time.\niter(dataset)\nSeeIterableDatasetfor more details.\nIterableDataset\nNote\nWhen using aIterableDatasetwithmulti-process data loading. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. SeeIterableDatasetdocumentations for how to\nachieve this.\nIterableDataset\nIterableDataset\n\n## Data Loading Order andSampler#\n\nSampler\nForiterable-style datasets, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time).\nThe rest of this section concerns the case withmap-style datasets.torch.utils.data.Samplerclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets. E.g., in the\ncommon case with stochastic gradient decent (SGD), aSamplercould randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD.\ntorch.utils.data.Sampler\nSampler\nA sequential or shuffled sampler will be automatically constructed based on theshuffleargument to aDataLoader.\nAlternatively, users may use thesamplerargument to specify a\ncustomSamplerobject that at each time yields\nthe next index/key to fetch.\nshuffle\nDataLoader\nsampler\nSampler\nA customSamplerthat yields a list of batch\nindices at a time can be passed as thebatch_samplerargument.\nAutomatic batching can also be enabled viabatch_sizeanddrop_lastarguments. Seethe next sectionfor more details\non this.\nSampler\nbatch_sampler\nbatch_size\ndrop_last\nNote\nNeithersamplernorbatch_sampleris compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex.\nsampler\nbatch_sampler\n\n## Loading Batched and Non-Batched Data#\n\nDataLoadersupports automatically collating\nindividual fetched data samples into batches via argumentsbatch_size,drop_last,batch_sampler, andcollate_fn(which has a default function).\nDataLoader\nbatch_size\ndrop_last\nbatch_sampler\ncollate_fn\n\n## Automatic batching (default)#\n\nThis is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first).\nWhenbatch_size(default1) is notNone, the data loader yields\nbatched samples instead of individual samples.batch_sizeanddrop_lastarguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecifybatch_sampler, which yields a list of keys at a time.\nbatch_size\n1\nNone\nbatch_size\ndrop_last\nbatch_sampler\nNote\nThebatch_sizeanddrop_lastarguments essentially are used\nto construct abatch_samplerfromsampler. For map-style\ndatasets, thesampleris either provided by user or constructed\nbased on theshuffleargument. For iterable-style datasets, thesampleris a dummy infinite one. Seethis sectionon more details on\nsamplers.\nbatch_size\ndrop_last\nbatch_sampler\nsampler\nsampler\nshuffle\nsampler\nNote\nWhen fetching fromiterable-style datasetswithmulti-processingthedrop_lastargument drops the last non-full batch of each worker\u2019s dataset replica.\ndrop_last\nAfter fetching a list of samples using the indices from sampler, the function\npassed as thecollate_fnargument is used to collate lists of samples\ninto batches.\ncollate_fn\nIn this case, loading from a map-style dataset is roughly equivalent with:\n\n```python\nfor indices in batch_sampler:\n    yield collate_fn([dataset[i] for i in indices])\n\n```\n\nand loading from an iterable-style dataset is roughly equivalent with:\n\n```python\ndataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])\n\n```\n\nA customcollate_fncan be used to customize collation, e.g., padding\nsequential data to max length of a batch. Seethis sectionon more aboutcollate_fn.\ncollate_fn\ncollate_fn\n\n## Disable automatic batching#\n\nIn certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples. Under these scenarios, it\u2019s likely\nbetter to not use automatic batching (wherecollate_fnis used to\ncollate the samples), but let the data loader directly return each member of\nthedatasetobject.\ncollate_fn\ndataset\nWhen bothbatch_sizeandbatch_samplerareNone(default\nvalue forbatch_sampleris alreadyNone), automatic batching is\ndisabled. Each sample obtained from thedatasetis processed with the\nfunction passed as thecollate_fnargument.\nbatch_size\nbatch_sampler\nNone\nbatch_sampler\nNone\ndataset\ncollate_fn\nWhen automatic batching is disabled, the defaultcollate_fnsimply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.\ncollate_fn\nIn this case, loading from a map-style dataset is roughly equivalent with:\n\n```python\nfor index in sampler:\n    yield collate_fn(dataset[index])\n\n```\n\nand loading from an iterable-style dataset is roughly equivalent with:\n\n```python\nfor data in iter(dataset):\n    yield collate_fn(data)\n\n```\n\nSeethis sectionon more aboutcollate_fn.\ncollate_fn\n\n## Working withcollate_fn#\n\ncollate_fn\nThe use ofcollate_fnis slightly different when automatic batching is\nenabled or disabled.\ncollate_fn\nWhen automatic batching is disabled,collate_fnis called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the defaultcollate_fnsimply converts NumPy\narrays in PyTorch tensors.\ncollate_fn\ncollate_fn\nWhen automatic batching is enabled,collate_fnis called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes the behavior of the defaultcollate_fn(default_collate()).\ncollate_fn\ncollate_fn\ndefault_collate()\nFor instance, if each data sample consists of a 3-channel image and an integral\nclass label, i.e., each element of the dataset returns a tuple(image,class_index), the defaultcollate_fncollates a list of\nsuch tuples into a single tuple of a batched image tensor and a batched class\nlabel Tensor. In particular, the defaultcollate_fnhas the following\nproperties:\n(image,class_index)\ncollate_fn\ncollate_fn\nIt always prepends a new dimension as the batch dimension.\nIt automatically converts NumPy arrays and Python numerical values into\nPyTorch Tensors.\nIt preserves the data structure, e.g., if each sample is a dictionary, it\noutputs a dictionary with the same set of keys but batched Tensors as values\n(or lists if the values can not be converted into Tensors). Same\nforlists,tuples,namedtuples, etc.\nlist\ntuple\nnamedtuple\nUsers may use customizedcollate_fnto achieve custom batching, e.g.,\ncollating along a dimension other than the first, padding sequences of\nvarious lengths, or adding support for custom data types.\ncollate_fn\nIf you run into a situation where the outputs ofDataLoaderhave dimensions or type that is different from your expectation, you may\nwant to check yourcollate_fn.\nDataLoader\ncollate_fn\n\n## Single- and Multi-process Data Loading#\n\nADataLoaderuses single-process data loading by\ndefault.\nDataLoader\nWithin a Python process, theGlobal Interpreter Lock (GIL)prevents true fully parallelizing Python code across threads. To avoid blocking\ncomputation code with data loading, PyTorch provides an easy switch to perform\nmulti-process data loading by simply setting the argumentnum_workersto a positive integer.\nnum_workers\n\n## Single-process data loading (default)#\n\nIn this mode, data fetching is done in the same process aDataLoaderis initialized. Therefore, data loading\nmay block computing. However, this mode may be preferred when resource(s) used\nfor sharing data among processes (e.g., shared memory, file descriptors) is\nlimited, or when the entire dataset is small and can be loaded entirely in\nmemory. Additionally, single-process loading often shows more readable error\ntraces and thus is useful for debugging.\nDataLoader\n\n## Multi-process data loading#\n\nSetting the argumentnum_workersas a positive integer will\nturn on multi-process data loading with the specified number of loader worker\nprocesses.\nnum_workers\nWarning\nAfter several iterations, the loader worker processes will consume\nthe same amount of CPU memory as the parent process for all Python\nobjects in the parent process which are accessed from the worker\nprocesses. This can be problematic if the Dataset contains a lot of\ndata (e.g., you are loading a very large list of filenames at Dataset\nconstruction time) and/or you are using a lot of workers (overall\nmemory usage isnumberofworkers*sizeofparentprocess). The\nsimplest workaround is to replace Python objects with non-refcounted\nrepresentations such as Pandas, Numpy or PyArrow objects. Check outissue #13246for more details on why this occurs and example code for how to\nworkaround these problems.\nnumberofworkers*sizeofparentprocess\nIn this mode, each time an iterator of aDataLoaderis created (e.g., when you callenumerate(dataloader)),num_workersworker processes are created. At this point, thedataset,collate_fn, andworker_init_fnare passed to each\nworker, where they are used to initialize, and fetch data. This means that\ndataset access together with its internal IO, transforms\n(includingcollate_fn) runs in the worker process.\nDataLoader\nenumerate(dataloader)\nnum_workers\ndataset\ncollate_fn\nworker_init_fn\ncollate_fn\ntorch.utils.data.get_worker_info()returns various useful information\nin a worker process (including the worker id, dataset replica, initial seed,\netc.), and returnsNonein main process. Users may use this function in\ndataset code and/orworker_init_fnto individually configure each\ndataset replica, and to determine whether the code is running in a worker\nprocess. For example, this can be particularly helpful in sharding the dataset.\ntorch.utils.data.get_worker_info()\nNone\nworker_init_fn\nFor map-style datasets, the main process generates the indices usingsamplerand sends them to the workers. So any shuffle randomization is\ndone in the main process which guides loading by assigning indices to load.\nsampler\nFor iterable-style datasets, since each worker process gets a replica of thedatasetobject, naive multi-process loading will often result in\nduplicated data. Usingtorch.utils.data.get_worker_info()and/orworker_init_fn, users may configure each replica independently. (SeeIterableDatasetdocumentations for how to achieve\nthis. ) For similar reasons, in multi-process loading, thedrop_lastargument drops the last non-full batch of each worker\u2019s iterable-style dataset\nreplica.\ndataset\ntorch.utils.data.get_worker_info()\nworker_init_fn\nIterableDataset\ndrop_last\nWorkers are shut down once the end of the iteration is reached, or when the\niterator becomes garbage collected.\nWarning\nIt is generally not recommended to return CUDA tensors in multi-process\nloading because of many subtleties in using CUDA and sharing CUDA tensors in\nmultiprocessing (seeCUDA in multiprocessing). Instead, we recommend\nusingautomatic memory pinning(i.e., settingpin_memory=True), which enables fast data transfer to CUDA-enabled\nGPUs.\npin_memory=True\nSince workers rely on Pythonmultiprocessing, worker launch behavior is\ndifferent on Windows compared to Unix.\nmultiprocessing\nOn Unix,fork()is the defaultmultiprocessingstart method.\nUsingfork(), child workers typically can access thedatasetand\nPython argument functions directly through the cloned address space.\nfork()\nmultiprocessing\nfork()\ndataset\nOn Windows or MacOS,spawn()is the defaultmultiprocessingstart method.\nUsingspawn(), another interpreter is launched which runs your main script,\nfollowed by the internal worker function that receives thedataset,collate_fnand other arguments throughpickleserialization.\nspawn()\nmultiprocessing\nspawn()\ndataset\ncollate_fn\npickle\nThis separate serialization means that you should take two steps to ensure you\nare compatible with Windows while using multi-process data loading:\nWrap most of you main script\u2019s code withinif__name__=='__main__':block,\nto make sure it doesn\u2019t run again (most likely generating error) when each worker\nprocess is launched. You can place your dataset andDataLoaderinstance creation logic here, as it doesn\u2019t need to be re-executed in workers.\nif__name__=='__main__':\nDataLoader\nMake sure that any customcollate_fn,worker_init_fnordatasetcode is declared as top level definitions, outside of the__main__check. This ensures that they are available in worker processes.\n(this is needed since functions are pickled as references only, notbytecode.)\ncollate_fn\nworker_init_fn\ndataset\n__main__\nbytecode\nBy default, each worker will have its PyTorch seed set tobase_seed+worker_id,\nwherebase_seedis a long generated by main process using its RNG (thereby,\nconsuming a RNG state mandatorily) or a specifiedgenerator. However, seeds for other\nlibraries may be duplicated upon initializing workers, causing each worker to return\nidentical random numbers. (Seethis sectionin FAQ.).\nbase_seed+worker_id\nbase_seed\ngenerator\nInworker_init_fn, you may access the PyTorch seed set for each worker\nwith eithertorch.utils.data.get_worker_info().seedortorch.initial_seed(), and use it to seed other libraries before data\nloading.\nworker_init_fn\ntorch.utils.data.get_worker_info().seed\ntorch.initial_seed()\n\n## Memory Pinning#\n\nHost to GPU copies are much faster when they originate from pinned (page-locked)\nmemory. SeeUse pinned memory buffersfor more details on when and how to use\npinned memory generally.\nFor data loading, passingpin_memory=Trueto aDataLoaderwill automatically put the fetched data\nTensors in pinned memory, and thus enables faster data transfer to CUDA-enabled\nGPUs.\npin_memory=True\nDataLoader\nThe default memory pinning logic only recognizes Tensors and maps and iterables\ncontaining Tensors. By default, if the pinning logic sees a batch that is a\ncustom type (which will occur if you have acollate_fnthat returns a\ncustom batch type), or if each element of your batch is a custom type, the\npinning logic will not recognize them, and it will return that batch (or those\nelements) without pinning the memory. To enable memory pinning for custom\nbatch or data type(s), define apin_memory()method on your custom\ntype(s).\ncollate_fn\npin_memory()\nSee the example below.\nExample:\n\n```python\nclass SimpleCustomBatch:\n    def __init__(self, data):\n        transposed_data = list(zip(*data))\n        self.inp = torch.stack(transposed_data[0], 0)\n        self.tgt = torch.stack(transposed_data[1], 0)\n\n    # custom memory pinning method on custom type\n    def pin_memory(self):\n        self.inp = self.inp.pin_memory()\n        self.tgt = self.tgt.pin_memory()\n        return self\n\ndef collate_wrapper(batch):\n    return SimpleCustomBatch(batch)\n\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ndataset = TensorDataset(inps, tgts)\n\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                    pin_memory=True)\n\nfor batch_ndx, sample in enumerate(loader):\n    print(sample.inp.is_pinned())\n    print(sample.tgt.is_pinned())\n\n```\n\nData loader combines a dataset and a sampler, and provides an iterable over the given dataset.\nTheDataLoadersupports both map-style and\niterable-style datasets with single- or multi-process loading, customizing\nloading order and optional automatic batching (collation) and memory pinning.\nDataLoader\nSeetorch.utils.datadocumentation page for more details.\ntorch.utils.data\ndataset(Dataset) \u2013 dataset from which to load the data.\nbatch_size(int,optional) \u2013 how many samples per batch to load\n(default:1).\n1\nshuffle(bool,optional) \u2013 set toTrueto have the data reshuffled\nat every epoch (default:False).\nTrue\nFalse\nsampler(SamplerorIterable,optional) \u2013 defines the strategy to draw\nsamples from the dataset. Can be anyIterablewith__len__implemented. If specified,shufflemust not be specified.\nIterable\n__len__\nshuffle\nbatch_sampler(SamplerorIterable,optional) \u2013 likesampler, but\nreturns a batch of indices at a time. Mutually exclusive withbatch_size,shuffle,sampler,\nanddrop_last.\nsampler\nbatch_size\nshuffle\nsampler\ndrop_last\nnum_workers(int,optional) \u2013 how many subprocesses to use for data\nloading.0means that the data will be loaded in the main process.\n(default:0)\n0\n0\ncollate_fn(Callable,optional) \u2013 merges a list of samples to form a\nmini-batch of Tensor(s).  Used when using batched loading from a\nmap-style dataset.\npin_memory(bool,optional) \u2013 IfTrue, the data loader will copy Tensors\ninto device/CUDA pinned memory before returning them.  If your data elements\nare a custom type, or yourcollate_fnreturns a batch that is a custom type,\nsee the example below.\nTrue\ncollate_fn\ndrop_last(bool,optional) \u2013 set toTrueto drop the last incomplete batch,\nif the dataset size is not divisible by the batch size. IfFalseand\nthe size of dataset is not divisible by the batch size, then the last batch\nwill be smaller. (default:False)\nTrue\nFalse\nFalse\ntimeout(numeric,optional) \u2013 if positive, the timeout value for collecting a batch\nfrom workers. Should always be non-negative. (default:0)\n0\nworker_init_fn(Callable,optional) \u2013 If notNone, this will be called on each\nworker subprocess with the worker id (an int in[0,num_workers-1]) as\ninput, after seeding and before data loading. (default:None)\nNone\n[0,num_workers-1]\nNone\nmultiprocessing_context(strormultiprocessing.context.BaseContext,optional) \u2013 IfNone, the defaultmultiprocessing context# noqa: D401\nof your operating system will\nbe used. (default:None)\nNone\nNone\ngenerator(torch.Generator,optional) \u2013 If notNone, this RNG will be used\nby RandomSampler to generate random indexes and multiprocessing to generatebase_seedfor workers. (default:None)\nNone\nbase_seed\nNone\nprefetch_factor(int,optional,keyword-only arg) \u2013 Number of batches loaded\nin advance by each worker.2means there will be a total of\n2 * num_workers batches prefetched across all workers. (default value depends\non the set value for num_workers. If value of num_workers=0 default isNone.\nOtherwise, if value ofnum_workers>0default is2).\n2\nNone\nnum_workers>0\n2\npersistent_workers(bool,optional) \u2013 IfTrue, the data loader will not shut down\nthe worker processes after a dataset has been consumed once. This allows to\nmaintain the workersDatasetinstances alive. (default:False)\nTrue\nFalse\npin_memory_device(str,optional) \u2013 Deprecated, the currentacceleratorwill be used as the device ifpin_memory=True.\npin_memory=True\nin_order(bool,optional) \u2013 IfFalse, the data loader will not enforce that batches\nare returned in a first-in, first-out order. Only applies whennum_workers>0. (default:True)\nFalse\nnum_workers>0\nTrue\nWarning\nIf thespawnstart method is used,worker_init_fncannot be an unpicklable object, e.g., a lambda function. SeeMultiprocessing best practiceson more details related\nto multiprocessing in PyTorch.\nspawn\nworker_init_fn\nWarning\nlen(dataloader)heuristic is based on the length of the sampler used.\nWhendatasetis anIterableDataset,\nit instead returns an estimate based onlen(dataset)/batch_size, with proper\nrounding depending ondrop_last, regardless of multi-process loading\nconfigurations. This represents the best guess PyTorch can make because PyTorch\ntrusts userdatasetcode in correctly handling multi-process\nloading to avoid duplicate data.\nlen(dataloader)\ndataset\nIterableDataset\nlen(dataset)/batch_size\ndrop_last\ndataset\nHowever, if sharding results in multiple workers having incomplete last batches,\nthis estimate can still be inaccurate, because (1) an otherwise complete batch can\nbe broken into multiple ones and (2) more than one batch worth of samples can be\ndropped whendrop_lastis set. Unfortunately, PyTorch can not detect such\ncases in general.\ndrop_last\nSeeDataset Typesfor more details on these two types of datasets and howIterableDatasetinteracts withMulti-process data loading.\nIterableDataset\nWarning\nSeeReproducibility, andMy data loader workers return identical random numbers, andRandomness in multi-process data loadingnotes for random seed related questions.\nWarning\nSettingin_ordertoFalsecan harm reproducibility and may lead to a skewed data\ndistribution being fed to the trainer in cases with imbalanced data.\nAn abstract class representing aDataset.\nDataset\nAll datasets that represent a map from keys to data samples should subclass\nit. All subclasses should overwrite__getitem__(), supporting fetching a\ndata sample for a given key. Subclasses could also optionally overwrite__len__(), which is expected to return the size of the dataset by manySamplerimplementations and the default options\nofDataLoader. Subclasses could also\noptionally implement__getitems__(), for speedup batched samples\nloading. This method accepts list of indices of samples of batch and returns\nlist of samples.\n__getitem__()\n__len__()\nSampler\nDataLoader\n__getitems__()\nNote\nDataLoaderby default constructs an index\nsampler that yields integral indices.  To make it work with a map-style\ndataset with non-integral indices/keys, a custom sampler must be provided.\nDataLoader\nAn iterable Dataset.\nAll datasets that represent an iterable of data samples should subclass it.\nSuch form of datasets is particularly useful when data come from a stream.\nAll subclasses should overwrite__iter__(), which would return an\niterator of samples in this dataset.\n__iter__()\nWhen a subclass is used withDataLoader, each\nitem in the dataset will be yielded from theDataLoaderiterator. Whennum_workers>0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers.get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s__iter__()method or theDataLoader\u2018sworker_init_fnoption to modify each copy\u2019s behavior.\nDataLoader\nDataLoader\nnum_workers>0\nget_worker_info()\n__iter__()\nDataLoader\nworker_init_fn\nExample 1: splitting workload across all workers in__iter__():\n__iter__()\n\n```python\n>>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n\n>>> # Multi-process loading with two worker processes\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=12)))\n[tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n\n```\n\nExample 2: splitting workload across all workers usingworker_init_fn:\nworker_init_fn\n\n```python\n>>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         return iter(range(self.start, self.end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n>>>\n>>> # Directly doing multi-process loading yields duplicate data\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 3, 4, 4, 5, 5, 6, 6]\n\n>>> # Define a `worker_init_fn` that configures each dataset copy differently\n>>> def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n...\n\n>>> # Mult-process loading with the custom `worker_init_fn`\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=12, worker_init_fn=worker_init_fn)))\n[3, 4, 5, 6]\n\n```\n\nDataset wrapping tensors.\nEach sample will be retrieved by indexing tensors along the first dimension.\n*tensors(Tensor) \u2013 tensors that have the same size of the first dimension.\nDataset as a stacking of multiple datasets.\nThis class is useful to assemble different parts of complex input data, given as datasets.\nExample\n\n```python\n>>> images = ImageDataset()\n>>> texts = TextDataset()\n>>> tuple_stack = StackDataset(images, texts)\n>>> tuple_stack[0] == (images[0], texts[0])\n>>> dict_stack = StackDataset(image=images, text=texts)\n>>> dict_stack[0] == {\"image\": images[0], \"text\": texts[0]}\n\n```\n\n*args(Dataset) \u2013 Datasets for stacking returned as tuple.\n**kwargs(Dataset) \u2013 Datasets for stacking returned as dict.\nDataset as a concatenation of multiple datasets.\nThis class is useful to assemble different existing datasets.\ndatasets(sequence) \u2013 List of datasets to be concatenated\nDataset for chaining multipleIterableDatasets.\nIterableDataset\nThis class is useful to assemble different existing dataset streams. The\nchaining operation is done on-the-fly, so concatenating large-scale\ndatasets with this class will be efficient.\ndatasets(iterableofIterableDataset) \u2013 datasets to be chained together\nSubset of a dataset at specified indices.\ndataset(Dataset) \u2013 The whole Dataset\nindices(sequence) \u2013 Indices in the whole set selected for subset\nGeneral collate function that handles collection type of element within each batch.\nThe function also opens function registry to deal with specific element types.default_collate_fn_mapprovides default collate functions for tensors, numpy arrays, numbers and strings.\nbatch\u2013 a single batch to be collated\ncollate_fn_map(Optional[dict[Union[type,tuple[type,...]],Callable]]) \u2013 Optional dictionary mapping from element type to the corresponding collate function.\nIf the element type isn\u2019t present in this dictionary,\nthis function will go through each key of the dictionary in the insertion order to\ninvoke the corresponding collate function if the element type is a subclass of the key.\nExamples\n\n```python\n>>> def collate_tensor_fn(batch, *, collate_fn_map):\n...     # Extend this function to handle batch of tensors\n...     return torch.stack(batch, 0)\n>>> def custom_collate(batch):\n...     collate_map = {torch.Tensor: collate_tensor_fn}\n...     return collate(batch, collate_fn_map=collate_map)\n>>> # Extend `default_collate` by in-place modifying `default_collate_fn_map`\n>>> default_collate_fn_map.update({torch.Tensor: collate_tensor_fn})\n\n```\n\nNote\nEach collate function requires a positional argument for batch and a keyword argument\nfor the dictionary of collate functions ascollate_fn_map.\nTake in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\nThe exact output type can be atorch.Tensor, aSequenceoftorch.Tensor, a\nCollection oftorch.Tensor, or left unchanged, depending on the input type.\nThis is used as the default function for collation whenbatch_sizeorbatch_sampleris defined inDataLoader.\ntorch.Tensor\ntorch.Tensor\ntorch.Tensor\nDataLoader\nHere is the general input type (based on the type of the element within the batch) to output type mapping:\ntorch.Tensor->torch.Tensor(with an added outer dimension batch size)\ntorch.Tensor\ntorch.Tensor\nNumPy Arrays ->torch.Tensor\ntorch.Tensor\nfloat->torch.Tensor\ntorch.Tensor\nint->torch.Tensor\ntorch.Tensor\nstr->str(unchanged)\nbytes->bytes(unchanged)\nMapping[K, V_i]->Mapping[K, default_collate([V_1, V_2, \u2026])]\nNamedTuple[V1_i, V2_i, \u2026]->NamedTuple[default_collate([V1_1, V1_2, \u2026]),\ndefault_collate([V2_1, V2_2, \u2026]), \u2026]\nSequence[V1_i, V2_i, \u2026]->Sequence[default_collate([V1_1, V1_2, \u2026]),\ndefault_collate([V2_1, V2_2, \u2026]), \u2026]\nbatch\u2013 a single batch to be collated\nExamples\n\n```python\n>>> # Example with a batch of `int`s:\n>>> default_collate([0, 1, 2, 3])\ntensor([0, 1, 2, 3])\n>>> # Example with a batch of `str`s:\n>>> default_collate([\"a\", \"b\", \"c\"])\n['a', 'b', 'c']\n>>> # Example with `Map` inside the batch:\n>>> default_collate([{\"A\": 0, \"B\": 1}, {\"A\": 100, \"B\": 100}])\n{'A': tensor([  0, 100]), 'B': tensor([  1, 100])}\n>>> # Example with `NamedTuple` inside the batch:\n>>> Point = namedtuple(\"Point\", [\"x\", \"y\"])\n>>> default_collate([Point(0, 0), Point(1, 1)])\nPoint(x=tensor([0, 1]), y=tensor([0, 1]))\n>>> # Example with `Tuple` inside the batch:\n>>> default_collate([(0, 1), (2, 3)])\n[tensor([0, 2]), tensor([1, 3])]\n>>> # Example with `List` inside the batch:\n>>> default_collate([[0, 1], [2, 3]])\n[tensor([0, 2]), tensor([1, 3])]\n>>> # Two options to extend `default_collate` to handle specific type\n>>> # Option 1: Write custom collate function and invoke `default_collate`\n>>> def custom_collate(batch):\n...     elem = batch[0]\n...     if isinstance(elem, CustomType):  # Some custom condition\n...         return ...\n...     else:  # Fall back to `default_collate`\n...         return default_collate(batch)\n>>> # Option 2: In-place modify `default_collate_fn_map`\n>>> def collate_customtype_fn(batch, *, collate_fn_map=None):\n...     return ...\n>>> default_collate_fn_map.update(CustomType, collate_customtype_fn)\n>>> default_collate(batch)  # Handle `CustomType` automatically\n\n```\n\nConvert each NumPy array element into atorch.Tensor.\ntorch.Tensor\nIf the input is aSequence,Collection, orMapping, it tries to convert each element inside to atorch.Tensor.\nIf the input is not an NumPy array, it is left unchanged.\nThis is used as the default function for collation when bothbatch_samplerandbatch_sizeare NOT defined inDataLoader.\ntorch.Tensor\nDataLoader\nThe general input type to output type mapping is similar to that\nofdefault_collate(). See the description there for more details.\ndefault_collate()\ndata\u2013 a single data point to be converted\nExamples\n\n```python\n>>> # Example with `int`\n>>> default_convert(0)\n0\n>>> # Example with NumPy array\n>>> default_convert(np.array([0, 1]))\ntensor([0, 1])\n>>> # Example with NamedTuple\n>>> Point = namedtuple(\"Point\", [\"x\", \"y\"])\n>>> default_convert(Point(0, 0))\nPoint(x=0, y=0)\n>>> default_convert(Point(np.array(0), np.array(0)))\nPoint(x=tensor(0), y=tensor(0))\n>>> # Example with List\n>>> default_convert([np.array([0, 1]), np.array([2, 3])])\n[tensor([0, 1]), tensor([2, 3])]\n\n```\n\nReturns the information about the currentDataLoaderiterator worker process.\nDataLoader\nWhen called in a worker, this returns an object guaranteed to have the\nfollowing attributes:\nid: the current worker id.\nid\nnum_workers: the total number of workers.\nnum_workers\nseed: the random seed set for the current worker. This value is\ndetermined by main process RNG and the worker id. SeeDataLoader\u2019s documentation for more details.\nseed\nDataLoader\ndataset: the copy of the dataset object inthisprocess. Note\nthat this will be a different object in a different process than the one\nin the main process.\ndataset\nWhen called in the main process, this returnsNone.\nNone\nNote\nWhen used in aworker_init_fnpassed over toDataLoader, this method can be useful to\nset up each worker process differently, for instance, usingworker_idto configure thedatasetobject to only read a specific fraction of a\nsharded dataset, or useseedto seed other libraries used in dataset\ncode.\nworker_init_fn\nDataLoader\nworker_id\ndataset\nseed\nOptional[WorkerInfo]\nRandomly split a dataset into non-overlapping new datasets of given lengths.\nIf a list of fractions that sum up to 1 is given,\nthe lengths will be computed automatically as\nfloor(frac * len(dataset)) for each fraction provided.\nAfter computing the lengths, if there are any remainders, 1 count will be\ndistributed in round-robin fashion to the lengths\nuntil there are no remainders left.\nOptionally fix the generator for reproducible results, e.g.:\nExample\n\n```python\n>>> generator1 = torch.Generator().manual_seed(42)\n>>> generator2 = torch.Generator().manual_seed(42)\n>>> random_split(range(10), [3, 7], generator=generator1)\n>>> random_split(range(30), [0.3, 0.3, 0.4], generator=generator2)\n\n```\n\ndataset(Dataset) \u2013 Dataset to be split\nlengths(sequence) \u2013 lengths or fractions of splits to be produced\ngenerator(Generator) \u2013 Generator used for the random permutation.\nlist[torch.utils.data.dataset.Subset[~_T]]\nBase class for all Samplers.\nEvery Sampler subclass has to provide an__iter__()method, providing a\nway to iterate over indices or lists of indices (batches) of dataset elements,\nand may provide a__len__()method that returns the length of the returned iterators.\n__iter__()\n__len__()\ndata_source(Dataset) \u2013 This argument is not used and will be removed in 2.2.0.\nYou may still have custom implementation that utilizes it.\nExample\n\n```python\n>>> class AccedingSequenceLengthSampler(Sampler[int]):\n>>>     def __init__(self, data: List[str]) -> None:\n>>>         self.data = data\n>>>\n>>>     def __len__(self) -> int:\n>>>         return len(self.data)\n>>>\n>>>     def __iter__(self) -> Iterator[int]:\n>>>         sizes = torch.tensor([len(x) for x in self.data])\n>>>         yield from torch.argsort(sizes).tolist()\n>>>\n>>> class AccedingSequenceLengthBatchSampler(Sampler[List[int]]):\n>>>     def __init__(self, data: List[str], batch_size: int) -> None:\n>>>         self.data = data\n>>>         self.batch_size = batch_size\n>>>\n>>>     def __len__(self) -> int:\n>>>         return (len(self.data) + self.batch_size - 1) // self.batch_size\n>>>\n>>>     def __iter__(self) -> Iterator[List[int]]:\n>>>         sizes = torch.tensor([len(x) for x in self.data])\n>>>         for batch in torch.chunk(torch.argsort(sizes), len(self)):\n>>>             yield batch.tolist()\n\n```\n\nNote\nThe__len__()method isn\u2019t strictly required byDataLoader, but is expected in any\ncalculation involving the length of aDataLoader.\n__len__()\nDataLoader\nDataLoader\nSamples elements sequentially, always in the same order.\ndata_source(Dataset) \u2013 dataset to sample from\nSamples elements randomly. If without replacement, then sample from a shuffled dataset.\nIf with replacement, then user can specifynum_samplesto draw.\nnum_samples\ndata_source(Dataset) \u2013 dataset to sample from\nreplacement(bool) \u2013 samples are drawn on-demand with replacement ifTrue, default=``False``\nTrue\nnum_samples(int) \u2013 number of samples to draw, default=`len(dataset)`.\ngenerator(Generator) \u2013 Generator used in sampling.\nSamples elements randomly from a given list of indices, without replacement.\nindices(sequence) \u2013 a sequence of indices\ngenerator(Generator) \u2013 Generator used in sampling.\nSamples elements from[0,..,len(weights)-1]with given probabilities (weights).\n[0,..,len(weights)-1]\nweights(sequence) \u2013 a sequence of weights, not necessary summing up to one\nnum_samples(int) \u2013 number of samples to draw\nreplacement(bool) \u2013 ifTrue, samples are drawn with replacement.\nIf not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row.\nTrue\ngenerator(Generator) \u2013 Generator used in sampling.\nExample\n\n```python\n>>> list(\n...     WeightedRandomSampler(\n...         [0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True\n...     )\n... )\n[4, 4, 1, 4, 5]\n>>> list(\n...     WeightedRandomSampler(\n...         [0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False\n...     )\n... )\n[0, 1, 4, 3, 2]\n\n```\n\nWraps another sampler to yield a mini-batch of indices.\nsampler(SamplerorIterable) \u2013 Base sampler. Can be any iterable object\nbatch_size(int) \u2013 Size of mini-batch.\ndrop_last(bool) \u2013 IfTrue, the sampler will drop the last batch if\nits size would be less thanbatch_size\nTrue\nbatch_size\nExample\n\n```python\n>>> list(\n...     BatchSampler(\n...         SequentialSampler(range(10)), batch_size=3, drop_last=False\n...     )\n... )\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n>>> list(\n...     BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True)\n... )\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n\n```\n\nSampler that restricts data loading to a subset of the dataset.\nIt is especially useful in conjunction withtorch.nn.parallel.DistributedDataParallel. In such a case, each\nprocess can pass aDistributedSamplerinstance as aDataLoadersampler, and load a subset of the\noriginal dataset that is exclusive to it.\ntorch.nn.parallel.DistributedDataParallel\nDistributedSampler\nDataLoader\nNote\nDataset is assumed to be of constant size and that any instance of it always\nreturns the same elements in the same order.\ndataset(Dataset) \u2013 Dataset used for sampling.\nnum_replicas(int,optional) \u2013 Number of processes participating in\ndistributed training. By default,world_sizeis retrieved from the\ncurrent distributed group.\nworld_size\nrank(int,optional) \u2013 Rank of the current process withinnum_replicas.\nBy default,rankis retrieved from the current distributed\ngroup.\nnum_replicas\nrank\nshuffle(bool,optional) \u2013 IfTrue(default), sampler will shuffle the\nindices.\nTrue\nseed(int,optional) \u2013 random seed used to shuffle the sampler ifshuffle=True. This number should be identical across all\nprocesses in the distributed group. Default:0.\nshuffle=True\n0\ndrop_last(bool,optional) \u2013 ifTrue, then the sampler will drop the\ntail of the data to make it evenly divisible across the number of\nreplicas. IfFalse, the sampler will add extra indices to make\nthe data evenly divisible across the replicas. Default:False.\nTrue\nFalse\nFalse\nWarning\nIn distributed mode, calling theset_epoch()method at\nthe beginning of each epochbeforecreating theDataLoaderiterator\nis necessary to make shuffling work properly across multiple epochs. Otherwise,\nthe same ordering will be always used.\nset_epoch()\nDataLoader\nExample:\n\n```python\n>>> sampler = DistributedSampler(dataset) if is_distributed else None\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\n...                     sampler=sampler)\n>>> for epoch in range(start_epoch, n_epochs):\n...     if is_distributed:\n...         sampler.set_epoch(epoch)\n...     train(loader)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/data.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "bdad930d5de4a900474b46da6a63c980",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/nn.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f5cccd235fca1156750c968093fc6fc2",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_fake_tensor.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f13e36e30a7fed9a89f467a8849cc214",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_api.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b4b5c7b385fac06940b05a832391fcb5",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/checkpoint.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "de68d31ddc1de9f70fb0ba4974793448",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.reporting_issues.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2c158a017fba277460d3b6b46d5877b6",
    "source": "pytorch_docs",
    "title": "PyTorch documentation \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch documentation#\n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\nFeatures described in this documentation are classified by release status:\nStable (API-Stable):These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).\nUnstable (API-Unstable):Encompasses all features that are under active development where APIs may change based on user feedback, requisite performance improvements or because coverage across operators is not yet complete.\nThe APIs and performance characteristics of these features may change.\n\n## Indices and tables#\n\nIndex\nModule Index",
    "url": "https://pytorch.org/docs/stable/index.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8df8fa2968a44f606968e0a603d82fed",
    "source": "pytorch_docs",
    "title": "AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models \u2014 PyTorch 2.9 documentation",
    "text": "\n## AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models#\n\nCreated On: Jun 13, 2025 | Last Updated On: Aug 14, 2025\nWarning\nAOTInductor and its related features are in prototype status and are\nsubject to backwards compatibility breaking changes.\nAOTInductor is a specialized version ofTorchInductor,\ndesigned to process exported PyTorch models, optimize them, and produce shared libraries as well\nas other relevant artifacts.\nThese compiled artifacts are specifically crafted for deployment in non-Python environments,\nwhich are frequently employed for inference deployments on the server side.\nIn this tutorial, you will gain insight into the process of taking a PyTorch model, exporting it,\ncompiling it into an artifact, and conducting model predictions using C++.\n\n## Model Compilation#\n\nTo compile a model using AOTInductor, we first need to usetorch.export.export()to capture a given PyTorch model into a\ncomputational graph.torch.exportprovides soundness\nguarantees and a strict specification on the IR captured, which AOTInductor\nrelies on.\ntorch.export.export()\nWe will then usetorch._inductor.aoti_compile_and_package()to compile the\nexported program using TorchInductor, and save the compiled artifacts into one\npackage. The package is in the format of aPT2 Archive Spec.\ntorch._inductor.aoti_compile_and_package()\nNote\nIf you have a CUDA-enabled device on your machine and you installed PyTorch with CUDA support,\nthe following code will compile the model into a shared library for CUDA execution.\nOtherwise, the compiled artifact will run on CPU. For better performance during CPU inference,\nit is suggested to enable freezing by settingexportTORCHINDUCTOR_FREEZING=1before running the Python script below. The same behavior works in an environment with Intel\u00ae\nGPU as well.\nexportTORCHINDUCTOR_FREEZING=1\n\n```python\nimport os\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 16)\n        self.relu = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(16, 1)\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\nwith torch.no_grad():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = Model().to(device=device)\n    example_inputs=(torch.randn(8, 10, device=device),)\n    batch_dim = torch.export.Dim(\"batch\", min=1, max=1024)\n    # [Optional] Specify the first dimension of the input x as dynamic.\n    exported = torch.export.export(model, example_inputs, dynamic_shapes={\"x\": {0: batch_dim}})\n    # [Note] In this example we directly feed the exported module to aoti_compile_and_package.\n    # Depending on your use case, e.g. if your training platform and inference platform\n    # are different, you may choose to save the exported model using torch.export.save and\n    # then load it back using torch.export.load on your inference platform to run AOT compilation.\n    output_path = torch._inductor.aoti_compile_and_package(\n        exported,\n        # [Optional] Specify the generated shared library path. If not specified,\n        # the generated artifact is stored in your system temp directory.\n        package_path=os.path.join(os.getcwd(), \"model.pt2\"),\n    )\n\n```\n\nIn this illustrative example, theDimparameter is employed to designate the first dimension of\nthe input variable \u201cx\u201d as dynamic. Notably, the path and name of the compiled library remain unspecified,\nresulting in the shared library being stored in a temporary directory.\nTo access this path from the C++ side, we save it to a file for later retrieval within the C++ code.\nDim\n\n## Inference in Python#\n\nThere are multiple ways to deploy the compiled artifact for inference, and one of that is using Python.\nWe have provided a convenient utility API in Pythontorch._inductor.aoti_load_package()for loading\nand running the artifact, as shown in the following example:\ntorch._inductor.aoti_load_package()\n\n```python\nimport os\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = torch._inductor.aoti_load_package(os.path.join(os.getcwd(), \"model.pt2\"))\nprint(model(torch.randn(8, 10, device=device)))\n\n```\n\nThe input at inference time should have the same size, dtype, and stride as the input at export time.\n\n## Inference in C++#\n\nNext, we use the following example C++ fileinference.cppto load the compiled artifact,\nenabling us to conduct model predictions directly within a C++ environment.\ninference.cpp\n\n```python\n#include <iostream>\n#include <vector>\n\n#include <torch/torch.h>\n#include <torch/csrc/inductor/aoti_package/model_package_loader.h>\n\nint main() {\n    c10::InferenceMode mode;\n\n    torch::inductor::AOTIModelPackageLoader loader(\"model.pt2\");\n    // Assume running on CUDA\n    std::vector<torch::Tensor> inputs = {torch::randn({8, 10}, at::kCUDA)};\n    std::vector<torch::Tensor> outputs = loader.run(inputs);\n    std::cout << \"Result from the first inference:\"<< std::endl;\n    std::cout << outputs[0] << std::endl;\n\n    // The second inference uses a different batch size and it works because we\n    // specified that dimension as dynamic when compiling model.pt2.\n    std::cout << \"Result from the second inference:\"<< std::endl;\n    // Assume running on CUDA\n    std::cout << loader.run({torch::randn({1, 10}, at::kCUDA)})[0] << std::endl;\n\n    return 0;\n}\n\n```\n\nFor building the C++ file, you can make use of the providedCMakeLists.txtfile, which\nautomates the process of invokingpythonmodel.pyfor AOT compilation of the model and compilinginference.cppinto an executable binary namedaoti_example.\nCMakeLists.txt\npythonmodel.py\ninference.cpp\naoti_example\n\n```python\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\nproject(aoti_example)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(aoti_example inference.cpp model.pt2)\n\nadd_custom_command(\n    OUTPUT model.pt2\n    COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/model.py\n    DEPENDS model.py\n)\n\ntarget_link_libraries(aoti_example \"${TORCH_LIBRARIES}\")\nset_property(TARGET aoti_example PROPERTY CXX_STANDARD 17)\n\n```\n\nProvided the directory structure resembles the following, you can execute the subsequent commands\nto construct the binary. It is essential to note that theCMAKE_PREFIX_PATHvariable\nis crucial for CMake to locate the LibTorch library, and it should be set to an absolute path.\nPlease be mindful that your path may vary from the one illustrated in this example.\nCMAKE_PREFIX_PATH\n\n```python\naoti_example/\n    CMakeLists.txt\n    inference.cpp\n    model.py\n\n```\n\n\n```python\n$ mkdir build\n$ cd build\n$ CMAKE_PREFIX_PATH=/path/to/python/install/site-packages/torch/share/cmake cmake ..\n$ cmake --build . --config Release\n\n```\n\nAfter theaoti_examplebinary has been generated in thebuilddirectory, executing it will\ndisplay results akin to the following:\naoti_example\nbuild\n\n```python\n$ ./aoti_example\nResult from the first inference:\n0.4866\n0.5184\n0.4462\n0.4611\n0.4744\n0.4811\n0.4938\n0.4193\n[ CUDAFloatType{8,1} ]\nResult from the second inference:\n0.4883\n0.4703\n[ CUDAFloatType{2,1} ]\n\n```\n\n\n## Troubleshooting#\n\nBelow are some useful tools for debugging AOT Inductor.\nDebugging Tools\nTo enable runtime checks on inputs, set the environment variableAOTI_RUNTIME_CHECK_INPUTSto 1. This will raise aRuntimeErrorif the inputs to the compiled model differ in size, data type, or strides from those used during export.\nAOTI_RUNTIME_CHECK_INPUTS\nRuntimeError\n\n## API Reference#\n\nCompiles the exported program with AOTInductor, and packages it into a .pt2\nartifact specified by the input package_path. To load the package, you can\ncalltorch._inductor.aoti_load_package(package_path).\ntorch._inductor.aoti_load_package(package_path)\nAn example usage is as follows:\n\n```python\nep = torch.export.export(M(), ...)\naoti_file = torch._inductor.aoti_compile_and_package(\n    ep, package_path=\"my_package.pt2\"\n)\ncompiled_model = torch._inductor.aoti_load_package(\"my_package.pt2\")\n\n```\n\nTo compile and save multiple models into a single.pt2artifact, you can do\nthe following:\n.pt2\n\n```python\nep1 = torch.export.export(M1(), ...)\naoti_file1 = torch._inductor.aot_compile(\n    ep1, ..., options={\"aot_inductor.package\": True}\n)\nep2 = torch.export.export(M2(), ...)\naoti_file2 = torch._inductor.aot_compile(\n    ep2, ..., options={\"aot_inductor.package\": True}\n)\n\nfrom torch._inductor.package import package_aoti, load_package\n\npackage_aoti(\"my_package.pt2\", {\"model1\": aoti_file1, \"model2\": aoti_file2})\n\ncompiled_model1 = load_package(\"my_package.pt2\", \"model1\")\ncompiled_model2 = load_package(\"my_package.pt2\", \"model2\")\n\n```\n\nexported_program(ExportedProgram) \u2013 An exported program created through a call from torch.export\npackage_path(Optional[FileLike]) \u2013 Optional specified path to the generated .pt2 artifact.\ninductor_configs(Optional[dict[str,Any]]) \u2013 Optional dictionary of configs to control inductor.\nPath to the generated artifact\nstr\nLoads the model from the PT2 package.\nIf multiple models were packaged into the PT2, this will load the default\nmodel. To load a specific model, you can directly call the load API\n\n```python\nfrom torch._inductor.package import load_package\n\ncompiled_model1 = load_package(\"my_package.pt2\", \"model1\")\ncompiled_model2 = load_package(\"my_package.pt2\", \"model2\")\n\n```\n\npath(FileLike) \u2013 Path to the .pt2 package\nrun_single_threaded(bool) \u2013 Whether the model should be run without\nthread synchronization logic. This is useful to avoid conflicts with\nCUDAGraphs.\ndevice_index(int) \u2013 The index of the device to which the PT2 package is\nto be loaded. By default,device_index=-1is used, which corresponds\nto the devicecudawhen using CUDA. Passingdevice_index=1would\nload the package tocuda:1, for example.\nAOTICompiledModel",
    "url": "https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "51cfb37cce931ad002e01e4c60bc9550",
    "source": "pytorch_docs",
    "title": "torch.compiler API reference \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.compiler API reference#\n\nCreated On: Jun 02, 2023 | Last Updated On: Jun 22, 2025\nFor a quick overview oftorch.compiler, seetorch.compiler.\ntorch.compiler\ncompile\n\ncompile\nSeetorch.compile()for details on the arguments for this function.\ntorch.compile()\nreset\n\nreset\nThis function clears all compilation caches and restores the system to its initial state.\nallow_in_graph\n\nallow_in_graph\nTells the compiler frontend (Dynamo) to skip symbolic introspection of the function and instead directly write it to the graph when encountered.\nsubstitute_in_graph\n\nsubstitute_in_graph\nRegister a polyfill handler for a function, usually a C function from the C extension, to be used in place of the original function when inlining the original function in the graph.\nassume_constant_result\n\nassume_constant_result\nThis function is used to mark a functionfnas having a constant result.\nlist_backends\n\nlist_backends\nReturn valid strings that can be passed totorch.compile(..., backend=\"name\").\ndisable\n\ndisable\nThis function provides a decorator to disable compilation on a function.\nset_stance\n\nset_stance\nSet the current stance of the compiler.\nset_enable_guard_collectives\n\nset_enable_guard_collectives\nEnables use of collectivesduringguard evaluation to synchronize behavior across ranks.\ncudagraph_mark_step_begin\n\ncudagraph_mark_step_begin\nIndicates that a new iteration of inference or training is about to begin.\nis_compiling\n\nis_compiling\nIndicates whether a graph is executed/traced as part of torch.compile() or torch.export().\nis_dynamo_compiling\n\nis_dynamo_compiling\nIndicates whether a graph is traced via TorchDynamo.\nis_exporting\n\nis_exporting\nIndicated whether we're under exporting.\nskip_guard_on_inbuilt_nn_modules_unsafe\n\nskip_guard_on_inbuilt_nn_modules_unsafe\nA common function to skip guards on the inbuilt nn modules like torch.nn.Linear.\nskip_guard_on_all_nn_modules_unsafe\n\nskip_guard_on_all_nn_modules_unsafe\nA common function to skip guards on all nn modules, both user defined as well inbuilt nn modules (like torch.nn.Linear).\nkeep_tensor_guards_unsafe\n\nkeep_tensor_guards_unsafe\nA common function to keep tensor guards on all tensors.\nskip_guard_on_globals_unsafe\n\nskip_guard_on_globals_unsafe\nA common function to skip guards on all globals.\nnested_compile_region\n\nnested_compile_region\nTells``torch.compile``that the marked set of operations forms a nested compile region (which is often repeated in the full model) whose code can be compiled once and safely reused.",
    "url": "https://pytorch.org/docs/stable/torch.compiler_api.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a5dddee08df5692c78557edc1ff0e17e",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/linalg.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "54d09665dcfe6e2bb3e73c5be6da28be",
    "source": "pytorch_docs",
    "title": "torch.accelerator \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.accelerator#\n\nCreated On: Oct 27, 2024 | Last Updated On: Aug 08, 2025\nThis package introduces support for the currentacceleratorin python.\ndevice_count\n\ndevice_count\nReturn the number of currentacceleratoravailable.\nis_available\n\nis_available\nCheck if the current accelerator is available at runtime: it was build, all the required drivers are available and at least one device is visible.\ncurrent_accelerator\n\ncurrent_accelerator\nReturn the device of the accelerator available at compilation time.\nset_device_index\n\nset_device_index\nSet the current device index to a given device.\nset_device_idx\n\nset_device_idx\n(Deprecated) Set the current device index to a given device.\ncurrent_device_index\n\ncurrent_device_index\nReturn the index of a currently selected device for the currentaccelerator.\ncurrent_device_idx\n\ncurrent_device_idx\n(Deprecated) Return the index of a currently selected device for the currentaccelerator.\nset_stream\n\nset_stream\nSet the current stream to a given stream.\ncurrent_stream\n\ncurrent_stream\nReturn the currently selected stream for a given device.\nsynchronize\n\nsynchronize\nWait for all kernels in all streams on the given device to complete.\ndevice_index\n\ndevice_index\nContext manager to set the current device index for the currentaccelerator.\n\n## Memory management#\n\nempty_cache\n\nempty_cache\nRelease all unoccupied cached memory currently held by the caching allocator so that those can be used in other application.\nmax_memory_allocated\n\nmax_memory_allocated\nReturn the currentacceleratormaximum device memory occupied by tensors in bytes for a given device index.\nmax_memory_reserved\n\nmax_memory_reserved\nReturn the currentacceleratormaximum device memory managed by the caching allocator in bytes for a given device index.\nmemory_allocated\n\nmemory_allocated\nReturn the currentacceleratordevice memory occupied by tensors in bytes for a given device index.\nmemory_reserved\n\nmemory_reserved\nReturn the currentacceleratordevice memory managed by the caching allocator in bytes for a given device index.\nmemory_stats\n\nmemory_stats\nReturn a dictionary of accelerator device memory allocator statistics for a given device index.\nreset_accumulated_memory_stats\n\nreset_accumulated_memory_stats\nReset the \"accumulated\" (historical) stats tracked by the currentacceleratormemory allocator for a given device index.\nreset_peak_memory_stats\n\nreset_peak_memory_stats\nReset the \"peak\" stats tracked by the currentacceleratormemory allocator for a given device index.",
    "url": "https://pytorch.org/docs/stable/accelerator.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3de999a08c673a80cf19c452237b0b88",
    "source": "pytorch_docs",
    "title": "AOTInductor Debugging Guide \u2014 PyTorch 2.9 documentation",
    "text": "\n## AOTInductor Debugging Guide#\n\nCreated On: Aug 14, 2025 | Last Updated On: Aug 14, 2025\nIf you encounter CUDA illegal memory access (IMA) errors while usingAOT Inductor, this guide provides a systematic approach to debug such errors. AOT Inductor is part of the PT2 stack, similar to torch.compile, but it produces a compilation artifact that can work in a C++ environment. CUDA illegal memory errors can happen non-deterministically and even appear transient at times.\nOn a high-level, there are three main steps in debugging CUDA IMA errors:\nSanity checks: Use basic debugging flags to catch common issues before diving deeper.\nPinpoint the CUDA IMA: Make the error deterministic and identify the problematic kernel.\nIdentify problematic kernels: Use intermediate value debugging to inspect kernel inputs and outputs.\n\n## Step 1: Sanity Checks#\n\nBefore diving deep into reliably reproducing the error, try out some existing debugging flags:\n\n```python\nAOTI_RUNTIME_CHECK_INPUTS=1\nTORCHINDUCTOR_NAN_ASSERTS=1\n\n```\n\nThese flags take effect at compilation time (more precisely, at codegen time):\nAOTI_RUNTIME_CHECK_INPUTS=1checks if the inputs satisfy the same set of guards used during compilation. Seetorch.compile Troubleshootingfor more details.\nAOTI_RUNTIME_CHECK_INPUTS=1\nTORCHINDUCTOR_NAN_ASSERTS=1adds codegen before and after each Inductor\u2019s kernel to check for NaN.\nTORCHINDUCTOR_NAN_ASSERTS=1\n\n## Step 2: Pinpoint the CUDA IMA#\n\nOne hard part is CUDA IMA errors can be non-deterministic. They can happen at different locations, and sometimes not happen at all (though that just means the numerics are silently incorrect). With the following two flags, we can trigger the error deterministically:\n\n```python\nPYTORCH_NO_CUDA_MEMORY_CACHING=1\nCUDA_LAUNCH_BLOCKING=1\n\n```\n\nThese flags take effect at runtime:\nPYTORCH_NO_CUDA_MEMORY_CACHING=1disables PyTorch\u2019s Caching Allocator, which allocates a bigger buffer than needed immediately to reduce the number of buffer allocations. This is usually the reason why CUDA illegal memory access errors are non-deterministic.Figure: How PyTorch\u2019s caching allocator can mask CUDA illegal memory access errors\nPYTORCH_NO_CUDA_MEMORY_CACHING=1\nCUDA_LAUNCH_BLOCKING=1forces the kernels to launch one at a time. Without this, we would get the famous \u201cCUDA kernel errors might be asynchronously reported at some other API call\u201d warning since kernels are launched asynchronously.\nCUDA_LAUNCH_BLOCKING=1\n\n## Step 3: Identify Problematic Kernels with Intermediate Value Debugger#\n\nThe AOTI Intermediate Value Debugger can help pinpoint the problematic kernel and get information about the inputs and outputs of said kernel.\nFirst, use:\n\n```python\nAOT_INDUCTOR_DEBUG_INTERMEDIATE_VALUE_PRINTER=3\n\n```\n\nThis flag takes effect at compilation time and prints the kernels one by one at runtime. Together with the previous flags, this would let us know which kernel was launched right before the error happened.\nHowever, it is important to note that just because the error happened in that kernel, it doesn\u2019t mean that kernel is problematic. For example, it can happen that an earlier kernel is problematic and produces some wrong outputs. So the natural next step is to inspect the inputs to the problematic kernel:\n\n```python\nAOT_INDUCTOR_FILTERED_KERNELS_TO_PRINT=\"triton_poi_fused_add_ge_logical_and_logical_or_lt_231,_add_position_embeddings_kernel_5\" AOT_INDUCTOR_DEBUG_INTERMEDIATE_VALUE_PRINTER=2\n\n```\n\nThe filtered kernels to print environment variable has the names of the kernels you want to inspect. If the inputs to the kernel are not as expected, you then inspect the kernel that produces the bad input.\n\n## Additional Debugging Tools#\n\n\n## Logging and Tracing#\n\ntlparse / TORCH_TRACE: Provides complete output codes for inspection and records the set of guards used. Seetlparse / TORCH_TRACEfor more details.\nTORCH_LOGS: UseTORCH_LOGS=\"+inductor,output_code\"to see more PT2 internal logs. SeeTORCH_LOGSfor more details.\nTORCH_LOGS=\"+inductor,output_code\"\nTORCH_SHOW_CPP_STACKTRACES: SetTORCH_SHOW_CPP_STACKTRACES=1to potentially see more stack traces.\nTORCH_SHOW_CPP_STACKTRACES=1\n\n## Common Sources of Issues#\n\nDynamic shapes: Historically a source of many IMAs. Pay special attention when debugging dynamic shape scenarios.\nCustom ops: Especially when implemented in C++ and used with dynamic shapes. There is a need to Symint\u2019ify the meta function.",
    "url": "https://pytorch.org/docs/stable/torch.compiler_aot_inductor_debugging_guide.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "aa7807e9cba63a18cd6865ae2f93685f",
    "source": "pytorch_docs",
    "title": "TorchElastic Kubernetes \u2014 PyTorch 2.9 documentation",
    "text": "\n## TorchElastic Kubernetes#\n\nCreated On: May 04, 2021 | Last Updated On: Jan 23, 2023\nPlease refer to our GitHub\u2019sKubernetes READMEfor more information on Elastic Job Controller and custom resource definition.",
    "url": "https://pytorch.org/docs/stable/elastic/kubernetes.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "507d0eee7b511bd1e3372ba9944c3afb",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/hip.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "80891cc0bb58f9bd7e92c5eef692b014",
    "source": "pytorch_docs",
    "title": "torch.distributed.tensor \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.distributed.tensor#\n\nCreated On: Jun 13, 2025 | Last Updated On: Aug 23, 2025\nNote\ntorch.distributed.tensoris currently in alpha state and under\ndevelopment, we are committing backward compatibility for the most APIs listed\nin the doc, but there might be API changes if necessary.\ntorch.distributed.tensor\n\n## PyTorch DTensor (Distributed Tensor)#\n\nPyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed\nlogic, including sharded storage, operator computation and collective communications across devices/hosts.DTensorcould be used to build different parallelism solutions and support sharded state_dict representation\nwhen working with multi-dimensional sharding.\nDTensor\nPlease see examples from the PyTorch native parallelism solutions that are built on top ofDTensor:\nDTensor\nTensor Parallel\nFSDP2\nDTensorfollows the SPMD (single program, multiple data) programming model to empower users to\nwrite distributed program as if it\u2019s asingle-device program with the same convergence property. It\nprovides a uniform tensor sharding layout (DTensor Layout) through specifying theDeviceMeshandPlacement:\nDTensor\nDeviceMesh\nPlacement\nDeviceMeshrepresents the device topology and the communicators of the cluster using\nan n-dimensional array.\nDeviceMesh\nPlacementdescribes the sharding layout of the logical tensor on theDeviceMesh.\nDTensor supports three types of placements:Shard,ReplicateandPartial.\nPlacement\nDeviceMesh\nShard\nReplicate\nPartial\n\n## DTensor Class APIs#\n\nDTensoris atorch.Tensorsubclass. This means once aDTensoris created, it could be\nused in very similar way totorch.Tensor, including running different types of PyTorch operators as if\nrunning them in a single device, allowing proper distributed computation for PyTorch operators.\nDTensor\ntorch.Tensor\nDTensor\ntorch.Tensor\nIn addition to existingtorch.Tensormethods, it also offers a set of additional methods to interact withtorch.Tensor,redistributethe DTensor Layout to a new DTensor, get the full tensor content\non all devices, etc.\ntorch.Tensor\ntorch.Tensor\nredistribute\nDTensor(Distributed Tensor) is a subclass oftorch.Tensorthat provides single-device like\nabstraction to program with multi-devicetorch.Tensor. It describes the distributed tensor sharding\nlayout (DTensor Layout) through theDeviceMeshand following types ofPlacement:\nDTensor\ntorch.Tensor\ntorch.Tensor\nDeviceMesh\nPlacement\nShard: Tensor sharded on the tensor dimensiondimon the devices of theDeviceMeshdimension\nShard\ndim\nDeviceMesh\nReplicate: Tensor replicated on the devices of theDeviceMeshdimension\nReplicate\nDeviceMesh\nPartial: Tensor is pending reduction on the devices of theDeviceMeshdimension\nPartial\nDeviceMesh\nWhen calling PyTorch operators,DTensoroverrides the PyTorch operators to perform sharded computation and issue\ncommunications whenever necessary. Along with the operator computation,DTensorwill transform or propagate the\nplacements (DTensor Layout) properly (based on the operator semantic itself) and generate newDTensoroutputs.\nDTensor\nDTensor\nDTensor\nTo ensure numerical correctness of theDTensorsharded computation when calling PyTorch operators,DTensorrequires every Tensor argument of the operator be DTensor.\nDTensor\nDTensor\nNote\nDirectly using the Tensor subclass constructor here is not the recommended way to create aDTensor(i.e. it does not handle autograd correctly hence is not the public API). Please refer to thecreate_dtensorsection to see how to create aDTensor.\nDTensor\nDTensor\nDTensor\nReturn a list of ChunkStorageMetadata, which is a dataclass that describes the size/offset of the local shard/replica\non current rank. For DTensor, each rank will have a single local shard/replica, so the returned list usually only\nhas one element.\nThis dunder method is primariy used for distributed checkpoint purpose.\nA List[ChunkStorageMetadata] object that represents the shard size/offset on the current rank.\nChunkStorageMetadata\nCreate aDTensorfrom a local torch.Tensor on each rank\naccording to thedevice_meshandplacementsspecified.\nDTensor\ndevice_mesh\nplacements\nlocal_tensor(torch.Tensor) \u2013 local torch.Tensor on each rank.\ndevice_mesh(DeviceMesh, optional) \u2013 DeviceMesh to place the\ntensor, if not specified, must be called under a DeviceMesh\ncontext manager, default: None\nDeviceMesh\nplacements(List[Placement], optional) \u2013 the placements that\ndescribes how to place the local torch.Tensor on DeviceMesh, must\nhave the same number of elements asdevice_mesh.ndim.\nPlacement\ndevice_mesh.ndim\nrun_check(bool,optional) \u2013 at a cost of extra communications, perform\nsanity check across ranks to check each local tensor\u2019s meta information\nto ensure correctness. If haveReplicateinplacements, the\ndata on first rank of the device mesh dimension will be broadcasted\nto other ranks. default: False\nReplicate\nplacements\nshape(torch.Size,optional) \u2013 A List of int which specifies the size of\nDTensor which build on top oflocal_tensor. Note this needs to be\nprovided if the shape oflocal_tensorare different across the ranks.\nIf not provided,shapewill be computed assuming the given distributed\ntensor is evenly sharded across ranks. default: None\nlocal_tensor\nshape\nstride(tuple,optional) \u2013 A List of int which specifies the stride of DTensor.\nIf not provided,stridewill be computed assuming the given distributed\ntensor is evenly sharded across ranks. default: None\nstride\nADTensorobject\nDTensor\nDTensor\nNote\nWhenrun_check=False, it is the user\u2019s responsibility to ensure the\nlocal tensor passed in is correct across ranks (i.e. the tensor is sharded for\ntheShard(dim)placement or replicated for theReplicate()placement).\nIf not, the behavior of the created DTensor is undefined.\nrun_check=False\nShard(dim)\nReplicate()\nNote\nfrom_localis differentiable, therequires_gradof the createdDTensorobject will depend on iflocal_tensorrequires_grad or not.\nfrom_local\nReturn the full tensor of this DTensor. It will perform necessary collectives\nto gather the local tensors from other ranks in its DeviceMesh and concatenate\nthem together. It\u2019s a syntactic sugar of the following code:\ndtensor.redistribute(placements=[Replicate()]*mesh.ndim).to_local()\ndtensor.redistribute(placements=[Replicate()]*mesh.ndim).to_local()\ngrad_placements(List[Placement], optional) \u2013 the placements describes\nthe future layout of any gradient layout of the full Tensor returned from this\nfunction.full_tensorconverts DTensor to a full torch.Tensor and the returned torch.tensor\nmight not be used as the original replicated DTensor layout later in the code. This\nargument is the hint that user can give to autograd in case the gradient\nlayout of the returned tensor does not match the original replicated DTensor layout.\nIf not specified, we will assume the gradient layout of the full tensor be replicated.\nPlacement\nAtorch.Tensorobject that represents the full tensor of this DTensor.\ntorch.Tensor\nTensor\nNote\nfull_tensoris differentiable.\nfull_tensor\nredistributeperforms necessary collective operations that redistribute the current\nDTensor from its current placements to a new placements, or from its current DeviceMesh\nto a new DeviceMesh. i.e. we can turn a Sharded DTensor to a Replicated DTensor by\nspecifying a Replicate placement for each dimension of the DeviceMesh.\nredistribute\nWhen redistributing from current to the new placements on one device mesh dimension, we\nwill perform the following operations including communication collective or local operation:\nShard(dim)->Replicate():all_gather\nShard(dim)\nReplicate()\nall_gather\nShard(src_dim)->Shard(dst_dim):all_to_all\nShard(src_dim)\nShard(dst_dim)\nall_to_all\nReplicate()->Shard(dim): local chunking (i.e.torch.chunk)\nReplicate()\nShard(dim)\ntorch.chunk\nPartial()->Replicate():all_reduce\nPartial()\nReplicate()\nall_reduce\nPartial()->Shard(dim):reduce_scatter\nPartial()\nShard(dim)\nreduce_scatter\nredistributewould correctly figure out the necessary redistribute steps for DTensors\nthat are created either on 1-D or N-D DeviceMesh.\nredistribute\ndevice_mesh(DeviceMesh, optional) \u2013 DeviceMesh to place the\nDTensor. If not specified, it would use the current DTensor\u2019s DeviceMesh.\ndefault: None\nDeviceMesh\nplacements(List[Placement], optional) \u2013 the new placements that\ndescribes how to place the DTensor into the DeviceMesh, must\nhave the same number of elements asdevice_mesh.ndim.\ndefault: replicate on all mesh dimensions\nPlacement\ndevice_mesh.ndim\nasync_op(bool,optional) \u2013 whether to perform the DTensor redistribute operation\nasynchronously or not. Default: False\nforward_dtype(torch.dtype,optional) \u2013 the local tensor datatype can be converted toforward_dtypebefore redistributing the local tensor in its forward.\nThe result DTensor will be inforward_dtypeDefault: None.\nforward_dtype\nforward_dtype\nbackward_dtype(torch.dtype,optional) \u2013 the local tensor datatype can be converted tobackward_dtypebefore redistributing the local tensor in its backward.\nThe result DTensor gradient would be converted back to the current DTensor dtype. Default: None\nbackward_dtype\nADTensorobject\nDTensor\nDTensor\nNote\nredistributeis differentiable, which means user do not need to worry about\nthe backward formula of the redistribute operation.\nredistribute\nNote\nredistributecurrently only supports redistributing DTensor on the same DeviceMesh,\nPlease file an issue if you need to redistribute DTensor to different DeviceMesh.\nredistribute\nGet the local tensor of this DTensor on its current rank. For sharding it returns\na local shard of the logical tensor view, for replication it returns the replica on\nits current rank.\ngrad_placements(List[Placement], optional) \u2013 the placements describes\nthe future layout of any gradient layout of the Tensor returned from this\nfunction.to_localconverts DTensor to local tensor and the returned local tensor\nmight not be used as the original DTensor layout later in the code. This\nargument is the hint that user can give to autograd in case the gradient\nlayout of the returned tensor does not match the original DTensor layout.\nIf not specified, we will assume the gradient layout remains the same\nas the original DTensor and use that for gradient computation.\nPlacement\nAtorch.TensororAsyncCollectiveTensorobject. it represents the\nlocal tensor on its current rank. When anAsyncCollectiveTensorobject is returned,\nit means the local tensor is not ready yet (i.e. communication is not finished). In this\ncase, user needs to callwaitto wait the local tensor to be ready.\ntorch.Tensor\nAsyncCollectiveTensor\nAsyncCollectiveTensor\nwait\nTensor\nNote\nto_localis differentiable, therequires_gradof the local tensor returned\nwill depend on if theDTensorrequires_grad or not.\nto_local\nrequires_grad\nTheDeviceMeshattribute that associates with this DTensor object.\nDeviceMesh\nNote\ndevice_meshis a read-only property, it can not be set.\ndevice_mesh\nThe placements attribute of this DTensor that describes the layout of this\nDTensor on the its DeviceMesh.\nNote\nplacementsis a read-only property, it can not be set.\nplacements\n\n## DeviceMesh as the distributed communicator#\n\nDeviceMeshwas built from DTensor as the abstraction to describe cluster\u2019s device topology and represent\nmulti-dimensional communicators (on top ofProcessGroup). To see the details of how to create/use a DeviceMesh,\nplease refer to theDeviceMesh recipe.\nDeviceMesh\nProcessGroup\n\n## DTensor Placement Types#\n\nDTensor supports the following types ofPlacementon eachDeviceMeshdimension:\nPlacement\nDeviceMesh\nTheShard(dim)placement describes the DTensor sharding on tensor dimensiondimover a correspondingDeviceMeshdimension, where each rank on the\nDeviceMesh dimension only holds a shard/piece of the global Tensor. TheShard(dim)placement follows thetorch.chunk(dim)semantic, where the\nlast few shards on the DeviceMesh dimension might be empty when the tensor dimension\nis not evenly divisible on the DeviceMesh dimension. TheShardplacement can be\nused by all DTensor APIs (i.e. distribute_tensor, from_local, etc.)\nShard(dim)\ndim\nDeviceMesh\nShard(dim)\ntorch.chunk(dim)\nShard\ndim(int) \u2013 The tensor dimension that describes the DTensor is sharded over its\ncorresponding DeviceMesh dimension.\nWarning\nsharding on a tensor dimension where the tensor dimension size is not\nevenly divisible on a DeviceMesh dimension is currently experimental and subject to change.\nTheReplicate()placement describes the DTensor replicating on a correspondingDeviceMeshdimension, where each rank on the DeviceMesh dimension holds a\nreplica of the global Tensor. TheReplicateplacement can be used by all\nDTensor APIs (i.e.distribute_tensor,DTensor.from_local, etc.)\nReplicate()\nDeviceMesh\nReplicate\ndistribute_tensor\nDTensor.from_local\nThePartial(reduce_op)placement describes the DTensor that is pending\nreduction on a specifiedDeviceMeshdimension, where each rank on the\nDeviceMesh dimension holds the partial value of the global Tensor. User can\nredistribute thePartialDTensor to aReplicateorShard(dim)placement on the specifiedDeviceMeshdimension usingredistribute,\nwhich would trigger necessary communication operations under the hood (i.e.allreduce,reduce_scatter).\nPartial(reduce_op)\nDeviceMesh\nPartial\nReplicate\nShard(dim)\nDeviceMesh\nredistribute\nallreduce\nreduce_scatter\nreduce_op(str,optional) \u2013 The reduction op to be used for the partial DTensor\nto produce Replicated/Sharded DTensor. Only element-wise reduction operations\nare supported, including: \u201csum\u201d, \u201cavg\u201d, \u201cproduct\u201d, \u201cmax\u201d, \u201cmin\u201d, default: \u201csum\u201d.\nNote\nThePartialplacement can be generated as a result of the DTensor operators,\nand can only be used by theDTensor.from_localAPI.\nPartial\nDTensor.from_local\nThe base class for the Placement type, where it describes how a DTensor is placed onto theDeviceMesh.PlacementandDeviceMeshtogether could describe the DTensor Layout.\nIt is the base class of the three main DTensor Placement types:Shard,Replicate,\nandPartial.\nDeviceMesh\nPlacement\nDeviceMesh\nShard\nReplicate\nPartial\nThis class is not meant to be used directly, mainly served as a typing stub.\nbool\nbool\nbool\n\n## Different ways to create a DTensor#\n\nDTensor\ndistribute_tensor()creates aDTensorfrom a logical or \u201cglobal\u201dtorch.Tensoron\neach rank. This could be used to shard the leaftorch.Tensors (i.e. model parameters/buffers\nand inputs).\ndistribute_tensor()\nDTensor\ntorch.Tensor\ntorch.Tensor\nDTensor.from_local()creates aDTensorfrom a localtorch.Tensoron each rank, which can\nbe used to createDTensorfrom a non-leaftorch.Tensors (i.e. intermediate activation\ntensors during forward/backward).\nDTensor.from_local()\nDTensor\ntorch.Tensor\nDTensor\ntorch.Tensor\nDTensor provides dedicated tensor factory functions (e.g.empty(),ones(),randn(), etc.)\nto allow differentDTensorcreations by directly specifying theDeviceMeshandPlacement. Compare todistribute_tensor(), this could directly materializing the sharded memory\non device, instead of performing sharding after initializing the logical Tensor memory.\nempty()\nones()\nrandn()\nDTensor\nDeviceMesh\nPlacement\ndistribute_tensor()\n\n## Create DTensor from a logical torch.Tensor#\n\nThe SPMD (single program, multiple data) programming model intorch.distributedlaunches multiple processes\n(i.e. viatorchrun) to execute the same program, this means that the model inside the program would be\ninitialized on different processes first (i.e. the model might be initialized on CPU, or meta device, or directly\non GPU if enough memory).\ntorch.distributed\ntorchrun\nDTensoroffers adistribute_tensor()API that could shard the model weights or Tensors toDTensors,\nwhere it would create a DTensor from the \u201clogical\u201d Tensor on each process. This would empower the createdDTensors to comply with the single device semantic, which is critical fornumerical correctness.\nDTensor\ndistribute_tensor()\nDTensor\nDTensor\nDistribute a leaftorch.Tensor(i.e. nn.Parameter/buffers) to thedevice_meshaccording\nto theplacementsspecified. The rank ofdevice_meshandplacementsmust be the\nsame. Thetensorto distribute is the logical or \u201cglobal\u201d tensor, and the API would use\nthetensorfrom first rank of the DeviceMesh dimension as the source of truth to preserve\nthe single-device semantic. If you want to construct a DTensor in the middle of the Autograd\ncomputation, please useDTensor.from_local()instead.\ntorch.Tensor\ndevice_mesh\nplacements\ndevice_mesh\nplacements\ntensor\ntensor\nDTensor.from_local()\ntensor(torch.Tensor) \u2013 torch.Tensor to be distributed. Note that if you\nwant to shard a tensor on a dimension that is not evenly divisible by\nthe number of devices in that mesh dimension, we usetorch.chunksemantic to shard the tensor and scatter the shards. The uneven sharding\nbehavior is experimental and subject to change.\ntorch.chunk\ndevice_mesh(DeviceMesh, optional) \u2013 DeviceMesh to distribute the\ntensor, if not specified, must be called under a DeviceMesh context\nmanager, default: None\nDeviceMesh\nplacements(List[Placement], optional) \u2013 the placements that\ndescribes how to place the tensor on DeviceMesh, must have the same\nnumber of elements asdevice_mesh.ndim. If not specified, we will\nby default replicate the tensor across thedevice_meshfrom the\nfirst rank of each dimension of thedevice_mesh.\nPlacement\ndevice_mesh.ndim\ndevice_mesh\nsrc_data_rank(int,optional) \u2013 the rank of the source data for the logical/global tensor, it is\nused bydistribute_tensor()to scatter/broadcast the shards/replicas to other ranks.\nBy default, we usegroup_rank=0on each DeviceMesh dimension as the source data to preserve\nthe single-device semantic. If passingNoneexplicitly,distribute_tensor()simply uses\nits local data instead of trying to preserve the single-device semantic via scatter/broadcast.\nDefault: 0\ndistribute_tensor()\ngroup_rank=0\nNone\ndistribute_tensor()\nADTensororXLAShardedTensorobject.\nDTensor\nXLAShardedTensor\nDTensor\nNote\nWhen initialize the DeviceMesh with thexladevice_type,distribute_tensorreturnXLAShardedTensorinstead. seethis issuefor more details. The XLA integration is experimental and subject to change.\nxla\ndistribute_tensor\nAlong withdistribute_tensor(), DTensor also offers adistribute_module()API to allow easier\nsharding on thenn.Modulelevel\ndistribute_tensor()\ndistribute_module()\nnn.Module\nThis function expose three functions to control the parameters/inputs/outputs of the module:\n1. To perform sharding on the module before runtime execution by specifying thepartition_fn(i.e. allow user to convert Module parameters toDTensorparameters according to thepartition_fnspecified).\n2. To control the inputs or outputs of the module during runtime execution by\nspecifying theinput_fnandoutput_fn. (i.e. convert the input toDTensor, convert the output back totorch.Tensor)\npartition_fn\nDTensor\ninput_fn\noutput_fn\nDTensor\ntorch.Tensor\nmodule(nn.Module) \u2013 user module to be partitioned.\nnn.Module\ndevice_mesh(DeviceMesh) \u2013 the device mesh to place the module.\nDeviceMesh\npartition_fn(Callable) \u2013 the function to partition parameters (i.e. shard certain\nparameters across thedevice_mesh). Ifpartition_fnis not specified,\nby default we replicate all module parameters ofmoduleacross the mesh.\ndevice_mesh\npartition_fn\nmodule\ninput_fn(Callable) \u2013 specify the input distribution, i.e. could control how the\ninput of the module is sharded.input_fnwill be installed as a moduleforward_pre_hook(pre forward hook).\ninput_fn\nforward_pre_hook\noutput_fn(Callable) \u2013 specify the output distribution, i.e. could control how the\noutput is sharded, or convert it back to torch.Tensor.output_fnwill be\ninstalled as a moduleforward_hook(post forward hook).\noutput_fn\nforward_hook\nA module that contains parameters/buffers that are allDTensors.\nDTensor\nModule\n\nNote\nWhen initialize the DeviceMesh with thexladevice_type,distribute_modulereturn nn.Module with PyTorch/XLA SPMD annotated parameters. Seethis issuefor more details. The XLA integration is experimental and subject to change.\nxla\ndistribute_module\n\n## DTensor Factory Functions#\n\nDTensor also provides dedicated tensor factory functions to allow creatingDTensordirectly\nusing torch.Tensor like factory function APIs (i.e. torch.ones, torch.empty, etc), by additionally\nspecifying theDeviceMeshandPlacementfor theDTensorcreated:\nDTensor\nDeviceMesh\nPlacement\nDTensor\nReturns aDTensorfilled with the scalar value 0.\nDTensor\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: zeros(1,2,3..) or zeros([1,2,3..]) or zeros((1,2,3..))\nDTensor\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returnedDTensor.\nDefault:torch.strided.\ntorch.layout\nDTensor\ntorch.strided\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled with the scalar value 1, with the shape defined\nby the variable argumentsize.\nDTensor\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))\nDTensor\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returned DTensor.\nDefault:torch.strided.\ntorch.layout\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled with uninitialized data. The shape of theDTensoris defined by the variable argumentsize.\nDTensor\nDTensor\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: empty(1,2,3..) or empty([1,2,3..]) or empty((1,2,3..))\nDTensor\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).        layout (torch.layout, optional): the desired layout of returnedDTensor.\nDefault:torch.strided.\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\ntorch.layout\nDTensor\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled withfill_valueaccording todevice_meshandplacements, with the shape defined by the argumentsize.\nDTensor\nfill_value\ndevice_mesh\nplacements\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))\nDTensor\nfill_value(Scalar) \u2013 the value to fill the output tensor with.\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returned DTensor.\nDefault:torch.strided.\ntorch.layout\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks.\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled with random numbers from a uniform distribution\non the interval[0,1). The shape of the tensor is defined by the variable\nargumentsize.\nDTensor\n[0,1)\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))\nDTensor\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returned DTensor.\nDefault:torch.strided.\ntorch.layout\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks.\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\nReturns aDTensorfilled with random numbers from a normal distribution\nwith mean 0 and variance 1. The shape of the tensor is defined by the variable\nargumentsize.\nDTensor\nsize\nsize(int...) \u2013 a sequence of integers defining the shape of the outputDTensor.\nCan be a variable number of arguments or a collection like a list or tuple.\nE.g.: ones(1,2,3..) or ones([1,2,3..]) or ones((1,2,3..))\nDTensor\ndtype(torch.dtype, optional) \u2013 the desired data type of returnedDTensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nDTensor\nNone\ntorch.set_default_dtype()\nlayout(torch.layout, optional) \u2013 the desired layout of returned DTensor.\nDefault:torch.strided.\ntorch.layout\ntorch.strided\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturnedDTensor. Default:False.\nDTensor\nFalse\ndevice_mesh\u2013DeviceMeshtype, contains the mesh info of ranks.\nDeviceMesh\nplacements\u2013 a sequence ofPlacementtype:Shard,Replicate\nPlacement\nShard\nReplicate\nADTensorobject on each rank\nDTensor\nDTensor\n\n## Random Operations#\n\nDTensor provides distributed RNG functionality to ensure that random operations on sharded tensors get unique values, and random operations on replicated tensors get the same values. This system requires that all participating\nranks (e.g. SPMD ranks) start out using the same generator state before each dtensor random operation is performed,\nand if this is true, it ensures they all end up at the same state after each dtensor random operation completes. There is no communication performed during random operations to synchronize RNG states.\nOperators that accept ageneratorkwarg will utilize the user-passed generator, if passed, or the default generator for the device otherwise. Whichever generator is used, it will be advanced after the DTensor operation.  It is valid to use the same generator for both DTensor and non-DTensor operations, but care must be taken to ensure the non-DTensor operations advance the generator state equally on all ranks if so.\ngenerator\nWhen using DTensor together with Pipeline Parallelism, ranks for each pipeline stage should use a distinct seed, and ranks within a pipeline stage should use the same seed.\nDTensor\u2019s RNG infra is based on the philox based RNG algorithm, and supports any philox based backend (cuda, and other cuda-like devices), but unfortunately does not yet support the CPU backend.\n\n## Debugging#\n\n\n## Logging#\n\nWhen launching the program, you can turn on additional logging using theTORCH_LOGSenvironment variable fromtorch._logging:\nTORCH_LOGS\nTORCH_LOGS=+dtensorwill displaylogging.DEBUGmessages and all levels above it.\nTORCH_LOGS=+dtensor\nlogging.DEBUG\nTORCH_LOGS=dtensorwill displaylogging.INFOmessages and above.\nTORCH_LOGS=dtensor\nlogging.INFO\nTORCH_LOGS=-dtensorwill displaylogging.WARNINGmessages and above.\nTORCH_LOGS=-dtensor\nlogging.WARNING\n\n## Debugging Tools#\n\nTo debug the program that applied DTensor, and understand more details about what collectives happened under the\nhood, DTensor provides aCommDebugMode:\nCommDebugMode\nCommDebugModeis a context manager that counts the number of\nfunctional collectives within its context. It does this using aTorchDispatchMode.\nCommDebugMode\nTorchDispatchMode\nNote\nNot all collectives are supported yet.\nExample usage\n\n```python\nmod = ...\ncomm_mode = CommDebugMode()\nwith comm_mode:\n    mod.sum().backward()\nprint(comm_mode.get_comm_counts())\n\n```\n\nGenerates detailed table displaying operations and collective tracing information\non a module level. Amount of information is dependent on noise_level\nprints module-level collective counts\nprints dTensor operations not included in trivial operations, module information\nprints operations not included in trivial operations\nprints all operations\nCreates json file used to build browser visual\n0. prints module-level collective counts\n1. prints dTensor operations not included in trivial operations\n2. prints operations not included in trivial operations\n3. prints all operations\nReturns the communication counts as a dictionary.\nThe communication counts as a dictionary.\nDict[Any,int]\ndict[str,dict[str,Any]]\ndict[str,dict[str,Any]]\nint\nAlternative to console CommDebugMode output, writes to file specified by the user\nTo visualize the sharding of a DTensor that have less than 3 dimensions, DTensor providesvisualize_sharding():\nvisualize_sharding()\nVisualizes sharding in the terminal forDTensorthat are 1D or 2D.\nDTensor\nNote\nThis requires thetabulatepackage, orrichandmatplotlib.\nNo sharding info will be printed for empty tensors\ntabulate\nrich\nmatplotlib\n\n## Experimental Features#\n\nDTensoralso provides a set of experimental features. These features are either in prototyping stage, or the basic\nfunctionality is done and but looking for user feedbacks. Please submit a issue to PyTorch if you have feedbacks to\nthese features.\nDTensor\ncontext_parallelis an experimental API to enable context\nparallelism (CP). This API performs two actions: 1) patch the SDPA\n(torch.nn.functional.scaled_dot_product_attention) with the CP-enabled\none, 2) shardbuffersalong the sequence dimension and each rank will\npreserve the corresponding shard accordingmesh.\ncontext_parallel\ntorch.nn.functional.scaled_dot_product_attention\nbuffers\nmesh\nmesh(DeviceMesh) \u2013 the device mesh for the context parallelism.\nDeviceMesh\nbuffers(Optional[List[torch.Tensor]]) \u2013 buffers that the usage depend\non the sequence dimension. Examples are input batch, labels and\npositional embedding buffers. These buffers must be sharded along\nthe sequence dimension to ensure the accuracy. The sharding will\nhappen in-place, the buffer\u2019s shape will change within the context.\nThe buffers will be restored after the context finishes.no_restore_bufferscan be used to specify which buffers don\u2019t\nneed to be restored. Note thatbuffersshould not contain any\nnn.Parameter.\nno_restore_buffers\nbuffers\nbuffer_seq_dims(Optional[List[int]]) \u2013 the sequence dimensions ofbuffers.\nbuffers\nno_restore_buffers(Optional[Set[torch.Tensor]]) \u2013 buffers in these set\nwon\u2019t be restored after the context exits. This set must be a subset\nofbuffers. If the buffers won\u2019t be used after the context exits,\nthese buffers can be put in this list to avoid extra restore time.\nbuffers\nGenerator[None, None, None]\nWarning\ntorch.distributed.tensor.experimental.context_parallelis a\nprototype feature in PyTorch. The API is subject to change.\nlocal_map()is an experimental API that allows users to passDTensors\nto a function that is written to be applied ontorch.Tensors. It is done by extracting\nthe local components ofDTensor, call the function, and wrap the outputs toDTensoraccording to theout_placements.\nlocal_map()\nDTensor\ntorch.Tensor\nDTensor\nDTensor\nout_placements\nfunc(Callable) \u2013 the function to be applied on each local shard ofDTensors.\nDTensor\nout_placements(Union[PlacementType, Tuple[PlacementType, \u2026]]) \u2013 the desired placements of theDTensors infunc\u2019s flattened output.\nIf the flattenedoutputis a single value, theout_placementsshould be\nof typePlacementType. Otherwise if the flattenedoutputhas multiple\nvalues, theout_placementsshould be a tuple ofPlacementTypevalues 1:1\nmapping to the flattenedoutput.\nBesides, forTensoroutput, we usePlacementTypeas its\nplacements (aTuple[Placement]value). For non-Tensor output, thePlacementTypeshould beNone.\nNote that the only exception is when noDTensorargument is passed\nin. In this case, even ifout_placementsis notNone, the result function\nshould ignore the desired placements because the function is not running withDTensors.\nDTensor\nfunc\noutput\nout_placements\noutput\nout_placements\noutput\nTensor\nDTensor\nDTensor\nin_placements(Tuple[PlacementType, \u2026], optional) \u2013 the required placements of theDTensors in the flattened inputs offunc.\nIfin_placementsis specified,local_map()would examine whether the\nplacements of eachDTensorargument is the same as the required\nplacements or not. If the placements are not the same andredistribute_inputsisFalse, an exception will be raised. Otherwise ifredistribute_inputsisTrue, the argument will be first redistributed to\nthe required sharding placements before passing its local tensor tofunc.\nThe only exception is when required placements are notNoneand the\nargument is atorch.Tensor. In this case, the placements examination\nwill be skipped and the argument will be directly passed tofunc.\nIfin_placementsisNone, no placements examination will be performed.\nDefault: None\nDTensor\nfunc\nin_placements\nlocal_map()\nDTensor\nredistribute_inputs\nFalse\nredistribute_inputs\nTrue\nfunc\nNone\ntorch.Tensor\nfunc\nin_placements\nNone\nin_grad_placements(Tuple[PlacementType, \u2026], optional) \u2013 the placements hint of theDTensors gradient corresponds\nto the flattened input DTensor. This argument is the hint that user\ncan give toto_local()in case the gradient layout of the\nlocal tensor input does not match itsDTensorinput layout.\nIf not specified, we will assume the gradient layout of the local\ntensor input remains the same as the originalDTensorinput\nand use that for gradient computation. Default: None.\nDTensor\nto_local()\nDTensor\nDTensor\ndevice_mesh(DeviceMesh, optional) \u2013 the device mesh that the outputDTensors are placed on. If not\nspecified, this will be inferred from the first inputDTensor\u2019s device\nmesh. Default: None.\nDeviceMesh\nDTensor\nDTensor\nredistribute_inputs(bool,optional) \u2013 the bool value indicating whether to reshard the inputDTensors when\ntheir placements are different from the required input placements. If this\nvalue isFalseand someDTensorinput has a different placement,\nan exception will be raised. Default: False.\nDTensor\nFalse\nDTensor\nACallablethat appliesfuncto each local shard of the inputDTensorand returns aDTensorconstructed from the return value offunc.\nCallable\nfunc\nDTensor\nDTensor\nfunc\nAssertionError\u2013 For any non-DTensor output, we require its corresponding\n    output placement inout_placementsbe None. An AssertionError will be raised\n    if this is not the case.\nout_placements\nValueError\u2013 Ifredistribute_inputs=Falsebut the inputDTensorneeds\n    a redistribution according toin_placements.\nredistribute_inputs=False\nDTensor\nin_placements\nExample\n\n```python\n>>> def mm_allreduce_forward(device_mesh, W, X):\n>>>     partial_sum_tensor = torch.mm(W, X)\n>>>     reduced_tensor = funcol.all_reduce(partial_sum_tensor, \"sum\", device_mesh)\n>>>     return reduced_tensor\n>>>\n>>> W = torch.randn(12, 8, requires_grad=False)\n>>> X = torch.randn(8, 16, requires_grad=False)\n>>> Y = torch.mm(W, X)\n>>> row_wise = [Shard(0)]  # row-wise sharding placements on 1-d mesh\n>>> col_wise = [Shard(1)]  # col-wise sharding placements on 1-d mesh\n>>>\n>>> # local_mm_allreduce_forward is the function wrapped with DTensor/Tensor conversion\n>>> local_mm_allreduce_forward = local_map(\n>>>     mm_allreduce_forward,\n>>>     out_placements=[Replicate()],\n>>>     in_placements=[col_wise, row_wise],\n>>>     device_mesh=device_mesh,\n>>> )\n>>>\n>>> W_dt = distribute_tensor(\n...     W, device_mesh, (col_wise)\n... )  # col-wisely sharded W tensor\n>>> X_dt = distribute_tensor(\n...     X, device_mesh, (row_wise)\n... )  # row-wisely sharded X tensor\n>>> Y_dt = local_mm_allreduce_forward(\n...     device_mesh, W_dt, X_dt\n... )  # apply local_mm_allreduce_forward to DTensors\n\n```\n\nNote\nThis API is currently experimental and subject to change\nregister_sharding()is an experimental API that allows users to register sharding\nstrategies for an operator when the tensor inputs and outputs are DTensor.\nIt can be useful when: (1) there doesn\u2019t exist a default sharding strategy forop,\ne.g. whenopis a custom operator that is not supported byDTensor; (2)\nwhen users would like to overwrite default sharding strategies of existing operators.\nregister_sharding()\nop\nop\nDTensor\nop(Union[OpOverload,List[OpOverload]]) \u2013 An op or a list of ops to register the customized sharding function.\nA function decorator which can be used to wrap a function that defines the sharding\nstrategy for the operator specified inop. The defined sharding strategy will be\nregistered to DTensor and will override the default sharding strategy if DTensor has\nalready implemented the operator. The customized sharding function takes the same inputs\nas the original op (except that if an arg is atorch.Tensor, it will be\nreplaced by a tensor-like object that DTensor uses internally). The function should\nreturn a sequence of 2-tuples, each specifying acceptable output placements and its\ncorresponding input placements.\nop\ntorch.Tensor\nExample\n\n```python\n>>> @register_sharding(aten._softmax.default)\n>>> def custom_softmax_sharding(x, dim, half_to_float):\n>>>     softmax_dim = dim if dim >= 0 else dim + x.ndim\n>>>     acceptable_shardings = []\n>>>\n>>>     all_replicate = ([Replicate()], [Replicate(), None, None])\n>>>     acceptable_shardings.append(all_replicate)\n>>>\n>>>     for sharding_dim in range(x.ndim):\n>>>         if sharding_dim != softmax_dim:\n>>>             all_sharded = (\n>>>                 [Shard(sharding_dim)],\n>>>                 [Shard(sharding_dim), None, None],\n>>>             )\n>>>             acceptable_shardings.append(all_sharded)\n>>>\n>>>     return acceptable_shardings\n\n```\n\nNote\nThis API is currently experimental and subject to change",
    "url": "https://pytorch.org/docs/stable/distributed.tensor.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b7afc32f31f632d0323df512e732397a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/cuda.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b6af5e9e5f93bb40957558c84554995c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/monitor.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "09caa721f32dfd97d43bc0cdfd498b19",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/xpu.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "213c21dbdd8f25e3f7a4c6006466a616",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/community/index.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3e0051c0a74ebf7efae6ef0f1dec2243",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/modules.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "179daf32a0cb75524f8e222a33b5f38c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/quantization-support.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8e420bc3916ce89132f3053b19c0aaa3",
    "source": "pytorch_docs",
    "title": "torch.distributed.fsdp.fully_shard \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.distributed.fsdp.fully_shard#\n\nCreated On: Dec 04, 2024 | Last Updated On: Jun 16, 2025\n\n## PyTorch FSDP2 (fully_shard)#\n\nfully_shard\nPyTorch FSDP2 (RFC) provides\na fully sharded data parallelism (FSDP) implementation targeting performant\neager-mode while using per-parameter sharding for improved usability\nSee theGetting Started with FSDP2tutorial for more information.\nIf you are currently using FSDP1, consider migrating to FSDP2 using ourmigration guide.\nThe user contract forfully_shard(model)is as follows\nfully_shard(model)\nFor model initialization, fully_shard converts model.parameters() from\nplain torch.Tensor to DTensor in-place. The parameters are moved to the\nappropriate device according to the device mesh.\nBefore forward and backward passes, pre-forward/backward hooks are\nresponsible for all-gathering the parameters and converting model.parameters()\nfrom DTensor to plain torch.Tensor.\nAfter forward and backward passes, post-forward/backward hooks free\nthe unsharded parameters (no communication needed) and convert\nmodel.parameters() from plain torch.Tensor back to DTensor.\nFor the optimizer, it must be initialized with the DTensor model.parameters(),\nand the optimizer step should be performed on DTensor parameters.\nCallmodel(input)instead ofmodel.forward(input)to trigger pre-forward\nhooks to all-gather parameters. To make model.forward(input) work, users must\neither callmodel.unshard()explicitly or useregister_fsdp_forward_method(model,\"forward\")to register the forward method for hooking.\nmodel(input)\nmodel.forward(input)\nmodel.unshard()\nregister_fsdp_forward_method(model,\"forward\")\nfully_shard groups parameters together for a single all-gather. User should apply\nfully_shard in a bottom-up manner. For example, in a Transformer model, fully_shard\nshould be applied to each layer before applying it to the root model. When applied\nto the root model, fully_shard excludes model.parameters() from each layer and groups\nthe remaining parameters (e.g., embeddings, output projection) into a single\nall-gather group.\ntype(model)is \u201cunioned\u201d withFSDPModulein-place. For example, if model\nis originally of type nn.Linear, then fully_shard changestype(model)from\nnn.Linear toFSDPLinearin-place.FSDPLinearis an instance of both\nnn.Linear andFSDPModule. It retains all methods of nn.Linear while also\nexposing FSDP2-specific APIs under FSDPModule, such asreshard()andunshard().\ntype(model)\nFSDPModule\ntype(model)\nFSDPLinear\nFSDPLinear\nFSDPModule\nreshard()\nunshard()\nFully Qualified Names (FQNs) for parameters remain unchanged. If we callmodel.state_dict(), the FQNs are the same before and after applying\nfully_shard. This is because fully_shard does not wrap the module but only\nregisters hooks to the original module.\nmodel.state_dict()\nCompared to PyTorch FSDP1 (FullyShardedDataParallel):\nFullyShardedDataParallel\nFSDP2 usesDTensor-based dim-0 per-parameter sharding for a simpler\nsharding representation compared to FSDP1\u2019s flat-parameter sharding, while\npreserving similar throughput performance. More specifically, FSDP2 chunks\neach parameter on dim-0 across the data parallel workers (usingtorch.chunk(dim=0)), whereas FSDP1 flattens, concatenates, and chunks a\ngroup of tensors together, making reasoning about what data is present on\neach worker and resharding to different parallelisms complex. Per-parameter\nsharding provides a more intuitive user experience, relaxes constraints\naround frozen parameters, and allows for communication-free (sharded) state\ndicts, which otherwise require all-gathers in FSDP1.\nDTensor\ntorch.chunk(dim=0)\nFSDP2 implements a different memory management approach to handle the\nmulti-stream usages that avoidstorch.Tensor.record_stream. This ensures\ndeterministic and expected memory usage and does not require blocking the CPU\nlike in FSDP1\u2019slimit_all_gathers=True.\ntorch.Tensor.record_stream\nlimit_all_gathers=True\nFSDP2 exposes APIs for manual control over prefetching and collective\nscheduling, allowing power users more customization. See the methods onFSDPModulebelow for details.\nFSDPModule\nFSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly\nsupport full state dicts. Instead, users can reshard the sharded state dicts\ncontainingDTensors to full state dicts themselves usingDTensorAPIs likeDTensor.full_tensor()or by using higher-level APIs likePyTorch Distributed Checkpoint\u2018s\ndistributed state dict APIs. Also, some other args have been removed; seeherefor\ndetails.\nDTensor\nDTensor\nDTensor.full_tensor()\nThe frontend API isfully_shardthat can be called on amodule:\nfully_shard\nmodule\nApply fully sharded data parallelism (FSDP) tomodule, where FSDP\nshards module parameters, gradients, and optimizer states across data\nparallel workers to save memory at the cost of communication.\nmodule\nAt initialization, FSDP shards the module\u2019s parameters across the data\nparallel workers given bymesh. Before forward, FSDP all-gathers the\nsharded parameters across the data-parallel workers to get the unsharded\nparameters for forward computation. Ifreshard_after_forwardisTrue, then FSDP frees the unsharded parameters after forward and\nre-all-gathers them in backward before gradient computation. After gradient\ncomputation, FSDP frees the unsharded parameters and reduce-scatters the\nunsharded gradients across data-parallel workers.\nmesh\nreshard_after_forward\nTrue\nThis implementation represents the sharded parameters asDTensors\nsharded on dim-0, while the unsharded parameters will be like the original\nparameters onmodule(e.g.torch.Tensorif originallytorch.Tensor). A moduleforward pre-hookonmoduleall-gathers the parameters, and a moduleforward hookonmodulefrees them (if needed). Similar backward hooks all-gather\nparameters and later free parameters and reduce-scatter gradients.\nDTensor\nmodule\ntorch.Tensor\ntorch.Tensor\nmodule\nmodule\nSince grouping multiple tensors together for one collective is critical for\ncommunication efficiency, this implementation makes this grouping first\nclass. Callingfully_shard()onmoduleconstructs one group that\nincludes the parameters inmodule.parameters()except those already\nassigned to a group from an earlier call on a submodule. This means thatfully_shard()should be called bottom-up on your model. Each group\u2019s\nparameters are all-gathered in one collective, and its gradients are\nreduce-scattered in one collective. Partitioning the model into multiple\ngroups (\u201clayer by layer\u201d) allows for peak memory savings and communication/computation\noverlap. Users generally shouldnotcallfully_shard()only on the\ntopmost root module.\nfully_shard()\nmodule\nmodule.parameters()\nfully_shard()\nfully_shard()\nmodule(Union[nn.Module,List[nn.Module]) \u2013 The module or modules to\nshard with FSDP and group together for communication.\nmesh(Optional[DeviceMesh]) \u2013 This data parallel mesh defines the\nsharding and device. If 1D, then parameters are fully sharded\nacross the 1D mesh (FSDP) with(Shard(0),)placement. If 2D,\nthen parameters are sharded across the 1st dim and replicated\nacross the 0th dim (HSDP) with(Replicate(),Shard(0))placement. The mesh\u2019s device type gives the device type used for\ncommunication; if a CUDA or CUDA-like device type, then we use the\ncurrent device.\n(Shard(0),)\n(Replicate(),Shard(0))\nreshard_after_forward(Optional[Union[bool,int]]) \u2013This controls the parameter\nbehavior after forward and can trade off memory and communication:IfTrue, then this reshards parameters after forward and\nre-all-gathers in backward.IfFalse, then this keeps the unsharded parameters in memory\nafter forward and avoids the all-gather in backward. For best performance,\nwe usually setFalsefor the root module, because the root module\nis typically required immediately when the backward pass begins.IfNone, it is set toTruefor non-root modules andFalsefor root modules.If anint, then this represents the world size to reshard to\nafter forward. It should be a non-trivial divisor of themeshshard dim size (i.e. excluding 1 and the dim size itself). A\nchoice may be the intra-node size (e.g.torch.cuda.device_count()).\nThis allows the all-gather in backward to be over a smaller world\nsize at the cost of higher memory usage than setting toTrue.After forward, the parameters registered to the module depend on\nto this: The registered parameters are the sharded parameters ifTrue; unsharded parameters ifFalse; and the parameters\nresharded to the smaller mesh otherwise. To modify the parameters\nbetween forward and backward, the registered parameters must be\nthe sharded parameters. ForFalseor anint, this can be\ndone by manually resharding viareshard().\nThis controls the parameter\nbehavior after forward and can trade off memory and communication:\nIfTrue, then this reshards parameters after forward and\nre-all-gathers in backward.\nTrue\nIfFalse, then this keeps the unsharded parameters in memory\nafter forward and avoids the all-gather in backward. For best performance,\nwe usually setFalsefor the root module, because the root module\nis typically required immediately when the backward pass begins.\nFalse\nFalse\nIfNone, it is set toTruefor non-root modules andFalsefor root modules.\nNone\nTrue\nFalse\nIf anint, then this represents the world size to reshard to\nafter forward. It should be a non-trivial divisor of themeshshard dim size (i.e. excluding 1 and the dim size itself). A\nchoice may be the intra-node size (e.g.torch.cuda.device_count()).\nThis allows the all-gather in backward to be over a smaller world\nsize at the cost of higher memory usage than setting toTrue.\nint\nmesh\ntorch.cuda.device_count()\nTrue\nAfter forward, the parameters registered to the module depend on\nto this: The registered parameters are the sharded parameters ifTrue; unsharded parameters ifFalse; and the parameters\nresharded to the smaller mesh otherwise. To modify the parameters\nbetween forward and backward, the registered parameters must be\nthe sharded parameters. ForFalseor anint, this can be\ndone by manually resharding viareshard().\nTrue\nFalse\nFalse\nint\nreshard()\nshard_placement_fn(Optional[Callable[[nn.Parameter],Optional[Shard]]]) \u2013 This callable can be used to override the sharding placement for a\nparameter to shard a parameter on a dimension other than dim-0. If\nthis callable returns aShardplacement (notNone),\nthen FSDP will shard according to that placement (e.g.Shard(1)).\nIf sharding on a nonzero dim, we currently require even sharding,\ni.e. the tensor dim size on that dim must be divisible by the FSDP\nshard mesh size.\nShard\nNone\nShard(1)\nmp_policy(MixedPrecisionPolicy) \u2013 This controls the mixed precision\npolicy, which offers parameter/reduction mixed precision for this\nmodule. SeeMixedPrecisionPolicyfor details.\nMixedPrecisionPolicy\noffload_policy(OffloadPolicy) \u2013 This controls the offloading policy,\nwhich offers parameter/gradient/optimizer state offloading. SeeOffloadPolicyand its subclasses for details.\nOffloadPolicy\nignored_params(Optional[set[nn.Parameter]]) \u2013 Optional(Set[nn.Parameter]): The set of parameters to be\nignored by FSDP. They will not be sharded, nor moved to the device\nduring init, nor have their gradients reduced in backward.\nThe module with FSDP applied (in-place).\nFSDPModule\nReshards the module\u2019s parameters, freeing the unsharded parameters if\nthey are allocated and registering the sharded parameters to the\nmodule. This method isnotrecursive.\nhook(Callable[[torch.Tensor],None]) \u2013 User-defined all-reduce hook\nwith expected signaturehook(reduce_output:torch.Tensor)->Nonewherereduce_outputis the reduce-scatter output if only\nusing FSDP or the all-reduce output if using native HSDP.\nhook(reduce_output:torch.Tensor)->None\nreduce_output\nstream(Optional[torch.cuda.Stream]) \u2013 Stream to run the all-reduce\nhook in. This should only be set if not using native HSDP. If\nusing native HSDP, the hook will run in the internally defined\nall-reduce stream used by the native HSDP all-reduce.\nSets whether the temporary staging buffers used to send and receive data\nover collective communications should be allocated using the custom\noptimized allocator provided by the ProcessGroup itself (if any). This\nmight allow the ProcessGroup to be more efficient. For example, when\nusing NCCL, this enables it to leverage zero-copy transfers over SHARP\n(for NVLink and/or InfiniBand).\nThis cannot be used together withset_custom_all_gather()orset_custom_reduce_scatter()as those APIs allow for\nfiner-grained control over each communication, and this method cannot\ndetermine their staging buffer allocation strategy.\nset_custom_all_gather()\nset_custom_reduce_scatter()\nenable(bool) \u2013 Whether to turn on ProcessGroup allocation.\nOverrides the defaultall_gathercommunication behavior,\nto have better control over the communication and memory usage.\nSeeCommandReduceScatterfor details.\nall_gather\ncomm(AllGather) \u2013 Custom all-gather communication.\nOverrides the defaultreduce_scattercommunication behavior,\nto have better control over the communication and memory usage.\nSeeCommandReduceScatterfor details.\nreduce_scatter\ncomm(ReduceScatter) \u2013 Custom reduce_scatter communication.\nSets whether to require the low-level collective communication\nprimitives to exclusively use \u201csum\u201d-type reductions, even if it comes\nat the cost of separate additional pre- or post-scaling operations.\nThis is needed for example because NCCL currently supports zero-copy\ntransfers only for this kind of collectives.\nNB: for MTIA devices, this is always implicitly enabled.\nNB: ifset_all_reduce_hookis used under FSDP setup, the caller needs\nto ensure the custom all-reduce across FSDP units follow this strategy\nas well, as FSDP can no longer automatically handle that.\nenable(bool) \u2013 Whether to only ever use ReduceOp.SUM for comms.\nSets a custom divide factor for the gradient reduction. This might use\na custom reduce op using NCCL\u2019s PreMulSum, which allows multiplying by\nthe factor before reduction.\nfactor(float) \u2013 Custom divide factor.\nSets whether the next backward is the last one. On the last backward,\nFSDP waits on pending gradient reduction and clears internal data\ndata structures for backward prefetching. This can be useful for\nmicrobatching.\nSets the FSDP modules for which this FSDP module should explicitly\nprefetch all-gathers in backward. This overrides the default backward\npretching implementation that prefetches the next FSDP module based on\nthe reverse post-forward order.\nPassing a singleton list containing the previous FSDP module gives the\nsame all-gather overlap behavior as the default overlap behavior.\nPassing a list with at least length two is required for more aggressive\noverlap and will use more reserved memory.\nmodules(List[FSDPModule]) \u2013 FSDP modules to prefetch.\nSets the FSDP modules for which this FSDP module should explicitly\nprefetch all-gathers in forward. The prefetching runs after this\nmodule\u2019s all-gather copy-out.\nPassing a singleton list containing the next FSDP module gives the same\nall-gather overlap behavior as the default overlap behavior, except the\nprefetched all-gather is issued earlier from the CPU. Passing a list\nwith at least length two is required for more aggressive overlap and\nwill use more reserved memory.\nmodules(List[FSDPModule]) \u2013 FSDP modules to prefetch.\nSets a post-optimizer-step event for the root FSDP module to wait the\nall-gather streams on.\nBy default, the root FSDP module waits the all-gather streams on the\ncurrent stream to ensure that the optimizer step has finished before\nall-gathering. However, this may introduce false dependencies if\nthere is unrelated computation after the optimizer step. This API\nallows the user to provide their own event to wait on. After the root\nwaits on the event, the event is discarded, so this API should be\ncalled with a new event each iteration.\nevent(torch.Event) \u2013 Event recorded after the optimizer step\nto wait all-gather streams on.\nUseset_gradient_divide_factor()instead\nset_gradient_divide_factor()\nSets if the module should all-reduce gradients. This can be used to\nimplement gradient accumulation with only reduce-scatter but not\nall-reduce for HSDP.\nSets if the module should sync gradients. This can be used to implement\ngradient accumulationwithout communication. For HSDP, this controls\nboth reduce-scatter and all-reduce together. This is the equivalence ofno_syncin FSDP1.\nrequires_gradient_sync(bool) \u2013 Whether to reduce gradients for the\nmodule\u2019s parameters.\nrecurse(bool) \u2013 Whether to set for all FSDP submodules or just the\npassed-in module.\nSets if the module should reshard parameters after backward. This can\nbe used during gradient accumulation to trade off higher memory for\nreduced communication since the unsharded parameters do not need to be\nre-all-gathered before the next forward.\nreshard_after_backward(bool) \u2013 Whether to reshard parameters after\nbackward.\nrecurse(bool) \u2013 Whether to set for all FSDP submodules or just the\npassed-in module.\nSets if the module should reshard parameters after forward. This can be\nused to change thereshard_after_forwardFSDP arg at runtime. For\nexample, this can be used to set the FSDP root module\u2019s value toTrue(since it is otherwise specially set toFalse), or it can\nset an FSDP module\u2019s value toFalsefor running evals and set back\ntoTruefor training.\nreshard_after_forward\nTrue\nFalse\nFalse\nTrue\nreshard_after_forward(bool) \u2013 Whether to reshard parameters after\nforward.\nrecurse(bool) \u2013 Whether to set for all FSDP submodules or just the\npassed-in module.\nSets whether the FSDP module\u2019s parameters need to be unsharded in\nbackward. This can be used in expert cases when the user knows that all\nparameters in this FSDP module\u2019s parameter group are not needed for\nbackward computation (e.g. embedding).\nUnshards the module\u2019s parameters by allocating memory and all-gathering\nthe parameters. This method isnotrecursive. The unshard follows theMixedPrecisionPolicy, so it will all-gather followingparam_dtypeif set.\nMixedPrecisionPolicy\nparam_dtype\nasync_op(bool) \u2013 IfTrue, then returns aUnshardHandlethat has await()method to wait on the unshard op. IfFalse, then returnsNoneand waits on the handle inside\nthis function.\nTrue\nUnshardHandle\nwait()\nFalse\nNone\nOptional[UnshardHandle]\nNote\nIfasync_op=True, then FSDP will wait on the pending\nunshard in the module\u2019s pre-forward for the user. The user only\nneeds to callwait()explicitly if the wait should happen\nbefore pre-forward.\nasync_op=True\nwait()\nA handle to wait on aFSDPModule.unshard()op.\nFSDPModule.unshard()\nWaits on the unshard op. This ensures that the current stream can use\nthe unsharded parameters, which are now registered to the module.\nRegisters a method onmoduleto be considered a forward method for\nFSDP.\nmodule\nFSDP all-gathers parameters pre-forward and optionally frees parameters\npost-forward (depending onreshard_after_forward). FSDP only knows to\ndo this fornn.Module.forward()by default. This function patches a\nuser-specified method to run the pre/post-forward hooks before/after the\nmethod, respectively. Ifmoduleis not anFSDPModule, then\nthis is a no-op.\nreshard_after_forward\nnn.Module.forward()\nmodule\nFSDPModule\nmodule(nn.Module) \u2013 Module to register the forward method on.\nmethod_name(str) \u2013 Name of the forward method.\nThis configures FSDP\u2019s mixed precision. Unlike autocast, this applies mixed\nprecision at the module level, not op level, which means low-precision\nactivations are saved for backward and high-to-low-precision casts are\nincurred only at module boundaries.\nFSDP works well with module-level mixed precision since it keeps the\nhigh-precision sharded parameters in memory anyway. In other words, FSDP\ndoes not require any extra memory to keep a high-precision copy of the\nparameters for the optimizer step.\nparam_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\nthe unsharded parameter and hence the dtype for forward/backward\ncomputation and the parameter all-gather. If this isNone, then\nthe unsharded parameter uses the original dtype. The optimizer step\nuses the sharded parameter in the original dtype. (Default:None)\nNone\nNone\nreduce_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\ngradient reduction (i.e. reduce-scatter or all-reduce). If this isNonebutparam_dtypeis notNone, then the reduction\nuses the compute dtype. This can be used to run gradient reduction\nin full precision while using low precision for compute. If also\ngradient reduction is disabled viaset_requires_gradient_sync(),\nthen FSDP will accumulate gradients usingreduce_dtype.\n(Default:None)\nNone\nparam_dtype\nNone\nset_requires_gradient_sync()\nreduce_dtype\nNone\noutput_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\ncasting floating-point forward outputs. This can be used to\nhelp implement cases where different modules have different mixed\nprecision policies. (Default:None)\nNone\ncast_forward_inputs(bool) \u2013 This specifies whether FSDP should cast the\nforward\u2019s floating-point input tensors toparam_dtypeor not.\nparam_dtype\nThis base class represents the policy of no offloading and is only used as\nthe default value for theoffload_policyarg.\noffload_policy\nThis offload policy offloads parameters, gradients, and optimizer states to\nCPU. Sharded parameters are copied host-to-device before all-gather. The\nall-gathered parameters are freed according toreshard_after_forward.\nSharded gradients are copied device-to-host in backward, and the optimizer\nstep runs on CPU with CPU optimizer states.\nreshard_after_forward\npin_memory(bool) \u2013 Whether to pin sharded parameter and gradient\nmemory. Pinning memory allows both more efficient H2D/D2H copies\nand for the copies to overlap with compute. However, the pinned\nmemory cannot be used by other processes. Set this toFalseif\nyou have insufficient CPU memory. (Default:True)\nFalse\nTrue",
    "url": "https://pytorch.org/docs/stable/distributed.fsdp.fully_shard.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4544b3316eeb63164e9826fc87132bf5",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/timer.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a8e4e076432a819453eab23eda73771f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_dynamo_deepdive.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b0eb64b165f1e1a0700ef9132b19fcc6",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/signal.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "458483a9fe3ed24a2ffec89097803275",
    "source": "pytorch_docs",
    "title": "Customization \u2014 PyTorch 2.9 documentation",
    "text": "\n## Customization#\n\nCreated On: May 04, 2021 | Last Updated On: May 04, 2021\nThis section describes how to customize TorchElastic to fit your needs.\n\n## Launcher#\n\nThe launcher program that ships with TorchElastic\nshould be sufficient for most use-cases (seetorchrun (Elastic Launch)).\nYou can implement a custom launcher by\nprogrammatically creating an agent and passing it specs for your workers as\nshown below.\n\n```python\n# my_launcher.py\n\nif __name__ == \"__main__\":\n  args = parse_args(sys.argv[1:])\n  rdzv_handler = RendezvousHandler(...)\n  spec = WorkerSpec(\n      local_world_size=args.nproc_per_node,\n      fn=trainer_entrypoint_fn,\n      args=(trainer_entrypoint_fn args.fn_args,...),\n      rdzv_handler=rdzv_handler,\n      max_restarts=args.max_restarts,\n      monitor_interval=args.monitor_interval,\n  )\n\n  agent = LocalElasticAgent(spec, start_method=\"spawn\")\n  try:\n      run_result = agent.run()\n      if run_result.is_failed():\n          print(f\"worker 0 failed with: run_result.failures[0]\")\n      else:\n          print(f\"worker 0 return value is: run_result.return_values[0]\")\n  except Exception ex:\n      # handle exception\n\n```\n\n\n## Rendezvous Handler#\n\nTo implement your own rendezvous, extendtorch.distributed.elastic.rendezvous.RendezvousHandlerand implement its methods.\ntorch.distributed.elastic.rendezvous.RendezvousHandler\nWarning\nRendezvous handlers are tricky to implement. Before you begin\nmake sure you completely understand the properties of rendezvous.\nPlease refer toRendezvousfor more information.\nOnce implemented you can pass your custom rendezvous handler to the worker\nspec when creating the agent.\n\n```python\nspec = WorkerSpec(\n    rdzv_handler=MyRendezvousHandler(params),\n    ...\n)\nelastic_agent = LocalElasticAgent(spec, start_method=start_method)\nelastic_agent.run(spec.role)\n\n```\n\n\n## Metric Handler#\n\nTorchElastic emits platform level metrics (seeMetrics).\nBy default metrics are emitted to/dev/nullso you will not see them.\nTo have the metrics pushed to a metric handling service in your infrastructure,\nimplement atorch.distributed.elastic.metrics.MetricHandlerandconfigureit in your\ncustom launcher.\n\n```python\n# my_launcher.py\n\nimport torch.distributed.elastic.metrics as metrics\n\nclass MyMetricHandler(metrics.MetricHandler):\n    def emit(self, metric_data: metrics.MetricData):\n        # push metric_data to your metric sink\n\ndef main():\n  metrics.configure(MyMetricHandler())\n\n  spec = WorkerSpec(...)\n  agent = LocalElasticAgent(spec)\n  agent.run()\n\n```\n\n\n## Events Handler#\n\nTorchElastic supports events recording (seeEvents).\nThe events module defines API that allows you to record events and\nimplement custom EventHandler. EventHandler is used for publishing events\nproduced during torchelastic execution to different sources, e.g.  AWS CloudWatch.\nBy default it usestorch.distributed.elastic.events.NullEventHandlerthat ignores\nevents. To configure custom events handler you need to implementtorch.distributed.elastic.events.EventHandlerinterface andconfigureit\nin your custom launcher.\n\n```python\n# my_launcher.py\n\nimport torch.distributed.elastic.events as events\n\nclass MyEventHandler(events.EventHandler):\n    def record(self, event: events.Event):\n        # process event\n\ndef main():\n  events.configure(MyEventHandler())\n\n  spec = WorkerSpec(...)\n  agent = LocalElasticAgent(spec)\n  agent.run()\n\n```\n",
    "url": "https://pytorch.org/docs/stable/elastic/customization.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4cf985dfec70a5fb76e8a6bbed219d2e",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/numerical_accuracy.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d2c493d5680a9f4df51ecc37989ebccf",
    "source": "pytorch_docs",
    "title": "Gradcheck mechanics \u2014 PyTorch 2.9 documentation",
    "text": "\n## Gradcheck mechanics#\n\nCreated On: Apr 27, 2021 | Last Updated On: Jun 18, 2025\nThis note presents an overview of how thegradcheck()andgradgradcheck()functions work.\ngradcheck()\ngradgradcheck()\nIt will cover both forward and backward mode AD for both real and complex-valued functions as well as higher-order derivatives.\nThis note also covers both the default behavior of gradcheck as well as the case wherefast_mode=Trueargument is passed (referred to as fast gradcheck below).\nfast_mode=True\nNotations and background information\nDefault backward mode gradcheck behavior\nReal-to-real functions\nComplex-to-real functions\nFunctions with complex outputs\nFast backward mode gradcheck\nFast gradcheck for real-to-real functions\nFast gradcheck for complex-to-real functions\nFast gradcheck for functions with complex outputs\nGradgradcheck implementation\n\n## Notations and background information#\n\nThroughout this note, we will use the following convention:\nxxx,yyy,aaa,bbb,vvv,uuu,urururanduiuiuiare real-valued vectors andzzzis a complex-valued vector that can be rewritten in terms of two real-valued vectors asz=a+ibz = a + i bz=a+ib.\nNNNandMMMare two integers that we will use for the dimension of the input and output space respectively.\nf:RN\u2192RMf: \\mathcal{R}^N \\to \\mathcal{R}^Mf:RN\u2192RMis our basic real-to-real function such thaty=f(x)y = f(x)y=f(x).\ng:CN\u2192RMg: \\mathcal{C}^N \\to \\mathcal{R}^Mg:CN\u2192RMis our basic complex-to-real function such thaty=g(z)y = g(z)y=g(z).\nFor the simple real-to-real case, we write asJfJ_fJf\u200bthe Jacobian matrix associated withfffof sizeM\u00d7NM \\times NM\u00d7N.\nThis matrix contains all the partial derivatives such that the entry at position(i,j)(i, j)(i,j)contains\u2202yi\u2202xj\\frac{\\partial y_i}{\\partial x_j}\u2202xj\u200b\u2202yi\u200b\u200b.\nBackward mode AD is then computing, for a given vectorvvvof sizeMMM, the quantityvTJfv^T J_fvTJf\u200b.\nForward mode AD on the other hand is computing, for a given vectoruuuof sizeNNN, the quantityJfuJ_f uJf\u200bu.\nFor functions that contain complex values, the story is a lot more complex. We only provide the gist here and the full description can be found atAutograd for Complex Numbers.\nThe constraints to satisfy complex differentiability (Cauchy-Riemann equations) are too restrictive for all real-valued loss functions, so we instead opted to use Wirtinger calculus.\nIn a basic setting of Wirtinger calculus, the chain rule requires access to both the Wirtinger derivative (calledWWWbelow) and the Conjugate Wirtinger derivative (calledCWCWCWbelow).\nBothWWWandCWCWCWneed to be propagated because in general, despite their name, one is not the complex conjugate of the other.\nTo avoid having to propagate both values, for backward mode AD, we always work under the assumption that the function whose derivative is being calculated is either a real-valued function or is part of a bigger real-valued function. This assumption means that all the intermediary gradients we compute during the backward pass are also associated with real-valued functions.\nIn practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers).\nUnder this assumption, usingWWWandCWCWCWdefinitions, we can show thatW=CW\u2217W = CW^*W=CW\u2217(we use\u2217*\u2217to denote complex conjugation here) and so only one of the two values actually need to be \u201cbackwarded through the graph\u201d as the other one can easily be recovered.\nTo simplify internal computations, PyTorch uses2\u2217CW2 * CW2\u2217CWas the value it backwards and returns when the user asks for gradients.\nSimilarly to the real case, when the output is actually inRM\\mathcal{R}^MRM, backward mode AD does not compute2\u2217CW2 * CW2\u2217CWbut onlyvT(2\u2217CW)v^T (2 * CW)vT(2\u2217CW)for a given vectorv\u2208RMv \\in \\mathcal{R}^Mv\u2208RM.\nFor forward mode AD, we use a similar logic, in this case, assuming that the function is part of a larger function whose input is inR\\mathcal{R}R. Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is inR\\mathcal{R}Rand in this case, usingWWWandCWCWCWdefinitions, we can show thatW=CWW = CWW=CWfor the intermediary functions.\nTo make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes2\u2217CW2 * CW2\u2217CW.\nSimilarly to the real case, when the input is actually inRN\\mathcal{R}^NRN, forward mode AD does not compute2\u2217CW2 * CW2\u2217CWbut only(2\u2217CW)u(2 * CW) u(2\u2217CW)ufor a given vectoru\u2208RNu \\in \\mathcal{R}^Nu\u2208RN.\n\n## Default backward mode gradcheck behavior#\n\n\n## Real-to-real functions#\n\nTo test a functionf:RN\u2192RM,x\u2192yf: \\mathcal{R}^N \\to \\mathcal{R}^M, x \\to yf:RN\u2192RM,x\u2192y, we reconstruct the full Jacobian matrixJfJ_fJf\u200bof sizeM\u00d7NM \\times NM\u00d7Nin two ways: analytically and numerically.\nThe analytical version uses our backward mode AD while the numerical version uses finite difference.\nThe two reconstructed Jacobian matrices are then compared elementwise for equality.\nIf we consider the elementary case of a one-dimensional function (N=M=1N = M = 1N=M=1), then we can use the basic finite difference formula fromthe wikipedia article. We use the \u201ccentral difference\u201d for better numerical properties:\nThis formula easily generalizes for multiple outputs (M>1M \\gt 1M>1) by having\u2202y\u2202x\\frac{\\partial y}{\\partial x}\u2202x\u2202y\u200bbe a column vector of sizeM\u00d71M \\times 1M\u00d71likef(x+eps)f(x + eps)f(x+eps).\nIn that case, the above formula can be reused as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namelyf(x+eps)f(x + eps)f(x+eps)andf(x\u2212eps)f(x - eps)f(x\u2212eps)).\nIt is more computationally expensive to handle the case with multiple inputs (N>1N \\gt 1N>1). In this scenario, we loop over all the inputs one after the other and apply theepsepsepsperturbation for each element ofxxxone after the other. This allows us to reconstruct theJfJ_fJf\u200bmatrix column by column.\nFor the analytical evaluation, we use the fact, as described above, that backward mode AD computesvTJfv^T J_fvTJf\u200b.\nFor functions with a single output, we simply usev=1v = 1v=1to recover the full Jacobian matrix with a single backward pass.\nFor functions with more than one output, we resort to a for-loop which iterates over the outputs where eachvvvis a one-hot vector corresponding to each output one after the other. This allows to reconstruct theJfJ_fJf\u200bmatrix row by row.\n\n## Complex-to-real functions#\n\nTo test a functiong:CN\u2192RM,z\u2192yg: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to yg:CN\u2192RM,z\u2192ywithz=a+ibz = a + i bz=a+ib, we reconstruct the (complex-valued) matrix that contains2\u2217CW2 * CW2\u2217CW.\nConsider the elementary case whereN=M=1N = M = 1N=M=1first. We know from (chapter 3 of)this research paperthat:\nNote that\u2202y\u2202a\\frac{\\partial y}{\\partial a}\u2202a\u2202y\u200band\u2202y\u2202b\\frac{\\partial y}{\\partial b}\u2202b\u2202y\u200b, in the above equation, areR\u2192R\\mathcal{R} \\to \\mathcal{R}R\u2192Rderivatives.\nTo evaluate these numerically, we use the method described above for the real-to-real case.\nThis allows us to compute theCWCWCWmatrix and then multiply it by222.\nNote that the code, as of time of writing, computes this value in a slightly convoluted way:\n\n```python\n# Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105\n# Notation changes in this code block:\n# s here is y above\n# x, y here are a, b above\n\nds_dx = compute_gradient(eps)\nds_dy = compute_gradient(eps * 1j)\n# conjugate wirtinger derivative\nconj_w_d = 0.5 * (ds_dx + ds_dy * 1j)\n# wirtinger derivative\nw_d = 0.5 * (ds_dx - ds_dy * 1j)\nd[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj()\n\n# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.\n\n```\n\nSince backward mode AD computes exactly twice theCWCWCWderivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs.\n\n## Functions with complex outputs#\n\nIn this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued.\nThis means that using autograd directly on this function is not well defined.\nTo solve this, we will replace the test of the functionh:PN\u2192CMh: \\mathcal{P}^N \\to \\mathcal{C}^Mh:PN\u2192CM(whereP\\mathcal{P}Pcan be eitherR\\mathcal{R}RorC\\mathcal{C}C), with two functions:hrhrhrandhihihisuch that:\nwhereq\u2208Pq \\in \\mathcal{P}q\u2208P.\nWe then do a basic gradcheck for bothhrhrhrandhihihiusing either the real-to-real or complex-to-real case described above, depending onP\\mathcal{P}P.\nNote that, the code, as of time of writing, does not create these functions explicitly but perform the chain rule with therealrealrealorimagimagimagfunctions manually by passing thegrad_out\\text{grad\\_out}grad_outarguments to the different functions.\nWhengrad_out=1\\text{grad\\_out} = 1grad_out=1, then we are consideringhrhrhr.\nWhengrad_out=1j\\text{grad\\_out} = 1jgrad_out=1j, then we are consideringhihihi.\n\n## Fast backward mode gradcheck#\n\nWhile the above formulation of gradcheck is great, both, to ensure correctness and debuggability, it is very slow because it reconstructs the full Jacobian matrices.\nThis section presents a way to perform gradcheck in a faster way without affecting its correctness.\nThe debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user.\nThe high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians.\n\n## Fast gradcheck for real-to-real functions#\n\nThe scalar quantity that we want to compute here isvTJfuv^T J_f uvTJf\u200bufor a given random vectorv\u2208RMv \\in \\mathcal{R}^Mv\u2208RMand a random unit norm vectoru\u2208RNu \\in \\mathcal{R}^Nu\u2208RN.\nFor the numerical evaluation, we can efficiently compute\nWe then perform the dot product between this vector andvvvto get the scalar value of interest.\nFor the analytical version, we can use backward mode AD to computevTJfv^T J_fvTJf\u200bdirectly. We then perform the dot product withuuuto get the expected value.\n\n## Fast gradcheck for complex-to-real functions#\n\nSimilar to the real-to-real case, we want to perform a reduction of the full matrix. But the2\u2217CW2 * CW2\u2217CWmatrix is complex-valued and so in this case, we will compare to complex scalars.\nDue to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value:\nwherev\u2208RMv \\in \\mathcal{R}^Mv\u2208RM,ur\u2208RNur \\in \\mathcal{R}^Nur\u2208RNandui\u2208RNui \\in \\mathcal{R}^Nui\u2208RN.\nWe first consider how to computessswith a numerical method. To do so, keeping in mind that we\u2019re consideringg:CN\u2192RM,z\u2192yg: \\mathcal{C}^N \\to \\mathcal{R}^M, z \\to yg:CN\u2192RM,z\u2192ywithz=a+ibz = a + i bz=a+ib, and thatCW=12\u2217(\u2202y\u2202a+i\u2202y\u2202b)CW = \\frac{1}{2} * (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})CW=21\u200b\u2217(\u2202a\u2202y\u200b+i\u2202b\u2202y\u200b),  we rewrite it as follows:\nIn this formula, we can see that\u2202y\u2202aur\\frac{\\partial y}{\\partial a} ur\u2202a\u2202y\u200burand\u2202y\u2202bui\\frac{\\partial y}{\\partial b} ui\u2202b\u2202y\u200buican be evaluated the same way as the fast version for the real-to-real case.\nOnce these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valuedvvvvector.\nFor the analytical case, things are simpler and we rewrite the formula as:\nWe can thus use the fact that the backward mode AD provides us with an efficient way to computevT(2\u2217CW)v^T (2 * CW)vT(2\u2217CW)and then perform a dot product of the real part withurururand the imaginary part withuiuiuibefore reconstructing the final complex scalarsss.\nAt this point, you might be wondering why we did not select a complexuuuand just performed the reduction2\u2217vTCWu\u20322 * v^T CW u'2\u2217vTCWu\u2032.\nTo dive into this, in this paragraph, we will use the complex version ofuuunotedu\u2032=ur\u2032+iui\u2032u' = ur' + i ui'u\u2032=ur\u2032+iui\u2032.\nUsing such complexu\u2032u'u\u2032, the problem is that when doing the numerical evaluation, we would need to compute:\nWhich would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above).\nSince this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above.\n\n## Fast gradcheck for functions with complex outputs#\n\nJust like in the slow case, we consider two real-valued functions and use the appropriate rule from above for each function.\n\n## Gradgradcheck implementation#\n\nPyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing.\nThis feature is implemented by considering the functionF:x,v\u2192vTJfF: x, v \\to v^T J_fF:x,v\u2192vTJf\u200band use the gradcheck defined above on this function.\nNote thatvvvin this case is just a random vector with the same type asf(x)f(x)f(x).\nThe fast version of gradgradcheck is implemented by using the fast version of gradcheck on that same functionFFF.",
    "url": "https://pytorch.org/docs/stable/notes/gradcheck.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d2c5fb030b6cf2c0c454d828c13d9cbb",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributed.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8c31f7cfb80ccf4f7b837fff4a10090a",
    "source": "pytorch_docs",
    "title": "torch.export IR Specification \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.export IR Specification#\n\nCreated On: Oct 05, 2023 | Last Updated On: Jul 16, 2025\nExport IR is an intermediate representation (IR) for compilers, which bears\nsimilarities toMLIRand TorchScript. It is specifically designed to express the\nsemantics of PyTorch programs. Export IR primarily represents computation in a\nstreamlined list of operations, with limited support for dynamism such as\ncontrol flows.\nTo create an Export IR graph, a frontend can be used that soundly captures a\nPyTorch program via a trace-specializing mechanism. The resulting Export IR can\nthen be optimized and executed by a backend. This can be done today throughtorch.export.export().\ntorch.export.export()\nThe key concepts that will be covered in this document include:\nExportedProgram: the data structure containing the Export IR program\nGraph: which consists of a list of nodes.\nNodes: which represents operations, control flow, and metadata stored on this node.\nValues are produced and consumed by nodes.\nTypes are associated with values and nodes.\nThe size and memory layout of values are also defined.\n\n## Assumptions#\n\nThis doc assumes that the audience is sufficiently familiar with PyTorch,\nspecifically withtorch.fxand its related toolings. Thus it will stop\ndescribing contents present intorch.fxdocumentation and paper.\ntorch.fx\ntorch.fx\n\n## What is Export IR#\n\nExport IR is a graph-based intermediate representation IR of PyTorch programs.\nExport IR is realized on top oftorch.fx.Graph. In other words,all\nExport IR graphs are also valid FX graphs, and if interpreted using standard\nFX semantics, Export IR can be interpreted soundly. One implication is that an\nexported graph can be converted to a valid Python program via standard FX\ncodegen.\ntorch.fx.Graph\nThis documentation will primarily focus on highlighting areas where Export IR\ndiffers from FX in terms of its strictness, while skipping parts where it shares\nsimilarities with FX.\n\n## ExportedProgram#\n\nThe top-level Export IR construct is antorch.export.ExportedProgramclass. It bundles the computational graph of a PyTorch model (which is usually atorch.nn.Module) with the parameters or weights that this model\nconsumes.\ntorch.export.ExportedProgram\ntorch.nn.Module\nSome notable attributes of thetorch.export.ExportedProgramclass are:\ntorch.export.ExportedProgram\ngraph_module(torch.fx.GraphModule): Data structure containing\nthe flattened computational graph of the PyTorch model. The graph can be\ndirectly accessed throughExportedProgram.graph.\ngraph_module\ntorch.fx.GraphModule\nExportedProgram.graph\ngraph_signature(torch.export.ExportGraphSignature): The graph\nsignature, which specifies the parameters and buffer names used and mutated\nwithin the graph. Instead of storing parameters and buffers as attributes of\nthe graph, they are lifted as inputs to the graph. The graph_signature is\nutilized to keep track of additional information on these parameters and\nbuffers.\ngraph_signature\ntorch.export.ExportGraphSignature\nstate_dict(Dict[str,Union[torch.Tensor,torch.nn.Parameter]]): Data\nstructure containing the parameters and buffers.\nstate_dict\nDict[str,Union[torch.Tensor,torch.nn.Parameter]]\nrange_constraints(Dict[sympy.Symbol,RangeConstraint]): For programs\nthat are exported with data dependent behavior, the metadata on each node will\ncontain symbolic shapes (which look likes0,i0). This attribute maps\nthe symbolic shapes to their lower/upper ranges.\nrange_constraints\nDict[sympy.Symbol,RangeConstraint]\ns0\ni0\n\n## Graph#\n\nAn Export IR Graph is a PyTorch program represented in the form of a DAG\n(directed acyclic graph). Each node in this graph represents a particular\ncomputation or operation, and edges of this graph consist of references between\nnodes.\nWe can view Graph having this schema:\n\n```python\nclass Graph:\n  nodes: List[Node]\n\n```\n\nIn practice, Export IR\u2019s graph is realized astorch.fx.GraphPython class.\ntorch.fx.Graph\nAn Export IR graph contains the following nodes (Nodes will be described in more\ndetails in the next section):\n0 or more nodes of op typeplaceholder\nplaceholder\n0 or more nodes of op typecall_function\ncall_function\nexactly 1 node of op typeoutput\noutput\nCollorary:The smallest valid Graph will be of one node. i.e. nodes is never empty.\nDefinition:The set ofplaceholdernodes of a Graph represents theinputsof the\nGraph of GraphModule. Theoutputnode of a Graph represents theoutputsof the Graph of GraphModule.\nplaceholder\noutput\nExample:\n\n```python\nimport torch\nfrom torch import nn\n\nclass MyModule(nn.Module):\n\n    def forward(self, x, y):\n      return x + y\n\nexample_args = (torch.randn(1), torch.randn(1))\nmod = torch.export.export(MyModule(), example_args)\nprint(mod.graph)\n\n```\n\n\n```python\ngraph():\n  %x : [num_users=1] = placeholder[target=x]\n  %y : [num_users=1] = placeholder[target=y]\n  %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x, %y), kwargs = {})\n  return (add,)\n\n```\n\nThe above is the textual representation of a Graph, with each line being a node.\n\n## Node#\n\nA Node represents a particular computation or operation and is represented in\nPython using thetorch.fx.Nodeclass. Edges between nodes are\nrepresented as direct references to other nodes via theargsproperty of the\nNode class. Using the same FX machinery, we can represent the following\noperations that a computational graph typically needs, such as operator calls,\nplaceholders (aka inputs), conditionals, and loops.\ntorch.fx.Node\nargs\nThe Node has the following schema:\n\n```python\nclass Node:\n  name: str # name of node\n  op_name: str  # type of operation\n\n  # interpretation of the fields below depends on op_name\n  target: [str|Callable]\n  args: List[object]\n  kwargs: Dict[str, object]\n  meta: Dict[str, object]\n\n```\n\nFX Text Format\nAs in the example above, notice that each line has this format:\n\n```python\n%<name>:[...] = <op_name>[target=<target>](args = (%arg1, %arg2, arg3, arg4, \u2026)), kwargs = {\"keyword\": arg5})\n\n```\n\nThis format captures everything present in the Node class, with the exception ofmeta, in a compact format.\nmeta\nConcretely:\nis the name of the node as it would appear innode.name.\nnode.name\n<op_name>is thenode.opfield, which must be one of these:<call_function>,<placeholder>,<get_attr>, or<output>.\nnode.op\n<call_function>\n<placeholder>\n<get_attr>\n<output>\nis the target of the node asnode.target. The meaning of this\nfield depends onop_name.\nnode.target\nop_name\nargs1, \u2026 args 4\u2026are what is listed in thenode.argstuple. If a\nvalue in the list is antorch.fx.Node, then it will be especially\nindicated with a leading%.\nnode.args\ntorch.fx.Node\nFor example, a call to the add operator would appear as:\n\n```python\n%add1 = call_function[target = torch.op.aten.add.Tensor](args = (%x, %y), kwargs = {})\n\n```\n\nWhere%x,%yare two other Nodes that have names x and y. Worth noting\nthat the stringtorch.op.aten.add.Tensorrepresents the callable object that\nis actually stored in the target field, not merely its string name.\n%x\n%y\ntorch.op.aten.add.Tensor\nThe final line of this text format is:\n\n```python\nreturn [add]\n\n```\n\nwhich is a Node withop_name=output, indicating that we are returning this\none element.\nop_name=output\n\n## call_function#\n\nAcall_functionnode represents a call to an operator.\ncall_function\nDefinitions\nFunctional:We say a callable is \u201cfunctional\u201d if it satisfies all the\nfollowing requirements:\nNon-mutating: The operator does not mutate the value of its input (for\ntensors, this includes both metadata and data).\nNo side effects: The operator does not mutate states that are visible\nfrom outside, like changing values of module parameters.\nOperator:is a functional callable with a predefined schema. Examples of\nsuch operators include functional ATen operators.\nRepresentation in FX\n\n```python\n%name = call_function[target = operator](args = (%x, %y, \u2026), kwargs = {})\n\n```\n\nDifferences from vanilla FX call_function\nIn FX graph, a call_function can refer to any callable, in Export IR, we\nrestrict it to only a select subset of ATen operators, custom operators, and\ncontrol flow operators.\nIn Export IR, constant arguments will be embedded within the graph.\nIn FX graph, a get_attr node can represent reading any attribute stored in\nthe graph module. However, in Export IR this is restricted to reading only\nsubmodules as all parameters/buffers will be passed in as inputs to the graph\nmodule.\nNode.metais a dict attached to every FX node. However, the FX spec does not\nspecify what metadata can or will be there. Export IR provides a stronger\ncontract, specifically allcall_functionnodes will guarantee having and\nonly having the following metadata fields:\nNode.meta\ncall_function\nnode.meta[\"stack_trace\"]is a string containing the Python stack trace\nreferencing the original Python source code. An example stack trace looks\nlike:\nnode.meta[\"stack_trace\"]\n\n```python\nFile \"my_module.py\", line 19, in forward\nreturn x + dummy_helper(y)\nFile \"helper_utility.py\", line 89, in dummy_helper\nreturn y + 1\n\n```\n\nnode.meta[\"val\"]describes the output of running the operation. It can be\nof type<symint>,<FakeTensor>, aList[Union[FakeTensor,SymInt]], orNone.\nnode.meta[\"val\"]\n<symint>\n<FakeTensor>\nList[Union[FakeTensor,SymInt]]\nNone\nnode.meta[\"nn_module_stack\"]describes the \u201cstacktrace\u201d of thetorch.nn.Modulefrom which the node came, if it was from atorch.nn.Modulecall. For example, if a node containing theaddmmop called from atorch.nn.Linearmodule inside of atorch.nn.Sequentialmodule, thenn_module_stackwould look\nsomething like:\nnode.meta[\"nn_module_stack\"]\ntorch.nn.Module\ntorch.nn.Module\naddmm\ntorch.nn.Linear\ntorch.nn.Sequential\nnn_module_stack\n\n```python\n{'self_linear': ('self.linear', <class 'torch.nn.Linear'>), 'self_sequential': ('self.sequential', <class 'torch.nn.Sequential'>)}\n\n```\n\nnode.meta[\"source_fn_stack\"]contains the torch function or the leaftorch.nn.Moduleclass this node was called from before decomposition.\nFor example, a node containing theaddmmop from atorch.nn.Linearmodule call would containtorch.nn.Linearin\ntheirsource_fn, and a node containing theaddmmop from atorch.nn.functional.Linearmodule call would containtorch.nn.functional.Linearin theirsource_fn.\nnode.meta[\"source_fn_stack\"]\ntorch.nn.Module\naddmm\ntorch.nn.Linear\ntorch.nn.Linear\nsource_fn\naddmm\ntorch.nn.functional.Linear\ntorch.nn.functional.Linear\nsource_fn\n\n## placeholder#\n\nPlaceholder represents an input to a graph. Its semantics are exactly the same as in FX.\nPlaceholder nodes must be the first N nodes in the nodes list of a graph. N can be zero.\nRepresentation in FX\n\n```python\n%name = placeholder[target = name](args = ())\n\n```\n\nThe target field is a string which is the name of input.\nargs, if non-empty, should be of size 1 representing the default value of this input.\nargs\nMetadata\nPlaceholder nodes also havemeta[\u2018val\u2019], likecall_functionnodes. Thevalfield in this case represents the input shape/dtype that the graph is\nexpected to receive for this input parameter.\nmeta[\u2018val\u2019]\ncall_function\nval\n\n## output#\n\nAn output call represents a return statement in a function; it thus terminates the\ncurrent graph. There is one and only one output node, and it will always be the\nlast node of the graph.\nRepresentation in FX\n\n```python\noutput[](args = (%something, \u2026))\n\n```\n\nThis has the exact semantics as intorch.fx.argsrepresents the node\nto be returned.\ntorch.fx\nargs\nMetadata\nOutput node has the same metadata ascall_functionnodes.\ncall_function\n\n## get_attr#\n\nget_attrnodes represent reading a submodule from the encapsulatingtorch.fx.GraphModule. Unlike a vanilla FX graph fromtorch.fx.symbolic_trace()in whichget_attrnodes are used to read\nattributes such as parameters and buffers from the top-leveltorch.fx.GraphModule, parameters and buffers are passed in as\ninputs to the graph module, and stored in the top-leveltorch.export.ExportedProgram.\nget_attr\ntorch.fx.GraphModule\ntorch.fx.symbolic_trace()\nget_attr\ntorch.fx.GraphModule\ntorch.export.ExportedProgram\nRepresentation in FX\n\n```python\n%name = get_attr[target = name](args = ())\n\n```\n\nExample\nConsider the following model:\n\n```python\nfrom functorch.experimental.control_flow import cond\n\ndef true_fn(x):\n    return x.sin()\n\ndef false_fn(x):\n    return x.cos()\n\ndef f(x, y):\n    return cond(y, true_fn, false_fn, [x])\n\n```\n\nGraph:\n\n```python\ngraph():\n    %x_1 : [num_users=1] = placeholder[target=x_1]\n    %y_1 : [num_users=1] = placeholder[target=y_1]\n    %true_graph_0 : [num_users=1] = get_attr[target=true_graph_0]\n    %false_graph_0 : [num_users=1] = get_attr[target=false_graph_0]\n    %conditional : [num_users=1] = call_function[target=torch.ops.higher_order.cond](args = (%y_1, %true_graph_0, %false_graph_0, [%x_1]), kwargs = {})\n    return conditional\n\n```\n\nThe line,%true_graph_0:[num_users=1]=get_attr[target=true_graph_0],\nreads the submoduletrue_graph_0which contains thesinoperator.\n%true_graph_0:[num_users=1]=get_attr[target=true_graph_0]\ntrue_graph_0\nsin\n\n## References#\n\n\n## SymInt#\n\nA SymInt is an object that can either be a literal integer or a symbol that represents\nan Integer (represented in Python bysympy.Symbolclass). When SymInt is a\nsymbol, it describes a variable of type integer that is unknown to the graph at\ncompile time, that is, its value is only known at runtime.\nsympy.Symbol\n\n## FakeTensor#\n\nA FakeTensor is an object that contains the metadata of a tensor. It can be\nviewed as having the following metadata.\n\n```python\nclass FakeTensor:\n  size: List[SymInt]\n  dtype: torch.dtype\n  device: torch.device\n  dim_order: List[int]  # This doesn't exist yet\n\n```\n\nThe size field of FakeTensor is a list of integers or SymInts. If SymInts are\npresent, this means this tensor has a dynamic shape. If integers are present, it\nis assumed that the tensor will have that exact static shape. The rank of the\nTensorMeta is never dynamic. The dtype field represents the dtype of the\noutput of that node. There are no implicit type promotions in Edge IR. There\nare no strides in FakeTensor.\nIn other words:\nIf the operator in node.target returns a Tensor, thennode.meta['val']is a\nFakeTensor describing that tensor.\nnode.meta['val']\nIf the operator in node.target returns an n-tuple of Tensors, thennode.meta['val']is an n-tuple of FakeTensors describing each tensor.\nnode.meta['val']\nIf the operator in node.target returns an int/float/scalar that is known at\ncompile time, thennode.meta['val']is None.\nnode.meta['val']\nIf the operator in node.target returns an int/float/scalar that is not known\nat compile time, thennode.meta['val']is of type SymInt.\nnode.meta['val']\nFor example:\naten::addreturns a Tensor; so its spec will be a FakeTensor with dtype\nand size of the tensor returned by this operator.\naten::add\naten::sym_sizereturns an integer; so its val will be a SymInt because its\nvalue is only available at runtime.\naten::sym_size\nmax_pool2d_with_indexesreturns a tuple of (Tensor, Tensor); so the spec\nwill also be a 2-tuple of FakeTensor objects, the first TensorMeta describes\nthe first element of the return value etc.\nmax_pool2d_with_indexes\nPython code:\n\n```python\ndef add_one(x):\n  return torch.ops.aten(x, 1)\n\n```\n\nGraph:\n\n```python\ngraph():\n  %ph_0 : [#users=1] = placeholder[target=ph_0]\n  %add_tensor : [#users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%ph_0, 1), kwargs = {})\n  return [add_tensor]\n\n```\n\nFakeTensor:\n\n```python\nFakeTensor(dtype=torch.int, size=[2,], device=CPU)\n\n```\n\n\n## Pytree-able Types#\n\nWe define a type \u201cPytree-able\u201d, if it is either a leaf type or a container type\nthat contains other Pytree-able types.\nNote:\nThe concept of pytree is the same as the one documentedherefor JAX:\nThe following types are defined asleaf type:\nType\nDefinition\nTensor\ntorch.Tensor\ntorch.Tensor\nScalar\nAny numerical types from Python, including integral types, floating point types, and zero dimensional tensors.\nint\nPython int (bound as int64_t in C++)\nfloat\nPython float (bound as double in C++)\nbool\nPython bool\nstr\nPython string\nScalarType\ntorch.dtype\ntorch.dtype\nLayout\ntorch.layout\ntorch.layout\nMemoryFormat\ntorch.memory_format\ntorch.memory_format\nDevice\ntorch.device\ntorch.device\nThe following types are defined ascontainer type:\nType\nDefinition\nTuple\nPython tuple\nList\nPython list\nDict\nPython dict with Scalar keys\nNamedTuple\nPython namedtuple\nDataclass\nMust be registered throughregister_dataclass\nCustom class\nAny custom class defined with_register_pytree_node",
    "url": "https://pytorch.org/docs/stable/export/ir_spec.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "369953c0827d1555313403924bc47d71",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/masked.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8e0064d2ed9eddc09824827f0c5424d8",
    "source": "pytorch_docs",
    "title": "torch.export \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.export#\n\nCreated On: Jun 12, 2025 | Last Updated On: Aug 11, 2025\n\n## Overview#\n\ntorch.export.export()takes atorch.nn.Moduleand produces a traced graph\nrepresenting only the Tensor computation of the function in an Ahead-of-Time\n(AOT) fashion, which can subsequently be executed with different outputs or\nserialized.\ntorch.export.export()\ntorch.nn.Module\n\n```python\nimport torch\nfrom torch.export import export, ExportedProgram\n\nclass Mod(torch.nn.Module):\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        a = torch.sin(x)\n        b = torch.cos(y)\n        return a + b\n\nexample_args = (torch.randn(10, 10), torch.randn(10, 10))\n\nexported_program: ExportedProgram = export(Mod(), args=example_args)\nprint(exported_program)\n\n```\n\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[10, 10]\", y: \"f32[10, 10]\"):\n             # File: /tmp/ipykernel_210/2550508656.py:6 in forward, code: a = torch.sin(x)\n            sin: \"f32[10, 10]\" = torch.ops.aten.sin.default(x);  x = None\n            \n             # File: /tmp/ipykernel_210/2550508656.py:7 in forward, code: b = torch.cos(y)\n            cos: \"f32[10, 10]\" = torch.ops.aten.cos.default(y);  y = None\n            \n             # File: /tmp/ipykernel_210/2550508656.py:8 in forward, code: return a + b\n            add: \"f32[10, 10]\" = torch.ops.aten.add.Tensor(sin, cos);  sin = cos = None\n            return (add,)\n            \nGraph signature: \n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n    \n    # outputs\n    add: USER_OUTPUT\n    \nRange constraints: {}\n\n```\n\ntorch.exportproduces a clean intermediate representation (IR) with the\nfollowing invariants. More specifications about the IR can be foundhere.\ntorch.export\nSoundness: It is guaranteed to be a sound representation of the original\nprogram, and maintains the same calling conventions of the original program.\nNormalized: There are no Python semantics within the graph. Submodules\nfrom the original programs are inlined to form one fully flattened\ncomputational graph.\nGraph properties: The graph is purely functional, meaning it does not\ncontain operations with side effects such as mutations or aliasing. It does\nnot mutate any intermediate values, parameters, or buffers.\nMetadata: The graph contains metadata captured during tracing, such as a\nstacktrace from user\u2019s code.\nUnder the hood,torch.exportleverages the following latest technologies:\ntorch.export\nTorchDynamo (torch._dynamo)is an internal API that uses a CPython feature\ncalled the Frame Evaluation API to safely trace PyTorch graphs. This\nprovides a massively improved graph capturing experience, with much fewer\nrewrites needed in order to fully trace the PyTorch code.\nAOT Autogradprovides a functionalized PyTorch graph and ensures the graph\nis decomposed/lowered to the ATen operator set.\nTorch FX (torch.fx)is the underlying representation of the graph,\nallowing flexible Python-based transformations.\n\n## Existing frameworks#\n\ntorch.compile()also utilizes the same PT2 stack astorch.export, but\nis slightly different:\ntorch.compile()\ntorch.export\nJIT vs. AOT:torch.compile()is a JIT compiler whereas\nwhich is not intended to be used to produce compiled artifacts outside of\ndeployment.\ntorch.compile()\nPartial vs. Full Graph Capture: Whentorch.compile()runs into an\nuntraceable part of a model, it will \u201cgraph break\u201d and fall back to running\nthe program in the eager Python runtime. In comparison,torch.exportaims\nto get a full graph representation of a PyTorch model, so it will error out\nwhen something untraceable is reached. Sincetorch.exportproduces a full\ngraph disjoint from any Python features or runtime, this graph can then be\nsaved, loaded, and run in different environments and languages.\ntorch.compile()\ntorch.export\ntorch.export\nUsability tradeoff: Sincetorch.compile()is able to fallback to the\nPython runtime whenever it reaches something untraceable, it is a lot more\nflexible.torch.exportwill instead require users to provide more\ninformation or rewrite their code to make it traceable.\ntorch.compile()\ntorch.export\nCompared totorch.fx.symbolic_trace(),torch.exporttraces using\nTorchDynamo which operates at the Python bytecode level, giving it the ability\nto trace arbitrary Python constructs not limited by what Python operator\noverloading supports. Additionally,torch.exportkeeps fine-grained track of\ntensor metadata, so that conditionals on things like tensor shapes do not\nfail tracing. In general,torch.exportis expected to work on more user\nprograms, and produce lower-level graphs (at thetorch.ops.atenoperator\nlevel). Note that users can still usetorch.fx.symbolic_trace()as a\npreprocessing step beforetorch.export.\ntorch.fx.symbolic_trace()\ntorch.export\ntorch.export\ntorch.export\ntorch.ops.aten\ntorch.fx.symbolic_trace()\ntorch.export\nCompared totorch.jit.script(),torch.exportdoes not capture Python\ncontrol flow or data structures, unless using explicitcontrol flow operators,\nbut it supports more Python language features due to its comprehensive coverage\nover Python bytecodes. The resulting graphs are simpler and only have straight\nline control flow, except for explicit control flow operators.\ntorch.jit.script()\ntorch.export\nCompared totorch.jit.trace(),torch.exportis sound:\nit can trace code that performs integer computation on sizes and records\nall of the side-conditions necessary to ensure that a particular\ntrace is valid for other inputs.\ntorch.jit.trace()\ntorch.export\n\n## Exporting a PyTorch Model#\n\nThe main entrypoint is throughtorch.export.export(), which takes atorch.nn.Moduleand sample inputs, and\ncaptures the computation graph into antorch.export.ExportedProgram. An\nexample:\ntorch.export.export()\ntorch.nn.Module\ntorch.export.ExportedProgram\n\n```python\nimport torch\nfrom torch.export import export, ExportedProgram\n\n# Simple module for demonstration\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels=3, out_channels=16, kernel_size=3, padding=1\n        )\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3)\n\n    def forward(self, x: torch.Tensor, *, constant=None) -> torch.Tensor:\n        a = self.conv(x)\n        a.add_(constant)\n        return self.maxpool(self.relu(a))\n\nexample_args = (torch.randn(1, 3, 256, 256),)\nexample_kwargs = {\"constant\": torch.ones(1, 16, 256, 256)}\n\nexported_program: ExportedProgram = export(\n    M(), args=example_args, kwargs=example_kwargs\n)\nprint(exported_program)\n\n# To run the exported program, we can use the `module()` method\nprint(exported_program.module()(torch.randn(1, 3, 256, 256), constant=torch.ones(1, 16, 256, 256)))\n\n```\n\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, p_conv_weight: \"f32[16, 3, 3, 3]\", p_conv_bias: \"f32[16]\", x: \"f32[1, 3, 256, 256]\", constant: \"f32[1, 16, 256, 256]\"):\n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n            conv2d: \"f32[1, 16, 256, 256]\" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias, [1, 1], [1, 1]);  x = p_conv_weight = p_conv_bias = None\n            \n             # File: /tmp/ipykernel_210/2848084713.py:16 in forward, code: a.add_(constant)\n            add_: \"f32[1, 16, 256, 256]\" = torch.ops.aten.add_.Tensor(conv2d, constant);  conv2d = constant = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu: \"f32[1, 16, 256, 256]\" = torch.ops.aten.relu.default(add_);  add_ = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(\n            max_pool2d: \"f32[1, 16, 85, 85]\" = torch.ops.aten.max_pool2d.default(relu, [3, 3], [3, 3]);  relu = None\n            return (max_pool2d,)\n            \nGraph signature: \n    # inputs\n    p_conv_weight: PARAMETER target='conv.weight'\n    p_conv_bias: PARAMETER target='conv.bias'\n    x: USER_INPUT\n    constant: USER_INPUT\n    \n    # outputs\n    max_pool2d: USER_OUTPUT\n    \nRange constraints: {}\n\ntensor([[[[1.9458, 1.6273, 1.7019,  ..., 2.1504, 2.5507, 1.8787],\n          [2.2365, 0.7755, 1.9928,  ..., 2.3413, 1.2623, 2.3057],\n          [1.9247, 1.5907, 1.6548,  ..., 2.1173, 1.7600, 1.5872],\n          ...,\n          [1.9187, 1.5949, 1.6456,  ..., 1.9207, 1.7531, 2.1677],\n          [2.3853, 2.1632, 1.7806,  ..., 2.0037, 1.6852, 1.4049],\n          [1.7645, 1.9489, 1.3452,  ..., 2.0768, 2.1393, 1.7513]],\n\n         [[1.5012, 1.6937, 1.3029,  ..., 1.4494, 1.9466, 1.9828],\n          [1.8593, 1.8719, 2.7506,  ..., 1.5848, 1.0953, 1.7283],\n          [2.4618, 1.8617, 1.6075,  ..., 2.2223, 1.7467, 1.6623],\n          ...,\n          [2.0089, 1.4795, 1.7225,  ..., 1.8013, 1.7134, 1.3521],\n          [2.2312, 1.6322, 1.8930,  ..., 2.0357, 1.3973, 1.7693],\n          [1.9949, 1.6649, 1.3789,  ..., 1.7138, 2.0039, 1.9342]],\n\n         [[1.7960, 1.8803, 2.0474,  ..., 1.8290, 1.7625, 1.9906],\n          [1.6626, 1.8577, 2.1852,  ..., 1.8916, 1.9693, 1.2295],\n          [1.6173, 1.6056, 2.2389,  ..., 1.7685, 1.7084, 2.2667],\n          ...,\n          [1.4691, 1.9432, 2.1351,  ..., 1.9473, 2.8299, 1.8726],\n          [2.4949, 2.1844, 1.6385,  ..., 2.6726, 1.8075, 2.0171],\n          [2.3629, 2.2791, 1.7623,  ..., 2.2238, 1.7401, 1.8280]],\n\n         ...,\n\n         [[1.4885, 1.7708, 1.8946,  ..., 2.2083, 1.3406, 1.5833],\n          [2.2450, 2.3450, 2.0719,  ..., 2.3678, 1.9186, 1.9807],\n          [2.1043, 1.7939, 2.1791,  ..., 2.0423, 1.5829, 1.3758],\n          ...,\n          [2.1712, 1.4382, 1.6501,  ..., 1.4854, 1.6417, 1.7364],\n          [1.6107, 2.1640, 2.0138,  ..., 1.4675, 1.5564, 1.8517],\n          [1.5922, 1.9649, 1.9984,  ..., 1.5279, 1.3116, 1.6866]],\n\n         [[1.3002, 2.0303, 1.3138,  ..., 1.2582, 1.9901, 1.7969],\n          [1.8442, 2.0318, 1.7786,  ..., 1.6682, 0.8281, 1.2347],\n          [2.2114, 1.9100, 1.5499,  ..., 2.5738, 1.4758, 1.6086],\n          ...,\n          [1.5610, 1.9579, 2.1318,  ..., 2.2424, 1.9802, 1.9594],\n          [1.5059, 1.6121, 1.0169,  ..., 1.7745, 1.3896, 1.0430],\n          [1.9281, 1.7772, 1.6407,  ..., 2.3685, 2.1403, 1.5720]],\n\n         [[1.8390, 2.7730, 2.1496,  ..., 2.0973, 1.8250, 2.3001],\n          [2.5140, 2.3918, 2.0092,  ..., 1.7859, 2.6470, 2.0772],\n          [1.7845, 2.2558, 2.2929,  ..., 2.2216, 2.1517, 1.8377],\n          ...,\n          [1.8539, 2.0280, 2.3912,  ..., 2.1246, 2.1039, 2.5728],\n          [2.0437, 1.5667, 2.3570,  ..., 2.0728, 2.2498, 2.1027],\n          [1.4684, 2.0560, 1.7849,  ..., 1.7504, 2.0461, 2.0553]]]],\n       grad_fn=<MaxPool2DWithIndicesBackward0>)\n\n```\n\nInspecting theExportedProgram, we can note the following:\nExportedProgram\nThetorch.fx.Graphcontains the computation graph of the original\nprogram, along with records of the original code for easy debugging.\ntorch.fx.Graph\nThe graph contains onlytorch.ops.atenoperators foundhereand custom operators.\ntorch.ops.aten\nThe parameters (weight and bias to conv) are lifted as inputs to the graph,\nresulting in noget_attrnodes in the graph, which previously existed in\nthe result oftorch.fx.symbolic_trace().\nget_attr\ntorch.fx.symbolic_trace()\nThetorch.export.ExportGraphSignaturemodels the input and output\nsignature, along with specifying which inputs are parameters.\ntorch.export.ExportGraphSignature\nThe resulting shape and dtype of tensors produced by each node in the graph is\nnoted. For example, theconv2dnode will result in a tensor of dtypetorch.float32and shape (1, 16, 256, 256).\nconv2d\ntorch.float32\n\n## Expressing Dynamism#\n\nBy defaulttorch.exportwill trace the program assuming all input shapes arestatic, and specializing the exported program to those dimensions. One\nconsequence of this is that at runtime, the program won\u2019t work on inputs with\ndifferent shapes, even if they\u2019re valid in eager mode.\ntorch.export\nAn example:\n\n```python\nimport torch\nimport traceback as tb\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\n\nep = torch.export.export(M(), example_args)\nprint(ep)\n\nexample_args2 = (torch.randn(64, 64), torch.randn(64, 128))\ntry:\n    ep.module()(*example_args2)  # fails\nexcept Exception:\n    tb.print_exc()\n\n```\n\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, p_branch1_0_weight: \"f32[32, 64]\", p_branch1_0_bias: \"f32[32]\", p_branch2_0_weight: \"f32[64, 128]\", p_branch2_0_bias: \"f32[64]\", c_buffer: \"f32[32]\", x1: \"f32[32, 64]\", x2: \"f32[32, 128]\"):\n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n            linear: \"f32[32, 32]\" = torch.ops.aten.linear.default(x1, p_branch1_0_weight, p_branch1_0_bias);  x1 = p_branch1_0_weight = p_branch1_0_bias = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu: \"f32[32, 32]\" = torch.ops.aten.relu.default(linear);  linear = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n            linear_1: \"f32[32, 64]\" = torch.ops.aten.linear.default(x2, p_branch2_0_weight, p_branch2_0_bias);  x2 = p_branch2_0_weight = p_branch2_0_bias = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu_1: \"f32[32, 64]\" = torch.ops.aten.relu.default(linear_1);  linear_1 = None\n            \n             # File: /tmp/ipykernel_210/1522925308.py:19 in forward, code: return (out1 + self.buffer, out2)\n            add: \"f32[32, 32]\" = torch.ops.aten.add.Tensor(relu, c_buffer);  relu = c_buffer = None\n            return (add, relu_1)\n            \nGraph signature: \n    # inputs\n    p_branch1_0_weight: PARAMETER target='branch1.0.weight'\n    p_branch1_0_bias: PARAMETER target='branch1.0.bias'\n    p_branch2_0_weight: PARAMETER target='branch2.0.weight'\n    p_branch2_0_bias: PARAMETER target='branch2.0.bias'\n    c_buffer: CONSTANT_TENSOR target='buffer'\n    x1: USER_INPUT\n    x2: USER_INPUT\n    \n    # outputs\n    add: USER_OUTPUT\n    relu_1: USER_OUTPUT\n    \nRange constraints: {}\n\n```\n\n\n```python\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_210/1522925308.py\", line 28, in <module>\n    ep.module()(*example_args2)  # fails\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 837, in call_wrapped\n    return self._wrapped_call(self, *args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 413, in __call__\n    raise e\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/graph_module.py\", line 400, in __call__\n    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1881, in _call_impl\n    return inner()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1829, in inner\n    result = forward_call(*args, **kwargs)\n  File \"<eval_with_key>.25\", line 11, in forward\n    _guards_fn = self._guards_fn(x1, x2);  _guards_fn = None\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"<string>\", line 3, in _\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/__init__.py\", line 2185, in _assert\n    assert condition, message\nAssertionError: Guard failed: x1.size()[0] == 32\n\n```\n\nHowever, some dimensions, such as a batch dimension, can be dynamic and vary\nfrom run to run. Such dimensions must be specified by using thetorch.export.Dim()API to create them and by passing them intotorch.export.export()through thedynamic_shapesargument.\ntorch.export.Dim()\ntorch.export.export()\ndynamic_shapes\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\n\n# Create a dynamic batch size\nbatch = torch.export.Dim(\"batch\")\n# Specify that the first dimension of each input is that batch size\ndynamic_shapes = {\"x1\": {0: batch}, \"x2\": {0: batch}}\n\nep = torch.export.export(\n    M(), args=example_args, dynamic_shapes=dynamic_shapes\n)\nprint(ep)\n\nexample_args2 = (torch.randn(64, 64), torch.randn(64, 128))\nep.module()(*example_args2)  # success\n\n```\n\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, p_branch1_0_weight: \"f32[32, 64]\", p_branch1_0_bias: \"f32[32]\", p_branch2_0_weight: \"f32[64, 128]\", p_branch2_0_bias: \"f32[64]\", c_buffer: \"f32[32]\", x1: \"f32[s24, 64]\", x2: \"f32[s24, 128]\"):\n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n            linear: \"f32[s24, 32]\" = torch.ops.aten.linear.default(x1, p_branch1_0_weight, p_branch1_0_bias);  x1 = p_branch1_0_weight = p_branch1_0_bias = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu: \"f32[s24, 32]\" = torch.ops.aten.relu.default(linear);  linear = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n            linear_1: \"f32[s24, 64]\" = torch.ops.aten.linear.default(x2, p_branch2_0_weight, p_branch2_0_bias);  x2 = p_branch2_0_weight = p_branch2_0_bias = None\n            \n             # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n            relu_1: \"f32[s24, 64]\" = torch.ops.aten.relu.default(linear_1);  linear_1 = None\n            \n             # File: /tmp/ipykernel_210/3456136871.py:18 in forward, code: return (out1 + self.buffer, out2)\n            add: \"f32[s24, 32]\" = torch.ops.aten.add.Tensor(relu, c_buffer);  relu = c_buffer = None\n            return (add, relu_1)\n            \nGraph signature: \n    # inputs\n    p_branch1_0_weight: PARAMETER target='branch1.0.weight'\n    p_branch1_0_bias: PARAMETER target='branch1.0.bias'\n    p_branch2_0_weight: PARAMETER target='branch2.0.weight'\n    p_branch2_0_bias: PARAMETER target='branch2.0.bias'\n    c_buffer: CONSTANT_TENSOR target='buffer'\n    x1: USER_INPUT\n    x2: USER_INPUT\n    \n    # outputs\n    add: USER_OUTPUT\n    relu_1: USER_OUTPUT\n    \nRange constraints: {s24: VR[0, int_oo]}\n\n```\n\n\n```python\n(tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 2.0279],\n         [1.0000, 1.0000, 1.0000,  ..., 1.1377, 1.0730, 1.0000],\n         [1.0000, 1.2866, 1.0000,  ..., 1.0000, 1.2756, 1.0392],\n         ...,\n         [1.0000, 1.0000, 1.4028,  ..., 2.4600, 1.1347, 1.0000],\n         [1.0000, 1.5510, 1.0000,  ..., 1.0000, 1.1089, 1.1483],\n         [1.3684, 1.3657, 1.0000,  ..., 1.0855, 1.0000, 1.0822]],\n        grad_fn=<AddBackward0>),\n tensor([[0.0067, 0.0000, 0.1880,  ..., 0.0000, 0.2531, 0.0000],\n         [0.0000, 0.0000, 0.0000,  ..., 0.4300, 0.0000, 0.0000],\n         [0.8456, 0.3823, 0.0000,  ..., 0.1809, 0.0000, 0.8871],\n         ...,\n         [0.3800, 0.0000, 0.3322,  ..., 0.0000, 0.0000, 0.0000],\n         [0.1636, 0.4819, 0.0000,  ..., 0.0525, 0.0000, 0.0000],\n         [0.0000, 0.3364, 0.0000,  ..., 0.0000, 0.0000, 0.7609]],\n        grad_fn=<ReluBackward0>))\n\n```\n\nSome additional things to note:\nThrough thetorch.export.Dim()API and thedynamic_shapesargument, we specified the first\ndimension of each input to be dynamic. Looking at the inputsx1andx2, they have a symbolic shape of(s0,64)and(s0,128), instead of\nthe(32,64)and(32,128)shaped tensors that we passed in as example inputs.s0is a symbol representing that this dimension can be a range\nof values.\ntorch.export.Dim()\ndynamic_shapes\nx1\nx2\n(s0,64)\n(s0,128)\n(32,64)\n(32,128)\ns0\nexported_program.range_constraintsdescribes the ranges of each symbol\nappearing in the graph. In this case, we see thats0has the range\n[0, int_oo]. For technical reasons that are difficult to explain here, they are\nassumed to be not 0 or 1. This is not a bug, and does not necessarily mean\nthat the exported program will not work for dimensions 0 or 1. SeeThe 0/1 Specialization Problemfor an in-depth discussion of this topic.\nexported_program.range_constraints\ns0\nIn the example, we usedDim(\"batch\")to create a dynamic dimension. This is\nthe most explicit way to specify dynamism. We can also useDim.DYNAMICandDim.AUTOto specify dynamism. We will go over both methods in the next section.\nDim(\"batch\")\nDim.DYNAMIC\nDim.AUTO\n\n## Named Dims#\n\nFor every dimension specified withDim(\"name\"), we will allocate a symbolic\nshape. Specifying aDimwith the same name will result in the same symbol\nto be generated. This allows users to specify what symbols are allocated for\neach input dimension.\nDim(\"name\")\nDim\n\n```python\nbatch = Dim(\"batch\")\ndynamic_shapes = {\"x1\": {0: dim}, \"x2\": {0: batch}}\n\n```\n\nFor eachDim, we can specify minimum and maximum values. We also allow\nspecifying relations betweenDims in univariate linear expressions:A*dim+B.\nThis allows users to specify more complex constraints like integer divisibility\nfor dynamic dimensions. These features allow for users to place explicit\nrestrictions on the dynamic behavior of theExportedProgramproduced.\nDim\nDim\nA*dim+B\nExportedProgram\n\n```python\ndx = Dim(\"dx\", min=4, max=256)\ndh = Dim(\"dh\", max=512)\ndynamic_shapes = {\n    \"x\": (dx, None),\n    \"y\": (2 * dx, dh),\n}\n\n```\n\nHowever,ConstraintViolationErrorswill be raised if the while tracing, we emit guards\nthat conflict with the relations or static/dynamic specifications given. For\nexample, in the above specification, the following is asserted:\nConstraintViolationErrors\nx.shape[0]is to have range[4,256], and related toy.shape[0]byy.shape[0]==2*x.shape[0].\nx.shape[0]\n[4,256]\ny.shape[0]\ny.shape[0]==2*x.shape[0]\nx.shape[1]is static.\nx.shape[1]\ny.shape[1]has range[0,512], and is unrelated to any other dimension.\ny.shape[1]\n[0,512]\nIf any of these assertions are found to be incorrect while tracing (ex.x.shape[0]is static, ory.shape[1]has a smaller range, ory.shape[0]!=2*x.shape[0]), then aConstraintViolationErrorwill be\nraised, and the user will need to change theirdynamic_shapesspecification.\nx.shape[0]\ny.shape[1]\ny.shape[0]!=2*x.shape[0]\nConstraintViolationError\ndynamic_shapes\n\n## Dim Hints#\n\nInstead of explicitly specifying dynamism usingDim(\"name\"), we can lettorch.exportinfer the ranges and relationships of the dynamic values usingDim.DYNAMIC. This is also a more convenient way to specify dynamism when you\ndon\u2019t know specificallyhowdynamic your dynamic values are.\nDim(\"name\")\ntorch.export\nDim.DYNAMIC\n\n```python\ndynamic_shapes = {\n    \"x\": (Dim.DYNAMIC, None),\n    \"y\": (Dim.DYNAMIC, Dim.DYNAMIC),\n}\n\n```\n\nWe can also specify min/max values forDim.DYNAMIC, which will serve as hints\nto export. But if while tracing export found the range to be different, it will\nautomatically update the range without raising an error. We also cannot specify\nrelationships between dynamic values. Instead, this will be inferred by export,\nand exposed to users through an inspection of assertions within the graph.  In\nthis method of specifying dynamism,ConstraintViolationErrorswillonlybe\nraised if the specified value is inferred to bestatic.\nDim.DYNAMIC\nConstraintViolationErrors\nAn even more convenient way to specify dynamism is to useDim.AUTO, which will\nbehave likeDim.DYNAMIC, but willnotraise an error if the dimension is\ninferred to be static. This is useful for when you have no idea what the dynamic\nvalues are, and want to export the program with a \u201cbest effort\u201d dynamic approach.\nDim.AUTO\nDim.DYNAMIC\n\n## ShapesCollection#\n\nWhen specifying which inputs are dynamic viadynamic_shapes, we must specify\nthe dynamism of every input. For example, given the following inputs:\ndynamic_shapes\n\n```python\nargs = {\"x\": tensor_x, \"others\": [tensor_y, tensor_z]}\n\n```\n\nwe would need to specify the dynamism oftensor_x,tensor_y, andtensor_zalong with the dynamic shapes:\ntensor_x\ntensor_y\ntensor_z\n\n```python\n# With named-Dims\ndim = torch.export.Dim(...)\ndynamic_shapes = {\"x\": {0: dim, 1: dim + 1}, \"others\": [{0: dim * 2}, None]}\n\ntorch.export(..., args, dynamic_shapes=dynamic_shapes)\n\n```\n\nHowever, this is particularly complicated as we need to specify thedynamic_shapesspecification in the same nested input structure as the input\narguments. Instead, an easier way to specify dynamic shapes is with the helper\nutilitytorch.export.ShapesCollection, where instead of specifying the\ndynamism of every single input, we can just assign directly which input\ndimensions are dynamic.\ndynamic_shapes\ntorch.export.ShapesCollection\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def forward(self, inp):\n        x = inp[\"x\"] * 1\n        y = inp[\"others\"][0] * 2\n        z = inp[\"others\"][1] * 3\n        return x, y, z\n\ntensor_x = torch.randn(3, 4, 8)\ntensor_y = torch.randn(6)\ntensor_z = torch.randn(6)\nargs = {\"x\": tensor_x, \"others\": [tensor_y, tensor_z]}\n\ndim = torch.export.Dim(\"dim\")\nsc = torch.export.ShapesCollection()\nsc[tensor_x] = (dim, dim + 1, 8)\nsc[tensor_y] = {0: dim * 2}\n\nprint(sc.dynamic_shapes(M(), (args,)))\nep = torch.export.export(M(), (args,), dynamic_shapes=sc)\nprint(ep)\n\n```\n\n\n```python\n{'inp': {'x': (Dim('dim', min=0), dim + 1, 8), 'others': [{0: 2*dim}, None]}}\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, inp_x: \"f32[s96, s96 + 1, 8]\", inp_others_0: \"f32[2*s96]\", inp_others_1: \"f32[6]\"):\n             # File: /tmp/ipykernel_210/1070110726.py:5 in forward, code: x = inp[\"x\"] * 1\n            mul: \"f32[s96, s96 + 1, 8]\" = torch.ops.aten.mul.Tensor(inp_x, 1);  inp_x = None\n            \n             # File: /tmp/ipykernel_210/1070110726.py:6 in forward, code: y = inp[\"others\"][0] * 2\n            mul_1: \"f32[2*s96]\" = torch.ops.aten.mul.Tensor(inp_others_0, 2);  inp_others_0 = None\n            \n             # File: /tmp/ipykernel_210/1070110726.py:7 in forward, code: z = inp[\"others\"][1] * 3\n            mul_2: \"f32[6]\" = torch.ops.aten.mul.Tensor(inp_others_1, 3);  inp_others_1 = None\n            return (mul, mul_1, mul_2)\n            \nGraph signature: \n    # inputs\n    inp_x: USER_INPUT\n    inp_others_0: USER_INPUT\n    inp_others_1: USER_INPUT\n    \n    # outputs\n    mul: USER_OUTPUT\n    mul_1: USER_OUTPUT\n    mul_2: USER_OUTPUT\n    \nRange constraints: {s96: VR[0, int_oo], s96 + 1: VR[1, int_oo], 2*s96: VR[0, int_oo]}\n\n```\n\n\n## AdditionalInputs#\n\nIn the case where you don\u2019t know how dynamic your inputs are, but you have an\nample set of testing or profiling data that can provide a fair sense of\nrepresentative inputs for a model, you can usetorch.export.AdditionalInputsin place ofdynamic_shapes. You can\nspecify all the possible inputs used to trace the program, andAdditionalInputswill infer which inputs are dynamic based on which input\nshapes are changing.\ntorch.export.AdditionalInputs\ndynamic_shapes\nAdditionalInputs\nExample:\n\n```python\nimport dataclasses\nimport torch\nimport torch.utils._pytree as pytree\n\n@dataclasses.dataclass\nclass D:\n    b: bool\n    i: int\n    f: float\n    t: torch.Tensor\n\npytree.register_dataclass(D)\n\nclass M(torch.nn.Module):\n    def forward(self, d: D):\n        return d.i + d.f + d.t\n\ninput1 = (D(True, 3, 3.0, torch.ones(3)),)\ninput2 = (D(True, 4, 3.0, torch.ones(4)),)\nai = torch.export.AdditionalInputs()\nai.add(input1)\nai.add(input2)\n\nprint(ai.dynamic_shapes(M(), input1))\nep = torch.export.export(M(), input1, dynamic_shapes=ai)\nprint(ep)\n\n```\n\n\n```python\n{'d': [None, _DimHint(type=<_DimHintType.DYNAMIC: 3>, min=None, max=None, _factory=True), None, (_DimHint(type=<_DimHintType.DYNAMIC: 3>, min=None, max=None, _factory=True),)]}\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, d_b, d_i: \"Sym(s37)\", d_f, d_t: \"f32[s99]\"):\n             # File: /tmp/ipykernel_210/829931439.py:16 in forward, code: return d.i + d.f + d.t\n            sym_float: \"Sym(ToFloat(s37))\" = torch.sym_float(d_i);  d_i = None\n            add: \"Sym(ToFloat(s37) + 3.0)\" = sym_float + 3.0;  sym_float = None\n            add_1: \"f32[s99]\" = torch.ops.aten.add.Tensor(d_t, add);  d_t = add = None\n            return (add_1,)\n            \nGraph signature: \n    # inputs\n    d_b: USER_INPUT\n    d_i: USER_INPUT\n    d_f: USER_INPUT\n    d_t: USER_INPUT\n    \n    # outputs\n    add_1: USER_OUTPUT\n    \nRange constraints: {s37: VR[0, int_oo], s99: VR[2, int_oo]}\n\n```\n\n\n## Serialization#\n\nTo save theExportedProgram, users can use thetorch.export.save()andtorch.export.load()APIs. The resulting file is a zipfile with a specific\nstructure. The details of the structure are defined in thePT2 Archive Spec.\nExportedProgram\ntorch.export.save()\ntorch.export.load()\nAn example:\n\n```python\nimport torch\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nexported_program = torch.export.export(MyModule(), (torch.randn(5),))\n\ntorch.export.save(exported_program, 'exported_program.pt2')\nsaved_exported_program = torch.export.load('exported_program.pt2')\n\n```\n\n\n## Export IR, Decompositions#\n\nThe graph produced bytorch.exportreturns a graph containing onlyATen operators, which are the basic unit of\ncomputation in PyTorch. As there are over\n3000 ATen operators, export provides a way to narrow down the operator set used\nin the graph based on certain characteristics, creating different IRs.\ntorch.export\nBy default, export produces the most generic IR which contains all ATen\noperators, including both functional and non-functional operators. A functional\noperator is one that does not contain any mutations or aliasing of the inputs.\nYou can find a list of all ATen operatorshereand you can inspect if an operator is functional by checkingop._schema.is_mutable.\nop._schema.is_mutable\nThis generic IR can be used to train in eager PyTorch Autograd.\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\nprint(ep_for_training.graph_module.print_readable(print_output=False))\n\n```\n\n\n```python\nclass GraphModule(torch.nn.Module):\n    def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"):\n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        conv2d: \"f32[1, 3, 3, 3]\" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias);  x = p_conv_weight = p_conv_bias = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:173 in forward, code: self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n        add_: \"i64[]\" = torch.ops.aten.add_.Tensor(b_bn_num_batches_tracked, 1);  b_bn_num_batches_tracked = add_ = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n        batch_norm: \"f32[1, 3, 3, 3]\" = torch.ops.aten.batch_norm.default(conv2d, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05, True);  conv2d = p_bn_weight = p_bn_bias = b_bn_running_mean = b_bn_running_var = None\n        return (batch_norm,)\n        \n\n```\n\nHowever, if you want to use the IR for inference, or decrease the amount of\noperators being used, you can lower the graph through theExportedProgram.run_decompositions()API. This method decomposes the\nATen operators into the ones specified in the decomposition table, and\nfunctionalizes the graph.\nExportedProgram.run_decompositions()\nBy specifying an empty set, we\u2019re only performing functionalization, and does\nnot do any additional decompositions. This results in an IR which contains ~2000\noperators (instead of the 3000 operators above), and is ideal for inference cases.\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\nwith torch.no_grad():\n    ep_for_inference = ep_for_training.run_decompositions(decomp_table={})\nprint(ep_for_inference.graph_module.print_readable(print_output=False))\n\n```\n\n\n```python\nclass GraphModule(torch.nn.Module):\n    def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"):\n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        conv2d: \"f32[1, 3, 3, 3]\" = torch.ops.aten.conv2d.default(x, p_conv_weight, p_conv_bias);  x = p_conv_weight = p_conv_bias = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:173 in forward, code: self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n        add: \"i64[]\" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1);  b_bn_num_batches_tracked = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n        _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(conv2d, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05);  conv2d = p_bn_weight = p_bn_bias = b_bn_running_mean = b_bn_running_var = None\n        getitem: \"f32[1, 3, 3, 3]\" = _native_batch_norm_legit_functional[0]\n        getitem_3: \"f32[3]\" = _native_batch_norm_legit_functional[3]\n        getitem_4: \"f32[3]\" = _native_batch_norm_legit_functional[4];  _native_batch_norm_legit_functional = None\n        return (getitem_3, getitem_4, add, getitem)\n        \n\n```\n\nAs we can see, the previously in-place operator,torch.ops.aten.add_.defaulthas now been replaced withtorch.ops.aten.add.default, a functional operator.\ntorch.ops.aten.add_.default\ntorch.ops.aten.add.default\nWe can also further lower this exported program to an operator set which only\ncontains theCoreATenOperatorSet<https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir>__,\nwhich is a collection of only ~180 operators. This IR is optimal for backends\nwho do not want to reimplement all ATen operators.\nCoreATenOperatorSet<https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir>\n\n```python\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\nwith torch.no_grad():\n    core_aten_ir = ep_for_training.run_decompositions(decomp_table=None)\nprint(core_aten_ir.graph_module.print_readable(print_output=False))\n\n```\n\n\n```python\nclass GraphModule(torch.nn.Module):\n    def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"):\n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        convolution: \"f32[1, 3, 3, 3]\" = torch.ops.aten.convolution.default(x, p_conv_weight, p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  x = p_conv_weight = p_conv_bias = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:173 in forward, code: self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n        add: \"i64[]\" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1);  b_bn_num_batches_tracked = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n        _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(convolution, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05);  convolution = p_bn_weight = p_bn_bias = b_bn_running_mean = b_bn_running_var = None\n        getitem: \"f32[1, 3, 3, 3]\" = _native_batch_norm_legit_functional[0]\n        getitem_3: \"f32[3]\" = _native_batch_norm_legit_functional[3]\n        getitem_4: \"f32[3]\" = _native_batch_norm_legit_functional[4];  _native_batch_norm_legit_functional = None\n        return (getitem_3, getitem_4, add, getitem)\n        \n\n```\n\nWe now see thattorch.ops.aten.conv2d.defaulthas been decomposed\nintotorch.ops.aten.convolution.default. This is becauseconvolutionis a more \u201ccore\u201d operator, as operations likeconv1dandconv2dcan be\nimplemented using the same op.\ntorch.ops.aten.conv2d.default\ntorch.ops.aten.convolution.default\nconvolution\nconv1d\nconv2d\nWe can also specify our own decomposition behaviors:\n\n```python\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\n\nmy_decomp_table = torch.export.default_decompositions()\n\ndef my_awesome_custom_conv2d_function(x, weight, bias, stride=[1, 1], padding=[0, 0], dilation=[1, 1], groups=1):\n    return 2 * torch.ops.aten.convolution(x, weight, bias, stride, padding, dilation, False, [0, 0], groups)\n\nmy_decomp_table[torch.ops.aten.conv2d.default] = my_awesome_custom_conv2d_function\nmy_ep = ep_for_training.run_decompositions(my_decomp_table)\nprint(my_ep.graph_module.print_readable(print_output=False))\n\n```\n\n\n```python\nclass GraphModule(torch.nn.Module):\n    def forward(self, p_conv_weight: \"f32[3, 1, 1, 1]\", p_conv_bias: \"f32[3]\", p_bn_weight: \"f32[3]\", p_bn_bias: \"f32[3]\", b_bn_running_mean: \"f32[3]\", b_bn_running_var: \"f32[3]\", b_bn_num_batches_tracked: \"i64[]\", x: \"f32[1, 1, 3, 3]\"):\n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n        convolution: \"f32[1, 3, 3, 3]\" = torch.ops.aten.convolution.default(x, p_conv_weight, p_conv_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  x = p_conv_weight = p_conv_bias = None\n        mul: \"f32[1, 3, 3, 3]\" = torch.ops.aten.mul.Tensor(convolution, 2);  convolution = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:173 in forward, code: self.num_batches_tracked.add_(1)  # type: ignore[has-type]\n        add: \"i64[]\" = torch.ops.aten.add.Tensor(b_bn_num_batches_tracked, 1);  b_bn_num_batches_tracked = None\n        \n         # File: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n        _native_batch_norm_legit_functional = torch.ops.aten._native_batch_norm_legit_functional.default(mul, p_bn_weight, p_bn_bias, b_bn_running_mean, b_bn_running_var, True, 0.1, 1e-05);  mul = p_bn_weight = p_bn_bias = b_bn_running_mean = b_bn_running_var = None\n        getitem: \"f32[1, 3, 3, 3]\" = _native_batch_norm_legit_functional[0]\n        getitem_3: \"f32[3]\" = _native_batch_norm_legit_functional[3]\n        getitem_4: \"f32[3]\" = _native_batch_norm_legit_functional[4];  _native_batch_norm_legit_functional = None\n        return (getitem_3, getitem_4, add, getitem)\n        \n\n```\n\nNotice that instead oftorch.ops.aten.conv2d.defaultbeing decomposed\nintotorch.ops.aten.convolution.default, it is now decomposed intotorch.ops.aten.convolution.defaultandtorch.ops.aten.mul.Tensor,\nwhich matches our custom decomposition rule.\ntorch.ops.aten.conv2d.default\ntorch.ops.aten.convolution.default\ntorch.ops.aten.convolution.default\ntorch.ops.aten.mul.Tensor\n\n## Limitations of torch.export#\n\nAstorch.exportis a one-shot process for capturing a computation graph from\na PyTorch program, it might ultimately run into untraceable parts of programs as\nit is nearly impossible to support tracing all PyTorch and Python features. In\nthe case oftorch.compile, an unsupported operation will cause a \u201cgraph\nbreak\u201d and the unsupported operation will be run with default Python evaluation.\nIn contrast,torch.exportwill require users to provide additional\ninformation or rewrite parts of their code to make it traceable.\ntorch.export\ntorch.compile\ntorch.export\nDraft-exportis a great resource for listing out\ngraphs breaks that will be encountered when tracing the program, along with\nadditional debug information to solve those errors.\nExportDBis also great resource for learning about the\nkinds of programs that are supported and unsupported, along with ways to rewrite\nprograms to make them traceable.\n\n## TorchDynamo unsupported#\n\nWhen usingtorch.exportwithstrict=True, this will use TorchDynamo to\nevaluate the program at the Python bytecode level to trace the program into a\ngraph. Compared to previous tracing frameworks, there will be significantly\nfewer rewrites required to make a program traceable, but there will still be\nsome Python features that are unsupported. An option to get past dealing with\nthis graph breaks is by usingnon-strict exportthrough changing thestrictflag\ntostrict=False.\ntorch.export\nstrict=True\nstrict\nstrict=False\n\n## Data/Shape-Dependent Control Flow#\n\nGraph breaks can also be encountered on data-dependent control flow (ifx.shape[0]>2) when shapes are not being specialized, as a tracing compiler cannot\npossibly deal with without generating code for a combinatorially exploding\nnumber of paths. In such cases, users will need to rewrite their code using\nspecial control flow operators. Currently, we supporttorch.condto express if-else like control flow (more coming soon!).\nifx.shape[0]>2\nYou can also refer to thistutorialfor more ways of addressing data-dependent errors.\n\n## Missing Fake/Meta Kernels for Operators#\n\nWhen tracing, a FakeTensor kernel (aka meta kernel) is required for all\noperators. This is used to reason about the input/output shapes for this\noperator.\nPlease see thistutorialfor more details.\nIn the unfortunate case where your model uses an ATen operator that is does not\nhave a FakeTensor kernel implementation yet, please file an issue.\n\n## Read More#\n\nAdditional Links for Export Users\nDeep Dive for PyTorch Developers",
    "url": "https://pytorch.org/docs/stable/export.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "47a678050e75f03c1bca362a6893fd52",
    "source": "pytorch_docs",
    "title": "Quickstart \u2014 PyTorch 2.9 documentation",
    "text": "\n## Quickstart#\n\nCreated On: May 04, 2021 | Last Updated On: Feb 09, 2023\nTo launch afault-tolerantjob, run the following on all nodes.\n\n```python\ntorchrun\n   --nnodes=NUM_NODES\n   --nproc-per-node=TRAINERS_PER_NODE\n   --max-restarts=NUM_ALLOWED_FAILURES\n   --rdzv-id=JOB_ID\n   --rdzv-backend=c10d\n   --rdzv-endpoint=HOST_NODE_ADDR\n   YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\nTo launch anelasticjob, run the following on at leastMIN_SIZEnodes\nand at mostMAX_SIZEnodes.\nMIN_SIZE\nMAX_SIZE\n\n```python\ntorchrun\n    --nnodes=MIN_SIZE:MAX_SIZE\n    --nproc-per-node=TRAINERS_PER_NODE\n    --max-restarts=NUM_ALLOWED_FAILURES_OR_MEMBERSHIP_CHANGES\n    --rdzv-id=JOB_ID\n    --rdzv-backend=c10d\n    --rdzv-endpoint=HOST_NODE_ADDR\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\nNote\nTorchElastic models failures as membership changes. When a node fails,\nthis is treated as a \u201cscale down\u201d event. When the failed node is replaced by\nthe scheduler, it is a \u201cscale up\u201d event. Hence for both fault tolerant\nand elastic jobs,--max-restartsis used to control the total number of\nrestarts before giving up, regardless of whether the restart was caused\ndue to a failure or a scaling event.\n--max-restarts\nHOST_NODE_ADDR, in form <host>[:<port>] (e.g. node1.example.com:29400),\nspecifies the node and the port on which the C10d rendezvous backend should be\ninstantiated and hosted. It can be any node in your training cluster, but\nideally you should pick a node that has a high bandwidth.\nHOST_NODE_ADDR\nNote\nIf no port number is specifiedHOST_NODE_ADDRdefaults to 29400.\nHOST_NODE_ADDR\nNote\nThe--standaloneoption can be passed to launch a single node job with a\nsidecar rendezvous backend. You don\u2019t have to pass--rdzv-id,--rdzv-endpoint, and--rdzv-backendwhen the--standaloneoption\nis used.\n--standalone\n--rdzv-id\n--rdzv-endpoint\n--rdzv-backend\n--standalone\nNote\nLearn more about writing your distributed training scripthere.\nIftorchrundoes not meet your requirements you may use our APIs directly\nfor more powerful customization. Start by taking a look at theelastic agentAPI.\ntorchrun",
    "url": "https://pytorch.org/docs/stable/elastic/quickstart.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "61ab648d2b140766c3f4d37dc56d8334",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_troubleshooting_old.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "78e29d87ef986ad6f90818875ea4b04f",
    "source": "pytorch_docs",
    "title": "PyTorch Governance | Build + CI \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch Governance | Build + CI#\n\nCreated On: Aug 31, 2022 | Last Updated On: Apr 16, 2025\n\n## How to Add a New Maintainer#\n\nFor the person to be a maintainer, a person needs to:\nLand at least six commits to the related part of the PyTorch repository\nAt least one of these commits must be submitted in the last six months\nTo add a qualified person to the maintainers\u2019 list, please create\na PR that adds a person to thepersons of interestspage andmerge_rulesfiles. Current maintainers will cast their votes of\nsupport. Decision criteria for approving the PR:\nNot earlier than two business days passed before merging (ensure the majority of the contributors have seen it)\nPR has the correct label (module: ci)\nThere are no objections from the current maintainers\nThere are at least three netthumbs upfrom current maintainers (or all maintainers votethumbs upwhen the module has less than 3 maintainers).",
    "url": "https://pytorch.org/docs/stable/community/build_ci_governance.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f5755357666cc45bbf4cb470f3f881b7",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributed.elastic.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "cf23dbc3c638b698120afda5c1fac21f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/serialization.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f26b51b4ad1996174dddd0ceb246183a",
    "source": "pytorch_docs",
    "title": "torch.nn \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.nn#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jul 25, 2025\nThese are the basic building blocks for graphs:\ntorch.nn\nContainers\nConvolution Layers\nPooling layers\nPadding Layers\nNon-linear Activations (weighted sum, nonlinearity)\nNon-linear Activations (other)\nNormalization Layers\nRecurrent Layers\nTransformer Layers\nLinear Layers\nDropout Layers\nSparse Layers\nDistance Functions\nLoss Functions\nVision Layers\nShuffle Layers\nDataParallel Layers (multi-GPU, distributed)\nUtilities\nQuantized Functions\nLazy Modules Initialization\nBuffer\n\nBuffer\nA kind of Tensor that should not be considered a model parameter.\nParameter\n\nParameter\nA kind of Tensor that is to be considered a module parameter.\nUninitializedParameter\n\nUninitializedParameter\nA parameter that is not initialized.\nUninitializedBuffer\n\nUninitializedBuffer\nA buffer that is not initialized.\n\n## Containers#\n\nModule\n\nModule\nBase class for all neural network modules.\nSequential\n\nSequential\nA sequential container.\nModuleList\n\nModuleList\nHolds submodules in a list.\nModuleDict\n\nModuleDict\nHolds submodules in a dictionary.\nParameterList\n\nParameterList\nHolds parameters in a list.\nParameterDict\n\nParameterDict\nHolds parameters in a dictionary.\nGlobal Hooks For Module\nregister_module_forward_pre_hook\n\nregister_module_forward_pre_hook\nRegister a forward pre-hook common to all modules.\nregister_module_forward_hook\n\nregister_module_forward_hook\nRegister a global forward hook for all the modules.\nregister_module_backward_hook\n\nregister_module_backward_hook\nRegister a backward hook common to all the modules.\nregister_module_full_backward_pre_hook\n\nregister_module_full_backward_pre_hook\nRegister a backward pre-hook common to all the modules.\nregister_module_full_backward_hook\n\nregister_module_full_backward_hook\nRegister a backward hook common to all the modules.\nregister_module_buffer_registration_hook\n\nregister_module_buffer_registration_hook\nRegister a buffer registration hook common to all modules.\nregister_module_module_registration_hook\n\nregister_module_module_registration_hook\nRegister a module registration hook common to all modules.\nregister_module_parameter_registration_hook\n\nregister_module_parameter_registration_hook\nRegister a parameter registration hook common to all modules.\n\n## Convolution Layers#\n\nnn.Conv1d\nnn.Conv1d\nApplies a 1D convolution over an input signal composed of several input planes.\nnn.Conv2d\nnn.Conv2d\nApplies a 2D convolution over an input signal composed of several input planes.\nnn.Conv3d\nnn.Conv3d\nApplies a 3D convolution over an input signal composed of several input planes.\nnn.ConvTranspose1d\nnn.ConvTranspose1d\nApplies a 1D transposed convolution operator over an input image composed of several input planes.\nnn.ConvTranspose2d\nnn.ConvTranspose2d\nApplies a 2D transposed convolution operator over an input image composed of several input planes.\nnn.ConvTranspose3d\nnn.ConvTranspose3d\nApplies a 3D transposed convolution operator over an input image composed of several input planes.\nnn.LazyConv1d\nnn.LazyConv1d\nAtorch.nn.Conv1dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.Conv1d\nin_channels\nnn.LazyConv2d\nnn.LazyConv2d\nAtorch.nn.Conv2dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.Conv2d\nin_channels\nnn.LazyConv3d\nnn.LazyConv3d\nAtorch.nn.Conv3dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.Conv3d\nin_channels\nnn.LazyConvTranspose1d\nnn.LazyConvTranspose1d\nAtorch.nn.ConvTranspose1dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.ConvTranspose1d\nin_channels\nnn.LazyConvTranspose2d\nnn.LazyConvTranspose2d\nAtorch.nn.ConvTranspose2dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.ConvTranspose2d\nin_channels\nnn.LazyConvTranspose3d\nnn.LazyConvTranspose3d\nAtorch.nn.ConvTranspose3dmodule with lazy initialization of thein_channelsargument.\ntorch.nn.ConvTranspose3d\nin_channels\nnn.Unfold\nnn.Unfold\nExtracts sliding local blocks from a batched input tensor.\nnn.Fold\nnn.Fold\nCombines an array of sliding local blocks into a large containing tensor.\n\n## Pooling layers#\n\nnn.MaxPool1d\nnn.MaxPool1d\nApplies a 1D max pooling over an input signal composed of several input planes.\nnn.MaxPool2d\nnn.MaxPool2d\nApplies a 2D max pooling over an input signal composed of several input planes.\nnn.MaxPool3d\nnn.MaxPool3d\nApplies a 3D max pooling over an input signal composed of several input planes.\nnn.MaxUnpool1d\nnn.MaxUnpool1d\nComputes a partial inverse ofMaxPool1d.\nMaxPool1d\nnn.MaxUnpool2d\nnn.MaxUnpool2d\nComputes a partial inverse ofMaxPool2d.\nMaxPool2d\nnn.MaxUnpool3d\nnn.MaxUnpool3d\nComputes a partial inverse ofMaxPool3d.\nMaxPool3d\nnn.AvgPool1d\nnn.AvgPool1d\nApplies a 1D average pooling over an input signal composed of several input planes.\nnn.AvgPool2d\nnn.AvgPool2d\nApplies a 2D average pooling over an input signal composed of several input planes.\nnn.AvgPool3d\nnn.AvgPool3d\nApplies a 3D average pooling over an input signal composed of several input planes.\nnn.FractionalMaxPool2d\nnn.FractionalMaxPool2d\nApplies a 2D fractional max pooling over an input signal composed of several input planes.\nnn.FractionalMaxPool3d\nnn.FractionalMaxPool3d\nApplies a 3D fractional max pooling over an input signal composed of several input planes.\nnn.LPPool1d\nnn.LPPool1d\nApplies a 1D power-average pooling over an input signal composed of several input planes.\nnn.LPPool2d\nnn.LPPool2d\nApplies a 2D power-average pooling over an input signal composed of several input planes.\nnn.LPPool3d\nnn.LPPool3d\nApplies a 3D power-average pooling over an input signal composed of several input planes.\nnn.AdaptiveMaxPool1d\nnn.AdaptiveMaxPool1d\nApplies a 1D adaptive max pooling over an input signal composed of several input planes.\nnn.AdaptiveMaxPool2d\nnn.AdaptiveMaxPool2d\nApplies a 2D adaptive max pooling over an input signal composed of several input planes.\nnn.AdaptiveMaxPool3d\nnn.AdaptiveMaxPool3d\nApplies a 3D adaptive max pooling over an input signal composed of several input planes.\nnn.AdaptiveAvgPool1d\nnn.AdaptiveAvgPool1d\nApplies a 1D adaptive average pooling over an input signal composed of several input planes.\nnn.AdaptiveAvgPool2d\nnn.AdaptiveAvgPool2d\nApplies a 2D adaptive average pooling over an input signal composed of several input planes.\nnn.AdaptiveAvgPool3d\nnn.AdaptiveAvgPool3d\nApplies a 3D adaptive average pooling over an input signal composed of several input planes.\n\n## Padding Layers#\n\nnn.ReflectionPad1d\nnn.ReflectionPad1d\nPads the input tensor using the reflection of the input boundary.\nnn.ReflectionPad2d\nnn.ReflectionPad2d\nPads the input tensor using the reflection of the input boundary.\nnn.ReflectionPad3d\nnn.ReflectionPad3d\nPads the input tensor using the reflection of the input boundary.\nnn.ReplicationPad1d\nnn.ReplicationPad1d\nPads the input tensor using replication of the input boundary.\nnn.ReplicationPad2d\nnn.ReplicationPad2d\nPads the input tensor using replication of the input boundary.\nnn.ReplicationPad3d\nnn.ReplicationPad3d\nPads the input tensor using replication of the input boundary.\nnn.ZeroPad1d\nnn.ZeroPad1d\nPads the input tensor boundaries with zero.\nnn.ZeroPad2d\nnn.ZeroPad2d\nPads the input tensor boundaries with zero.\nnn.ZeroPad3d\nnn.ZeroPad3d\nPads the input tensor boundaries with zero.\nnn.ConstantPad1d\nnn.ConstantPad1d\nPads the input tensor boundaries with a constant value.\nnn.ConstantPad2d\nnn.ConstantPad2d\nPads the input tensor boundaries with a constant value.\nnn.ConstantPad3d\nnn.ConstantPad3d\nPads the input tensor boundaries with a constant value.\nnn.CircularPad1d\nnn.CircularPad1d\nPads the input tensor using circular padding of the input boundary.\nnn.CircularPad2d\nnn.CircularPad2d\nPads the input tensor using circular padding of the input boundary.\nnn.CircularPad3d\nnn.CircularPad3d\nPads the input tensor using circular padding of the input boundary.\n\n## Non-linear Activations (weighted sum, nonlinearity)#\n\nnn.ELU\nnn.ELU\nApplies the Exponential Linear Unit (ELU) function, element-wise.\nnn.Hardshrink\nnn.Hardshrink\nApplies the Hard Shrinkage (Hardshrink) function element-wise.\nnn.Hardsigmoid\nnn.Hardsigmoid\nApplies the Hardsigmoid function element-wise.\nnn.Hardtanh\nnn.Hardtanh\nApplies the HardTanh function element-wise.\nnn.Hardswish\nnn.Hardswish\nApplies the Hardswish function, element-wise.\nnn.LeakyReLU\nnn.LeakyReLU\nApplies the LeakyReLU function element-wise.\nnn.LogSigmoid\nnn.LogSigmoid\nApplies the Logsigmoid function element-wise.\nnn.MultiheadAttention\nnn.MultiheadAttention\nAllows the model to jointly attend to information from different representation subspaces.\nnn.PReLU\nnn.PReLU\nApplies the element-wise PReLU function.\nnn.ReLU\nnn.ReLU\nApplies the rectified linear unit function element-wise.\nnn.ReLU6\nnn.ReLU6\nApplies the ReLU6 function element-wise.\nnn.RReLU\nnn.RReLU\nApplies the randomized leaky rectified linear unit function, element-wise.\nnn.SELU\nnn.SELU\nApplies the SELU function element-wise.\nnn.CELU\nnn.CELU\nApplies the CELU function element-wise.\nnn.GELU\nnn.GELU\nApplies the Gaussian Error Linear Units function.\nnn.Sigmoid\nnn.Sigmoid\nApplies the Sigmoid function element-wise.\nnn.SiLU\nnn.SiLU\nApplies the Sigmoid Linear Unit (SiLU) function, element-wise.\nnn.Mish\nnn.Mish\nApplies the Mish function, element-wise.\nnn.Softplus\nnn.Softplus\nApplies the Softplus function element-wise.\nnn.Softshrink\nnn.Softshrink\nApplies the soft shrinkage function element-wise.\nnn.Softsign\nnn.Softsign\nApplies the element-wise Softsign function.\nnn.Tanh\nnn.Tanh\nApplies the Hyperbolic Tangent (Tanh) function element-wise.\nnn.Tanhshrink\nnn.Tanhshrink\nApplies the element-wise Tanhshrink function.\nnn.Threshold\nnn.Threshold\nThresholds each element of the input Tensor.\nnn.GLU\nnn.GLU\nApplies the gated linear unit function.\n\n## Non-linear Activations (other)#\n\nnn.Softmin\nnn.Softmin\nApplies the Softmin function to an n-dimensional input Tensor.\nnn.Softmax\nnn.Softmax\nApplies the Softmax function to an n-dimensional input Tensor.\nnn.Softmax2d\nnn.Softmax2d\nApplies SoftMax over features to each spatial location.\nnn.LogSoftmax\nnn.LogSoftmax\nApplies thelog\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))function to an n-dimensional input Tensor.\nnn.AdaptiveLogSoftmaxWithLoss\nnn.AdaptiveLogSoftmaxWithLoss\nEfficient softmax approximation.\n\n## Normalization Layers#\n\nnn.BatchNorm1d\nnn.BatchNorm1d\nApplies Batch Normalization over a 2D or 3D input.\nnn.BatchNorm2d\nnn.BatchNorm2d\nApplies Batch Normalization over a 4D input.\nnn.BatchNorm3d\nnn.BatchNorm3d\nApplies Batch Normalization over a 5D input.\nnn.LazyBatchNorm1d\nnn.LazyBatchNorm1d\nAtorch.nn.BatchNorm1dmodule with lazy initialization.\ntorch.nn.BatchNorm1d\nnn.LazyBatchNorm2d\nnn.LazyBatchNorm2d\nAtorch.nn.BatchNorm2dmodule with lazy initialization.\ntorch.nn.BatchNorm2d\nnn.LazyBatchNorm3d\nnn.LazyBatchNorm3d\nAtorch.nn.BatchNorm3dmodule with lazy initialization.\ntorch.nn.BatchNorm3d\nnn.GroupNorm\nnn.GroupNorm\nApplies Group Normalization over a mini-batch of inputs.\nnn.SyncBatchNorm\nnn.SyncBatchNorm\nApplies Batch Normalization over a N-Dimensional input.\nnn.InstanceNorm1d\nnn.InstanceNorm1d\nApplies Instance Normalization.\nnn.InstanceNorm2d\nnn.InstanceNorm2d\nApplies Instance Normalization.\nnn.InstanceNorm3d\nnn.InstanceNorm3d\nApplies Instance Normalization.\nnn.LazyInstanceNorm1d\nnn.LazyInstanceNorm1d\nAtorch.nn.InstanceNorm1dmodule with lazy initialization of thenum_featuresargument.\ntorch.nn.InstanceNorm1d\nnum_features\nnn.LazyInstanceNorm2d\nnn.LazyInstanceNorm2d\nAtorch.nn.InstanceNorm2dmodule with lazy initialization of thenum_featuresargument.\ntorch.nn.InstanceNorm2d\nnum_features\nnn.LazyInstanceNorm3d\nnn.LazyInstanceNorm3d\nAtorch.nn.InstanceNorm3dmodule with lazy initialization of thenum_featuresargument.\ntorch.nn.InstanceNorm3d\nnum_features\nnn.LayerNorm\nnn.LayerNorm\nApplies Layer Normalization over a mini-batch of inputs.\nnn.LocalResponseNorm\nnn.LocalResponseNorm\nApplies local response normalization over an input signal.\nnn.RMSNorm\nnn.RMSNorm\nApplies Root Mean Square Layer Normalization over a mini-batch of inputs.\n\n## Recurrent Layers#\n\nnn.RNNBase\nnn.RNNBase\nBase class for RNN modules (RNN, LSTM, GRU).\nnn.RNN\nnn.RNN\nApply a multi-layer Elman RNN withtanh\u2061\\tanhtanhorReLU\\text{ReLU}ReLUnon-linearity to an input sequence.\nnn.LSTM\nnn.LSTM\nApply a multi-layer long short-term memory (LSTM) RNN to an input sequence.\nnn.GRU\nnn.GRU\nApply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\nnn.RNNCell\nnn.RNNCell\nAn Elman RNN cell with tanh or ReLU non-linearity.\nnn.LSTMCell\nnn.LSTMCell\nA long short-term memory (LSTM) cell.\nnn.GRUCell\nnn.GRUCell\nA gated recurrent unit (GRU) cell.\n\n## Transformer Layers#\n\nnn.Transformer\nnn.Transformer\nA basic transformer layer.\nnn.TransformerEncoder\nnn.TransformerEncoder\nTransformerEncoder is a stack of N encoder layers.\nnn.TransformerDecoder\nnn.TransformerDecoder\nTransformerDecoder is a stack of N decoder layers.\nnn.TransformerEncoderLayer\nnn.TransformerEncoderLayer\nTransformerEncoderLayer is made up of self-attn and feedforward network.\nnn.TransformerDecoderLayer\nnn.TransformerDecoderLayer\nTransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n\n## Linear Layers#\n\nnn.Identity\nnn.Identity\nA placeholder identity operator that is argument-insensitive.\nnn.Linear\nnn.Linear\nApplies an affine linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b.\nnn.Bilinear\nnn.Bilinear\nApplies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b.\nnn.LazyLinear\nnn.LazyLinear\nAtorch.nn.Linearmodule wherein_featuresis inferred.\ntorch.nn.Linear\n\n## Dropout Layers#\n\nnn.Dropout\nnn.Dropout\nDuring training, randomly zeroes some of the elements of the input tensor with probabilityp.\np\nnn.Dropout1d\nnn.Dropout1d\nRandomly zero out entire channels.\nnn.Dropout2d\nnn.Dropout2d\nRandomly zero out entire channels.\nnn.Dropout3d\nnn.Dropout3d\nRandomly zero out entire channels.\nnn.AlphaDropout\nnn.AlphaDropout\nApplies Alpha Dropout over the input.\nnn.FeatureAlphaDropout\nnn.FeatureAlphaDropout\nRandomly masks out entire channels.\n\n## Sparse Layers#\n\nnn.Embedding\nnn.Embedding\nA simple lookup table that stores embeddings of a fixed dictionary and size.\nnn.EmbeddingBag\nnn.EmbeddingBag\nCompute sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.\n\n## Distance Functions#\n\nnn.CosineSimilarity\nnn.CosineSimilarity\nReturns cosine similarity betweenx1x_1x1\u200bandx2x_2x2\u200b, computed alongdim.\nnn.PairwiseDistance\nnn.PairwiseDistance\nComputes the pairwise distance between input vectors, or between columns of input matrices.\n\n## Loss Functions#\n\nnn.L1Loss\nnn.L1Loss\nCreates a criterion that measures the mean absolute error (MAE) between each element in the inputxxxand targetyyy.\nnn.MSELoss\nnn.MSELoss\nCreates a criterion that measures the mean squared error (squared L2 norm) between each element in the inputxxxand targetyyy.\nnn.CrossEntropyLoss\nnn.CrossEntropyLoss\nThis criterion computes the cross entropy loss between input logits and target.\nnn.CTCLoss\nnn.CTCLoss\nThe Connectionist Temporal Classification loss.\nnn.NLLLoss\nnn.NLLLoss\nThe negative log likelihood loss.\nnn.PoissonNLLLoss\nnn.PoissonNLLLoss\nNegative log likelihood loss with Poisson distribution of target.\nnn.GaussianNLLLoss\nnn.GaussianNLLLoss\nGaussian negative log likelihood loss.\nnn.KLDivLoss\nnn.KLDivLoss\nThe Kullback-Leibler divergence loss.\nnn.BCELoss\nnn.BCELoss\nCreates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:\nnn.BCEWithLogitsLoss\nnn.BCEWithLogitsLoss\nThis loss combines aSigmoidlayer and theBCELossin one single class.\nnn.MarginRankingLoss\nnn.MarginRankingLoss\nCreates a criterion that measures the loss given inputsx1x1x1,x2x2x2, two 1D mini-batch or 0DTensors, and a label 1D mini-batch or 0DTensoryyy(containing 1 or -1).\nnn.HingeEmbeddingLoss\nnn.HingeEmbeddingLoss\nMeasures the loss given an input tensorxxxand a labels tensoryyy(containing 1 or -1).\nnn.MultiLabelMarginLoss\nnn.MultiLabelMarginLoss\nCreates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 2DTensorof target class indices).\nnn.HuberLoss\nnn.HuberLoss\nCreates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.\nnn.SmoothL1Loss\nnn.SmoothL1Loss\nCreates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.\nnn.SoftMarginLoss\nnn.SoftMarginLoss\nCreates a criterion that optimizes a two-class classification logistic loss between input tensorxxxand target tensoryyy(containing 1 or -1).\nnn.MultiLabelSoftMarginLoss\nnn.MultiLabelSoftMarginLoss\nCreates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between inputxxxand targetyyyof size(N,C)(N, C)(N,C).\nnn.CosineEmbeddingLoss\nnn.CosineEmbeddingLoss\nCreates a criterion that measures the loss given input tensorsx1x_1x1\u200b,x2x_2x2\u200band aTensorlabelyyywith values 1 or -1.\nnn.MultiMarginLoss\nnn.MultiMarginLoss\nCreates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between inputxxx(a 2D mini-batchTensor) and outputyyy(which is a 1D tensor of target class indices,0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121):\nnn.TripletMarginLoss\nnn.TripletMarginLoss\nCreates a criterion that measures the triplet loss given an input tensorsx1x1x1,x2x2x2,x3x3x3and a margin with a value greater than000.\nnn.TripletMarginWithDistanceLoss\nnn.TripletMarginWithDistanceLoss\nCreates a criterion that measures the triplet loss given input tensorsaaa,ppp, andnnn(representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n\n## Vision Layers#\n\nnn.PixelShuffle\nnn.PixelShuffle\nRearrange elements in a tensor according to an upscaling factor.\nnn.PixelUnshuffle\nnn.PixelUnshuffle\nReverse the PixelShuffle operation.\nnn.Upsample\nnn.Upsample\nUpsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.\nnn.UpsamplingNearest2d\nnn.UpsamplingNearest2d\nApplies a 2D nearest neighbor upsampling to an input signal composed of several input channels.\nnn.UpsamplingBilinear2d\nnn.UpsamplingBilinear2d\nApplies a 2D bilinear upsampling to an input signal composed of several input channels.\n\n## Shuffle Layers#\n\nnn.ChannelShuffle\nnn.ChannelShuffle\nDivides and rearranges the channels in a tensor.\n\n## DataParallel Layers (multi-GPU, distributed)#\n\nnn.DataParallel\nnn.DataParallel\nImplements data parallelism at the module level.\nnn.parallel.DistributedDataParallel\nnn.parallel.DistributedDataParallel\nImplement distributed data parallelism based ontorch.distributedat module level.\ntorch.distributed\n\n## Utilities#\n\nFrom thetorch.nn.utilsmodule:\ntorch.nn.utils\nUtility functions to clip parameter gradients.\nclip_grad_norm_\n\nclip_grad_norm_\nClip the gradient norm of an iterable of parameters.\nclip_grad_norm\n\nclip_grad_norm\nClip the gradient norm of an iterable of parameters.\nclip_grad_value_\n\nclip_grad_value_\nClip the gradients of an iterable of parameters at specified value.\nget_total_norm\n\nget_total_norm\nCompute the norm of an iterable of tensors.\nclip_grads_with_norm_\n\nclip_grads_with_norm_\nScale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm.\nUtility functions to flatten and unflatten Module parameters to and from a single vector.\nparameters_to_vector\n\nparameters_to_vector\nFlatten an iterable of parameters into a single vector.\nvector_to_parameters\n\nvector_to_parameters\nCopy slices of a vector into an iterable of parameters.\nUtility functions to fuse Modules with BatchNorm modules.\nfuse_conv_bn_eval\n\nfuse_conv_bn_eval\nFuse a convolutional module and a BatchNorm module into a single, new convolutional module.\nfuse_conv_bn_weights\n\nfuse_conv_bn_weights\nFuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters.\nfuse_linear_bn_eval\n\nfuse_linear_bn_eval\nFuse a linear module and a BatchNorm module into a single, new linear module.\nfuse_linear_bn_weights\n\nfuse_linear_bn_weights\nFuse linear module parameters and BatchNorm module parameters into new linear module parameters.\nUtility functions to convert Module parameter memory formats.\nconvert_conv2d_weight_memory_format\n\nconvert_conv2d_weight_memory_format\nConvertmemory_formatofnn.Conv2d.weighttomemory_format.\nmemory_format\nnn.Conv2d.weight\nmemory_format\nconvert_conv3d_weight_memory_format\n\nconvert_conv3d_weight_memory_format\nConvertmemory_formatofnn.Conv3d.weighttomemory_formatThe conversion recursively applies to nestednn.Module, includingmodule.\nmemory_format\nnn.Conv3d.weight\nmemory_format\nnn.Module\nmodule\nUtility functions to apply and remove weight normalization from Module parameters.\nweight_norm\n\nweight_norm\nApply weight normalization to a parameter in the given module.\nremove_weight_norm\n\nremove_weight_norm\nRemove the weight normalization reparameterization from a module.\nspectral_norm\n\nspectral_norm\nApply spectral normalization to a parameter in the given module.\nremove_spectral_norm\n\nremove_spectral_norm\nRemove the spectral normalization reparameterization from a module.\nUtility functions for initializing Module parameters.\nskip_init\n\nskip_init\nGiven a module class object and args / kwargs, instantiate the module without initializing parameters / buffers.\nUtility classes and functions for pruning Module parameters.\nprune.BasePruningMethod\nprune.BasePruningMethod\nAbstract base class for creation of new pruning techniques.\nprune.PruningContainer\nprune.PruningContainer\nContainer holding a sequence of pruning methods for iterative pruning.\nprune.Identity\nprune.Identity\nUtility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.\nprune.RandomUnstructured\nprune.RandomUnstructured\nPrune (currently unpruned) units in a tensor at random.\nprune.L1Unstructured\nprune.L1Unstructured\nPrune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.\nprune.RandomStructured\nprune.RandomStructured\nPrune entire (currently unpruned) channels in a tensor at random.\nprune.LnStructured\nprune.LnStructured\nPrune entire (currently unpruned) channels in a tensor based on their Ln-norm.\nn\nprune.CustomFromMask\nprune.CustomFromMask\n\nprune.identity\nprune.identity\nApply pruning reparametrization without pruning any units.\nprune.random_unstructured\nprune.random_unstructured\nPrune tensor by removing random (currently unpruned) units.\nprune.l1_unstructured\nprune.l1_unstructured\nPrune tensor by removing units with the lowest L1-norm.\nprune.random_structured\nprune.random_structured\nPrune tensor by removing random channels along the specified dimension.\nprune.ln_structured\nprune.ln_structured\nPrune tensor by removing channels with the lowest Ln-norm along the specified dimension.\nn\nprune.global_unstructured\nprune.global_unstructured\nGlobally prunes tensors corresponding to all parameters inparametersby applying the specifiedpruning_method.\nparameters\npruning_method\nprune.custom_from_mask\nprune.custom_from_mask\nPrune tensor corresponding to parameter callednameinmoduleby applying the pre-computed mask inmask.\nname\nmodule\nmask\nprune.remove\nprune.remove\nRemove the pruning reparameterization from a module and the pruning method from the forward hook.\nprune.is_pruned\nprune.is_pruned\nCheck if a module is pruned by looking for pruning pre-hooks.\nParametrizations implemented using the new parametrization functionality\nintorch.nn.utils.parameterize.register_parametrization().\ntorch.nn.utils.parameterize.register_parametrization()\nparametrizations.orthogonal\nparametrizations.orthogonal\nApply an orthogonal or unitary parametrization to a matrix or a batch of matrices.\nparametrizations.weight_norm\nparametrizations.weight_norm\nApply weight normalization to a parameter in the given module.\nparametrizations.spectral_norm\nparametrizations.spectral_norm\nApply spectral normalization to a parameter in the given module.\nUtility functions to parametrize Tensors on existing Modules.\nNote that these functions can be used to parametrize a given Parameter\nor Buffer given a specific function that maps from an input space to the\nparametrized space. They are not parameterizations that would transform\nan object into a parameter. See theParametrizations tutorialfor more information on how to implement your own parametrizations.\nparametrize.register_parametrization\nparametrize.register_parametrization\nRegister a parametrization to a tensor in a module.\nparametrize.remove_parametrizations\nparametrize.remove_parametrizations\nRemove the parametrizations on a tensor in a module.\nparametrize.cached\nparametrize.cached\nContext manager that enables the caching system within parametrizations registered withregister_parametrization().\nregister_parametrization()\nparametrize.is_parametrized\nparametrize.is_parametrized\nDetermine if a module has a parametrization.\nparametrize.transfer_parametrizations_and_params\nparametrize.transfer_parametrizations_and_params\nTransfer parametrizations and the parameters they parametrize fromfrom_moduletoto_module.\nfrom_module\nto_module\nparametrize.type_before_parametrizations\nparametrize.type_before_parametrizations\nReturn the module type before parametrizations were applied and if not, then it returns the module type.\nparametrize.ParametrizationList\nparametrize.ParametrizationList\nA sequential container that holds and manages the original parameters or buffers of a parametrizedtorch.nn.Module.\ntorch.nn.Module\nUtility functions to call a given Module in a stateless manner.\nstateless.functional_call\nstateless.functional_call\nPerform a functional call on the module by replacing the module parameters and buffers with the provided ones.\nUtility functions in other modules\nnn.utils.rnn.PackedSequence\nnn.utils.rnn.PackedSequence\nHolds the data and list ofbatch_sizesof a packed sequence.\nbatch_sizes\nnn.utils.rnn.pack_padded_sequence\nnn.utils.rnn.pack_padded_sequence\nPacks a Tensor containing padded sequences of variable length.\nnn.utils.rnn.pad_packed_sequence\nnn.utils.rnn.pad_packed_sequence\nPad a packed batch of variable length sequences.\nnn.utils.rnn.pad_sequence\nnn.utils.rnn.pad_sequence\nPad a list of variable length Tensors withpadding_value.\npadding_value\nnn.utils.rnn.pack_sequence\nnn.utils.rnn.pack_sequence\nPacks a list of variable length Tensors.\nnn.utils.rnn.unpack_sequence\nnn.utils.rnn.unpack_sequence\nUnpack PackedSequence into a list of variable length Tensors.\nnn.utils.rnn.unpad_sequence\nnn.utils.rnn.unpad_sequence\nUnpad padded Tensor into a list of variable length Tensors.\nnn.utils.rnn.invert_permutation\nnn.utils.rnn.invert_permutation\nReturns the inverse ofpermutation.\npermutation\nnn.parameter.is_lazy\nnn.parameter.is_lazy\nReturns whetherparamis anUninitializedParameterorUninitializedBuffer.\nparam\nUninitializedParameter\nUninitializedBuffer\nnn.factory_kwargs\nnn.factory_kwargs\nReturn a canonicalized dict of factory kwargs.\nnn.modules.flatten.Flatten\nnn.modules.flatten.Flatten\nFlattens a contiguous range of dims into a tensor.\nnn.modules.flatten.Unflatten\nnn.modules.flatten.Unflatten\nUnflattens a tensor dim expanding it to a desired shape.\n\n## Quantized Functions#\n\nQuantization refers to techniques for performing computations and storing tensors at lower bitwidths than\nfloating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to theQuantizationdocumentation.\n\n## Lazy Modules Initialization#\n\nnn.modules.lazy.LazyModuleMixin\nnn.modules.lazy.LazyModuleMixin\nA mixin for modules that lazily initialize parameters, also known as \"lazy modules\".",
    "url": "https://pytorch.org/docs/stable/nn.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "811815c5a03cebc6060d9779c7a044ec",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributions.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "28fa5bc7cf5baac75248cccc18ef5fd5",
    "source": "pytorch_docs",
    "title": "torch.special \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.special#\n\nCreated On: Mar 04, 2021 | Last Updated On: Jun 18, 2025\nThe torch.special module, modeled after SciPy\u2019sspecialmodule.\n\n## Functions#\n\nAiry functionAi(input)\\text{Ai}\\left(\\text{input}\\right)Ai(input).\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nBessel function of the first kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nBessel function of the first kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nBessel function of the second kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nBessel function of the second kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the first kindTn(input)T_{n}(\\text{input})Tn\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Ifn<6n < 6n<6or\u2223input\u2223>1|\\text{input}| > 1\u2223input\u2223>1the recursion:\nis evaluated. Otherwise, the explicit trigonometric formula:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the second kindUn(input)U_{n}(\\text{input})Un\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,2\u00d7input2 \\times \\text{input}2\u00d7inputis returned. Ifn<6n < 6n<6or\u2223input\u2223>1|\\text{input}| > 1\u2223input\u2223>1, the recursion:\nis evaluated. Otherwise, the explicit trigonometric formula:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the third kindVn\u2217(input)V_{n}^{\\ast}(\\text{input})Vn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the fourth kindWn\u2217(input)W_{n}^{\\ast}(\\text{input})Wn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nComputes the logarithmic derivative of the gamma function oninput.\ninput(Tensor) \u2013 the tensor to compute the digamma function on\nout(Tensor,optional) \u2013 the output tensor.\nNote\nThis function is similar to SciPy\u2019sscipy.special.digamma.\nNote\nFrom PyTorch 1.8 onwards, the digamma function returns-Inffor0.\nPreviously it returnedNaNfor0.\nExample:\n\n```python\n>>> a = torch.tensor([1, 0.5])\n>>> torch.special.digamma(a)\ntensor([-0.5772, -1.9635])\n\n```\n\nComputes the entropy oninput(as defined below), elementwise.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.arange(-0.5, 1, 0.5)\n>>> a\ntensor([-0.5000,  0.0000,  0.5000])\n>>> torch.special.entr(a)\ntensor([  -inf, 0.0000, 0.3466])\n\n```\n\nComputes the error function ofinput. The error function is defined as follows:\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])\n\n```\n\nComputes the complementary error function ofinput.\nThe complementary error function is defined as follows:\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])\n\n```\n\nComputes the scaled complementary error function for each element ofinput.\nThe scaled complementary error function is defined as follows:\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.erfcx(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 5.0090, 0.0561])\n\n```\n\nComputes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as:\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])\n\n```\n\nComputes the base two exponential function ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\ntensor([ 1.,  2.,  8., 16.])\n\n```\n\nComputes the expit (also known as the logistic sigmoid function) of the elements ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> t = torch.randn(4)\n>>> t\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n>>> torch.special.expit(t)\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\n\n```\n\nComputes the exponential of the elements minus 1\nofinput.\ninput\nNote\nThis function provides greater precision than exp(x) - 1 for small values of x.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])\n\n```\n\nComputes the regularized lower incomplete gamma function:\nwhere bothinputi\\text{input}_iinputi\u200bandotheri\\text{other}_iotheri\u200bare weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative thenouti=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5)in the equation above is the gamma function,\nSeetorch.special.gammaincc()andtorch.special.gammaln()for related functions.\ntorch.special.gammaincc()\ntorch.special.gammaln()\nSupportsbroadcasting to a common shapeand float inputs.\nNote\nThe backward pass with respect toinputis not yet supported.\nPlease open an issue on PyTorch\u2019s Github to request it.\ninput\ninput(Tensor) \u2013 the first non-negative input tensor\nother(Tensor) \u2013 the second non-negative input tensor\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.special.gammaincc(a1, a2)\ntensor([0.3528, 0.5665, 0.7350])\ntensor([0.3528, 0.5665, 0.7350])\n>>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)\ntensor([1., 1., 1.])\n\n```\n\nComputes the regularized upper incomplete gamma function:\nwhere bothinputi\\text{input}_iinputi\u200bandotheri\\text{other}_iotheri\u200bare weakly positive\nand at least one is strictly positive.\nIf both are zero or either is negative thenouti=nan\\text{out}_i=\\text{nan}outi\u200b=nan.\u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5)in the equation above is the gamma function,\nSeetorch.special.gammainc()andtorch.special.gammaln()for related functions.\ntorch.special.gammainc()\ntorch.special.gammaln()\nSupportsbroadcasting to a common shapeand float inputs.\nNote\nThe backward pass with respect toinputis not yet supported.\nPlease open an issue on PyTorch\u2019s Github to request it.\ninput\ninput(Tensor) \u2013 the first non-negative input tensor\nother(Tensor) \u2013 the second non-negative input tensor\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.special.gammaincc(a1, a2)\ntensor([0.6472, 0.4335, 0.2650])\n>>> b = torch.special.gammainc(a1, a2) + torch.special.gammaincc(a1, a2)\ntensor([1., 1., 1.])\n\n```\n\nComputes the natural logarithm of the absolute value of the gamma function oninput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.special.gammaln(a)\ntensor([ 0.5724,  0.0000, -0.1208])\n\n```\n\nPhysicist\u2019s Hermite polynomialHn(input)H_{n}(\\text{input})Hn\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Otherwise, the recursion:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nProbabilist\u2019s Hermite polynomialHen(input)He_{n}(\\text{input})Hen\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Otherwise, the recursion:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nComputes the zeroth order modified Bessel function of the first kind for each element ofinput.\ninput\ninput(Tensor) \u2013 the input tensor\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.i0(torch.arange(5, dtype=torch.float32))\ntensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])\n\n```\n\nComputes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\n\n```\n\nComputes the first order modified Bessel function of the first kind (as defined below)\nfor each element ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.i1(torch.arange(5, dtype=torch.float32))\ntensor([0.0000, 0.5652, 1.5906, 3.9534, 9.7595])\n\n```\n\nComputes the exponentially scaled first order modified Bessel function of the first kind (as defined below)\nfor each element ofinput.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.i1e(torch.arange(5, dtype=torch.float32))\ntensor([0.0000, 0.2079, 0.2153, 0.1968, 0.1788])\n\n```\n\nLaguerre polynomialLn(input)L_{n}(\\text{input})Ln\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Otherwise, the recursion:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nLegendre polynomialPn(input)P_{n}(\\text{input})Pn\u200b(input).\nIfn=0n = 0n=0,111is returned. Ifn=1n = 1n=1,input\\text{input}inputis returned. Otherwise, the recursion:\nis evaluated.\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nAlias fortorch.log1p().\ntorch.log1p()\nComputes the log of the area under the standard Gaussian probability density function,\nintegrated from minus infinity toinput, elementwise.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.log_ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))\ntensor([-6.6077 -3.7832 -1.841  -0.6931 -0.1728 -0.023  -0.0014])\n\n```\n\nComputes softmax followed by a logarithm.\nWhile mathematically equivalent to log(softmax(x)), doing these two\noperations separately is slower and numerically unstable. This function\nis computed as:\ninput(Tensor) \u2013 input\ndim(int) \u2013 A dimension along which log_softmax will be computed.\ndtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is cast todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.\ntorch.dtype\ndtype\nExample:\n\n```python\n>>> t = torch.ones(2, 2)\n>>> torch.special.log_softmax(t, 0)\ntensor([[-0.6931, -0.6931],\n        [-0.6931, -0.6931]])\n\n```\n\nReturns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN.\ninput\ninput\ninput\ninput\ninput(Tensor) \u2013 the input tensor.\neps(float,optional) \u2013 the epsilon for input clamp bound. Default:None\nNone\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.rand(5)\n>>> a\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\n>>> torch.special.logit(a, eps=1e-6)\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\n\n```\n\nAlias fortorch.logsumexp().\ntorch.logsumexp()\nModified Bessel function of the first kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nModified Bessel function of the first kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nModified Bessel function of the second kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nModified Bessel function of the second kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nComputes themultivariate log-gamma functionwith dimensionpppelement-wise, given by\nwhereC=log\u2061(\u03c0)\u22c5p(p\u22121)4C = \\log(\\pi) \\cdot \\frac{p (p - 1)}{4}C=log(\u03c0)\u22c54p(p\u22121)\u200band\u0393(\u2212)\\Gamma(-)\u0393(\u2212)is the Gamma function.\nAll elements must be greater thanp\u221212\\frac{p - 1}{2}2p\u22121\u200b, otherwise the behavior is undefined.\ninput(Tensor) \u2013 the tensor to compute the multivariate log-gamma function\np(int) \u2013 the number of dimensions\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.empty(2, 3).uniform_(1, 2)\n>>> a\ntensor([[1.6835, 1.8474, 1.1929],\n        [1.0475, 1.7162, 1.4180]])\n>>> torch.special.multigammaln(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n        [1.0311, 0.3901, 0.5049]])\n\n```\n\nComputes the area under the standard Gaussian probability density function,\nintegrated from minus infinity toinput, elementwise.\ninput\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.ndtr(torch.tensor([-3., -2, -1, 0, 1, 2, 3]))\ntensor([0.0013, 0.0228, 0.1587, 0.5000, 0.8413, 0.9772, 0.9987])\n\n```\n\nComputes the argument, x, for which the area under the Gaussian probability density function\n(integrated from minus infinity to x) is equal toinput, elementwise.\ninput\nNote\nAlso known as quantile function for Normal Distribution.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> torch.special.ndtri(torch.tensor([0, 0.25, 0.5, 0.75, 1]))\ntensor([   -inf, -0.6745,  0.0000,  0.6745,     inf])\n\n```\n\nComputes thenthn^{th}nthderivative of the digamma function oninput.n\u22650n \\geq 0n\u22650is called the order of the polygamma function.\ninput\nNote\nThis function is implemented only for nonnegative integersn\u22650n \\geq 0n\u22650.\nn(int) \u2013 the order of the polygamma function\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> a = torch.tensor([1, 0.5])\n>>> torch.special.polygamma(1, a)\ntensor([1.64493, 4.9348])\n>>> torch.special.polygamma(2, a)\ntensor([ -2.4041, -16.8288])\n>>> torch.special.polygamma(3, a)\ntensor([ 6.4939, 97.4091])\n>>> torch.special.polygamma(4, a)\ntensor([ -24.8863, -771.4742])\n\n```\n\nAlias fortorch.special.digamma().\ntorch.special.digamma()\nAlias fortorch.round().\ntorch.round()\nScaled modified Bessel function of the second kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nScaled modified Bessel function of the second kind of order111.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the first kindTn\u2217(input)T_{n}^{\\ast}(\\text{input})Tn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the second kindUn\u2217(input)U_{n}^{\\ast}(\\text{input})Un\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the third kindVn\u2217(input)V_{n}^{\\ast}(\\text{input})Vn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nChebyshev polynomial of the fourth kindWn\u2217(input)W_{n}^{\\ast}(\\text{input})Wn\u2217\u200b(input).\ninput(Tensor) \u2013 the input tensor.\nn(Tensor) \u2013 Degree of the polynomial.\nout(Tensor,optional) \u2013 the output tensor.\nComputes the normalized sinc ofinput.\ninput.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> t = torch.randn(4)\n>>> t\ntensor([ 0.2252, -0.2948,  1.0267, -1.1566])\n>>> torch.special.sinc(t)\ntensor([ 0.9186,  0.8631, -0.0259, -0.1300])\n\n```\n\nComputes the softmax function.\nSoftmax is defined as:\nSoftmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}Softmax(xi\u200b)=\u2211j\u200bexp(xj\u200b)exp(xi\u200b)\u200b\nIt is applied to all slices along dim, and will re-scale them so that the elements\nlie in the range[0, 1]and sum to 1.\ninput(Tensor) \u2013 input\ndim(int) \u2013 A dimension along which softmax will be computed.\ndtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is cast todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.\ntorch.dtype\ndtype\n\n```python\n>>> t = torch.ones(2, 2)\n>>> torch.special.softmax(t, 0)\ntensor([[0.5000, 0.5000],\n        [0.5000, 0.5000]])\n\n```\n\nSpherical Bessel function of the first kind of order000.\ninput(Tensor) \u2013 the input tensor.\nout(Tensor,optional) \u2013 the output tensor.\nComputesinput*log1p(other)with the following cases.\ninput*log1p(other)\nSimilar to SciPy\u2019sscipy.special.xlog1py.\ninput(NumberorTensor) \u2013 Multiplier\nother(NumberorTensor) \u2013 Argument\nNote\nAt least one ofinputorothermust be a tensor.\ninput\nother\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])\n\n```\n\nComputesinput*log(other)with the following cases.\ninput*log(other)\nSimilar to SciPy\u2019sscipy.special.xlogy.\ninput(NumberorTensor) \u2013 Multiplier\nother(NumberorTensor) \u2013 Argument\nNote\nAt least one ofinputorothermust be a tensor.\ninput\nother\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlogy(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlogy(x, y)\ntensor([1.0986, 1.3863, 0.0000])\n>>> torch.special.xlogy(x, 4)\ntensor([1.3863, 2.7726, 4.1589])\n>>> torch.special.xlogy(2, y)\ntensor([2.1972, 1.3863, 0.0000])\n\n```\n\nComputes the Hurwitz zeta function, elementwise.\ninput(Tensor) \u2013 the input tensor corresponding tox.\nother(Tensor) \u2013 the input tensor corresponding toq.\nNote\nThe Riemann zeta function corresponds to the case whenq = 1\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> x = torch.tensor([2., 4.])\n>>> torch.special.zeta(x, 1)\ntensor([1.6449, 1.0823])\n>>> torch.special.zeta(x, torch.tensor([1., 2.]))\ntensor([1.6449, 0.0823])\n>>> torch.special.zeta(2, torch.tensor([1., 2.]))\ntensor([1.6449, 0.6449])\n\n```\n",
    "url": "https://pytorch.org/docs/stable/special.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e4a0e9c9bb9defa183d157830a9a0044",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/nn.attention.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "54841643bc36155d672b018651890bdd",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_performance_dashboard.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "037b6890ba8e7c282bdd4c60888b6e64",
    "source": "pytorch_docs",
    "title": "Numerical accuracy \u2014 PyTorch 2.9 documentation",
    "text": "\n## Numerical accuracy#\n\nCreated On: Oct 13, 2021 | Last Updated On: Jul 16, 2025\nIn modern computers, floating point numbers are represented using IEEE 754 standard.\nFor more details on floating point arithmetic and IEEE 754 standard, please seeFloating point arithmeticIn particular, note that floating point provides limited accuracy (about 7 decimal digits\nfor single precision floating point numbers, about 16 decimal digits for double precision\nfloating point numbers) and that floating point addition and multiplication are not\nassociative, so the order of the operations affects the results.\nBecause of this, PyTorch is not guaranteed\nto produce bitwise identical results for floating point computations that are\nmathematically identical. Similarly, bitwise identical results are not guaranteed across\nPyTorch releases, individual commits, or different platforms. In particular, CPU and GPU\nresults can be different even for bitwise-identical inputs and even after controlling for\nthe sources of randomness.\n\n## Batched computations or slice computations#\n\nMany operations in PyTorch support batched computation, where the same operation is performed\nfor the elements of the batches of inputs. An example of this istorch.mm()andtorch.bmm(). It is possible to implement batched computation as a loop over batch elements,\nand apply the necessary math operations to the individual batch elements, for efficiency reasons\nwe are not doing that, and typically perform computation for the whole batch. The mathematical\nlibraries that we are calling, and PyTorch internal implementations of operations can produces\nslightly different results in this case, compared to non-batched computations. In particular,\nletAandBbe 3D tensors with the dimensions suitable for batched matrix multiplication.\nThen(A@B)[0](the first element of the batched result) is not guaranteed to be bitwise\nidentical toA[0]@B[0](the matrix product of the first elements of the input batches)\neven though mathematically it\u2019s an identical computation.\ntorch.mm()\ntorch.bmm()\nA\nB\n(A@B)[0]\nA[0]@B[0]\nSimilarly, an operation applied to a tensor slice is not guaranteed to produce results that are\nidentical to the slice of the result of the same operation applied to the full tensor. E.g. letAbe a 2-dimensional tensor.A.sum(-1)[0]is not guaranteed to be bitwise equal toA[:,0].sum().\nA\nA.sum(-1)[0]\nA[:,0].sum()\n\n## Extremal values#\n\nWhen inputs contain large values such that intermediate results may overflow the range of the\nused datatype, the end result may overflow too, even though it is representable in the original\ndatatype. E.g.:\n\n```python\nimport torch\na=torch.tensor([1e20, 1e20]) # fp32 type by default\na.norm() # produces tensor(inf)\na.double().norm() # produces tensor(1.4142e+20, dtype=torch.float64), representable in fp32\n\n```\n\n\n## Linear algebra (torch.linalg)#\n\ntorch.linalg\n\n## Non-finite values#\n\nThe external libraries (backends) thattorch.linalguses provide no guarantees on their behaviour\nwhen the inputs have non-finite values likeinforNaN. As such, neither does PyTorch.\nThe operations may return a tensor with non-finite values, or raise an exception, or even segfault.\ntorch.linalg\ninf\nNaN\nConsider usingtorch.isfinite()before calling these functions to detect this situation.\ntorch.isfinite()\n\n## Extremal values in linalg#\n\nFunctions withintorch.linalghave moreExtremal Valuesthan other PyTorch functions.\ntorch.linalg\nSolversandInversesassume that the input matrixAis invertible. If it is close to\nbeing non-invertible (for example, if it has a very small singular value), then these algorithms may silently return\nincorrect results. These matrices are said to beill-conditioned.\nIf provided with ill-conditioned inputs, the result of these functions they may vary when using the same inputs on different\ndevices or when using different backends via the keyworddriver.\nA\ndriver\nSpectral operations likesvd,eig, andeighmay also return incorrect results (and their gradients may be infinite)\nwhen their inputs have singular values that are close to each other. This is because the algorithms used to compute these decompositions\nstruggle to converge for these inputs.\nsvd\neig\neigh\nRunning the computation infloat64(as NumPy does by default) often helps, but it does not solve these issues in all cases.\nAnalyzing the spectrum of the inputs viatorch.linalg.svdvals()or their condition number viatorch.linalg.cond()may help to detect these issues.\nfloat64\ntorch.linalg.svdvals()\ntorch.linalg.cond()\n\n## TensorFloat-32(TF32) on Nvidia Ampere (and later) devices#\n\nOn Ampere (and later) Nvidia GPUs, PyTorch can use TensorFloat32 (TF32) to speed up mathematically intensive operations, in particular matrix multiplications and convolutions.\nWhen an operation is performed using TF32 tensor cores, only the first 10 bits of the input mantissa are read.\nThis may reduce accuracy and produce surprising results (e.g., multiplying a matrix by the identity matrix may produce results that are different from the input).\nBy default, TF32 tensor cores are disabled for matrix multiplications and enabled for convolutions, although most neural network workloads have the same convergence behavior when using TF32 as they have with fp32.\nWe recommend enabling TF32 tensor cores for matrix multiplications withtorch.backends.cuda.matmul.fp32_precision=\"tf32\"(`torch.backends.cuda.matmul.allow_tf32=Trueis going to be deprecated) if your network does not need full float32 precision.\nIf your network needs full float32 precision for both matrix multiplications and convolutions, then TF32 tensor cores can also be disabled for convolutions withtorch.backends.cudnn.conv.fp32_precision=\"ieee\"(torch.backends.cudnn.allow_tf32=Falseis going to be deprecated).\ntorch.backends.cuda.matmul.fp32_precision=\"tf32\"\n`torch.backends.cuda.matmul.allow_tf32=True\ntorch.backends.cudnn.conv.fp32_precision=\"ieee\"\ntorch.backends.cudnn.allow_tf32=False\nFor more information seeTensorFloat32.\n\n## Reduced Precision Reduction for FP16 and BF16 GEMMs#\n\nHalf-precision GEMM operations are typically done with intermediate accumulations (reduction) in single-precision for numerical accuracy and improved resilience to overflow. For performance, certain GPU architectures, especially more recent ones, allow a few truncations of the intermediate accumulation results to the reduced precision (e.g., half-precision). This change is often benign from the perspective of model convergence, though it may lead to unexpected results (e.g.,infvalues when the final result should be be representable in half-precision).\nIf reduced-precision reductions are problematic, they can be turned off withtorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction=False\ninf\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction=False\nA similar flag exists for BF16 GEMM operations and is turned on by default. If BF16\nreduced-precision reductions are problematic, they can be turned off withtorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction=False\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction=False\nFor more information seeallow_fp16_reduced_precision_reductionandallow_bf16_reduced_precision_reduction\n\n## Reduced Precision Reduction for FP16 and BF16 in Scaled Dot Product Attention (SDPA)#\n\nA naive SDPA math backend, when using FP16/BF16 inputs, can accumulate significant numerical errors due to the usage of low-precision intermediate buffers. To mitigate this issue, the default behavior now involves upcasting FP16/BF16 inputs to FP32. Computations are performed in FP32/TF32, and the final FP32 results are then downcasted back to FP16/BF16. This will improve numerical accuracy of the final output for the math backend with FP16/BF16 inputs, but increases memory usages and may cause the performance regressions in the math backend as computations shift from FP16/BF16 BMM to FP32/TF32 BMM/Matmul.\nFor scenarios where reduced-precision reductions are preferred for speed, they can be enabled with the following setting:torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)\ntorch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True)\n\n## Reduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices#\n\nOn AMD Instinct MI200 GPUs, the FP16 and BF16 V_DOT2 and MFMA matrix instructions flush input and output denormal values to zero. FP32 and FP64 MFMA matrix instructions do not flush input and output denormal values to zero. The affected instructions are only used by rocBLAS (GEMM) and MIOpen (convolution) kernels; all other PyTorch operations will not encounter this behavior. All other supported AMD GPUs will not encounter this behavior.\nrocBLAS and MIOpen provide alternate implementations for affected FP16 operations. Alternate implementations for BF16 operations are not provided; BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values. For the FP16 alternate implementations, FP16 input values are cast to an intermediate BF16 value and then cast back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged.\nWhen training using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. Denormal values more frequently occur in the backward pass of training during gradient calculation. PyTorch by default will use the rocBLAS and MIOpen alternate implementations during the backward pass. The default behavior can be overridden using environment variables, ROCBLAS_INTERNAL_FP16_ALT_IMPL and MIOPEN_DEBUG_CONVOLUTION_ATTRIB_FP16_ALT_IMPL. The behavior of these environment variables is as follows:\nforward\nbackward\nEnv unset\noriginal\nalternate\nEnv set to 1\nalternate\nalternate\nEnv set to 0\noriginal\noriginal\nThe following is the list of operations where rocBLAS may be used:\ntorch.addbmm\ntorch.addmm\ntorch.baddbmm\ntorch.bmm\ntorch.mm\ntorch.nn.GRUCell\ntorch.nn.LSTMCell\ntorch.nn.Linear\ntorch.sparse.addmm\nthe following torch._C._ConvBackend implementations:\nslowNd\nslowNd_transposed\nslowNd_dilated\nslowNd_dilated_transposed\nThe following is the list of operations where MIOpen may be used:\ntorch.nn.Conv[Transpose]Nd\nthe following torch._C._ConvBackend implementations:\nConvBackend::Miopen\nConvBackend::MiopenDepthwise\nConvBackend::MiopenTranspose",
    "url": "https://pytorch.org/docs/stable/notes/numerical_accuracy.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2528643c2b65d5d1c25f2e34fbb091d6",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/benchmark_utils.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e31bb26fe59483b91fa922d0f3166195",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributed.checkpoint.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "35e15e38239ce67c7f92a67b3f930d7f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_faq.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e5520a251a94a6240977c350389a4f74",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/accelerator.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ef340d57e5ad124df86c932957681908",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_transformations.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "43eabe60d31eb50b08313f3e1d94d530",
    "source": "pytorch_docs",
    "title": "TorchScript \u2014 PyTorch 2.9 documentation",
    "text": "\n## TorchScript#\n\nCreated On: Sep 07, 2018 | Last Updated On: Jul 16, 2025\nWarning\nTorchScript is deprecated, please usetorch.exportinstead.\n\n## Creating TorchScript Code#\n\nscript\n\nscript\nScript the function.\ntrace\n\ntrace\nTrace a function and return an executable  orScriptFunctionthat will be optimized using just-in-time compilation.\nScriptFunction\nscript_if_tracing\n\nscript_if_tracing\nCompilesfnwhen it is first called during tracing.\nfn\ntrace_module\n\ntrace_module\nTrace a module and return an executableScriptModulethat will be optimized using just-in-time compilation.\nScriptModule\nfork\n\nfork\nCreate an asynchronous task executingfuncand a reference to the value of the result of this execution.\nwait\n\nwait\nForce completion of atorch.jit.Future[T]asynchronous task, returning the result of the task.\nScriptModule\n\nScriptModule\nWrapper for C++ torch::jit::Module with methods, attributes, and parameters.\nScriptFunction\n\nScriptFunction\nFunctionally equivalent to aScriptModule, but represents a single function and does not have any attributes or Parameters.\nScriptModule\nfreeze\n\nfreeze\nFreeze ScriptModule, inline submodules, and attributes as constants.\noptimize_for_inference\n\noptimize_for_inference\nPerform a set of optimization passes to optimize a model for the purposes of inference.\nenable_onednn_fusion\n\nenable_onednn_fusion\nEnable or disables onednn JIT fusion based on the parameterenabled.\nonednn_fusion_enabled\n\nonednn_fusion_enabled\nReturn whether onednn JIT fusion is enabled.\nset_fusion_strategy\n\nset_fusion_strategy\nSet the type and number of specializations that can occur during fusion.\nstrict_fusion\n\nstrict_fusion\nGive errors if not all nodes have been fused in inference, or symbolically differentiated in training.\nsave\n\nsave\nSave an offline version of this module for use in a separate process.\nload\n\nload\nLoad aScriptModuleorScriptFunctionpreviously saved withtorch.jit.save.\nScriptModule\nScriptFunction\ntorch.jit.save\nignore\n\nignore\nThis decorator indicates to the compiler that a function or method should be ignored and left as a Python function.\nunused\n\nunused\nThis decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.\ninterface\n\ninterface\nDecorate to annotate classes or modules of different types.\nisinstance\n\nisinstance\nProvide container type refinement in TorchScript.\nAttribute\n\nAttribute\nThis method is a pass-through function that returnsvalue, mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type oftype.\nannotate\n\nannotate\nUse to give type ofthe_valuein TorchScript compiler.",
    "url": "https://pytorch.org/docs/stable/jit.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3e1494c70fc943814d3120282c24c5de",
    "source": "pytorch_docs",
    "title": "Features for large-scale deployments \u2014 PyTorch 2.9 documentation",
    "text": "\n## Features for large-scale deployments#\n\nCreated On: Jul 24, 2019 | Last Updated On: Jul 15, 2025\nFleet-wide operator profiling\nAPI usage logging\nCommon extension points\nThis note talks about several extension points and tricks that might be useful\nwhen running PyTorch within a larger system or operating multiple systems using\nPyTorch in a larger organization.\nThe note assumes that you either build PyTorch from source in your\norganization or have an ability to statically link additional code to be loaded\nwhen PyTorch is used. Therefore, many of the hooks are exposed as C++ APIs that\ncan be triggered once in a centralized place, e.g. in static initialization\ncode.\n\n## Fleet-wide operator profiling#\n\nPyTorch comes withtorch.autograd.profilercapable of measuring time\ntaken by individual operators on demand. One can use the same mechanism to do\n\u201calways ON\u201d measurements for any process running PyTorch. It might be useful for\ngathering information about PyTorch workloads running in a given process or\nacross the entire set of machines.\ntorch.autograd.profiler\nNew callbacks for any operator invocation can be added withtorch::addGlobalCallback. Hooks will be called withtorch::RecordFunctionstruct that describes invocation\ncontext (e.g.name). If enabled,RecordFunction::inputs()contains arguments\nof the function represented astorch::IValuevariant type. Note, that inputs\nlogging is relatively expensive and thus has to be enabled explicitly.\ntorch::addGlobalCallback\ntorch::RecordFunction\nRecordFunction::inputs()\ntorch::IValue\nThe operator callbacks also have access toc10::ThreadLocalDebugInfo::get()interface that returns a pointer to the struct holding the debug information.\nThis debug information can be set earlier by usingat::DebugInfoGuardobject.\nDebug information is propagated through the forward (including asyncforktasks) and backward passes and can be useful for passing some extra information\nabout execution environment (e.g. model id) from the higher layers of the\napplication down to the operator callbacks.\nc10::ThreadLocalDebugInfo::get()\nat::DebugInfoGuard\nfork\nInvoking callbacks adds some overhead, so usually it\u2019s useful to just randomly\nsample operator invocations. This can be enabled on per-callback basis with an\noptional sampling rate passed intotorch::addGlobalCallback.\ntorch::addGlobalCallback\nNote, thataddGlobalCallbackis not thread-safe and can be called only when no\nPyTorch operator is running. Usually, it\u2019s a good idea to call them once during\ninitialization.\naddGlobalCallback\nHere\u2019s an example:\n\n```python\n// Called somewhere in the program beginning\nvoid init() {\n    // Sample one in a hundred operator runs randomly\n    addGlobalCallback(\n      RecordFunctionCallback(\n        &onFunctionEnter,\n        &onFunctionExit)\n      .needsInputs(true)\n      .samplingProb(0.01)\n    );\n    // Note, to enable observers in the model calling thread,\n    // call enableRecordFunction() in the thread before running a model\n}\n\nvoid onFunctionEnter(const RecordFunction& fn) {\n    std::cerr << \"Before function \" << fn.name()\n              << \" with \" << fn.inputs().size() << \" inputs\" << std::endl;\n}\n\nvoid onFunctionExit(const RecordFunction& fn) {\n    std::cerr << \"After function \" << fn.name();\n}\n\n```\n\n\n## API usage logging#\n\nWhen running in a broader ecosystem, for example in managed job scheduler, it\u2019s\noften useful to track which binaries invoke particular PyTorch APIs. There\nexists simple instrumentation injected at several important API points that\ntriggers a given callback. Because usually PyTorch is invoked in one-off python\nscripts, the callback fires only once for a given process for each of the APIs.\nc10::SetAPIUsageHandlercan be used to register API usage instrumentation\nhandler. Passed argument is going to be an \u201capi key\u201d identifying used point, for\nexamplepython.importfor PyTorch extension import.\nc10::SetAPIUsageHandler\npython.import\n\n```python\nSetAPIUsageLogger([](const std::string& event_name) {\n    std::cerr << \"API was used: \" << event_name << std::endl;\n});\n\n```\n\nNote for developers: new API trigger points can be added in code withC10_LOG_API_USAGE_ONCE(\"my_api\")in C++ ortorch._C._log_api_usage_once(\"my.api\")in Python.\nC10_LOG_API_USAGE_ONCE(\"my_api\")\ntorch._C._log_api_usage_once(\"my.api\")\n\n## Common extension points#\n\nPyTorch APIs are generally loosely coupled and it\u2019s easy to replace a component\nwith specialized version. Common extension points include:\nCustom operators implemented in C++ - seetutorial for more details.\nCustom data reading can be often integrated directly by invoking corresponding python library. Existing functionality oftorch.utils.datacan be utilized by extendingDatasetorIterableDataset.\ntorch.utils.data\nDataset\nIterableDataset",
    "url": "https://pytorch.org/docs/stable/notes/large_scale_deployments.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "dceec10a6b9b5651d97d766b8bb1cf89",
    "source": "pytorch_docs",
    "title": "MPS backend \u2014 PyTorch 2.9 documentation",
    "text": "\n## MPS backend#\n\nCreated On: May 13, 2022 | Last Updated On: Jun 02, 2022\nmpsdevice enables high-performance\ntraining on GPU for MacOS devices with Metal programming framework.  It\nintroduces a new device to map Machine Learning computational graphs and\nprimitives on highly efficient Metal Performance Shaders Graph framework and\ntuned kernels provided by Metal Performance Shaders framework respectively.\nmps\nThe new MPS backend extends the PyTorch ecosystem and provides existing scripts\ncapabilities to setup and run operations on GPU.\nTo get started, simply move your Tensor and Module to thempsdevice:\nmps\n\n```python\n# Check that MPS is available\nif not torch.backends.mps.is_available():\n    if not torch.backends.mps.is_built():\n        print(\"MPS not available because the current PyTorch install was not \"\n              \"built with MPS enabled.\")\n    else:\n        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n              \"and/or you do not have an MPS-enabled device on this machine.\")\n\nelse:\n    mps_device = torch.device(\"mps\")\n\n    # Create a Tensor directly on the mps device\n    x = torch.ones(5, device=mps_device)\n    # Or\n    x = torch.ones(5, device=\"mps\")\n\n    # Any operation happens on the GPU\n    y = x * 2\n\n    # Move your model to mps just like any other device\n    model = YourFavoriteNet()\n    model.to(mps_device)\n\n    # Now every call runs on the GPU\n    pred = model(x)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/notes/mps.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0ce6d2817dd3283742382cea14d12a3c",
    "source": "pytorch_docs",
    "title": "Joint with descriptors \u2014 PyTorch 2.9 documentation",
    "text": "\n## Joint with descriptors#\n\nCreated On: Aug 11, 2025 | Last Updated On: Aug 11, 2025\nJoint with descriptors is an experimental API for exporting a traced joint\ngraph that supports all of torch.compile\u2019s features in full generality and,\nafter processing, can be converted back into a differentiable callable that\ncan be executed as normal.  For example, it is used to implement autoparallel,\na system that takes a model and reshards inputs and parameters to make it\na distributed SPMD program.\nThis API captures the joint graph for an nn.Module.  However, unlike\naot_export_joint_simple or aot_export_module(trace_joint=True), the\ncalling convention of the produced joint graph follows no fixed positional\nschema; for example, you cannot rely on the second argument of the traced\njoint graph to correspond to the second argument of the module you traced.\nHowever, the inputs and outputs of the traced graph are schematized\nwithdescriptors, annotated on meta[\u2018desc\u2019] on the placeholder and\nreturn FX nodes, which you can use to determine the meaning of arguments.\nThe major benefit of using this export rather than aot_export_joint_simple\nis that we have feature parity with all situations that torch.compile\nsupports (via aot_module_simplified), including handling for more\ncomplicated cases such as multiple differentiable outputs, input mutations\nthat must be handled outside of the graph, tensor subclasses, etc.\nWhat can you do with one of these joint graphs with descriptors?  The\nmotivating use case (autoparallel) involves taking the joint graph, doing\noptimizations on it, and then turning it back into a callable so it can be\ntorch.compile\u2019d at a later point in time.  This cannot be done as a\ntraditional torch.compile joint graph pass for two reasons:\nThe sharding of parameters must be decided before parameter\ninitialization / checkpoint load, far before torch.compile would\nordinarily run.\nWe need to change the meaning of parameters (e.g., we might replace\na replicated parameter with a sharded version of it, changing its\ninput size).  torch.compile is ordinarily semantics preserving, and\nnot allowed to change the meaning of inputs.\nSome descriptors can be quite exotic, so we recommend thinking carefully\nif there is a safe fallback you can apply to descriptors you don\u2019t understand.\nFor example, you should have some way to handle not finding a particular\ninput exactly as is in the final FX graph inputs.\nNote: When using this API, you must create and enter an ExitStack context\nmanager, which will be passed into this function.  This context manager\nmust remain active if you call the compile function to finish compilation.\n(TODO: We may relax this requirement by having AOTAutograd keep track of\nhow to reconstruct all the context managers at a later point in time.)\nNB: You\u2019re not obligated to do a /full/ compile in stage2; instead you can\nleave the forward/backward compilers unspecified in which case the\npartitioned FX graphs will directly run.  The overall autograd Function\ncan be allowed in graph so you can reprocess it in the context of a\n(potentially larger) compiled region later.\nNB: These APIs do NOT hit cache, as we only ever cache the final compile results,\nnot the intermediate export result.\nNB: If the passed nn.Module has parameters and buffers on it, we will\ngenerate extra implicit parameter/buffer arguments and assign ParamAOTInput\nand BufferAOTInput descriptors to them.  However, if you generate the input\nnn.Module from a mechanism like Dynamo, you will NOT get these descriptors\n(because Dynamo will already have taken care of lifting the parameters/buffers\ninto arguments!)  In that case, it would be necessary to analyze the Sources\nof the inputs to determine if inputs are parameters and their FQNs.\nJointWithDescriptors\nCompanion function for aot_export_joint_with_descriptors which compiles the joint\ngraph into a callable function that follows a standard calling convention.\nparams_flat all are arguments.\nNote: We do NOT instantiate the module; this gives you the flexibility to subclass it and\ncustomize its behavior without having to worry about FQN rebinding.\nTODO: Consider if we should allow_in_graph the result by default.\ncallable\n\n## Descriptors#\n\nDescribes where an input from an AOTAutograd produced FX graph comes from\nTrue if this input is a buffer or derived from a buffer (e.g., subclass attr)\nbool\nTrue if this input is a parameter or derived from a parameter (e.g., subclass attr)\nbool\nTrue if this input is a tangent or derived from a tangent (e.g., subclass attr)\nbool\nDescribes where an output from an AOTAutograd produced FX graph will\neventually be bundled into the final output\nTrue if this output is a grad or derived from a grad (e.g., subclass attr)\nbool\nThe world token which is threaded through side-effectful operations, for backwards\nThe world token output for side-effectful calls, returned so we cannot DCE it, backward only\nThe input is a buffer, whose FQN is target\nIn some circumstances, we want to call into a function that expects AOTInput, but\nwe don\u2019t actually care about that logic (most typically, because some code is being used\nfor both compile-time and run-time; AOTInput processing is not needed in this situation.\nPass a dummy in this situation; but it is better to just have a version of the function\nthat doesn\u2019t have this at all.\nFor cases when you don\u2019t actually care about descriptor propagation, do not use under normal\ncircumstances.\nAn output representing the computed gradient for a differentiable input, in the joint graph\nThe mutated value of an input tensor, returned so we can appropriately propagate autograd.\nAn intermediate base of multiple outputs which alias each other.  We only report ONE of\nthe outputs that contributed to this base\nThe input is a parameter, whose FQN is target\nThe offset for functionalized Philox RNG calls, specifically for backward graph.\nThe seed for functionalized Philox RNG calls, specifically for backward graph.\nThe offset for functionalized Philox RNG calls, specifically for forward graph.\nThe seed for functionalized Philox RNG calls, specifically for forward graph.\nThe final offset from the functionalized RNG calls, backward only\nThe final offset from the functionalized RNG calls, forward only\nThe input is a plain input, corresponding to a particular positional index.\nNote that AOTInput is always relative to a function with aflatcalling convention,\ne.g., as accepted byaot_module_simplified.  There are some AOTAutograd APIs that\nflatten pytrees, and we don\u2019t record PyTree key paths from the flattening (but we\ncould and should!)\nA plain tensor output at position idx of the output tuple\nSubclass inputs get unpacked into their constituent pieces before going into an FX\ngraph.  This tells you which particular attribute of the subclass this particular\ninput corresponds to (of the \u2018base\u2019 originally subclass argument.)\nThis output will be bundled into a subclass at this location\nWhich subclass this particular outer size SymInt input (at dim idx) came from.\nThis output size will be bundled into a subclass at this location\nWhich subclass this particular outer stride SymInt input (at dim idx) came from.\nThis output stride will be bundled into a subclass at this location\nThis is similar to ViewBaseAOTInput, but this happens when none of the views were differentiable, so\nwe weren\u2019t able to get our hands on the true original view and constructed a synthetic one instead\nfor the sake of autograd.\nWhen multiple differentiable inputs are views of the same input, AOTAutograd will replace all of these\nviews with a single input representing the base.  If this is undesirable, you can clone the views\nexample inputs before passing them into AOTAutograd.\nTODO: In principle we could report ALL of the inputs who this is a base of.\n\n## FX utilities#\n\nThis module contains utility functions for working with joint FX graphs with descriptors\nthat are produced by AOTAutograd.  They will NOT work on generic FX graphs.  See alsotorch._functorch.aot_autograd.aot_export_joint_with_descriptors().  We also\nrecommend reading :mod:torch._functorch._aot_autograd.descriptors`.\ntorch._functorch.aot_autograd.aot_export_joint_with_descriptors()\nGiven a joint graph with descriptors (meta[\u2018desc\u2019] on placeholders and\noutput), returns the node for every input and its corresponding grad\noutput node if it exists.  These tuples are in a dict that is indexed by\nthe AOTInput descriptor that describes the input.\nNB:allforward tensor inputs are returned, including non-differentiable\ninputs (which simply have a None grad), so it is safe to use this function\nto perform operations on all inputs.  (Non-tensor inputs like symbolic\nintegers, tokens or RNG state are NOT traversed by this function.)\ng(Graph) \u2013 The FX joint graph with descriptors\nA dictionary mapping each DifferentiableAOTInput descriptor to a tuple\ncontaining:\n- The input node itself\n- The grad (output) node if it exists, None otherwise\nRuntimeError\u2013 If the joint graph has subclass tensor inputs/outputs; this\nis not supported by API as there is not necessarily a 1-1 correspondence\u2013\nbetween inputs and grads when subclasses are involved.\u2013\ndict[torch._functorch._aot_autograd.descriptors.DifferentiableAOTInput,tuple[torch.fx.node.Node,Optional[torch.fx.node.Node]]]\nGet all output nodes and their corresponding tangent nodes from a joint graph.\nSimilar to get_all_input_and_grad_nodes, but returns output nodes paired with\ntheir tangent nodes (if they exist). This function traverses the graph to find\nall differentiable outputs and matches them with their corresponding tangent\ninputs used in forward-mode autodiff.\nNB:allforward tensor output sare turned, including non-differentiable outputs,\nso you can use this function to perform operations on all outputs.\ng(Graph) \u2013 The FX joint graph with descriptors\nA dictionary mapping each DifferentiableAOTOutput descriptor to a tuple\ncontaining:\n- The output node itself\n- The tangent (input) node if it exists, None otherwise\nRuntimeError\u2013 If the joint graph has subclass tensor inputs/outputs; this\nis not supported by API as there is not necessarily a 1-1 correspondence\u2013\nbetween outputs and tangents when subclasses are involved.\u2013\ndict[torch._functorch._aot_autograd.descriptors.DifferentiableAOTOutput,tuple[torch.fx.node.Node,Optional[torch.fx.node.Node]]]\nGet all buffer nodes from a graph as a list.\nYou can rely on this providing the correct order of buffers you need\nto feed into the joint graph (after parameters).\ngraph(Graph) \u2013 The FX joint graph with descriptors\nA list of FX nodes representing all buffers in the graph.\nRuntimeError\u2013 If subclass tensors are encountered (not yet supported), as\nit is not clear if you wanted each individual constituent pieceofthe\u2013\nsubclasses, orhave them grouped up in some way.\u2013\nlist[torch.fx.node.Node]\nGet buffer nodes mapped by their fully qualified names.\nThis function traverses the graph to find all buffer input nodes and\nreturns them in a dictionary where keys are the buffer names (FQNs)\nand values are the corresponding FX nodes.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nA dictionary mapping buffer names (str) to their corresponding FX nodes.\nRuntimeError\u2013 If subclass tensors are encountered (not yet supported), as\nwith subclasses a FQN does not necessarily map to a single plain tensor.\u2013\ndict[str,torch.fx.node.Node]\nGet parameter nodes mapped by their fully qualified names.\nThis function traverses the graph to find all parameter input nodes and\nreturns them in a dictionary where keys are the parameter names (FQNs)\nand values are the corresponding FX nodes.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nA dictionary mapping parameter names (str) to their corresponding FX nodes.\nRuntimeError\u2013 If subclass tensors are encountered (not yet supported), as\nwith subclasses a FQN does not necessarily map to a single plain tensor.\u2013\ndict[str,torch.fx.node.Node]\nGet parameter nodes and their corresponding gradient nodes from a joint graph.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nThe parameter input nodeThe gradient (output) node if it exists, None otherwise\nThe parameter input node\nThe gradient (output) node if it exists, None otherwise\nA dictionary mapping each ParamAOTInput descriptor to a tuple containing\nGet all parameter nodes from a graph as a list.\nYou can rely on this providing the correct order of parameters you need\nto feed into the joint graph (at the very beginning of the argument list,\nbefore buffers).\ngraph(Graph) \u2013 The FX joint graph with descriptors\nA list of FX nodes representing all parameters in the graph.\nRuntimeError\u2013 If subclass tensors are encountered (not yet supported), as\nit is not clear if you wanted each individual constituent pieceofthe\u2013\nsubclasses, orhave them grouped up in some way.\u2013\nlist[torch.fx.node.Node]\nGet plain input nodes and their corresponding gradient nodes from a joint graph.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nThe plain input nodeThe gradient (output) node if it exists, None otherwise\nThe plain input node\nThe gradient (output) node if it exists, None otherwise\nA dictionary mapping each PlainAOTInput descriptor to a tuple containing\nGet plain output nodes and their corresponding tangent nodes from a joint graph.\ngraph(Graph) \u2013 The FX joint graph with descriptors\nThe plain output nodeThe tangent (input) node if it exists, None otherwise\nThe plain output node\nThe tangent (input) node if it exists, None otherwise\nA dictionary mapping each PlainAOTOutput descriptor to a tuple containing",
    "url": "https://pytorch.org/docs/stable/export/joint_with_descriptors.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "815894710cb8be2fb3e45b4dbe87c8a3",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/add_scalars.png",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "61ba5c82acaaba1b7444bdb886c0c868",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributed.optim.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "190a3e92d6620815b029d8de61e85d38",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/func.api.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "386f3ebcf9a4c0509b941c9085c60f1e",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/library.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "668f55063b86f246575fc69e980d0556",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/name_inference.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a5a9aec2d4e562649449329f57710648",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/ddp.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b97472067199cbbe4d84d1845911f60c",
    "source": "pytorch_docs",
    "title": "PyTorch 2.0 NNModule Support \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch 2.0 NNModule Support#\n\nCreated On: Apr 06, 2023 | Last Updated On: Jun 10, 2025\nAuthor:Will Constable\ntorch.compilehas special handling for torch.nn.Module objects, tracing them differently than it traces\narbitrary python classes, with the intent of producing faster code by making assumptions about the structure.\ntorch.compile\nThis doc describes some of the tradeoffs or edge cases that come up due to this specialization.\n\n## NNModule Hooks Support#\n\nPreviously,torch.compilehad no support for hooks on nn.Modules, and if hooks were registered\nthey would simply be ignored in the compiled program. Indeed many users do not\nuse nn.Module hooks at all, or only use them for debug workflows, but there are valid use cases\nfor composing nn.Module hooks withtorch.compile.\ntorch.compile\ntorch.compile\nHooks that are orchestrated via nn.Module.callimplementation include_forward_pre_hooks,forward_hooks,_backward_pre_hooks, and_backward_hooks, and will be referred to as \u2018call hooks\u2019.\nThese hooks are partially supported bytorch.compilewith limitations described below.\n_forward_pre_hooks\nforward_hooks\n_backward_pre_hooks\n_backward_hooks\ntorch.compile\nAnother category of hooks includes_state_dict_hooksand itspreandload_variants, and are still\nunsupported bytorch.compile.\n_state_dict_hooks\npre\nload_\ntorch.compile\n\n## nn.Module.__call__Hooks Usage and limitations#\n\nnn.Module.__call__\nBy default,torch.compilewill trace the contents ofnn.Module.__call__which means it will encounter\nand run forward/pre-forward hooks.  If you install hooks before callingtorch.compileand then do not remove\nor alter the hooks later, your use case should be supported by default.\ntorch.compile\nnn.Module.__call__\ntorch.compile\nBackward/Pre-backward hooks are generally also supported, with similar caveats: currently graph-breaks in dynamo\noccur when accessing backward_hooks dicts, which is probably avoiable with some work.  Graph-breaks also impact the\ntiming of firing backward hooks, since graph-segments are run as autograd-functions which produce all their grads at\nthe same time.  Assuming it were possible for dynamo to not graph-break on the presence of backward-hooks, we would\nstill expect the backward hooks for a series of modules to all fire together after the whole compiled graph\u2019s backward\nran.\nhooks on \u2018allowed modules\u2019torch.compiletreats common modules such as torch.conv, as well as modules that are difficult to trace, specially\nby allowing them to be called opaquely in the dynamo graph instead of traced into by dynamo.  For such modules, hooks\ncurrently trigger a graph-break so that the affected modules run outside of dynamo.  Depending on the model, this could\nintroduce a significant performance regression, and additional work is required to improve this support.\ntorch.compile\nskip_nnmodule_hook_guardsBy default,torch._dynamo.config.skip_nnmodule_hook_guardsis set to True, meaning no guards will be installed\non each nn.Module hook dictionary, improving runtime by reducing guard execution time, at the cost of not noticing\nif any hook dict is changed after compilation.\ntorch._dynamo.config.skip_nnmodule_hook_guards\nIf you want to be able to remove or modify hooks after compilation and havetorch.compilereact appropriately\n(by recompiling), then you need to setskip_nnmodule_hook_guards=Falseand expect a runtime penalty for the added\nguards.\ntorch.compile\nskip_nnmodule_hook_guards=False\nTODO: confirm if backward/pre_backward hooks are working or not and document accordingly\n\n## state_dict Hooks#\n\nState dict hooks have not yet been supported intorch.compile.\ntorch.compile\nTODO: warn_once if graph-breaking on hooks.  warn_once to point to this doc if hooks are present.",
    "url": "https://pytorch.org/docs/stable/torch.compiler_nn_module.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f1dbac9a70cc3c819bd9d320932d426e",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/rpc/rref.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "cf1ddb4e9a3bdaa3469a9b96bc142997",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/get_start_xpu.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9c4e18ea94e00f4a4651131db9890ebc",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/rendezvous.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "31f87534fe30cd97a12d7d01a131c947",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/complex_numbers.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6dffd4d9a1c22fa6b3653c1d800c1b06",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/extending.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ceb610a450ade500175860592c6dbe05",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/mkldnn.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5e22ea19ae1779ae09cf915ebf256235",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/python.assert.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1cdd4821753e6b3186fe8edecfc4bd3f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/module_tracker.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "52535bcab9597c9cd04cd0c724f553e4",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/torch.map.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d5caf6725521414e03e15780a81aff93",
    "source": "pytorch_docs",
    "title": "MKLDNN backend \u2014 PyTorch 2.9 documentation",
    "text": "\n## MKLDNN backend#\n\nCreated On: May 10, 2025 | Last Updated On: Jul 17, 2025\nMKLDNN is an open-source cross-platform performance library of basic building blocks\nfor deep learning applications.\n\n```python\n# The flag below controls whether enable MKLDNN backend in Pytorch.\ntorch.backends.mkldnn.enabled = True\n\n```\n\nUsers can disable MKLDNN backend by:\n\n```python\ntorch.backends.mkldnn.enabled = False\n\n```\n\n\n## Bfloat16 (BF16) on MKLDNN backend#\n\nStarting in PyTorch 2.9, there is a set of APIs to control the internal computation precision\nforfloat32operators.\n\n```python\n# The flag below controls the internal computation precision for mkldnn matmul. Default ieee is float32.\ntorch.backends.mkldnn.matmul.fp32_precision = \"ieee\"\n\n# The flag below controls the internal computation precision for mkldnn conv. Default ieee is float32.\ntorch.backends.mkldnn.conv.fp32_precision = \"ieee\"\n\n# The flag below controls the internal computation precision for mkldnn rnn. Default ieee is float32.\ntorch.backends.mkldnn.rnn.fp32_precision = \"ieee\"\n\n```\n\nNote that besides matmuls and convolutions themselves, functions and nn modules that internally uses\nmatmuls or convolutions are also affected. These includetorch.nn.Linear,torch.nn._ConvNd,torch.cdist(),torch.tensordot(),torch.nn.functional.affine_grid()andtorch.nn.functional.grid_sample(),torch.nn.AdaptiveLogSoftmaxWithLoss,torch.nn.GRUandtorch.nn.LSTM.\ntorch.nn.Linear\ntorch.nn._ConvNd\ntorch.cdist()\ntorch.tensordot()\ntorch.nn.functional.affine_grid()\ntorch.nn.functional.grid_sample()\ntorch.nn.AdaptiveLogSoftmaxWithLoss\ntorch.nn.GRU\ntorch.nn.LSTM\nTo get an idea of the precision and speed, see the example code and benchmark data (on SPR) below:\n\n```python\ntorch.manual_seed(0)\na_full = torch.randn(10240, 10240, dtype=torch.double)\nb_full = torch.randn(10240, 10240, dtype=torch.double)\nab_full = a_full @ b_full\nmean = ab_full.abs().mean()  # 80.7451\n\na = a_full.float()\nb = b_full.float()\n\n# Do matmul at BF16 mode.\ntorch.backends.mkldnn.matmul.fp32_precision = 'bf16'\nab_bf16 = a @ b  # expected speedup with BF16 dot-product acceleration\nerror = (ab_bf16 - ab_full).abs().max()  # 1.3704\nrelative_error = error / mean  # 0.0170\nprint(error, relative_error)\n\n# Do matmul at TF32 mode.\ntorch.backends.mkldnn.matmul.fp32_precision = 'tf32'\nab_tf32 = a @ b  # expected speedup with TF32 dot-product acceleration\nerror = (ab_tf32 - ab_full).abs().max()  # 0.0004\nrelative_error = error / mean  # 0.00000552\nprint(error, relative_error)\n\n# Do matmul FP32 mode.\ntorch.backends.mkldnn.matmul.fp32_precision = 'ieee'\nab_fp32 = a @ b\nerror = (ab_fp32 - ab_full).abs().max()  # 0.0003\nrelative_error = error / mean  # 0.00000317\nprint(error, relative_error)\n\n```\n\nFrom the above example, we can see that with BF16, the speed is ~7x faster on SPR, and that\nrelative error compared to double precision is approximately 2 orders of magnitude larger.\nIf full FP32 precision is needed, users can disable BF16 by:\n\n```python\ntorch.backends.mkldnn.matmul.fp32_precision = 'ieee'\ntorch.backends.mkldnn.conv.fp32_precision = 'ieee'\ntorch.backends.mkldnn.rnn.fp32_precision = 'ieee'\n\n```\n\nTo toggle the BF16 flags off in C++, you can do\n\n```python\nat::globalContext().setFloat32Precision(\"ieee\", \"mkldnn\", \"matmul\");\nat::globalContext().setFloat32Precision(\"ieee\", \"mkldnn\", \"conv\");\nat::globalContext().setFloat32Precision(\"ieee\", \"mkldnn\", \"rnn\");\n\n```\n\nWe can override a generic setting for a specific operator or backend if the fp32_precision is set toieee.\n\n```python\ntorch.backends.fp32_precision = \"bf16\"\ntorch.backends.mkldnn.fp32_precision = \"ieee\"\ntorch.backends.mkldnn.matmul.fp32_precision = \"ieee\"\n\n```\n\nFor such case, bothtorch.backends.mkldnn.fp32_precisionandtorch.backends.mkldnn.matmul.fp32_precisionis overridden to bf16.",
    "url": "https://pytorch.org/docs/stable/notes/mkldnn.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f5788f266e7378201824743b6b1b7283",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/tensor_comparison.jpg",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "fd5906b7f30485a0de42c717a11f7b68",
    "source": "pytorch_docs",
    "title": "python.object-model \u2014 PyTorch 2.9 documentation",
    "text": "\n## python.object-model#\n\n\n## model_attr_mutation#\n\nNote\nTags:python.object-model\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nfrom torch._export.db.case import SupportLevel\n\n\nclass ModelAttrMutation(torch.nn.Module):\n    \"\"\"\n    Attribute mutation is not supported.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.attr_list = [torch.randn(3, 2), torch.randn(3, 2)]\n\n    def recreate_list(self):\n        return [torch.zeros(3, 2), torch.zeros(3, 2)]\n\n    def forward(self, x):\n        self.attr_list = self.recreate_list()\n        return x.sum() + self.attr_list[0].sum()\n\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.object-model\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\nmodel = ModelAttrMutation()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nAssertionError: Mutating module attribute attr_list during export.\n\n```\n\n\n## optional_input#\n\nNote\nTags:python.object-model\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nfrom torch._export.db.case import SupportLevel\n\n\nclass OptionalInput(torch.nn.Module):\n    \"\"\"\n    Tracing through optional input is not supported yet\n    \"\"\"\n\n    def forward(self, x, y=torch.randn(2, 3)):\n        if y is not None:\n            return x + y\n        return x\n\n\nexample_args = (torch.randn(2, 3),)\ntags = {\"python.object-model\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\nmodel = OptionalInput()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nUnsupported: Tracing through optional input is not supported yet\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/python.object-model.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "034ee2813b2c2c76b42e084892430fc1",
    "source": "pytorch_docs",
    "title": "torch.compiler.config \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.compiler.config#\n\nCreated On: Nov 01, 2024 | Last Updated On: Jun 11, 2025\nThis is the top-level configuration module for the compiler, containing\ncross-cutting configuration options that affect all parts of the compiler\nstack.\nYou may also be interested in the per-component configuration modules, which\ncontain configuration options that affect only a specific part of the compiler:\ntorch._dynamo.config\ntorch._dynamo.config\ntorch._inductor.config\ntorch._inductor.config\ntorch._functorch.config\ntorch._functorch.config\ntorch.fx.experimental.config\ntorch.fx.experimental.config\nSemantically, this should be an identifier that uniquely identifies, e.g., a\ntraining job.  You might have multiple attempts of the same job, e.g., if it was\npreempted or needed to be restarted, but each attempt should be running\nsubstantially the same workload with the same distributed topology.  You can\nset this by environment variable withTORCH_COMPILE_JOB_ID.\nTORCH_COMPILE_JOB_ID\nOperationally, this controls the effect of profile-guided optimization related\npersistent state.  PGO state can affect how we perform compilation across\nmultiple invocations of PyTorch, e.g., the first time you run your program we\nmay compile twice as we discover what inputs are dynamic, and then PGO will\nsave this state so subsequent invocations only need to compile once, because\nthey remember it is dynamic.  This profile information, however, is sensitive\nto what workload you are running, so we require you to tell us that two jobs\narerelated(i.e., are the same workload) before we are willing to reuse\nthis information.  Notably, PGO does nothing (even if explicitly enabled)\nunless a validjob_idis available.  In some situations, PyTorch can\nconfigured to automatically compute ajob_idbased on the environment it\nis running in.\njob_id\njob_id\nProfiles are always collected on a per rank basis, so different ranks may have\ndifferent profiles.  If you know your workload is truly SPMD, you can run withtorch._dynamo.config.enable_compiler_collectivesto ensure nodes get\nconsistent profiles across all ranks.\ntorch._dynamo.config.enable_compiler_collectives",
    "url": "https://pytorch.org/docs/stable/torch.compiler.config.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6b5d17c605c6243cb74c655dbf9cb8c3",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/tensor_attributes.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d5388d6932dac6b53d5bec10ff086e55",
    "source": "pytorch_docs",
    "title": "ExportDB \u2014 PyTorch 2.9 documentation",
    "text": "\n## ExportDB#\n\nExportDB is a centralized dataset of supported and unsupported export cases.\nIt is targeted towards users who want to understand specifically what types of\ncode are supported, the subtleties of export, and how to modify their existing\ncode to be compatible with export. Note that this is not an exhaustive set of\neverything that is supported by exportdb, but it covers the\nmost common and confusing use cases that users will run into.\nIf you have a feature that you think needs a stronger guarantee from us to\nsupport in export please create an issue in the pytorch/pytorch repo with a module:export tag.\nTags\n\n## Supported#\n\n\n## assume_constant_result#\n\nNote\nTags:torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nimport torch._dynamo as torchdynamo\n\n\nclass AssumeConstantResult(torch.nn.Module):\n    \"\"\"\n    Applying `assume_constant_result` decorator to burn make non-tracable code as constant.\n    \"\"\"\n\n    @torchdynamo.assume_constant_result\n    def get_item(self, y):\n        return y.int().item()\n\n    def forward(self, x, y):\n        return x[: self.get_item(y)]\n\nexample_args = (torch.randn(3, 2), torch.tensor(4))\ntags = {\"torch.escape-hatch\"}\nmodel = AssumeConstantResult()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"):\n                 slice_1: \"f32[3, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 4);  x = None\n            return (slice_1,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    slice_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## autograd_function#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass MyAutogradFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        return x.clone()\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output + 1\n\nclass AutogradFunction(torch.nn.Module):\n    \"\"\"\n    TorchDynamo does not keep track of backward() on autograd functions. We recommend to\n    use `allow_in_graph` to mitigate this problem.\n    \"\"\"\n\n    def forward(self, x):\n        return MyAutogradFunction.apply(x)\n\nexample_args = (torch.randn(3, 2),)\nmodel = AutogradFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 clone: \"f32[3, 2]\" = torch.ops.aten.clone.default(x);  x = None\n            return (clone,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    clone: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## class_method#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass ClassMethod(torch.nn.Module):\n    \"\"\"\n    Class methods are inlined during tracing.\n    \"\"\"\n\n    @classmethod\n    def method(cls, x):\n        return x + 1\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.linear = torch.nn.Linear(4, 2)\n\n    def forward(self, x):\n        x = self.linear(x)\n        return self.method(x) * self.__class__.method(x) * type(self).method(x)\n\nexample_args = (torch.randn(3, 4),)\nmodel = ClassMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, p_linear_weight: \"f32[2, 4]\", p_linear_bias: \"f32[2]\", x: \"f32[3, 4]\"):\n                 linear: \"f32[3, 2]\" = torch.ops.aten.linear.default(x, p_linear_weight, p_linear_bias);  x = p_linear_weight = p_linear_bias = None\n\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(linear, 1)\n            add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(linear, 1)\n\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(add, add_1);  add = add_1 = None\n\n                 add_2: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(linear, 1);  linear = None\n\n                 mul_1: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(mul, add_2);  mul = add_2 = None\n            return (mul_1,)\n\nGraph signature:\n    # inputs\n    p_linear_weight: PARAMETER target='linear.weight'\n    p_linear_bias: PARAMETER target='linear.bias'\n    x: USER_INPUT\n\n    # outputs\n    mul_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_class_method#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass MySubModule(torch.nn.Module):\n    def foo(self, x):\n        return x.cos()\n\n    def forward(self, x):\n        return self.foo(x)\n\nclass CondBranchClassMethod(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n\n    This example demonstrates using class method in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.subm = MySubModule()\n\n    def bar(self, x):\n        return x.sin()\n\n    def forward(self, x):\n        return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchClassMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 sin: \"f32[3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nested_function#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNestedFunction(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n    This example demonstrates using nested function in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        def true_fn(x):\n            def inner_true_fn(y):\n                return x + y\n\n            return inner_true_fn(x)\n\n        def false_fn(x):\n            def inner_false_fn(y):\n                return x - y\n\n            return inner_false_fn(x)\n\n        return cond(x.shape[0] < 10, true_fn, false_fn, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNestedFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 add: \"f32[3]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nonlocal_variables#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNonlocalVariables(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n    - both branches must take the same args, which must also match the branch args passed to cond.\n    - both branches must return a single tensor\n    - returned tensor must have the same tensor metadata, e.g. shape and dtype\n    - branch function can be free function, nested function, lambda, class methods\n    - branch function can not have closure variables\n    - no inplace mutations on inputs or global variables\n\n    This example demonstrates how to rewrite code to avoid capturing closure variables in branch functions.\n\n    The code below will not work because capturing closure variables is not supported.\n    ```\n    my_tensor_var = x + 100\n    my_primitive_var = 3.14\n\n    def true_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y + my_tensor_var + my_primitive_var\n\n    def false_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y - my_tensor_var - my_primitive_var\n\n    return cond(x.shape[0] > 5, true_fn, false_fn, [x])\n    ```\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        my_tensor_var = x + 100\n        my_primitive_var = 3.14\n\n        def true_fn(x, y, z):\n            return x + y + z\n\n        def false_fn(x, y, z):\n            return x - y - z\n\n        return cond(\n            x.shape[0] > 5,\n            true_fn,\n            false_fn,\n            [x, my_tensor_var, torch.tensor(my_primitive_var)],\n        )\n\nexample_args = (torch.randn(6),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNonlocalVariables()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, c_lifted_tensor_0: \"f32[]\", x: \"f32[6]\"):\n                 add: \"f32[6]\" = torch.ops.aten.add.Tensor(x, 100)\n\n                 lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(c_lifted_tensor_0);  c_lifted_tensor_0 = None\n            detach_: \"f32[]\" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None\n\n                 add_1: \"f32[6]\" = torch.ops.aten.add.Tensor(x, add);  x = add = None\n            add_2: \"f32[6]\" = torch.ops.aten.add.Tensor(add_1, detach_);  add_1 = detach_ = None\n            return (add_2,)\n\nGraph signature:\n    # inputs\n    c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0'\n    x: USER_INPUT\n\n    # outputs\n    add_2: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_closed_over_variable#\n\nNote\nTags:python.closure,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondClosedOverVariable(torch.nn.Module):\n    \"\"\"\n    torch.cond() supports branches closed over arbitrary variables.\n    \"\"\"\n\n    def forward(self, pred, x):\n        def true_fn(val):\n            return x * 2\n\n        def false_fn(val):\n            return x - 2\n\n        return cond(pred, true_fn, false_fn, [x + 1])\n\nexample_args = (torch.tensor(True), torch.randn(3, 2))\ntags = {\"torch.cond\", \"python.closure\"}\nmodel = CondClosedOverVariable()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pred: \"b8[]\", x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1);  add = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, (x,));  pred = true_graph_0 = false_graph_0 = x = None\n            getitem: \"f32[3, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, 2);  x = None\n                return (mul,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(x, 2);  x = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    pred: USER_INPUT\n    x: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_operands#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.export import Dim\n\nx = torch.randn(3, 2)\ny = torch.randn(2)\ndim0_x = Dim(\"dim0_x\")\n\nclass CondOperands(torch.nn.Module):\n    \"\"\"\n    The operands passed to cond() must be:\n    - a list of tensors\n    - match arguments of `true_fn` and `false_fn`\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x, y):\n        def true_fn(x, y):\n            return x + y\n\n        def false_fn(x, y):\n            return x - y\n\n        return torch.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n\nexample_args = (x, y)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nextra_inputs = (torch.randn(2, 2), torch.randn(2))\ndynamic_shapes = {\"x\": {0: dim0_x}, \"y\": None}\nmodel = CondOperands()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n             #\n            sym_size_int_1: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0)\n\n                 gt: \"Sym(s77 > 2)\" = sym_size_int_1 > 2;  sym_size_int_1 = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x, y));  gt = true_graph_0 = false_graph_0 = x = y = None\n            getitem: \"f32[s77, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         add: \"f32[s77, 2]\" = torch.ops.aten.add.Tensor(x, y);  x = y = None\n                return (add,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         sub: \"f32[s77, 2]\" = torch.ops.aten.sub.Tensor(x, y);  x = y = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {s77: VR[0, int_oo]}\n\n```\n\n\n## cond_predicate#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondPredicate(torch.nn.Module):\n    \"\"\"\n    The conditional statement (aka predicate) passed to cond() must be one of the following:\n      - torch.Tensor with a single element\n      - boolean expression\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        pred = x.dim() > 2 and x.shape[2] > 10\n\n        return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x])\n\nexample_args = (torch.randn(6, 4, 3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondPredicate()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[6, 4, 3]\"):\n                 sin: \"f32[6, 4, 3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## constrain_as_size_example#\n\nNote\nTags:torch.dynamic-value,torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass ConstrainAsSizeExample(torch.nn.Module):\n    \"\"\"\n    If the value is not known at tracing time, you can provide hint so that we\n    can trace further. Please look at torch._check and torch._check_is_size APIs.\n    torch._check_is_size is used for values that NEED to be used for constructing\n    tensor.\n    \"\"\"\n\n    def forward(self, x):\n        a = x.item()\n        torch._check_is_size(a)\n        torch._check(a <= 5)\n        return torch.zeros((a, 5))\n\n\nexample_args = (torch.tensor(4),)\ntags = {\n    \"torch.dynamic-value\",\n    \"torch.escape-hatch\",\n}\nmodel = ConstrainAsSizeExample()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"i64[]\"):\n                 item: \"Sym(u0)\" = torch.ops.aten.item.default(x);  x = None\n\n             #\n            sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item);  sym_constrain_range_for_size_default = None\n\n                 ge_1: \"Sym(u0 >= 0)\" = item >= 0\n            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\");  ge_1 = _assert_scalar_default = None\n            le_1: \"Sym(u0 <= 5)\" = item <= 5\n            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\");  le_1 = _assert_scalar_default_1 = None\n\n                 zeros: \"f32[u0, 5]\" = torch.ops.aten.zeros.default([item, 5], device = device(type='cpu'), pin_memory = False);  item = None\n            return (zeros,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    zeros: USER_OUTPUT\n\nRange constraints: {u0: VR[0, 5], u1: VR[0, 5]}\n\n```\n\n\n## constrain_as_value_example#\n\nNote\nTags:torch.dynamic-value,torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass ConstrainAsValueExample(torch.nn.Module):\n    \"\"\"\n    If the value is not known at tracing time, you can provide hint so that we\n    can trace further. Please look at torch._check and torch._check_is_size APIs.\n    torch._check is used for values that don't need to be used for constructing\n    tensor.\n    \"\"\"\n\n    def forward(self, x, y):\n        a = x.item()\n        torch._check(a >= 0)\n        torch._check(a <= 5)\n\n        if a < 6:\n            return y.sin()\n        return y.cos()\n\n\nexample_args = (torch.tensor(4), torch.randn(5, 5))\ntags = {\n    \"torch.dynamic-value\",\n    \"torch.escape-hatch\",\n}\nmodel = ConstrainAsValueExample()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"i64[]\", y: \"f32[5, 5]\"):\n                 item: \"Sym(u0)\" = torch.ops.aten.item.default(x);  x = None\n            ge_1: \"Sym(u0 >= 0)\" = item >= 0\n            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\");  ge_1 = _assert_scalar_default = None\n            le_1: \"Sym(u0 <= 5)\" = item <= 5;  item = None\n            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\");  le_1 = _assert_scalar_default_1 = None\n\n                 sin: \"f32[5, 5]\" = torch.ops.aten.sin.default(y);  y = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {u0: VR[0, 5], u1: VR[0, 5]}\n\n```\n\n\n## decorator#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport functools\n\nimport torch\n\ndef test_decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        return func(*args, **kwargs) + 1\n\n    return wrapper\n\nclass Decorator(torch.nn.Module):\n    \"\"\"\n    Decorators calls are inlined into the exported function during tracing.\n    \"\"\"\n\n    @test_decorator\n    def forward(self, x, y):\n        return x + y\n\nexample_args = (torch.randn(3, 2), torch.randn(3, 2))\nmodel = Decorator()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", y: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, y);  x = y = None\n\n                 add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(add, 1);  add = None\n            return (add_1,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    add_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dictionary#\n\nNote\nTags:python.data-structure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass Dictionary(torch.nn.Module):\n    \"\"\"\n    Dictionary structures are inlined and flattened along tracing.\n    \"\"\"\n\n    def forward(self, x, y):\n        elements = {}\n        elements[\"x2\"] = x * x\n        y = y * elements[\"x2\"]\n        return {\"y\": y}\n\nexample_args = (torch.randn(3, 2), torch.tensor(4))\ntags = {\"python.data-structure\"}\nmodel = Dictionary()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"):\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, x);  x = None\n\n                 mul_1: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(y, mul);  y = mul = None\n            return (mul_1,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    mul_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_assert#\n\nNote\nTags:python.assert\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeAssert(torch.nn.Module):\n    \"\"\"\n    A basic usage of python assertion.\n    \"\"\"\n\n    def forward(self, x):\n        # assertion with error message\n        assert x.shape[0] > 2, f\"{x.shape[0]} is greater than 2\"\n        # assertion without error message\n        assert x.shape[0] > 1\n        return x\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.assert\"}\nmodel = DynamicShapeAssert()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n            return (x,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    x: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_constructor#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeConstructor(torch.nn.Module):\n    \"\"\"\n    Tensor constructors should be captured with dynamic shape inputs rather\n    than being baked in with static shape.\n    \"\"\"\n\n    def forward(self, x):\n        return torch.zeros(x.shape[0] * 2)\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\"}\nmodel = DynamicShapeConstructor()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 zeros: \"f32[6]\" = torch.ops.aten.zeros.default([6], device = device(type='cpu'), pin_memory = False)\n            return (zeros,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    zeros: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_if_guard#\n\nNote\nTags:torch.dynamic-shape,python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeIfGuard(torch.nn.Module):\n    \"\"\"\n    `if` statement with backed dynamic shape predicate will be specialized into\n    one particular branch and generate a guard. However, export will fail if the\n    the dimension is marked as dynamic shape from higher level API.\n    \"\"\"\n\n    def forward(self, x):\n        if x.shape[0] == 3:\n            return x.cos()\n\n        return x.sin()\n\nexample_args = (torch.randn(3, 2, 2),)\ntags = {\"torch.dynamic-shape\", \"python.control-flow\"}\nmodel = DynamicShapeIfGuard()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2, 2]\"):\n                 cos: \"f32[3, 2, 2]\" = torch.ops.aten.cos.default(x);  x = None\n            return (cos,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    cos: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_map#\n\nNote\nTags:torch.dynamic-shape,torch.map\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import map\n\nclass DynamicShapeMap(torch.nn.Module):\n    \"\"\"\n    functorch map() maps a function over the first tensor dimension.\n    \"\"\"\n\n    def forward(self, xs, y):\n        def body(x, y):\n            return x + y\n\n        return map(body, xs, y)\n\nexample_args = (torch.randn(3, 2), torch.randn(2))\ntags = {\"torch.dynamic-shape\", \"torch.map\"}\nmodel = DynamicShapeMap()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, xs: \"f32[3, 2]\", y: \"f32[2]\"):\n                 body_graph_0 = self.body_graph_0\n            map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y]);  body_graph_0 = xs = y = None\n            getitem: \"f32[3, 2]\" = map_impl[0];  map_impl = None\n            return (getitem,)\n\n        class body_graph_0(torch.nn.Module):\n            def forward(self, xs: \"f32[2]\", y: \"f32[2]\"):\n                         add: \"f32[2]\" = torch.ops.aten.add.Tensor(xs, y);  xs = y = None\n                return (add,)\n\nGraph signature:\n    # inputs\n    xs: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_slicing#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeSlicing(torch.nn.Module):\n    \"\"\"\n    Slices with dynamic shape arguments should be captured into the graph\n    rather than being baked in.\n    \"\"\"\n\n    def forward(self, x):\n        return x[: x.shape[0] - 2, x.shape[1] - 1 :: 2]\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\"}\nmodel = DynamicShapeSlicing()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 slice_1: \"f32[1, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 1);  x = None\n            slice_2: \"f32[1, 1]\" = torch.ops.aten.slice.Tensor(slice_1, 1, 1, 9223372036854775807, 2);  slice_1 = None\n            return (slice_2,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    slice_2: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_view#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeView(torch.nn.Module):\n    \"\"\"\n    Dynamic shapes should be propagated to view arguments instead of being\n    baked into the exported graph.\n    \"\"\"\n\n    def forward(self, x):\n        new_x_shape = x.size()[:-1] + (2, 5)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1)\n\nexample_args = (torch.randn(10, 10),)\ntags = {\"torch.dynamic-shape\"}\nmodel = DynamicShapeView()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[10, 10]\"):\n                 view: \"f32[10, 2, 5]\" = torch.ops.aten.view.default(x, [10, 2, 5]);  x = None\n\n                 permute: \"f32[10, 5, 2]\" = torch.ops.aten.permute.default(view, [0, 2, 1]);  view = None\n            return (permute,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    permute: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## fn_with_kwargs#\n\nNote\nTags:python.data-structure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass FnWithKwargs(torch.nn.Module):\n    \"\"\"\n    Keyword arguments are not supported at the moment.\n    \"\"\"\n\n    def forward(self, pos0, tuple0, *myargs, mykw0, **mykwargs):\n        out = pos0\n        for arg in tuple0:\n            out = out * arg\n        for arg in myargs:\n            out = out * arg\n        out = out * mykw0\n        out = out * mykwargs[\"input0\"] * mykwargs[\"input1\"]\n        return out\n\nexample_args = (\n    torch.randn(4),\n    (torch.randn(4), torch.randn(4)),\n    *[torch.randn(4), torch.randn(4)]\n)\nexample_kwargs = {\n    \"mykw0\": torch.randn(4),\n    \"input0\": torch.randn(4),\n    \"input1\": torch.randn(4),\n}\ntags = {\"python.data-structure\"}\nmodel = FnWithKwargs()\n\n\ntorch.export.export(model, example_args, example_kwargs)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pos0: \"f32[4]\", tuple0_0: \"f32[4]\", tuple0_1: \"f32[4]\", myargs_0: \"f32[4]\", myargs_1: \"f32[4]\", mykw0: \"f32[4]\", input0: \"f32[4]\", input1: \"f32[4]\"):\n                 mul: \"f32[4]\" = torch.ops.aten.mul.Tensor(pos0, tuple0_0);  pos0 = tuple0_0 = None\n            mul_1: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul, tuple0_1);  mul = tuple0_1 = None\n\n                 mul_2: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_1, myargs_0);  mul_1 = myargs_0 = None\n            mul_3: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_2, myargs_1);  mul_2 = myargs_1 = None\n\n                 mul_4: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_3, mykw0);  mul_3 = mykw0 = None\n\n                 mul_5: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_4, input0);  mul_4 = input0 = None\n            mul_6: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_5, input1);  mul_5 = input1 = None\n            return (mul_6,)\n\nGraph signature:\n    # inputs\n    pos0: USER_INPUT\n    tuple0_0: USER_INPUT\n    tuple0_1: USER_INPUT\n    myargs_0: USER_INPUT\n    myargs_1: USER_INPUT\n    mykw0: USER_INPUT\n    input0: USER_INPUT\n    input1: USER_INPUT\n\n    # outputs\n    mul_6: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_contains#\n\nNote\nTags:python.data-structure,torch.dynamic-shape,python.assert\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass ListContains(torch.nn.Module):\n    \"\"\"\n    List containment relation can be checked on a dynamic shape or constants.\n    \"\"\"\n\n    def forward(self, x):\n        assert x.size(-1) in [6, 2]\n        assert x.size(0) not in [4, 5, 6]\n        assert \"monkey\" not in [\"cow\", \"pig\"]\n        return x + x\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"}\nmodel = ListContains()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_unpack#\n\nNote\nTags:python.data-structure,python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\n\nimport torch\n\nclass ListUnpack(torch.nn.Module):\n    \"\"\"\n    Lists are treated as static construct, therefore unpacking should be\n    erased after tracing.\n    \"\"\"\n\n    def forward(self, args: list[torch.Tensor]):\n        \"\"\"\n        Lists are treated as static construct, therefore unpacking should be\n        erased after tracing.\n        \"\"\"\n        x, *y = args\n        return x + y[0]\n\nexample_args = ([torch.randn(3, 2), torch.tensor(4), torch.tensor(5)],)\ntags = {\"python.control-flow\", \"python.data-structure\"}\nmodel = ListUnpack()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, args_0: \"f32[3, 2]\", args_1: \"i64[]\", args_2: \"i64[]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(args_0, args_1);  args_0 = args_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    args_0: USER_INPUT\n    args_1: USER_INPUT\n    args_2: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## nested_function#\n\nNote\nTags:python.closure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass NestedFunction(torch.nn.Module):\n    \"\"\"\n    Nested functions are traced through. Side effects on global captures\n    are not supported though.\n    \"\"\"\n\n    def forward(self, a, b):\n        x = a + b\n        z = a - b\n\n        def closure(y):\n            nonlocal x\n            x += 1\n            return x * y + z\n\n        return closure(x)\n\nexample_args = (torch.randn(3, 2), torch.randn(2))\ntags = {\"python.closure\"}\nmodel = NestedFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, a: \"f32[3, 2]\", b: \"f32[2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(a, b)\n\n                 sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(a, b);  a = b = None\n\n                 add_: \"f32[3, 2]\" = torch.ops.aten.add_.Tensor(add, 1);  add = None\n\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(add_, add_);  add_ = None\n            add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(mul, sub);  mul = sub = None\n            return (add_1,)\n\nGraph signature:\n    # inputs\n    a: USER_INPUT\n    b: USER_INPUT\n\n    # outputs\n    add_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## null_context_manager#\n\nNote\nTags:python.context-manager\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport contextlib\n\nimport torch\n\nclass NullContextManager(torch.nn.Module):\n    \"\"\"\n    Null context manager in Python will be traced out.\n    \"\"\"\n\n    def forward(self, x):\n        \"\"\"\n        Null context manager in Python will be traced out.\n        \"\"\"\n        ctx = contextlib.nullcontext()\n        with ctx:\n            return x.sin() + x.cos()\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.context-manager\"}\nmodel = NullContextManager()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 sin: \"f32[3, 2]\" = torch.ops.aten.sin.default(x)\n            cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(x);  x = None\n            add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(sin, cos);  sin = cos = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## pytree_flatten#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.utils import _pytree as pytree\n\nclass PytreeFlatten(torch.nn.Module):\n    \"\"\"\n    Pytree from PyTorch can be captured by TorchDynamo.\n    \"\"\"\n\n    def forward(self, x):\n        y, _spec = pytree.tree_flatten(x)\n        return y[0] + 1\n\nexample_args = ({1: torch.randn(3, 2), 2: torch.randn(3, 2)},),\nmodel = PytreeFlatten()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x_0_1: \"f32[3, 2]\", x_0_2: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x_0_1, 1);  x_0_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x_0_1: USER_INPUT\n    x_0_2: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## scalar_output#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.export import Dim\n\nx = torch.randn(3, 2)\ndim1_x = Dim(\"dim1_x\")\n\nclass ScalarOutput(torch.nn.Module):\n    \"\"\"\n    Returning scalar values from the graph is supported, in addition to Tensor\n    outputs. Symbolic shapes are captured and rank is specialized.\n    \"\"\"\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x):\n        return x.shape[1] + 1\n\nexample_args = (x,)\ntags = {\"torch.dynamic-shape\"}\ndynamic_shapes = {\"x\": {1: dim1_x}}\nmodel = ScalarOutput()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, s27]\"):\n             #\n            sym_size_int_1: \"Sym(s27)\" = torch.ops.aten.sym_size.int(x, 1);  x = None\n\n                 add: \"Sym(s27 + 1)\" = sym_size_int_1 + 1;  sym_size_int_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {s27: VR[0, int_oo]}\n\n```\n\n\n## specialized_attribute#\n\nNote\nTags:\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nfrom enum import Enum\n\nimport torch\n\nclass Animal(Enum):\n    COW = \"moo\"\n\nclass SpecializedAttribute(torch.nn.Module):\n    \"\"\"\n    Model attributes are specialized.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.a = \"moo\"\n        self.b = 4\n\n    def forward(self, x):\n        if self.a == Animal.COW.value:\n            return x * x + self.b\n        else:\n            raise ValueError(\"bad\")\n\nexample_args = (torch.randn(3, 2),)\nmodel = SpecializedAttribute()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, x);  x = None\n            add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(mul, 4);  mul = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## static_for_loop#\n\nNote\nTags:python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass StaticForLoop(torch.nn.Module):\n    \"\"\"\n    A for loop with constant number of iterations should be unrolled in the exported graph.\n    \"\"\"\n\n    def forward(self, x):\n        # constant\n        ret = [i + x for i in range(10)]\n        return ret\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.control-flow\"}\nmodel = StaticForLoop()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 0)\n            add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1)\n            add_2: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 2)\n            add_3: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 3)\n            add_4: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4)\n            add_5: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 5)\n            add_6: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 6)\n            add_7: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 7)\n            add_8: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 8)\n            add_9: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 9);  x = None\n            return (add, add_1, add_2, add_3, add_4, add_5, add_6, add_7, add_8, add_9)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n    add_1: USER_OUTPUT\n    add_2: USER_OUTPUT\n    add_3: USER_OUTPUT\n    add_4: USER_OUTPUT\n    add_5: USER_OUTPUT\n    add_6: USER_OUTPUT\n    add_7: USER_OUTPUT\n    add_8: USER_OUTPUT\n    add_9: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## static_if#\n\nNote\nTags:python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass StaticIf(torch.nn.Module):\n    \"\"\"\n    `if` statement with static predicate value should be traced through with the\n    taken branch.\n    \"\"\"\n\n    def forward(self, x):\n        if len(x.shape) == 3:\n            return x + torch.ones(1, 1, 1)\n\n        return x\n\nexample_args = (torch.randn(3, 2, 2),)\ntags = {\"python.control-flow\"}\nmodel = StaticIf()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2, 2]\"):\n                 ones: \"f32[1, 1, 1]\" = torch.ops.aten.ones.default([1, 1, 1], device = device(type='cpu'), pin_memory = False)\n            add: \"f32[3, 2, 2]\" = torch.ops.aten.add.Tensor(x, ones);  x = ones = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## tensor_setattr#\n\nNote\nTags:python.builtin\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass TensorSetattr(torch.nn.Module):\n    \"\"\"\n    setattr() call onto tensors is not supported.\n    \"\"\"\n    def forward(self, x, attr):\n        setattr(x, attr, torch.randn(3, 2))\n        return x + 4\n\nexample_args = (torch.randn(3, 2), \"attr\")\ntags = {\"python.builtin\"}\nmodel = TensorSetattr()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", attr):\n                 randn: \"f32[3, 2]\" = torch.ops.aten.randn.default([3, 2], device = device(type='cpu'), pin_memory = False);  randn = None\n\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    attr: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## type_reflection_method#\n\nNote\nTags:python.builtin\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass A:\n    @classmethod\n    def func(cls, x):\n        return 1 + x\n\nclass TypeReflectionMethod(torch.nn.Module):\n    \"\"\"\n    type() calls on custom objects followed by attribute accesses are not allowed\n    due to its overly dynamic nature.\n    \"\"\"\n\n    def forward(self, x):\n        a = A()\n        return type(a).func(x)\n\n\nexample_args = (torch.randn(3, 4),)\ntags = {\"python.builtin\"}\nmodel = TypeReflectionMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 4]\"):\n                 add: \"f32[3, 4]\" = torch.ops.aten.add.Tensor(x, 1);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## user_input_mutation#\n\nNote\nTags:torch.mutation\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass UserInputMutation(torch.nn.Module):\n    \"\"\"\n    Directly mutate user input in forward\n    \"\"\"\n\n    def forward(self, x):\n        x.mul_(2)\n        return x.cos()\n\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.mutation\"}\nmodel = UserInputMutation()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 mul_: \"f32[3, 2]\" = torch.ops.aten.mul_.Tensor(x, 2);  x = None\n\n                 cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(mul_);  mul_ = None\n            return (cos,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    cos: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## Not Supported Yet#\n\n\n## dynamic_shape_round#\n\nNote\nTags:python.builtin,torch.dynamic-shape\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch._export.db.case import SupportLevel\nfrom torch.export import Dim\n\nclass DynamicShapeRound(torch.nn.Module):\n    \"\"\"\n    Calling round on dynamic shapes is not supported.\n    \"\"\"\n\n    def forward(self, x):\n        return x[: round(x.shape[0] / 2)]\n\nx = torch.randn(3, 2)\ndim0_x = Dim(\"dim0_x\")\nexample_args = (x,)\ntags = {\"torch.dynamic-shape\", \"python.builtin\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\ndynamic_shapes = {\"x\": {0: dim0_x}}\nmodel = DynamicShapeRound()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nUnsupported: Constraints violated (dim0_x)! For more information, run with TORCH_LOGS=\"+dynamic\".\n\n```\n\n\n## model_attr_mutation#\n\nNote\nTags:python.object-model\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nfrom torch._export.db.case import SupportLevel\n\n\nclass ModelAttrMutation(torch.nn.Module):\n    \"\"\"\n    Attribute mutation is not supported.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.attr_list = [torch.randn(3, 2), torch.randn(3, 2)]\n\n    def recreate_list(self):\n        return [torch.zeros(3, 2), torch.zeros(3, 2)]\n\n    def forward(self, x):\n        self.attr_list = self.recreate_list()\n        return x.sum() + self.attr_list[0].sum()\n\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.object-model\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\nmodel = ModelAttrMutation()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nAssertionError: Mutating module attribute attr_list during export.\n\n```\n\n\n## optional_input#\n\nNote\nTags:python.object-model\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nfrom torch._export.db.case import SupportLevel\n\n\nclass OptionalInput(torch.nn.Module):\n    \"\"\"\n    Tracing through optional input is not supported yet\n    \"\"\"\n\n    def forward(self, x, y=torch.randn(2, 3)):\n        if y is not None:\n            return x + y\n        return x\n\n\nexample_args = (torch.randn(2, 3),)\ntags = {\"python.object-model\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\nmodel = OptionalInput()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nUnsupported: Tracing through optional input is not supported yet\n\n```\n\n\n## unsupported_operator#\n\nNote\nTags:torch.operator\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nfrom torch._export.db.case import SupportLevel\n\n\nclass TorchSymMin(torch.nn.Module):\n    \"\"\"\n    torch.sym_min operator is not supported in export.\n    \"\"\"\n\n    def forward(self, x):\n        return x.sum() + torch.sym_min(x.size(0), 100)\n\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.operator\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\nmodel = TorchSymMin()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nUnsupported: torch.* op returned non-Tensor\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/index.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2306d944d7e426b01a9c8ee18b7b8b75",
    "source": "pytorch_docs",
    "title": "PyTorch Contribution Guide \u2014 PyTorch 2.9 documentation",
    "text": "Note\nThis page has been deprecated. Please refer to theContribution Guideon the PyTorch Wiki.\n\n## PyTorch Contribution Guide#\n\nCreated On: Mar 11, 2019 | Last Updated On: Apr 27, 2025\nPyTorch is a GPU-accelerated Python tensor computation package for\nbuilding deep neural networks using a tape-based autograd systems.\n\n## Contribution Process#\n\nThe PyTorch organization is governed byPyTorch\nGovernanceand the technical guide to contributing\ncan be found inCONTRIBUTING.md.\nThe PyTorch development process involves a healthy amount of open\ndiscussions between the core development team and the community.\nPyTorch operates similarly to most open source projects on GitHub.\nHowever, if you\u2019ve never contributed to an open source project before,\nhere is the basic process.\nFigure out what you\u2019re going to work on.The majority of open\nsource contributions come from people scratching their own itches.\nHowever, if you don\u2019t know what you want to work on, or are just\nlooking to get more acquainted with the project, here are some tips\nfor how to find appropriate tasks:\nLook through theissue\ntrackerand see if\nthere are any issues you know how to fix. Issues that are\nconfirmed by other contributors tend to be better to investigate.\nWe also maintain some labels for issues that are likely to be\ngood for new people, e.g.,bootcampand1hr, although\nthese labels are less well maintained.\nJoin us ondev discussand let us know you\u2019re interested in getting to\nknow PyTorch. We\u2019re very happy to help out researchers and\npartners get up to speed with the codebase.\nFigure out the scope of your change and reach out for design\ncomments on a GitHub issue if it\u2019s large.The majority of pull\nrequests are small; in that case, no need to let us know about what\nyou want to do, just get cracking. But if the change is going to be\nlarge, it\u2019s usually a good idea to get some design comments about it\nfirst bysubmitting an RFC.\nIf you don\u2019t know how big a change is going to be, we can help you\nfigure it out! Just post about it onissuesordev discuss.\nSome feature additions are very standardized; for example, lots of\npeople add new operators or optimizers to PyTorch. Design\ndiscussion in these cases boils down mostly to, \u201cDo we want this\noperator/optimizer?\u201d Giving evidence for its utility, e.g., usage\nin peer reviewed papers, or existence in other frameworks, helps a\nbit when making this case.\nAdding operators / algorithms from recently-released researchis generally not accepted unless there is overwhelming evidence that\nthis newly published work has ground-breaking results and will eventually\nbecome a standard in the field. If you are not sure where your method falls,\nopen an issue first before implementing a PR.\nCore changes and refactors can be quite difficult to coordinate\nsince the pace of development on the PyTorch main branch is quite fast.\nDefinitely reach out about fundamental or cross-cutting changes;\nwe can often give guidance about how to stage such changes into\nmore easily reviewable pieces.\nCode it out!\nSee theCONTRIBUTING.mdfile for advice for working with PyTorch in a\ntechnical form.\nOpen a pull request.\nIf you are not ready for the pull request to be reviewed, create a draft\npull request first - you can later convert it to a full PR by pressing\n\u201cReady for review\u201d button. You can also prepend the title of the PR with\n\u201c[WIP]\u201d (\u201cwork in progress\u201d) while it\u2019s still in draft. We will ignore\ndraft PRs when doing review passes. If you are working on a complex change,\nit\u2019s good to start things off as a draft, because you will need to spend\ntime looking at CI results to see if things worked out or not.\nFind an appropriate reviewer for your change. We have some folks\nwho regularly go through the PR queue and try to review\neverything, but if you happen to know who the maintainer for a\ngiven subsystem affected by your patch is, feel free to include\nthem directly on the pull request. You can learn more aboutPersons of Interestthat could review your code.\nIterate on the pull request until it\u2019s accepted!\nWe\u2019ll try our best to minimize the number of review round trips and\nblock PRs only when there are major issues. For the most common\nissues in pull requests, take a look atCommon Mistakes.\nOnce a pull request is accepted and CI is passing, there is\nnothing else you need to do; we will merge the PR for you.\n\n## Getting Started#\n\n\n## Proposing New Features#\n\nNew feature ideas are best discussed on a specific issue. Please include\nas much information as you can, any accompanying data, and your proposed\nsolution. The PyTorch team and community frequently review new issues\nand comments where they think they can help. If you feel confident in\nyour solution, go ahead and implement it.\n\n## Reporting Issues#\n\nIf you\u2019ve identified an issue, first search through thelist of\nexisting issueson the\nrepo. If you are unable to find a similar issue, then create a new one.\nSupply as much information you can to reproduce the problematic\nbehavior. Also, include any additional insights like the behavior you\nexpect.\n\n## Implementing Features or Fixing Bugs#\n\nIf you want to fix a specific issue, it\u2019s best to comment on the\nindividual issue with your intent. However, we do not lock or assign\nissues except in cases where we have worked with the developer before.\nIt\u2019s best to strike up a conversation on the issue and discuss your\nproposed solution. The PyTorch team can provide guidance that saves you\ntime.\nIssues that are labeled first-new-issue, low, or medium priority provide\nthe best entrance points and are great places to start.\n\n## Adding Tutorials#\n\nA great deal of the tutorials onpytorch.orgcome from the community itself and we welcome additional contributions.\nTo learn more about how to contribute a new tutorial you can learn more\nhere:PyTorch.org Tutorial Contribution Guide on\nGitHub\n\n## Improving Documentation & Tutorials#\n\nWe aim to produce high quality documentation and tutorials. On rare\noccasions that content includes typos or bugs. If you find something you\ncan fix, send us a pull request for consideration.\nTake a look at theDocumentationsection to learn how our system\nworks.\n\n## Participating in Online Discussions#\n\nYou can find active discussions happening on thePyTorch Discussion\nForumsfor users as well as thePyTorch Dev Discussion Forumsfor developers and maintainers.\n\n## Submitting Pull Requests to Fix Open Issues#\n\nYou can view a list of all open issueshere. Commenting on an\nissue is a great way to get the attention of the team. From here you can\nshare your ideas and how you plan to resolve the issue.\nFor more challenging issues, the team will provide feedback and\ndirection for how to best solve the issue.\nIf you\u2019re not able to fix the issue yourself, commenting and sharing\nwhether you can reproduce the issue can help the team\nidentify problem areas.\n\n## Reviewing Open Pull Requests#\n\nWe appreciate your help reviewing and commenting on pull requests. Our\nteam strives to keep the number of open pull requests at a manageable\nsize, we respond quickly for more information if we need it, and we\nmerge PRs that we think are useful. However, due to the high level of\ninterest, additional eyes on the pull requests are always appreciated.\n\n## Improving Code Readability#\n\nImproving code readability helps everyone. It is often better to submit a\nsmall number of pull requests that touch a few files versus a large pull\nrequest that touches many files. Starting a discussion in the PyTorch\nforumhereor on an issue related to\nyour improvement is the best way to get started.\n\n## Adding Test Cases to Make the Codebase More Robust#\n\nAdditional test coverage is appreciated.\n\n## Promoting PyTorch#\n\nYour use of PyTorch in your projects, research papers, write ups, blogs,\nor general discussions around the internet helps to raise awareness for\nPyTorch and our growing community. Please reach out tomarketing@pytorch.orgfor marketing support.\n\n## Triaging Issues#\n\nIf you feel that an issue could benefit from a particular tag or level\nof complexity, comment on the issue and share your opinion. If you\nfeel an issue isn\u2019t categorized properly, comment and let the team know.\n\n## About Open Source Development#\n\nIf this is your first time contributing to an open source project, some\naspects of the development process may seem unusual to you.\nThere is no way to \u201cclaim\u201d issues.People often want to \u201cclaim\u201d\nan issue when they decide to work on it, to ensure that there isn\u2019t\nwasted work when someone else ends up working on it. This doesn\u2019t\nreally work too well in open source, since someone may decide to work\non something, and end up not having time to do it. Feel free to give\ninformation in an advisory fashion, but at the end of the day, we\nwill take running code and rough consensus to move forward quickly.\nThere is a high bar for new functionality.Unlike\nin a corporate environment, where the person who wrote code\nimplicitly \u201cowns\u201d it and can be expected to take care of it for the\ncode\u2019s lifetime, once a pull request is merged into an open\nsource project, it immediately becomes the collective responsibility\nof all maintainers on the project. When we merge code, we are saying\nthat we, the maintainers, can review subsequent changes and\nmake a bugfix to the code. This naturally leads to a higher standard\nof contribution.\n\n## Common Mistakes To Avoid#\n\nDid you add tests?(Or if the change is hard to test, did you\ndescribe how you tested your change?)\nWe have a few motivations for why we ask for tests:\nto help us tell if we break it later\nto help us tell if the patch is correct in the first place\n(yes, we did review it, but as Knuth says, \u201cbeware of the\nfollowing code, for I have not run it, merely proven it\ncorrect\u201d)\nWhen is it OK not to add a test? Sometimes a change can\u2019t be\nconveniently tested, or the change is so obviously correct (and\nunlikely to be broken) that it\u2019s OK not to test it. On the\ncontrary, if a change seems likely (or is known to be likely)\nto be accidentally broken, it\u2019s important to put in the time to\nwork out a testing strategy.\nIs your PR too long?\nIt\u2019s easier for us to review and merge small PRs. The difficulty of\nreviewing a PR scales nonlinearly with its size.\nWhen is it OK to submit a large PR? It helps a lot if there was a\ncorresponding design discussion in an issue, with sign off from\nthe people who are going to review your diff. We can also help\ngive advice about how to split up a large change into individually\nshippable parts. Similarly, it helps if there is a complete\ndescription of the contents of the PR: it\u2019s easier to review code\nif we know what\u2019s inside!\nComments for subtle things?In cases where the behavior of your code\nis nuanced, please include extra comments and documentation to allow\nus to better understand the intention of your code.\nDid you add a hack?Sometimes, the right answer is a hack. But\nusually, we will have to discuss it.\nDo you want to touch a very core component?To prevent\nmajor regressions, pull requests that touch core components receive\nextra scrutiny. Make sure you\u2019ve discussed your changes with the team\nbefore undertaking major changes.\nWant to add a new feature?If you want to add new features,\ncomment your intention on the related issue. Our team tries to\ncomment on and provide feedback to the community. It\u2019s better to have\nan open discussion with the team and the rest of the community before\nbuilding new features. This helps us stay aware of what you\u2019re\nworking on and increases the chance that it\u2019ll be merged.\nDid you touch code unrelated to the PR?To aid in code review,\nplease only include files in your pull request that are directly\nrelated to your changes.\n\n## Frequently Asked Questions#\n\nHow can I contribute as a reviewer?There is lots of value if\ncommunity developers reproduce issues, try out new functionality, or\notherwise help us identify or troubleshoot issues. Commenting on\ntasks or pull requests with your environment details is helpful and\nappreciated.\nCI tests failed, what does it mean?Maybe your PR is based\noff a broken main branch? You can try to rebase your change on top\nof the latest main branch. You can also see the current status of\nmain branch\u2019s CI athttps://hud.pytorch.org/.\nWhat are the most high risk changes?Anything that touches build\nconfiguration is a risky area. Please avoid changing these unless\nyou\u2019ve had a discussion with the team beforehand.\nHey, a commit showed up on my branch, what\u2019s up with that?Sometimes another community member will provide a patch or fix to\nyour pull request or branch. This is often needed for getting CI tests\nto pass.\n\n## On Documentation#\n\n\n## Python Docs#\n\nPyTorch documentation is generated from python source usingSphinx. Generated HTML is\ncopied to the docs folder in the main branch ofpytorch.org/docs,\nand is served via GitHub pages.\nSite:https://pytorch.org/docs\nGitHub:pytorch/pytorch\nServed from:https://pytorch.org/docs/main\n\n## C++ Docs#\n\nFor C++ code we use Doxygen to generate the content files. The C++ docs\nare built on a special server and the resulting files are copied to thepytorch/cppdocsrepo, and are served from GitHub\npages.\nSite:https://pytorch.org/cppdocs\nGitHub:pytorch/pytorch\nServed from:pytorch/cppdocs\n\n## Tutorials#\n\nPyTorch tutorials are documents used to help understand using PyTorch to\naccomplish specific tasks or to understand more holistic concepts.\nTutorials are built usingSphinx-Galleryfrom executable python source files, or from restructured-text (rst)\nfiles.\nSite:https://pytorch.org/tutorials\nGitHub:pytorch/tutorials\n\n## Tutorials Build Overview#\n\nFor tutorials,pull\nrequeststrigger a\nrebuild of the entire site using CircleCI to test the effects of the\nchange. This build is sharded into 9 worker builds and takes around 40\nminutes total. At the same time, we do a Netlify build usingmake\nhtml-noplot, which builds the site without rendering the notebook\noutput into pages for quick review.\nAfter a PR is accepted, the site is rebuilt and deployed using GitHub\nActions.\n\n## Contributing a New Tutorial#\n\nSeePyTorch.org Tutorial Contribution\nGuide.",
    "url": "https://pytorch.org/docs/stable/community/contribution_guide.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e5e1c79a892a3511e2b2a6ef4a7b3495",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/randomness.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d2657e6333a139b3b16a507b93e52de5",
    "source": "pytorch_docs",
    "title": "torch.map \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.map#\n\n\n## dynamic_shape_map#\n\nNote\nTags:torch.dynamic-shape,torch.map\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import map\n\nclass DynamicShapeMap(torch.nn.Module):\n    \"\"\"\n    functorch map() maps a function over the first tensor dimension.\n    \"\"\"\n\n    def forward(self, xs, y):\n        def body(x, y):\n            return x + y\n\n        return map(body, xs, y)\n\nexample_args = (torch.randn(3, 2), torch.randn(2))\ntags = {\"torch.dynamic-shape\", \"torch.map\"}\nmodel = DynamicShapeMap()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, xs: \"f32[3, 2]\", y: \"f32[2]\"):\n                 body_graph_0 = self.body_graph_0\n            map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y]);  body_graph_0 = xs = y = None\n            getitem: \"f32[3, 2]\" = map_impl[0];  map_impl = None\n            return (getitem,)\n\n        class body_graph_0(torch.nn.Module):\n            def forward(self, xs: \"f32[2]\", y: \"f32[2]\"):\n                         add: \"f32[2]\" = torch.ops.aten.add.Tensor(xs, y);  xs = y = None\n                return (add,)\n\nGraph signature:\n    # inputs\n    xs: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/torch.map.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f50dc6a93011c87eb120e9d540086c58",
    "source": "pytorch_docs",
    "title": "Named Tensors operator coverage \u2014 PyTorch 2.9 documentation",
    "text": "\n## Named Tensors operator coverage#\n\nCreated On: Oct 08, 2019 | Last Updated On: Jun 08, 2025\nPlease readNamed Tensorsfirst for an introduction to named tensors.\nThis document is a reference forname inference, a process that defines how\nnamed tensors:\nuse names to provide additional automatic runtime correctness checks\npropagate names from input tensors to output tensors\nBelow is a list of all operations that are supported with named tensors\nand their associated name inference rules.\nIf you don\u2019t see an operation listed here, but it would help your use case, pleasesearch if an issue has already been filedand if not,file one.\nWarning\nThe named tensor API is experimental and subject to change.\nAPI\nName inference rule\nTensor.abs(),torch.abs()\nTensor.abs()\ntorch.abs()\nKeeps input names\nTensor.abs_()\nTensor.abs_()\nKeeps input names\nTensor.acos(),torch.acos()\nTensor.acos()\ntorch.acos()\nKeeps input names\nTensor.acos_()\nTensor.acos_()\nKeeps input names\nTensor.add(),torch.add()\nTensor.add()\ntorch.add()\nUnifies names from inputs\nTensor.add_()\nTensor.add_()\nUnifies names from inputs\nTensor.addmm(),torch.addmm()\nTensor.addmm()\ntorch.addmm()\nContracts away dims\nTensor.addmm_()\nTensor.addmm_()\nContracts away dims\nTensor.addmv(),torch.addmv()\nTensor.addmv()\ntorch.addmv()\nContracts away dims\nTensor.addmv_()\nTensor.addmv_()\nContracts away dims\nTensor.align_as()\nTensor.align_as()\nSee documentation\nTensor.align_to()\nTensor.align_to()\nSee documentation\nTensor.all(),torch.all()\nTensor.all()\ntorch.all()\nNone\nTensor.any(),torch.any()\nTensor.any()\ntorch.any()\nNone\nTensor.asin(),torch.asin()\nTensor.asin()\ntorch.asin()\nKeeps input names\nTensor.asin_()\nTensor.asin_()\nKeeps input names\nTensor.atan(),torch.atan()\nTensor.atan()\ntorch.atan()\nKeeps input names\nTensor.atan2(),torch.atan2()\nTensor.atan2()\ntorch.atan2()\nUnifies names from inputs\nTensor.atan2_()\nTensor.atan2_()\nUnifies names from inputs\nTensor.atan_()\nTensor.atan_()\nKeeps input names\nTensor.bernoulli(),torch.bernoulli()\nTensor.bernoulli()\ntorch.bernoulli()\nKeeps input names\nTensor.bernoulli_()\nTensor.bernoulli_()\nNone\nTensor.bfloat16()\nTensor.bfloat16()\nKeeps input names\nTensor.bitwise_not(),torch.bitwise_not()\nTensor.bitwise_not()\ntorch.bitwise_not()\nKeeps input names\nTensor.bitwise_not_()\nTensor.bitwise_not_()\nNone\nTensor.bmm(),torch.bmm()\nTensor.bmm()\ntorch.bmm()\nContracts away dims\nTensor.bool()\nTensor.bool()\nKeeps input names\nTensor.byte()\nTensor.byte()\nKeeps input names\ntorch.cat()\ntorch.cat()\nUnifies names from inputs\nTensor.cauchy_()\nTensor.cauchy_()\nNone\nTensor.ceil(),torch.ceil()\nTensor.ceil()\ntorch.ceil()\nKeeps input names\nTensor.ceil_()\nTensor.ceil_()\nNone\nTensor.char()\nTensor.char()\nKeeps input names\nTensor.chunk(),torch.chunk()\nTensor.chunk()\ntorch.chunk()\nKeeps input names\nTensor.clamp(),torch.clamp()\nTensor.clamp()\ntorch.clamp()\nKeeps input names\nTensor.clamp_()\nTensor.clamp_()\nNone\nTensor.copy_()\nTensor.copy_()\nout function and in-place variants\nTensor.cos(),torch.cos()\nTensor.cos()\ntorch.cos()\nKeeps input names\nTensor.cos_()\nTensor.cos_()\nNone\nTensor.cosh(),torch.cosh()\nTensor.cosh()\ntorch.cosh()\nKeeps input names\nTensor.cosh_()\nTensor.cosh_()\nNone\nTensor.acosh(),torch.acosh()\nTensor.acosh()\ntorch.acosh()\nKeeps input names\nTensor.acosh_()\nTensor.acosh_()\nNone\nTensor.cpu()\nTensor.cpu()\nKeeps input names\nTensor.cuda()\nTensor.cuda()\nKeeps input names\nTensor.cumprod(),torch.cumprod()\nTensor.cumprod()\ntorch.cumprod()\nKeeps input names\nTensor.cumsum(),torch.cumsum()\nTensor.cumsum()\ntorch.cumsum()\nKeeps input names\nTensor.data_ptr()\nTensor.data_ptr()\nNone\nTensor.deg2rad(),torch.deg2rad()\nTensor.deg2rad()\ntorch.deg2rad()\nKeeps input names\nTensor.deg2rad_()\nTensor.deg2rad_()\nNone\nTensor.detach(),torch.detach()\nTensor.detach()\ntorch.detach()\nKeeps input names\nTensor.detach_()\nTensor.detach_()\nNone\nTensor.device,torch.device()\nTensor.device\ntorch.device()\nNone\nTensor.digamma(),torch.digamma()\nTensor.digamma()\ntorch.digamma()\nKeeps input names\nTensor.digamma_()\nTensor.digamma_()\nNone\nTensor.dim()\nTensor.dim()\nNone\nTensor.div(),torch.div()\nTensor.div()\ntorch.div()\nUnifies names from inputs\nTensor.div_()\nTensor.div_()\nUnifies names from inputs\nTensor.dot(),torch.dot()\nTensor.dot()\ntorch.dot()\nNone\nTensor.double()\nTensor.double()\nKeeps input names\nTensor.element_size()\nTensor.element_size()\nNone\ntorch.empty()\ntorch.empty()\nFactory functions\ntorch.empty_like()\ntorch.empty_like()\nFactory functions\nTensor.eq(),torch.eq()\nTensor.eq()\ntorch.eq()\nUnifies names from inputs\nTensor.erf(),torch.erf()\nTensor.erf()\ntorch.erf()\nKeeps input names\nTensor.erf_()\nTensor.erf_()\nNone\nTensor.erfc(),torch.erfc()\nTensor.erfc()\ntorch.erfc()\nKeeps input names\nTensor.erfc_()\nTensor.erfc_()\nNone\nTensor.erfinv(),torch.erfinv()\nTensor.erfinv()\ntorch.erfinv()\nKeeps input names\nTensor.erfinv_()\nTensor.erfinv_()\nNone\nTensor.exp(),torch.exp()\nTensor.exp()\ntorch.exp()\nKeeps input names\nTensor.exp_()\nTensor.exp_()\nNone\nTensor.expand()\nTensor.expand()\nKeeps input names\nTensor.expm1(),torch.expm1()\nTensor.expm1()\ntorch.expm1()\nKeeps input names\nTensor.expm1_()\nTensor.expm1_()\nNone\nTensor.exponential_()\nTensor.exponential_()\nNone\nTensor.fill_()\nTensor.fill_()\nNone\nTensor.flatten(),torch.flatten()\nTensor.flatten()\ntorch.flatten()\nSee documentation\nTensor.float()\nTensor.float()\nKeeps input names\nTensor.floor(),torch.floor()\nTensor.floor()\ntorch.floor()\nKeeps input names\nTensor.floor_()\nTensor.floor_()\nNone\nTensor.frac(),torch.frac()\nTensor.frac()\ntorch.frac()\nKeeps input names\nTensor.frac_()\nTensor.frac_()\nNone\nTensor.ge(),torch.ge()\nTensor.ge()\ntorch.ge()\nUnifies names from inputs\nTensor.get_device(),torch.get_device()\nTensor.get_device()\ntorch.get_device()\nNone\nTensor.grad\nTensor.grad\nNone\nTensor.gt(),torch.gt()\nTensor.gt()\ntorch.gt()\nUnifies names from inputs\nTensor.half()\nTensor.half()\nKeeps input names\nTensor.has_names()\nTensor.has_names()\nSee documentation\nTensor.index_fill(),torch.index_fill()\nTensor.index_fill()\ntorch.index_fill()\nKeeps input names\nTensor.index_fill_()\nTensor.index_fill_()\nNone\nTensor.int()\nTensor.int()\nKeeps input names\nTensor.is_contiguous()\nTensor.is_contiguous()\nNone\nTensor.is_cuda\nTensor.is_cuda\nNone\nTensor.is_floating_point(),torch.is_floating_point()\nTensor.is_floating_point()\ntorch.is_floating_point()\nNone\nTensor.is_leaf\nTensor.is_leaf\nNone\nTensor.is_pinned()\nTensor.is_pinned()\nNone\nTensor.is_shared()\nTensor.is_shared()\nNone\nTensor.is_signed(),torch.is_signed()\nTensor.is_signed()\ntorch.is_signed()\nNone\nTensor.is_sparse\nTensor.is_sparse\nNone\nTensor.is_sparse_csr\nTensor.is_sparse_csr\nNone\ntorch.is_tensor()\ntorch.is_tensor()\nNone\nTensor.item()\nTensor.item()\nNone\nTensor.itemsize\nTensor.itemsize\nNone\nTensor.kthvalue(),torch.kthvalue()\nTensor.kthvalue()\ntorch.kthvalue()\nRemoves dimensions\nTensor.le(),torch.le()\nTensor.le()\ntorch.le()\nUnifies names from inputs\nTensor.log(),torch.log()\nTensor.log()\ntorch.log()\nKeeps input names\nTensor.log10(),torch.log10()\nTensor.log10()\ntorch.log10()\nKeeps input names\nTensor.log10_()\nTensor.log10_()\nNone\nTensor.log1p(),torch.log1p()\nTensor.log1p()\ntorch.log1p()\nKeeps input names\nTensor.log1p_()\nTensor.log1p_()\nNone\nTensor.log2(),torch.log2()\nTensor.log2()\ntorch.log2()\nKeeps input names\nTensor.log2_()\nTensor.log2_()\nNone\nTensor.log_()\nTensor.log_()\nNone\nTensor.log_normal_()\nTensor.log_normal_()\nNone\nTensor.logical_not(),torch.logical_not()\nTensor.logical_not()\ntorch.logical_not()\nKeeps input names\nTensor.logical_not_()\nTensor.logical_not_()\nNone\nTensor.logsumexp(),torch.logsumexp()\nTensor.logsumexp()\ntorch.logsumexp()\nRemoves dimensions\nTensor.long()\nTensor.long()\nKeeps input names\nTensor.lt(),torch.lt()\nTensor.lt()\ntorch.lt()\nUnifies names from inputs\ntorch.manual_seed()\ntorch.manual_seed()\nNone\nTensor.masked_fill(),torch.masked_fill()\nTensor.masked_fill()\ntorch.masked_fill()\nKeeps input names\nTensor.masked_fill_()\nTensor.masked_fill_()\nNone\nTensor.masked_select(),torch.masked_select()\nTensor.masked_select()\ntorch.masked_select()\nAligns mask up to input and then unifies_names_from_input_tensors\nTensor.matmul(),torch.matmul()\nTensor.matmul()\ntorch.matmul()\nContracts away dims\nTensor.mean(),torch.mean()\nTensor.mean()\ntorch.mean()\nRemoves dimensions\nTensor.median(),torch.median()\nTensor.median()\ntorch.median()\nRemoves dimensions\nTensor.nanmedian(),torch.nanmedian()\nTensor.nanmedian()\ntorch.nanmedian()\nRemoves dimensions\nTensor.mm(),torch.mm()\nTensor.mm()\ntorch.mm()\nContracts away dims\nTensor.mode(),torch.mode()\nTensor.mode()\ntorch.mode()\nRemoves dimensions\nTensor.mul(),torch.mul()\nTensor.mul()\ntorch.mul()\nUnifies names from inputs\nTensor.mul_()\nTensor.mul_()\nUnifies names from inputs\nTensor.mv(),torch.mv()\nTensor.mv()\ntorch.mv()\nContracts away dims\nTensor.names\nTensor.names\nSee documentation\nTensor.narrow(),torch.narrow()\nTensor.narrow()\ntorch.narrow()\nKeeps input names\nTensor.nbytes\nTensor.nbytes\nNone\nTensor.ndim\nTensor.ndim\nNone\nTensor.ndimension()\nTensor.ndimension()\nNone\nTensor.ne(),torch.ne()\nTensor.ne()\ntorch.ne()\nUnifies names from inputs\nTensor.neg(),torch.neg()\nTensor.neg()\ntorch.neg()\nKeeps input names\nTensor.neg_()\nTensor.neg_()\nNone\ntorch.normal()\ntorch.normal()\nKeeps input names\nTensor.normal_()\nTensor.normal_()\nNone\nTensor.numel(),torch.numel()\nTensor.numel()\ntorch.numel()\nNone\ntorch.ones()\ntorch.ones()\nFactory functions\nTensor.pow(),torch.pow()\nTensor.pow()\ntorch.pow()\nUnifies names from inputs\nTensor.pow_()\nTensor.pow_()\nNone\nTensor.prod(),torch.prod()\nTensor.prod()\ntorch.prod()\nRemoves dimensions\nTensor.rad2deg(),torch.rad2deg()\nTensor.rad2deg()\ntorch.rad2deg()\nKeeps input names\nTensor.rad2deg_()\nTensor.rad2deg_()\nNone\ntorch.rand()\ntorch.rand()\nFactory functions\ntorch.rand()\ntorch.rand()\nFactory functions\ntorch.randn()\ntorch.randn()\nFactory functions\ntorch.randn()\ntorch.randn()\nFactory functions\nTensor.random_()\nTensor.random_()\nNone\nTensor.reciprocal(),torch.reciprocal()\nTensor.reciprocal()\ntorch.reciprocal()\nKeeps input names\nTensor.reciprocal_()\nTensor.reciprocal_()\nNone\nTensor.refine_names()\nTensor.refine_names()\nSee documentation\nTensor.register_hook()\nTensor.register_hook()\nNone\nTensor.register_post_accumulate_grad_hook()\nTensor.register_post_accumulate_grad_hook()\nNone\nTensor.rename()\nTensor.rename()\nSee documentation\nTensor.rename_()\nTensor.rename_()\nSee documentation\nTensor.requires_grad\nTensor.requires_grad\nNone\nTensor.requires_grad_()\nTensor.requires_grad_()\nNone\nTensor.resize_()\nTensor.resize_()\nOnly allow resizes that do not change shape\nTensor.resize_as_()\nTensor.resize_as_()\nOnly allow resizes that do not change shape\nTensor.round(),torch.round()\nTensor.round()\ntorch.round()\nKeeps input names\nTensor.round_()\nTensor.round_()\nNone\nTensor.rsqrt(),torch.rsqrt()\nTensor.rsqrt()\ntorch.rsqrt()\nKeeps input names\nTensor.rsqrt_()\nTensor.rsqrt_()\nNone\nTensor.select(),torch.select()\nTensor.select()\ntorch.select()\nRemoves dimensions\nTensor.short()\nTensor.short()\nKeeps input names\nTensor.sigmoid(),torch.sigmoid()\nTensor.sigmoid()\ntorch.sigmoid()\nKeeps input names\nTensor.sigmoid_()\nTensor.sigmoid_()\nNone\nTensor.sign(),torch.sign()\nTensor.sign()\ntorch.sign()\nKeeps input names\nTensor.sign_()\nTensor.sign_()\nNone\nTensor.sgn(),torch.sgn()\nTensor.sgn()\ntorch.sgn()\nKeeps input names\nTensor.sgn_()\nTensor.sgn_()\nNone\nTensor.sin(),torch.sin()\nTensor.sin()\ntorch.sin()\nKeeps input names\nTensor.sin_()\nTensor.sin_()\nNone\nTensor.sinh(),torch.sinh()\nTensor.sinh()\ntorch.sinh()\nKeeps input names\nTensor.sinh_()\nTensor.sinh_()\nNone\nTensor.asinh(),torch.asinh()\nTensor.asinh()\ntorch.asinh()\nKeeps input names\nTensor.asinh_()\nTensor.asinh_()\nNone\nTensor.size()\nTensor.size()\nNone\nTensor.softmax(),torch.softmax()\nTensor.softmax()\ntorch.softmax()\nKeeps input names\nTensor.split(),torch.split()\nTensor.split()\ntorch.split()\nKeeps input names\nTensor.sqrt(),torch.sqrt()\nTensor.sqrt()\ntorch.sqrt()\nKeeps input names\nTensor.sqrt_()\nTensor.sqrt_()\nNone\nTensor.squeeze(),torch.squeeze()\nTensor.squeeze()\ntorch.squeeze()\nRemoves dimensions\nTensor.std(),torch.std()\nTensor.std()\ntorch.std()\nRemoves dimensions\ntorch.std_mean()\ntorch.std_mean()\nRemoves dimensions\nTensor.stride()\nTensor.stride()\nNone\nTensor.sub(),torch.sub()\nTensor.sub()\ntorch.sub()\nUnifies names from inputs\nTensor.sub_()\nTensor.sub_()\nUnifies names from inputs\nTensor.sum(),torch.sum()\nTensor.sum()\ntorch.sum()\nRemoves dimensions\nTensor.tan(),torch.tan()\nTensor.tan()\ntorch.tan()\nKeeps input names\nTensor.tan_()\nTensor.tan_()\nNone\nTensor.tanh(),torch.tanh()\nTensor.tanh()\ntorch.tanh()\nKeeps input names\nTensor.tanh_()\nTensor.tanh_()\nNone\nTensor.atanh(),torch.atanh()\nTensor.atanh()\ntorch.atanh()\nKeeps input names\nTensor.atanh_()\nTensor.atanh_()\nNone\ntorch.tensor()\ntorch.tensor()\nFactory functions\nTensor.to()\nTensor.to()\nKeeps input names\nTensor.topk(),torch.topk()\nTensor.topk()\ntorch.topk()\nRemoves dimensions\nTensor.transpose(),torch.transpose()\nTensor.transpose()\ntorch.transpose()\nPermutes dimensions\nTensor.trunc(),torch.trunc()\nTensor.trunc()\ntorch.trunc()\nKeeps input names\nTensor.trunc_()\nTensor.trunc_()\nNone\nTensor.type()\nTensor.type()\nNone\nTensor.type_as()\nTensor.type_as()\nKeeps input names\nTensor.unbind(),torch.unbind()\nTensor.unbind()\ntorch.unbind()\nRemoves dimensions\nTensor.unflatten()\nTensor.unflatten()\nSee documentation\nTensor.uniform_()\nTensor.uniform_()\nNone\nTensor.var(),torch.var()\nTensor.var()\ntorch.var()\nRemoves dimensions\ntorch.var_mean()\ntorch.var_mean()\nRemoves dimensions\nTensor.zero_()\nTensor.zero_()\nNone\ntorch.zeros()\ntorch.zeros()\nFactory functions\n\n## Keeps input names#\n\nAll pointwise unary functions follow this rule as well as some other unary functions.\nCheck names: None\nPropagate names: input tensor\u2019s names are propagated to the output.\n\n```python\n>>> x = torch.randn(3, 3, names=('N', 'C'))\n>>> x.abs().names\n('N', 'C')\n\n```\n\n\n## Removes dimensions#\n\nAll reduction ops likesum()remove dimensions by reducing\nover the desired dimensions. Other operations likeselect()andsqueeze()remove dimensions.\nsum()\nselect()\nsqueeze()\nWherever one can pass an integer dimension index to an operator, one can also pass\na dimension name. Functions that take lists of dimension indices can also take in a\nlist of dimension names.\nCheck names: Ifdimordimsis passed in as a list of names,\ncheck that those names exist inself.\ndim\ndims\nself\nPropagate names: If the dimensions of the input tensor specified bydimordimsare not present in the output tensor, then the corresponding names\nof those dimensions do not appear inoutput.names.\ndim\ndims\noutput.names\n\n```python\n>>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.squeeze('N').names\n('C', 'H', 'W')\n\n>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.sum(['N', 'C']).names\n('H', 'W')\n\n# Reduction ops with keepdim=True don't actually remove dimensions.\n>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.sum(['N', 'C'], keepdim=True).names\n('N', 'C', 'H', 'W')\n\n```\n\n\n## Unifies names from inputs#\n\nAll binary arithmetic ops follow this rule. Operations that broadcast still\nbroadcast positionally from the right to preserve compatibility with unnamed\ntensors. To perform explicit broadcasting by names, useTensor.align_as().\nTensor.align_as()\nCheck names: All names must match positionally from the right. i.e., intensor+other,match(tensor.names[i],other.names[i])must be true for alliin(-min(tensor.dim(),other.dim())+1,-1].\ntensor+other\nmatch(tensor.names[i],other.names[i])\ni\n(-min(tensor.dim(),other.dim())+1,-1]\nCheck names: Furthermore, all named dimensions must be aligned from the right.\nDuring matching, if we match a named dimensionAwith an unnamed dimensionNone, thenAmust not appear in the tensor with the unnamed dimension.\nA\nNone\nA\nPropagate names: unify pairs of names from the right from both tensors to\nproduce output names.\nFor example,\n\n```python\n# tensor: Tensor[   N, None]\n# other:  Tensor[None,    C]\n>>> tensor = torch.randn(3, 3, names=('N', None))\n>>> other = torch.randn(3, 3, names=(None, 'C'))\n>>> (tensor + other).names\n('N', 'C')\n\n```\n\nCheck names:\nmatch(tensor.names[-1],other.names[-1])isTrue\nmatch(tensor.names[-1],other.names[-1])\nTrue\nmatch(tensor.names[-2],tensor.names[-2])isTrue\nmatch(tensor.names[-2],tensor.names[-2])\nTrue\nBecause we matchedNoneintensorwith'C',\ncheck to make sure'C'doesn\u2019t exist intensor(it does not).\nNone\ntensor\n'C'\n'C'\ntensor\nCheck to make sure'N'doesn\u2019t exists inother(it does not).\n'N'\nother\nFinally, the output names are computed with[unify('N',None),unify(None,'C')]=['N','C']\n[unify('N',None),unify(None,'C')]=['N','C']\nMore examples:\n\n```python\n# Dimensions don't match from the right:\n# tensor: Tensor[N, C]\n# other:  Tensor[   N]\n>>> tensor = torch.randn(3, 3, names=('N', 'C'))\n>>> other = torch.randn(3, names=('N',))\n>>> (tensor + other).names\nRuntimeError: Error when attempting to broadcast dims ['N', 'C'] and dims\n['N']: dim 'C' and dim 'N' are at the same position from the right but do\nnot match.\n\n# Dimensions aren't aligned when matching tensor.names[-1] and other.names[-1]:\n# tensor: Tensor[N, None]\n# other:  Tensor[      N]\n>>> tensor = torch.randn(3, 3, names=('N', None))\n>>> other = torch.randn(3, names=('N',))\n>>> (tensor + other).names\nRuntimeError: Misaligned dims when attempting to broadcast dims ['N'] and\ndims ['N', None]: dim 'N' appears in a different position from the right\nacross both lists.\n\n```\n\nNote\nIn both of the last examples, it is possible to align the tensors by names\nand then perform the addition. UseTensor.align_as()to align\ntensors by name orTensor.align_to()to align tensors to a custom\ndimension ordering.\nTensor.align_as()\nTensor.align_to()\n\n## Permutes dimensions#\n\nSome operations, likeTensor.t(), permute the order of dimensions. Dimension names\nare attached to individual dimensions so they get permuted as well.\nTensor.t()\nIf the operator takes in positional indexdim, it is also able to take a dimension\nname asdim.\ndim\ndim\nCheck names: Ifdimis passed as a name, check that it exists in the tensor.\ndim\nPropagate names: Permute dimension names in the same way as the dimensions that are\nbeing permuted.\n\n```python\n>>> x = torch.randn(3, 3, names=('N', 'C'))\n>>> x.transpose('N', 'C').names\n('C', 'N')\n\n```\n\n\n## Contracts away dims#\n\nMatrix multiply functions follow some variant of this. Let\u2019s go throughtorch.mm()first and then generalize the rule for batch matrix multiplication.\ntorch.mm()\nFortorch.mm(tensor,other):\ntorch.mm(tensor,other)\nCheck names: None\nPropagate names: result names are(tensor.names[-2],other.names[-1]).\n(tensor.names[-2],other.names[-1])\n\n```python\n>>> x = torch.randn(3, 3, names=('N', 'D'))\n>>> y = torch.randn(3, 3, names=('in', 'out'))\n>>> x.mm(y).names\n('N', 'out')\n\n```\n\nInherently, a matrix multiplication performs a dot product over two dimensions,\ncollapsing them. When two tensors are matrix-multiplied, the contracted dimensions\ndisappear and do not show up in the output tensor.\ntorch.mv(),torch.dot()work in a similar way: name inference does not\ncheck input names and removes the dimensions that are involved in the dot product:\ntorch.mv()\ntorch.dot()\n\n```python\n>>> x = torch.randn(3, 3, names=('N', 'D'))\n>>> y = torch.randn(3, names=('something',))\n>>> x.mv(y).names\n('N',)\n\n```\n\nNow, let\u2019s take a look attorch.matmul(tensor,other). Assume thattensor.dim()>=2andother.dim()>=2.\ntorch.matmul(tensor,other)\ntensor.dim()>=2\nother.dim()>=2\nCheck names: Check that the batch dimensions of the inputs are aligned and broadcastable.\nSeeUnifies names from inputsfor what it means for the inputs to be aligned.\nPropagate names: result names are obtained by unifying the batch dimensions and removing\nthe contracted dimensions:unify(tensor.names[:-2],other.names[:-2])+(tensor.names[-2],other.names[-1]).\nunify(tensor.names[:-2],other.names[:-2])+(tensor.names[-2],other.names[-1])\nExamples:\n\n```python\n# Batch matrix multiply of matrices Tensor['C', 'D'] and Tensor['E', 'F'].\n# 'A', 'B' are batch dimensions.\n>>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))\n>>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F'))\n>>> torch.matmul(x, y).names\n('A', 'B', 'C', 'F')\n\n```\n\nFinally, there are fusedaddversions of many matmul functions. i.e.,addmm()andaddmv(). These are treated as composing name inference for i.e.mm()and\nname inference foradd().\nadd\naddmm()\naddmv()\nmm()\nadd()\n\n## Factory functions#\n\nFactory functions now take a newnamesargument that associates a name\nwith each dimension.\nnames\n\n```python\n>>> torch.zeros(2, 3, names=('N', 'C'))\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], names=('N', 'C'))\n\n```\n\n\n## out function and in-place variants#\n\nA tensor specified as anout=tensor has the following behavior:\nout=\nIf it has no named dimensions, then the names computed from the operation\nget propagated to it.\nIf it has any named dimensions, then the names computed from the operation\nmust be exactly equal to the existing names. Otherwise, the operation errors.\nAll in-place methods modify inputs to have names equal to the computed names\nfrom name inference. For example:\n\n```python\n>>> x = torch.randn(3, 3)\n>>> y = torch.randn(3, 3, names=('N', 'C'))\n>>> x.names\n(None, None)\n\n>>> x += y\n>>> x.names\n('N', 'C')\n\n```\n",
    "url": "https://pytorch.org/docs/stable/name_inference.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e3327a723d52b0419083ada85b77a86c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/meta.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c166c41dc08e0b6a4829026bd8e8069c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/type_info.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2dd3cc6a2dddb53c0d1d546d2b96c179",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/fsdp.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "962e958e4753582524bc799352138f5d",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/future_mod.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c9321840a1e1a2e73a859a0d761dd9fe",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/train_script.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "63e3d12e606d607cc0159472c93248c2",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/rpc.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9687449ef272a5be61e65c219449b576",
    "source": "pytorch_docs",
    "title": "torch.Size \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.Size#\n\nCreated On: Apr 19, 2024 | Last Updated On: Jun 18, 2025\ntorch.Sizeis the result type of a call totorch.Tensor.size(). It describes the size of all dimensions\nof the original tensor. As a subclass oftuple, it supports common sequence operations like indexing and\nlength.\ntorch.Size\ntorch.Tensor.size()\ntuple\nExample:\n\n```python\n    >>> x = torch.ones(10, 20, 30)\n    >>> s = x.size()\n    >>> s\n    torch.Size([10, 20, 30])\n    >>> s[1]\n    20\n    >>> len(s)\n    3\n\n```\n\nReturn number of occurrences of value.\nReturn first index of value.\nRaises ValueError if the value is not present.\nReturns the number of elements atorch.Tensorwith the given size would contain.\ntorch.Tensor\nMore formally, for a tensorx=tensor.ones(10,10)with sizes=torch.Size([10,10]),x.numel()==x.size().numel()==s.numel()==100holds true.\nx=tensor.ones(10,10)\ns=torch.Size([10,10])\nx.numel()==x.size().numel()==s.numel()==100\nExample:\n\n```python\n>>> x=torch.ones(10, 10)\n>>> s=x.size()\n>>> s\ntorch.Size([10, 10])\n>>> s.numel()\n100\n>>> x.numel() == s.numel()\nTrue\n\n```\n\nWarning\nThis function does not return the number of dimensions described bytorch.Size, but instead the number\nof elements atorch.Tensorwith that size would contain.\ntorch.Size\ntorch.Tensor",
    "url": "https://pytorch.org/docs/stable/size.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c3b96c3b6b4e80c80af767705a3a04f8",
    "source": "pytorch_docs",
    "title": "PyTorch 2.0 Performance Dashboard \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch 2.0 Performance Dashboard#\n\nCreated On: May 04, 2023 | Last Updated On: Jun 10, 2025\nAuthor:Bin BaoandHuy Do\nPyTorch 2.0\u2019s performance is tracked nightly on thisdashboard.\nThe performance collection runs on 12 GCP A100 nodes every night. Each node contains a 40GB A100 Nvidia GPU and\na 6-core 2.2GHz Intel Xeon CPU. The corresponding CI workflow file can be foundhere.\n\n## How to read the dashboard?#\n\nThe landing page shows tables for all three benchmark suites we measure,TorchBench,Huggingface, andTIMM,\nand graphs for one benchmark suite with the default setting. For example, the default graphs currently show the AMP\ntraining performance trend in the past 7 days forTorchBench. Droplists on the top of that page can be\nselected to view tables and graphs with different options. In addition to the pass rate, there are 3 key\nperformance metrics reported there:Geometricmeanspeedup,Meancompilationtime, andPeakmemoryfootprintcompressionratio.\nBothGeometricmeanspeedupandPeakmemoryfootprintcompressionratioare compared against\nthe PyTorch eager performance, and the larger the better. Each individual performance number on those tables can be clicked,\nwhich will bring you to a view with detailed numbers for all the tests in that specific benchmark suite.\nTorchBench\nHuggingface\nTIMM\nTorchBench\nGeometricmeanspeedup\nMeancompilationtime\nPeakmemoryfootprintcompressionratio\nGeometricmeanspeedup\nPeakmemoryfootprintcompressionratio\n\n## What is measured on the dashboard?#\n\nAll the dashboard tests are defined in thisfunction.\nThe exact test configurations are subject to change, but at the moment, we measure both inference and training\nperformance with AMP precision on the three benchmark suites. We also measure different settings of TorchInductor,\nincludingdefault,with_cudagraphs(default+cudagraphs), anddynamic(default+dynamic_shapes).\ndefault\nwith_cudagraphs(default+cudagraphs)\ndynamic(default+dynamic_shapes)\n\n## Can I check if my PR affects TorchInductor\u2019s performance on the dashboard before merging?#\n\nIndividual dashboard runs can be triggered manually by clicking theRunworkflowbuttonhereand submitting with your PR\u2019s branch selected. This will kick off a whole dashboard run with your PR\u2019s changes.\nOnce it is done, you can check the results by selecting the corresponding branch name and commit ID\non the performance dashboard UI. Be aware that this is an expensive CI run. With the limited\nresources, please use this functionality wisely.\nRunworkflow\n\n## How can I run any performance test locally?#\n\nThe exact command lines used during a complete dashboard run can be found in any recent CI run logs.\nTheworkflow pageis a good place to look for logs from some of the recent runs.\nIn those logs, you can search for lines likepythonbenchmarks/dynamo/huggingface.py--performance--cold-start-latency--inference--amp--backendinductor--disable-cudagraphs--devicecudaand run them locally if you have a GPU working with PyTorch 2.0.pythonbenchmarks/dynamo/huggingface.py-hwill give you a detailed explanation on options of the benchmarking script.\npythonbenchmarks/dynamo/huggingface.py--performance--cold-start-latency--inference--amp--backendinductor--disable-cudagraphs--devicecuda\npythonbenchmarks/dynamo/huggingface.py-h",
    "url": "https://pytorch.org/docs/stable/torch.compiler_performance_dashboard.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c1ddfffc5085f6420516197857cf9592",
    "source": "pytorch_docs",
    "title": "torch.escape-hatch \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.escape-hatch#\n\n\n## assume_constant_result#\n\nNote\nTags:torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nimport torch._dynamo as torchdynamo\n\n\nclass AssumeConstantResult(torch.nn.Module):\n    \"\"\"\n    Applying `assume_constant_result` decorator to burn make non-tracable code as constant.\n    \"\"\"\n\n    @torchdynamo.assume_constant_result\n    def get_item(self, y):\n        return y.int().item()\n\n    def forward(self, x, y):\n        return x[: self.get_item(y)]\n\nexample_args = (torch.randn(3, 2), torch.tensor(4))\ntags = {\"torch.escape-hatch\"}\nmodel = AssumeConstantResult()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"):\n                 slice_1: \"f32[3, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 4);  x = None\n            return (slice_1,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    slice_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## constrain_as_size_example#\n\nNote\nTags:torch.dynamic-value,torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass ConstrainAsSizeExample(torch.nn.Module):\n    \"\"\"\n    If the value is not known at tracing time, you can provide hint so that we\n    can trace further. Please look at torch._check and torch._check_is_size APIs.\n    torch._check_is_size is used for values that NEED to be used for constructing\n    tensor.\n    \"\"\"\n\n    def forward(self, x):\n        a = x.item()\n        torch._check_is_size(a)\n        torch._check(a <= 5)\n        return torch.zeros((a, 5))\n\n\nexample_args = (torch.tensor(4),)\ntags = {\n    \"torch.dynamic-value\",\n    \"torch.escape-hatch\",\n}\nmodel = ConstrainAsSizeExample()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"i64[]\"):\n                 item: \"Sym(u0)\" = torch.ops.aten.item.default(x);  x = None\n\n             #\n            sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item);  sym_constrain_range_for_size_default = None\n\n                 ge_1: \"Sym(u0 >= 0)\" = item >= 0\n            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\");  ge_1 = _assert_scalar_default = None\n            le_1: \"Sym(u0 <= 5)\" = item <= 5\n            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\");  le_1 = _assert_scalar_default_1 = None\n\n                 zeros: \"f32[u0, 5]\" = torch.ops.aten.zeros.default([item, 5], device = device(type='cpu'), pin_memory = False);  item = None\n            return (zeros,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    zeros: USER_OUTPUT\n\nRange constraints: {u0: VR[0, 5], u1: VR[0, 5]}\n\n```\n\n\n## constrain_as_value_example#\n\nNote\nTags:torch.dynamic-value,torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass ConstrainAsValueExample(torch.nn.Module):\n    \"\"\"\n    If the value is not known at tracing time, you can provide hint so that we\n    can trace further. Please look at torch._check and torch._check_is_size APIs.\n    torch._check is used for values that don't need to be used for constructing\n    tensor.\n    \"\"\"\n\n    def forward(self, x, y):\n        a = x.item()\n        torch._check(a >= 0)\n        torch._check(a <= 5)\n\n        if a < 6:\n            return y.sin()\n        return y.cos()\n\n\nexample_args = (torch.tensor(4), torch.randn(5, 5))\ntags = {\n    \"torch.dynamic-value\",\n    \"torch.escape-hatch\",\n}\nmodel = ConstrainAsValueExample()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"i64[]\", y: \"f32[5, 5]\"):\n                 item: \"Sym(u0)\" = torch.ops.aten.item.default(x);  x = None\n            ge_1: \"Sym(u0 >= 0)\" = item >= 0\n            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\");  ge_1 = _assert_scalar_default = None\n            le_1: \"Sym(u0 <= 5)\" = item <= 5;  item = None\n            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\");  le_1 = _assert_scalar_default_1 = None\n\n                 sin: \"f32[5, 5]\" = torch.ops.aten.sin.default(y);  y = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {u0: VR[0, 5], u1: VR[0, 5]}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/torch.escape-hatch.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8f8dd3d9708f8c71fc4be8cb8c92e11d",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.dynamo_core_concepts.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "431481926a0757645ae879f8be77e6b1",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.overrides.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9c0cdcbe3770cc22864c6787b6e7b7a1",
    "source": "pytorch_docs",
    "title": "Automatic Mixed Precision examples \u2014 PyTorch 2.9 documentation",
    "text": "\n## Automatic Mixed Precision examples#\n\nCreated On: Feb 13, 2020 | Last Updated On: Sep 13, 2024\nOrdinarily, \u201cautomatic mixed precision training\u201d means training withtorch.autocastandtorch.amp.GradScalertogether.\ntorch.autocast\ntorch.amp.GradScaler\nInstances oftorch.autocastenable autocasting for chosen regions.\nAutocasting automatically chooses the precision for operations to improve performance\nwhile maintaining accuracy.\ntorch.autocast\nInstances oftorch.amp.GradScalerhelp perform the steps of\ngradient scaling conveniently.  Gradient scaling improves convergence for networks withfloat16(by default on CUDA and XPU)\ngradients by minimizing gradient underflow, as explainedhere.\ntorch.amp.GradScaler\nfloat16\ntorch.autocastandtorch.amp.GradScalerare modular.\nIn the samples below, each is used as its individual documentation suggests.\ntorch.autocast\ntorch.amp.GradScaler\n(Samples here are illustrative.  See theAutomatic Mixed Precision recipefor a runnable walkthrough.)\nTypical Mixed Precision Training\nWorking with Unscaled Gradients\nGradient clipping\nWorking with Scaled Gradients\nGradient accumulation\nGradient penalty\nWorking with Multiple Models, Losses, and Optimizers\nWorking with Multiple GPUs\nDataParallel in a single process\nDistributedDataParallel, one GPU per process\nDistributedDataParallel, multiple GPUs per process\nAutocast and Custom Autograd Functions\nFunctions with multiple inputs or autocastable ops\nFunctions that need a particulardtype\ndtype\n\n## Typical Mixed Precision Training#\n\n\n```python\n# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\n# Creates a GradScaler once at the beginning of training.\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Runs the forward pass with autocasting.\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        scaler.scale(loss).backward()\n\n        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n        # otherwise, optimizer.step() is skipped.\n        scaler.step(optimizer)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\n```\n\n\n## Working with Unscaled Gradients#\n\nAll gradients produced byscaler.scale(loss).backward()are scaled.  If you wish to modify or inspect\nthe parameters\u2019.gradattributes betweenbackward()andscaler.step(optimizer),  you should\nunscale them first.  For example, gradient clipping manipulates a set of gradients such that their global norm\n(seetorch.nn.utils.clip_grad_norm_()) or maximum magnitude (seetorch.nn.utils.clip_grad_value_())\nis<=<=<=some user-imposed threshold.  If you attempted to clipwithoutunscaling, the gradients\u2019 norm/maximum\nmagnitude would also be scaled, so your requested threshold (which was meant to be the threshold forunscaledgradients) would be invalid.\nscaler.scale(loss).backward()\n.grad\nbackward()\nscaler.step(optimizer)\ntorch.nn.utils.clip_grad_norm_()\ntorch.nn.utils.clip_grad_value_()\nscaler.unscale_(optimizer)unscales gradients held byoptimizer\u2019s assigned parameters.\nIf your model or models contain other parameters that were assigned to another optimizer\n(sayoptimizer2), you may callscaler.unscale_(optimizer2)separately to unscale those\nparameters\u2019 gradients as well.\nscaler.unscale_(optimizer)\noptimizer\noptimizer2\nscaler.unscale_(optimizer2)\n\n## Gradient clipping#\n\nCallingscaler.unscale_(optimizer)before clipping enables you to clip unscaled gradients as usual:\nscaler.unscale_(optimizer)\n\n```python\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # Unscales the gradients of optimizer's assigned params in-place\n        scaler.unscale_(optimizer)\n\n        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n\n        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n        # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n        scaler.step(optimizer)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\n```\n\nscalerrecords thatscaler.unscale_(optimizer)was already called for this optimizer\nthis iteration, soscaler.step(optimizer)knows not to redundantly unscale gradients before\n(internally) callingoptimizer.step().\nscaler\nscaler.unscale_(optimizer)\nscaler.step(optimizer)\noptimizer.step()\nWarning\nunscale_should only be called once per optimizer perstepcall,\nand only after all gradients for that optimizer\u2019s assigned parameters have been accumulated.\nCallingunscale_twice for a given optimizer between eachsteptriggers a RuntimeError.\nunscale_\nstep\nunscale_\nstep\n\n## Working with Scaled Gradients#\n\n\n## Gradient accumulation#\n\nGradient accumulation adds gradients over an effective batch of sizebatch_per_iter*iters_to_accumulate(*num_procsif distributed).  The scale should be calibrated for the effective batch, which means inf/NaN checking,\nstep skipping if inf/NaN grads are found, and scale updates should occur at effective-batch granularity.\nAlso, grads should remain scaled, and the scale factor should remain constant, while grads for a given effective\nbatch are accumulated.  If grads are unscaled (or the scale factor changes) before accumulation is complete,\nthe next backward pass will add scaled grads to unscaled grads (or grads scaled by a different factor)\nafter which it\u2019s impossible to recover the accumulated unscaled gradsstepmust apply.\nbatch_per_iter*iters_to_accumulate\n*num_procs\nstep\nTherefore, if you want tounscale_grads (e.g., to allow clipping unscaled grads),\ncallunscale_just beforestep, after all (scaled) grads for the upcomingstephave been accumulated.  Also, only callupdateat the end of iterations\nwhere you calledstepfor a full effective batch:\nunscale_\nunscale_\nstep\nstep\nupdate\nstep\n\n```python\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for i, (input, target) in enumerate(data):\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n            loss = loss / iters_to_accumulate\n\n        # Accumulates scaled gradients.\n        scaler.scale(loss).backward()\n\n        if (i + 1) % iters_to_accumulate == 0:\n            # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n```\n\n\n## Gradient penalty#\n\nA gradient penalty implementation commonly creates gradients usingtorch.autograd.grad(), combines them to create the penalty value,\nand adds the penalty value to the loss.\ntorch.autograd.grad()\nHere\u2019s an ordinary example of an L2 penalty without gradient scaling or autocasting:\n\n```python\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n\n        # Creates gradients\n        grad_params = torch.autograd.grad(outputs=loss,\n                                          inputs=model.parameters(),\n                                          create_graph=True)\n\n        # Computes the penalty term and adds it to the loss\n        grad_norm = 0\n        for grad in grad_params:\n            grad_norm += grad.pow(2).sum()\n        grad_norm = grad_norm.sqrt()\n        loss = loss + grad_norm\n\n        loss.backward()\n\n        # clip gradients here, if desired\n\n        optimizer.step()\n\n```\n\nTo implement a gradient penaltywithgradient scaling, theoutputsTensor(s)\npassed totorch.autograd.grad()should be scaled.  The resulting gradients\nwill therefore be scaled, and should be unscaled before being combined to create the\npenalty value.\noutputs\ntorch.autograd.grad()\nAlso, the penalty term computation is part of the forward pass, and therefore should be\ninside anautocastcontext.\nautocast\nHere\u2019s how that looks for the same L2 penalty:\n\n```python\nscaler = GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params\n        scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),\n                                                 inputs=model.parameters(),\n                                                 create_graph=True)\n\n        # Creates unscaled grad_params before computing the penalty. scaled_grad_params are\n        # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:\n        inv_scale = 1./scaler.get_scale()\n        grad_params = [p * inv_scale for p in scaled_grad_params]\n\n        # Computes the penalty term and adds it to the loss\n        with autocast(device_type='cuda', dtype=torch.float16):\n            grad_norm = 0\n            for grad in grad_params:\n                grad_norm += grad.pow(2).sum()\n            grad_norm = grad_norm.sqrt()\n            loss = loss + grad_norm\n\n        # Applies scaling to the backward call as usual.\n        # Accumulates leaf gradients that are correctly scaled.\n        scaler.scale(loss).backward()\n\n        # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n        # step() and update() proceed as usual.\n        scaler.step(optimizer)\n        scaler.update()\n\n```\n\n\n## Working with Multiple Models, Losses, and Optimizers#\n\nIf your network has multiple losses, you must callscaler.scaleon each of them individually.\nIf your network has multiple optimizers, you may callscaler.unscale_on any of them individually,\nand you must callscaler.stepon each of them individually.\nscaler.scale\nscaler.unscale_\nscaler.step\nHowever,scaler.updateshould only be called once,\nafter all optimizers used this iteration have been stepped:\nscaler.update\n\n```python\nscaler = torch.amp.GradScaler()\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer0.zero_grad()\n        optimizer1.zero_grad()\n        with autocast(device_type='cuda', dtype=torch.float16):\n            output0 = model0(input)\n            output1 = model1(input)\n            loss0 = loss_fn(2 * output0 + 3 * output1, target)\n            loss1 = loss_fn(3 * output0 - 5 * output1, target)\n\n        # (retain_graph here is unrelated to amp, it's present because in this\n        # example, both backward() calls share some sections of graph.)\n        scaler.scale(loss0).backward(retain_graph=True)\n        scaler.scale(loss1).backward()\n\n        # You can choose which optimizers receive explicit unscaling, if you\n        # want to inspect or modify the gradients of the params they own.\n        scaler.unscale_(optimizer0)\n\n        scaler.step(optimizer0)\n        scaler.step(optimizer1)\n\n        scaler.update()\n\n```\n\nEach optimizer checks its gradients for infs/NaNs and makes an independent decision\nwhether or not to skip the step.  This may result in one optimizer skipping the step\nwhile the other one does not.  Since step skipping occurs rarely (every several hundred iterations)\nthis should not impede convergence.  If you observe poor convergence after adding gradient scaling\nto a multiple-optimizer model, please report a bug.\n\n## Working with Multiple GPUs#\n\nThe issues described here only affectautocast.GradScaler\u2018s usage is unchanged.\nautocast\nGradScaler\n\n## DataParallel in a single process#\n\nEven iftorch.nn.DataParallelspawns threads to run the forward pass on each device.\nThe autocast state is propagated in each one and the following will work:\ntorch.nn.DataParallel\n\n```python\nmodel = MyModel()\ndp_model = nn.DataParallel(model)\n\n# Sets autocast in the main thread\nwith autocast(device_type='cuda', dtype=torch.float16):\n    # dp_model's internal threads will autocast.\n    output = dp_model(input)\n    # loss_fn also autocast\n    loss = loss_fn(output)\n\n```\n\n\n## DistributedDataParallel, one GPU per process#\n\ntorch.nn.parallel.DistributedDataParallel\u2019s documentation recommends one GPU per process for best\nperformance.  In this case,DistributedDataParalleldoes not spawn threads internally,\nso usages ofautocastandGradScalerare not affected.\ntorch.nn.parallel.DistributedDataParallel\nDistributedDataParallel\nautocast\nGradScaler\n\n## DistributedDataParallel, multiple GPUs per process#\n\nHeretorch.nn.parallel.DistributedDataParallelmay spawn a side thread to run the forward pass on each\ndevice, liketorch.nn.DataParallel.The fix is the same:\napply autocast as part of your model\u2019sforwardmethod to ensure it\u2019s enabled in side threads.\ntorch.nn.parallel.DistributedDataParallel\ntorch.nn.DataParallel\nforward\n\n## Autocast and Custom Autograd Functions#\n\nIf your network usescustom autograd functions(subclasses oftorch.autograd.Function), changes are required for\nautocast compatibility if any function\ntorch.autograd.Function\ntakes multiple floating-point Tensor inputs,\nwraps any autocastable op (see theAutocast Op Reference), or\nrequires a particulardtype(for example, if it wrapsCUDA extensionsthat were only compiled fordtype).\ndtype\ndtype\nIn all cases, if you\u2019re importing the function and can\u2019t alter its definition, a safe fallback\nis to disable autocast and force execution infloat32( ordtype) at any points of use where errors occur:\nfloat32\ndtype\n\n```python\nwith autocast(device_type='cuda', dtype=torch.float16):\n    ...\n    with autocast(device_type='cuda', dtype=torch.float16, enabled=False):\n        output = imported_function(input1.float(), input2.float())\n\n```\n\nIf you\u2019re the function\u2019s author (or can alter its definition) a better solution is to use thetorch.amp.custom_fwd()andtorch.amp.custom_bwd()decorators as shown in\nthe relevant case below.\ntorch.amp.custom_fwd()\ntorch.amp.custom_bwd()\n\n## Functions with multiple inputs or autocastable ops#\n\nApplycustom_fwdandcustom_bwd(with no arguments) toforwardandbackwardrespectively.  These ensureforwardexecutes with the current autocast state andbackwardexecutes with the same autocast state asforward(which can prevent type mismatch errors):\ncustom_fwd\ncustom_bwd\nforward\nbackward\nforward\nbackward\nforward\n\n```python\nclass MyMM(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd\n    def forward(ctx, a, b):\n        ctx.save_for_backward(a, b)\n        return a.mm(b)\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad):\n        a, b = ctx.saved_tensors\n        return grad.mm(b.t()), a.t().mm(grad)\n\n```\n\nNowMyMMcan be invoked anywhere, without disabling autocast or manually casting inputs:\nMyMM\n\n```python\nmymm = MyMM.apply\n\nwith autocast(device_type='cuda', dtype=torch.float16):\n    output = mymm(input1, input2)\n\n```\n\n\n## Functions that need a particulardtype#\n\ndtype\nConsider a custom function that requirestorch.float32inputs.\nApplycustom_fwd(device_type='cuda',cast_inputs=torch.float32)toforwardandcustom_bwd(device_type='cuda')tobackward.\nIfforwardruns in an autocast-enabled region, the decorators cast floating-point Tensor\ninputs tofloat32on designated device assigned by the argumentdevice_type,CUDAin this example, and locally disable autocast duringforwardandbackward:\ntorch.float32\ncustom_fwd(device_type='cuda',cast_inputs=torch.float32)\nforward\ncustom_bwd(device_type='cuda')\nbackward\nforward\nfloat32\nforward\nbackward\n\n```python\nclass MyFloat32Func(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd(device_type='cuda', cast_inputs=torch.float32)\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        ...\n        return fwd_output\n    @staticmethod\n    @custom_bwd(device_type='cuda')\n    def backward(ctx, grad):\n        ...\n\n```\n\nNowMyFloat32Funccan be invoked anywhere, without manually disabling autocast or casting inputs:\nMyFloat32Func\n\n```python\nfunc = MyFloat32Func.apply\n\nwith autocast(device_type='cuda', dtype=torch.float16):\n    # func will run in float32, regardless of the surrounding autocast state\n    output = func(input)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/notes/amp_examples.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a9fc6bf83194e2eec2c7fe5293654bf7",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_aot_inductor_minifier.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6980213a2875df4430302d72835c722d",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_inductor_provenance.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "886dbc73a3178ffe934445318ad1a430",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/cpu_threading_torchscript_inference.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f4e552c1f241d2f54d38864a6087a87d",
    "source": "pytorch_docs",
    "title": "Automatic Mixed Precision package - torch.amp \u2014 PyTorch 2.9 documentation",
    "text": "\n## Automatic Mixed Precision package - torch.amp#\n\nCreated On: Jun 12, 2025 | Last Updated On: Jun 12, 2025\ntorch.ampprovides convenience methods for mixed precision,\nwhere some operations use thetorch.float32(float) datatype and other operations\nuse lower precision floating point datatype (lower_precision_fp):torch.float16(half) ortorch.bfloat16. Some ops, like linear layers and convolutions,\nare much faster inlower_precision_fp. Other ops, like reductions, often require the dynamic\nrange offloat32. Mixed precision tries to match each op to its appropriate datatype.\ntorch.amp\ntorch.float32\nfloat\nlower_precision_fp\ntorch.float16\nhalf\ntorch.bfloat16\nlower_precision_fp\nfloat32\nOrdinarily, \u201cautomatic mixed precision training\u201d with datatype oftorch.float16usestorch.autocastandtorch.amp.GradScalertogether, as shown in theAutomatic Mixed Precision examplesandAutomatic Mixed Precision recipe.\nHowever,torch.autocastandtorch.GradScalerare modular, and may be used separately if desired.\nAs shown in the CPU example section oftorch.autocast, \u201cautomatic mixed precision training/inference\u201d on CPU with\ndatatype oftorch.bfloat16only usestorch.autocast.\ntorch.float16\ntorch.autocast\ntorch.amp.GradScaler\ntorch.autocast\ntorch.GradScaler\ntorch.autocast\ntorch.bfloat16\ntorch.autocast\nWarning\ntorch.cuda.amp.autocast(args...)andtorch.cpu.amp.autocast(args...)is deprecated. Please usetorch.amp.autocast(\"cuda\",args...)ortorch.amp.autocast(\"cpu\",args...)instead.torch.cuda.amp.GradScaler(args...)andtorch.cpu.amp.GradScaler(args...)is deprecated. Please usetorch.amp.GradScaler(\"cuda\",args...)ortorch.amp.GradScaler(\"cpu\",args...)instead.\ntorch.cuda.amp.autocast(args...)\ntorch.cpu.amp.autocast(args...)\ntorch.amp.autocast(\"cuda\",args...)\ntorch.amp.autocast(\"cpu\",args...)\ntorch.cuda.amp.GradScaler(args...)\ntorch.cpu.amp.GradScaler(args...)\ntorch.amp.GradScaler(\"cuda\",args...)\ntorch.amp.GradScaler(\"cpu\",args...)\ntorch.autocastandtorch.cpu.amp.autocastare new in version1.10.\ntorch.autocast\ntorch.cpu.amp.autocast\n1.10\nAutocasting\nGradient Scaling\nAutocast Op Reference\nOp Eligibility\nCUDA Op-Specific Behavior\nCUDA Ops that can autocast tofloat16\nfloat16\nCUDA Ops that can autocast tofloat32\nfloat32\nCUDA Ops that promote to the widest input type\nPreferbinary_cross_entropy_with_logitsoverbinary_cross_entropy\nbinary_cross_entropy_with_logits\nbinary_cross_entropy\nXPU Op-Specific Behavior (Experimental)\nXPU Ops that can autocast tofloat16\nfloat16\nXPU Ops that can autocast tofloat32\nfloat32\nXPU Ops that promote to the widest input type\nCPU Op-Specific Behavior\nCPU Ops that can autocast tobfloat16\nbfloat16\nCPU Ops that can autocast tofloat32\nfloat32\nCPU Ops that promote to the widest input type\n\n## Autocasting#\n\nReturn a bool indicating if autocast is available ondevice_type.\ndevice_type\ndevice_type(str) \u2013 Device type to use. Possible values are: \u2018cuda\u2019, \u2018cpu\u2019, \u2018mtia\u2019, \u2018maia\u2019, \u2018xpu\u2019, and so on.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\nbool\nInstances ofautocastserve as context managers or decorators that\nallow regions of your script to run in mixed precision.\nautocast\nIn these regions, ops run in an op-specific dtype chosen by autocast\nto improve performance while maintaining accuracy.\nSee theAutocast Op Referencefor details.\nWhen entering an autocast-enabled region, Tensors may be any type.\nYou should not callhalf()orbfloat16()on your model(s) or inputs when using autocasting.\nhalf()\nbfloat16()\nautocastshould wrap only the forward pass(es) of your network, including the loss\ncomputation(s).  Backward passes under autocast are not recommended.\nBackward ops run in the same type that autocast used for corresponding forward ops.\nautocast\nExample for CUDA Devices:\n\n```python\n# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with torch.autocast(device_type=\"cuda\"):\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n\n```\n\nSee theAutomatic Mixed Precision examplesfor usage (along with gradient scaling)\nin more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).\nautocastcan also be used as a decorator, e.g., on theforwardmethod of your model:\nautocast\nforward\n\n```python\nclass AutocastModel(nn.Module):\n    ...\n\n    @torch.autocast(device_type=\"cuda\")\n    def forward(self, input): ...\n\n```\n\nFloating-point Tensors produced in an autocast-enabled region may befloat16.\nAfter returning to an autocast-disabled region, using them with floating-point\nTensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)\nproduced in the autocast region back tofloat32(or other dtype if desired).\nIf a Tensor from the autocast region is alreadyfloat32, the cast is a no-op,\nand incurs no additional overhead.\nCUDA Example:\nfloat16\nfloat32\nfloat32\n\n```python\n# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n\n```\n\nCPU Training Example:\n\n```python\n# Creates model and optimizer in default precision\nmodel = Net()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Runs the forward pass with autocasting.\n        with torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        loss.backward()\n        optimizer.step()\n\n```\n\nCPU Inference Example:\n\n```python\n# Creates model in default precision\nmodel = Net().eval()\n\nwith torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n    for input in data:\n        # Runs the forward pass with autocasting.\n        output = model(input)\n\n```\n\nCPU Inference Example with Jit Trace:\n\n```python\nclass TestModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        return self.fc1(x)\n\n\ninput_size = 2\nnum_classes = 2\nmodel = TestModel(input_size, num_classes).eval()\n\n# For now, we suggest to disable the Jit Autocast Pass,\n# As the issue: https://github.com/pytorch/pytorch/issues/75956\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.cpu.amp.autocast(cache_enabled=False):\n    model = torch.jit.trace(model, torch.randn(1, input_size))\nmodel = torch.jit.freeze(model)\n# Models Run\nfor _ in range(3):\n    model(torch.randn(1, input_size))\n\n```\n\nType mismatch errorsinan autocast-enabled region are a bug; if this is what you observe,\nplease file an issue.\nautocast(enabled=False)subregions can be nested in autocast-enabled regions.\nLocally disabling autocast can be useful, for example, if you want to force a subregion\nto run in a particulardtype.  Disabling autocast gives you explicit control over\nthe execution type.  In the subregion, inputs from the surrounding region\nshould be cast todtypebefore use:\nautocast(enabled=False)\ndtype\ndtype\n\n```python\n# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    e_float16 = torch.mm(a_float32, b_float32)\n    with torch.autocast(device_type=\"cuda\", enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n\n```\n\nThe autocast state is thread-local.  If you want it enabled in a new thread, the context manager or decorator\nmust be invoked in that thread.  This affectstorch.nn.DataParallelandtorch.nn.parallel.DistributedDataParallelwhen used with more than one GPU per process\n(seeWorking with Multiple GPUs).\ntorch.nn.DataParallel\ntorch.nn.parallel.DistributedDataParallel\ndevice_type(str,required) \u2013 Device type to use. Possible values are: \u2018cuda\u2019, \u2018cpu\u2019, \u2018mtia\u2019, \u2018maia\u2019, \u2018xpu\u2019, and \u2018hpu\u2019.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\nenabled(bool,optional) \u2013 Whether autocasting should be enabled in the region.\nDefault:True\nTrue\ndtype(torch_dtype,optional) \u2013 Data type for ops run in autocast. It uses the default value\n(torch.float16for CUDA andtorch.bfloat16for CPU), given byget_autocast_dtype(), ifdtypeisNone.\nDefault:None\ntorch.float16\ntorch.bfloat16\nget_autocast_dtype()\ndtype\nNone\nNone\ncache_enabled(bool,optional) \u2013 Whether the weight cache inside autocast should be enabled.\nDefault:True\nTrue\nCreate a helper decorator forforwardmethods of custom autograd functions.\nforward\nAutograd functions are subclasses oftorch.autograd.Function.\nSee theexample pagefor more detail.\ntorch.autograd.Function\ndevice_type(str) \u2013 Device type to use. \u2018cuda\u2019, \u2018cpu\u2019, \u2018mtia\u2019, \u2018maia\u2019, \u2018xpu\u2019 and so on.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\ncast_inputs(torch.dtypeor None, optional, default=None) \u2013 If notNone,\nwhenforwardruns in an autocast-enabled region, casts incoming\nfloating-point Tensors to the target dtype (non-floating-point Tensors are not affected),\nthen executesforwardwith autocast disabled.\nIfNone,forward\u2019s internal ops execute with the current autocast state.\ntorch.dtype\nNone\nforward\nforward\nNone\nforward\nNote\nIf the decoratedforwardis called outside an autocast-enabled region,custom_fwdis a no-op andcast_inputshas no effect.\nforward\ncustom_fwd\ncast_inputs\nCreate a helper decorator for backward methods of custom autograd functions.\nAutograd functions are subclasses oftorch.autograd.Function.\nEnsures thatbackwardexecutes with the same autocast state asforward.\nSee theexample pagefor more detail.\ntorch.autograd.Function\nbackward\nforward\ndevice_type(str) \u2013 Device type to use. \u2018cuda\u2019, \u2018cpu\u2019, \u2018mtia\u2019, \u2018maia\u2019, \u2018xpu\u2019 and so on.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\nSeetorch.autocast.\ntorch.autocast\ntorch.cuda.amp.autocast(args...)is deprecated. Please usetorch.amp.autocast(\"cuda\",args...)instead.\ntorch.cuda.amp.autocast(args...)\ntorch.amp.autocast(\"cuda\",args...)\ntorch.cuda.amp.custom_fwd(args...)is deprecated. Please usetorch.amp.custom_fwd(args...,device_type='cuda')instead.\ntorch.cuda.amp.custom_fwd(args...)\ntorch.amp.custom_fwd(args...,device_type='cuda')\ntorch.cuda.amp.custom_bwd(args...)is deprecated. Please usetorch.amp.custom_bwd(args...,device_type='cuda')instead.\ntorch.cuda.amp.custom_bwd(args...)\ntorch.amp.custom_bwd(args...,device_type='cuda')\nSeetorch.autocast.torch.cpu.amp.autocast(args...)is deprecated. Please usetorch.amp.autocast(\"cpu\",args...)instead.\ntorch.autocast\ntorch.cpu.amp.autocast(args...)\ntorch.amp.autocast(\"cpu\",args...)\n\n## Gradient Scaling#\n\nIf the forward pass for a particular op hasfloat16inputs, the backward pass for\nthat op will producefloat16gradients.\nGradient values with small magnitudes may not be representable infloat16.\nThese values will flush to zero (\u201cunderflow\u201d), so the update for the corresponding parameters will be lost.\nfloat16\nfloat16\nfloat16\nTo prevent underflow, \u201cgradient scaling\u201d multiplies the network\u2019s loss(es) by a scale factor and\ninvokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are\nthen scaled by the same factor. In other words, gradient values have a larger magnitude,\nso they don\u2019t flush to zero.\nEach parameter\u2019s gradient (.gradattribute) should be unscaled before the optimizer\nupdates the parameters, so the scale factor does not interfere with the learning rate.\n.grad\nNote\nAMP/fp16 may not work for every model! For example, most bf16-pretrained models cannot operate in\nthe fp16 numerical range of max 65504 and will cause gradients to overflow instead of underflow. In\nthis case, the scale factor may decrease under 1 as an attempt to bring gradients to a number\nrepresentable in the fp16 dynamic range. While one may expect the scale to always be above 1, our\nGradScaler does NOT make this guarantee to maintain performance. If you encounter NaNs in your loss\nor gradients when running with AMP/fp16, verify your model is compatible.\nSeetorch.amp.GradScaler.torch.cuda.amp.GradScaler(args...)is deprecated. Please usetorch.amp.GradScaler(\"cuda\",args...)instead.\ntorch.amp.GradScaler\ntorch.cuda.amp.GradScaler(args...)\ntorch.amp.GradScaler(\"cuda\",args...)\nSeetorch.amp.GradScaler.torch.cpu.amp.GradScaler(args...)is deprecated. Please usetorch.amp.GradScaler(\"cpu\",args...)instead.\ntorch.amp.GradScaler\ntorch.cpu.amp.GradScaler(args...)\ntorch.amp.GradScaler(\"cpu\",args...)\n\n## Autocast Op Reference#\n\n\n## Op Eligibility#\n\nOps that run infloat64or non-floating-point dtypes are not eligible, and will\nrun in these types whether or not autocast is enabled.\nfloat64\nOnly out-of-place ops and Tensor methods are eligible.\nIn-place variants and calls that explicitly supply anout=...Tensor\nare allowed in autocast-enabled regions, but won\u2019t go through autocasting.\nFor example, in an autocast-enabled regiona.addmm(b,c)can autocast,\nbuta.addmm_(b,c)anda.addmm(b,c,out=d)cannot.\nFor best performance and stability, prefer out-of-place ops in autocast-enabled\nregions.\nout=...\na.addmm(b,c)\na.addmm_(b,c)\na.addmm(b,c,out=d)\nOps called with an explicitdtype=...argument are not eligible,\nand will produce output that respects thedtypeargument.\ndtype=...\ndtype\n\n## CUDA Op-Specific Behavior#\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of atorch.nn.Module,\nas a function, or as atorch.Tensormethod. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\ntorch.nn.Module\ntorch.Tensor\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type\nin which unlisted ops run if they\u2019re downstream from autocasted ops.\nIf an op is unlisted, we assume it\u2019s numerically stable infloat16.\nIf you believe an unlisted op is numerically unstable infloat16,\nplease file an issue.\nfloat16\nfloat16\nfloat16\n__matmul__,addbmm,addmm,addmv,addr,baddbmm,bmm,chain_matmul,multi_dot,conv1d,conv2d,conv3d,conv_transpose1d,conv_transpose2d,conv_transpose3d,GRUCell,linear,LSTMCell,matmul,mm,mv,prelu,RNNCell\n__matmul__\naddbmm\naddmm\naddmv\naddr\nbaddbmm\nbmm\nchain_matmul\nmulti_dot\nconv1d\nconv2d\nconv3d\nconv_transpose1d\nconv_transpose2d\nconv_transpose3d\nGRUCell\nlinear\nLSTMCell\nmatmul\nmm\nmv\nprelu\nRNNCell\nfloat32\n__pow__,__rdiv__,__rpow__,__rtruediv__,acos,asin,binary_cross_entropy_with_logits,cosh,cosine_embedding_loss,cdist,cosine_similarity,cross_entropy,cumprod,cumsum,dist,erfinv,exp,expm1,group_norm,hinge_embedding_loss,kl_div,l1_loss,layer_norm,log,log_softmax,log10,log1p,log2,margin_ranking_loss,mse_loss,multilabel_margin_loss,multi_margin_loss,nll_loss,norm,normalize,pdist,poisson_nll_loss,pow,prod,reciprocal,rsqrt,sinh,smooth_l1_loss,soft_margin_loss,softmax,softmin,softplus,sum,renorm,tan,triplet_margin_loss\n__pow__\n__rdiv__\n__rpow__\n__rtruediv__\nacos\nasin\nbinary_cross_entropy_with_logits\ncosh\ncosine_embedding_loss\ncdist\ncosine_similarity\ncross_entropy\ncumprod\ncumsum\ndist\nerfinv\nexp\nexpm1\ngroup_norm\nhinge_embedding_loss\nkl_div\nl1_loss\nlayer_norm\nlog\nlog_softmax\nlog10\nlog1p\nlog2\nmargin_ranking_loss\nmse_loss\nmultilabel_margin_loss\nmulti_margin_loss\nnll_loss\nnorm\nnormalize\npdist\npoisson_nll_loss\npow\nprod\nreciprocal\nrsqrt\nsinh\nsmooth_l1_loss\nsoft_margin_loss\nsoftmax\nsoftmin\nsoftplus\nsum\nrenorm\ntan\ntriplet_margin_loss\nThese ops don\u2019t require a particular dtype for stability, but take multiple inputs\nand require that the inputs\u2019 dtypes match. If all of the inputs arefloat16, the op runs infloat16. If any of the inputs isfloat32,\nautocast casts all inputs tofloat32and runs the op infloat32.\nfloat16\nfloat16\nfloat32\nfloat32\nfloat32\naddcdiv,addcmul,atan2,bilinear,cross,dot,grid_sample,index_put,scatter_add,tensordot\naddcdiv\naddcmul\natan2\nbilinear\ncross\ndot\ngrid_sample\nindex_put\nscatter_add\ntensordot\nSome ops not listed here (e.g., binary ops likeadd) natively promote\ninputs without autocasting\u2019s intervention. If inputs are a mixture offloat16andfloat32, these ops run infloat32and producefloat32output,\nregardless of whether autocast is enabled.\nadd\nfloat16\nfloat32\nfloat32\nfloat32\nbinary_cross_entropy_with_logits\nbinary_cross_entropy\nThe backward passes oftorch.nn.functional.binary_cross_entropy()(andtorch.nn.BCELoss, which wraps it)\ncan produce gradients that aren\u2019t representable infloat16. In autocast-enabled regions, the forward input\nmay befloat16, which means the backward gradient must be representable infloat16(autocastingfloat16forward inputs tofloat32doesn\u2019t help, because that cast must be reversed in backward).\nTherefore,binary_cross_entropyandBCELossraise an error in autocast-enabled regions.\ntorch.nn.functional.binary_cross_entropy()\ntorch.nn.BCELoss\nfloat16\nfloat16\nfloat16\nfloat16\nfloat32\nbinary_cross_entropy\nBCELoss\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers usingtorch.nn.functional.binary_cross_entropy_with_logits()ortorch.nn.BCEWithLogitsLoss.binary_cross_entropy_with_logitsandBCEWithLogitsare safe to autocast.\ntorch.nn.functional.binary_cross_entropy_with_logits()\ntorch.nn.BCEWithLogitsLoss\nbinary_cross_entropy_with_logits\nBCEWithLogits\n\n## XPU Op-Specific Behavior (Experimental)#\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of atorch.nn.Module,\nas a function, or as atorch.Tensormethod. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\ntorch.nn.Module\ntorch.Tensor\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type\nin which unlisted ops run if they\u2019re downstream from autocasted ops.\nIf an op is unlisted, we assume it\u2019s numerically stable infloat16.\nIf you believe an unlisted op is numerically unstable infloat16,\nplease file an issue.\nfloat16\nfloat16\nfloat16\naddbmm,addmm,addmv,addr,baddbmm,bmm,chain_matmul,multi_dot,conv1d,conv2d,conv3d,conv_transpose1d,conv_transpose2d,conv_transpose3d,GRUCell,linear,LSTMCell,matmul,mm,mv,RNNCell\naddbmm\naddmm\naddmv\naddr\nbaddbmm\nbmm\nchain_matmul\nmulti_dot\nconv1d\nconv2d\nconv3d\nconv_transpose1d\nconv_transpose2d\nconv_transpose3d\nGRUCell\nlinear\nLSTMCell\nmatmul\nmm\nmv\nRNNCell\nfloat32\n__pow__,__rdiv__,__rpow__,__rtruediv__,binary_cross_entropy_with_logits,cosine_embedding_loss,cosine_similarity,cumsum,dist,exp,group_norm,hinge_embedding_loss,kl_div,l1_loss,layer_norm,log,log_softmax,margin_ranking_loss,nll_loss,normalize,poisson_nll_loss,pow,reciprocal,rsqrt,soft_margin_loss,softmax,softmin,sum,triplet_margin_loss\n__pow__\n__rdiv__\n__rpow__\n__rtruediv__\nbinary_cross_entropy_with_logits\ncosine_embedding_loss\ncosine_similarity\ncumsum\ndist\nexp\ngroup_norm\nhinge_embedding_loss\nkl_div\nl1_loss\nlayer_norm\nlog\nlog_softmax\nmargin_ranking_loss\nnll_loss\nnormalize\npoisson_nll_loss\npow\nreciprocal\nrsqrt\nsoft_margin_loss\nsoftmax\nsoftmin\nsum\ntriplet_margin_loss\nThese ops don\u2019t require a particular dtype for stability, but take multiple inputs\nand require that the inputs\u2019 dtypes match. If all of the inputs arefloat16, the op runs infloat16. If any of the inputs isfloat32,\nautocast casts all inputs tofloat32and runs the op infloat32.\nfloat16\nfloat16\nfloat32\nfloat32\nfloat32\nbilinear,cross,grid_sample,index_put,scatter_add,tensordot\nbilinear\ncross\ngrid_sample\nindex_put\nscatter_add\ntensordot\nSome ops not listed here (e.g., binary ops likeadd) natively promote\ninputs without autocasting\u2019s intervention. If inputs are a mixture offloat16andfloat32, these ops run infloat32and producefloat32output,\nregardless of whether autocast is enabled.\nadd\nfloat16\nfloat32\nfloat32\nfloat32\n\n## CPU Op-Specific Behavior#\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of atorch.nn.Module,\nas a function, or as atorch.Tensormethod. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\ntorch.nn.Module\ntorch.Tensor\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type\nin which unlisted ops run if they\u2019re downstream from autocasted ops.\nIf an op is unlisted, we assume it\u2019s numerically stable inbfloat16.\nIf you believe an unlisted op is numerically unstable inbfloat16,\nplease file an issue.float16shares the lists ofbfloat16.\nbfloat16\nbfloat16\nfloat16\nbfloat16\nbfloat16\nconv1d,conv2d,conv3d,bmm,mm,linalg_vecdot,baddbmm,addmm,addbmm,linear,matmul,_convolution,conv_tbc,mkldnn_rnn_layer,conv_transpose1d,conv_transpose2d,conv_transpose3d,prelu,scaled_dot_product_attention,_native_multi_head_attention\nconv1d\nconv2d\nconv3d\nbmm\nmm\nlinalg_vecdot\nbaddbmm\naddmm\naddbmm\nlinear\nmatmul\n_convolution\nconv_tbc\nmkldnn_rnn_layer\nconv_transpose1d\nconv_transpose2d\nconv_transpose3d\nprelu\nscaled_dot_product_attention\n_native_multi_head_attention\nfloat32\navg_pool3d,binary_cross_entropy,grid_sampler,grid_sampler_2d,_grid_sampler_2d_cpu_fallback,grid_sampler_3d,polar,prod,quantile,nanquantile,stft,cdist,trace,view_as_complex,cholesky,cholesky_inverse,cholesky_solve,inverse,lu_solve,orgqr,inverse,ormqr,pinverse,max_pool3d,max_unpool2d,max_unpool3d,adaptive_avg_pool3d,reflection_pad1d,reflection_pad2d,replication_pad1d,replication_pad2d,replication_pad3d,mse_loss,cosine_embedding_loss,nll_loss,nll_loss2d,hinge_embedding_loss,poisson_nll_loss,cross_entropy_loss,l1_loss,huber_loss,margin_ranking_loss,soft_margin_loss,triplet_margin_loss,multi_margin_loss,ctc_loss,kl_div,multilabel_margin_loss,binary_cross_entropy_with_logits,fft_fft,fft_ifft,fft_fft2,fft_ifft2,fft_fftn,fft_ifftn,fft_rfft,fft_irfft,fft_rfft2,fft_irfft2,fft_rfftn,fft_irfftn,fft_hfft,fft_ihfft,linalg_cond,linalg_matrix_rank,linalg_solve,linalg_cholesky,linalg_svdvals,linalg_eigvals,linalg_eigvalsh,linalg_inv,linalg_householder_product,linalg_tensorinv,linalg_tensorsolve,fake_quantize_per_tensor_affine,geqrf,_lu_with_info,qr,svd,triangular_solve,fractional_max_pool2d,fractional_max_pool3d,adaptive_max_pool3d,multilabel_margin_loss_forward,linalg_qr,linalg_cholesky_ex,linalg_svd,linalg_eig,linalg_eigh,linalg_lstsq,linalg_inv_ex\navg_pool3d\nbinary_cross_entropy\ngrid_sampler\ngrid_sampler_2d\n_grid_sampler_2d_cpu_fallback\ngrid_sampler_3d\npolar\nprod\nquantile\nnanquantile\nstft\ncdist\ntrace\nview_as_complex\ncholesky\ncholesky_inverse\ncholesky_solve\ninverse\nlu_solve\norgqr\ninverse\normqr\npinverse\nmax_pool3d\nmax_unpool2d\nmax_unpool3d\nadaptive_avg_pool3d\nreflection_pad1d\nreflection_pad2d\nreplication_pad1d\nreplication_pad2d\nreplication_pad3d\nmse_loss\ncosine_embedding_loss\nnll_loss\nnll_loss2d\nhinge_embedding_loss\npoisson_nll_loss\ncross_entropy_loss\nl1_loss\nhuber_loss\nmargin_ranking_loss\nsoft_margin_loss\ntriplet_margin_loss\nmulti_margin_loss\nctc_loss\nkl_div\nmultilabel_margin_loss\nbinary_cross_entropy_with_logits\nfft_fft\nfft_ifft\nfft_fft2\nfft_ifft2\nfft_fftn\nfft_ifftn\nfft_rfft\nfft_irfft\nfft_rfft2\nfft_irfft2\nfft_rfftn\nfft_irfftn\nfft_hfft\nfft_ihfft\nlinalg_cond\nlinalg_matrix_rank\nlinalg_solve\nlinalg_cholesky\nlinalg_svdvals\nlinalg_eigvals\nlinalg_eigvalsh\nlinalg_inv\nlinalg_householder_product\nlinalg_tensorinv\nlinalg_tensorsolve\nfake_quantize_per_tensor_affine\ngeqrf\n_lu_with_info\nqr\nsvd\ntriangular_solve\nfractional_max_pool2d\nfractional_max_pool3d\nadaptive_max_pool3d\nmultilabel_margin_loss_forward\nlinalg_qr\nlinalg_cholesky_ex\nlinalg_svd\nlinalg_eig\nlinalg_eigh\nlinalg_lstsq\nlinalg_inv_ex\nThese ops don\u2019t require a particular dtype for stability, but take multiple inputs\nand require that the inputs\u2019 dtypes match. If all of the inputs arebfloat16, the op runs inbfloat16. If any of the inputs isfloat32,\nautocast casts all inputs tofloat32and runs the op infloat32.\nbfloat16\nbfloat16\nfloat32\nfloat32\nfloat32\ncat,stack,index_copy\ncat\nstack\nindex_copy\nSome ops not listed here (e.g., binary ops likeadd) natively promote\ninputs without autocasting\u2019s intervention. If inputs are a mixture ofbfloat16andfloat32, these ops run infloat32and producefloat32output,\nregardless of whether autocast is enabled.\nadd\nbfloat16\nfloat32\nfloat32\nfloat32",
    "url": "https://pytorch.org/docs/stable/amp.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d7cbd6271dca80ed9b3619b08f0eb872",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/onnx_dynamo_mlp_model.png",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "04f6bd73f632d6cba4f78439bd48b059",
    "source": "pytorch_docs",
    "title": "Extending PyTorch \u2014 PyTorch 2.9 documentation",
    "text": "\n## Extending PyTorch#\n\nCreated On: Jan 16, 2017 | Last Updated On: May 07, 2025\nIn this note we\u2019ll cover ways of extendingtorch.nn,torch.autograd,torch, and writing custom C++ extensions.\ntorch.nn\ntorch.autograd\ntorch\n\n## Adding new operators#\n\nPyTorch offers a large library of operators that work on Tensors (e.g.torch.add(),torch.sum(), etc). However, you may wish to bring a new custom operation to PyTorch\nand have it behave like PyTorch\u2019s built-in operators. In order to do so, you must\nregister the custom operation with PyTorch via the Pythontorch.libraryor C++ TORCH_LIBRARY\nAPIs.\ntorch.add()\ntorch.sum()\nPlease seePyTorch Custom Operators Landing Pagefor more details.\n\n## Extendingtorch.autograd#\n\ntorch.autograd\nAdding operations toautogradrequires implementing a newFunctionsubclass for each operation. Recall that Functions\nare whatautograduses to encode the operation history and compute\ngradients.\nautograd\nFunction\nautograd\nThe first part of this doc is focused on backward mode AD as it is the most widely used\nfeature. A section at the end discusses the extensions for forward mode AD.\n\n## When to use#\n\nIn general, implement a custom function if you want to perform computations in your model\nthat are not differentiable or rely on non-PyTorch libraries (e.g., NumPy), but\nstill wish for your operation to chain with other ops and work with the autograd engine.\nIn some situations, custom functions can also be used to improve performance and\nmemory usage: If you implemented your forward and backward passes using aC++ extension,\nyou can wrap them inFunctionto interface with the autograd\nengine. If you\u2019d like to reduce the number of buffers saved for the backward pass,\ncustom functions can be used to combine ops together.\nFunction\n\n## When not to use#\n\nIf you can already write your function in terms of PyTorch\u2019s built-in ops, its\nbackward graph is (most likely) already able to be recorded by autograd. In this case, you do\nnot need to implement the backward function yourself. Consider using a plain\nold Python function.\nIf you need to maintain state, i.e., trainable parameters, you should (also) use a\ncustom module. See the section below for more information on extendingtorch.nn.\ntorch.nn\nIf you\u2019d like to alter the gradients during the backward pass or perform a side\neffect, consider registering atensororModulehook.\n\n## How to use#\n\nTake the following steps:\n1. SubclassFunctionand implement theforward(),\n(optional)setup_context()andbackward()methods.\n2. Call the proper methods on thectxargument.\n3. Declare whether your function supportsdouble backward.\n4. Validate whether your gradients are correct using gradcheck.\nFunction\nforward()\nsetup_context()\nbackward()\nStep 1:After subclassingFunction, you\u2019ll need to define 3 methods:\nFunction\nforward()is the code that performs the operation. It can take\nas many arguments as you want, with some of them being optional, if you\nspecify the default values. All kinds of Python objects are accepted here.Tensorarguments that track history (i.e., withrequires_grad=True) will be converted to ones that don\u2019t track history\nbefore the call, and their use will be registered in the graph. Note that this\nlogic won\u2019t traverse lists/dicts/any other data structures and will only\nconsider tensors that are direct arguments to the call. You can\nreturn either a singleTensoroutput, or atupleof\ntensors if there are multiple outputs. Also, please refer to the\ndocs ofFunctionto find descriptions of useful methods that can be\ncalled only fromforward().\nforward()\nTensor\nrequires_grad=True\nTensor\ntuple\nFunction\nforward()\nsetup_context()(optional). One can either write a \u201ccombined\u201dforward()that\naccepts actxobject or (as of PyTorch 2.0) a separateforward()that does\nnot acceptctxand asetup_context()method where thectxmodification happens.\nTheforward()should have the compute andsetup_context()should\nonly be responsible for thectxmodification (and not have any compute).\nIn general the separateforward()andsetup_context()is closer to how\nPyTorch native operations work and therefore more composable with various PyTorch subsystems.\nSeeCombined or separate forward() and setup_context()for more details.\nsetup_context()\nforward()\nctx\nforward()\nctx\nsetup_context()\nctx\nforward()\nsetup_context()\nctx\nforward()\nsetup_context()\nbackward()(orvjp()) defines the gradient formula.\nIt will be given as manyTensorarguments as there were outputs, with each\nof them representing gradient w.r.t. that output. It is important NEVER to modify\nthese in-place. It should return as many tensors as there\nwere inputs, with each of them containing the gradient w.r.t. its\ncorresponding input. If your inputs didn\u2019t require gradient\n(needs_input_gradis a tuple of booleans indicating\nwhether each input needs gradient computation), or were non-Tensorobjects, you can returnpython:None. Also, if you have optional\narguments toforward()you can return more gradients than there\nwere inputs, as long as they\u2019re allNone.\nbackward()\nvjp()\nTensor\nneeds_input_grad\nTensor\npython:None\nforward()\nNone\nStep 2:It is your responsibility to use the functions inctxproperly in order to ensure that the newFunctionworks properly with\nthe autograd engine.\nctx\nFunction\nsave_for_backward()should be\nused to save any tensors needed for the backward pass (as opposed to\ndirectly onctx). You cannot usesave_for_backwardfor non-tensors;\nyou should store those directly onctx.\nsave_for_backward()\nctx\nsave_for_backward\nctx\nSaving tensors viasave_for_backward:\n1. Allows the autograd engine to clear\nthem as soon as the backward computation of theautograd.Functioncompletes.\n(If a tensor is stored directly onctxit will unnecessarily remain alive for the lifetime of the autograd graph \u2013\ntypically until the end of the iteration.)\n2. Helps avoid certain reference cycles, (e.g., since the tensor\noutput of theautograd.Functionitself keeps a reference to the ctx).\n3. Is important for compatibility with\nfeatures like activation checkpointing and offloading that rely ontorch.autograd.graph.saved_tensors_hooks.\nsave_for_backward\nautograd.Function\nctx\nautograd.Function\ntorch.autograd.graph.saved_tensors_hooks\nIf tensors that are neither inputs nor outputs are saved for backward yourFunctionmay not support double backward (see step 3).\nFunction\nmark_dirty()must be used to\nmark any input that is modified inplace by the forward function.\nmark_dirty()\nmark_non_differentiable()must\nbe used to tell the engine if an output is not differentiable. By\ndefault all output tensors that are of differentiable type will be set\nto require gradient. Tensors of non-differentiable type (i.e., integral types)\nare never marked as requiring gradients.\nmark_non_differentiable()\nset_materialize_grads()can be\nused to tell the autograd engine to optimize gradient computations in the cases where\nthe output does not depend on the input by not materializing grad tensors given to backward\nfunction. That is, if set to False, None object in Python or \u201cundefined tensor\u201d (tensor x for\nwhich x.defined() is False) in C++ will not be converted to a tensor filled with zeros prior\nto calling backward, and so your code will need to handle such objects as if they were\ntensors filled with zeros. The default value of this setting is True.\nset_materialize_grads()\nStep 3:If yourFunctiondoes not support double backward\nyou should explicitly declare this by decorating backward with theonce_differentiable(). With this decorator, attempts to\nperform double backward through your function will produce an error.\nSee our double backward tutorial for more information on double backward.\nFunction\nonce_differentiable()\nStep 4:It is recommended that you usetorch.autograd.gradcheck()to check whether your backward function correctly computes gradients of the\nforward by computing the Jacobian matrix using your backward function and\ncomparing the value element-wise with the Jacobian computed numerically using\nfinite-differencing.\ntorch.autograd.gradcheck()\n\n## Example#\n\nBelow you can find code for aLinearfunction, with\nadditional comments:\nLinear\n\n```python\n# Inherit from Function\nclass LinearFunction(Function):\n\n    # Note that forward, setup_context, and backward are @staticmethods\n    @staticmethod\n    def forward(input, weight, bias):\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    # inputs is a Tuple of all of the inputs passed to forward.\n    # output is the output of the forward().\n    def setup_context(ctx, inputs, output):\n        input, weight, bias = inputs\n        ctx.save_for_backward(input, weight, bias)\n\n    # This function has only a single output, so it gets only one gradient\n    @staticmethod\n    def backward(ctx, grad_output):\n        # This is a pattern that is very convenient - at the top of backward\n        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n        # None. Thanks to the fact that additional trailing Nones are\n        # ignored, the return statement is simple even when the function has\n        # optional inputs.\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        # These needs_input_grad checks are optional and there only to\n        # improve efficiency. If you want to make your code simpler, you can\n        # skip them. Returning gradients for inputs that don't require it is\n        # not an error.\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias\n\n```\n\nNow, to make it easier to use these custom ops, we recommend either aliasing\nthem or wrapping them in a function. Wrapping in a function lets us support\ndefault arguments and keyword arguments:\n\n```python\n# Option 1: alias\nlinear = LinearFunction.apply\n\n# Option 2: wrap in a function, to support default args and keyword args.\ndef linear(input, weight, bias=None):\n    return LinearFunction.apply(input, weight, bias)\n\n```\n\nHere, we give an additional example of a function that is parametrized by\nnon-Tensor arguments:\n\n```python\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        # ctx is a context object that can be used to stash information\n        # for backward computation\n        tensor, constant = inputs\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None\n\n```\n\nAnd here, we optimize the above example by calling set_materialize_grads(False):\n\n```python\nclass MulConstant(Function):\n    @staticmethod\n    def forward(tensor, constant):\n        return tensor * constant\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        tensor, constant = inputs\n        ctx.set_materialize_grads(False)\n        ctx.constant = constant\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Here we must handle None grad_output tensor. In this case we\n        # can skip unnecessary computations and just return None.\n        if grad_output is None:\n            return None, None\n\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n        return grad_output * ctx.constant, None\n\n```\n\nIf you need any \u201cintermediate\u201d Tensors computed inforward()to be saved,\neither they must be returned as outputs, or combineforwardandsetup_context()(seeCombined or separate forward() and setup_context()).\nNote that this means if you want gradients to flow through those intermediate values, you\nneed to define the gradient formula for them (see alsothe double backward tutorial):\nforward()\nforward\nsetup_context()\n\n```python\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        # We wish to save dx for backward. In order to do so, it must\n        # be returned as an output.\n        dx = 3 * x ** 2\n        result = x ** 3\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`,\n        # which is grad_dx * 6 * x.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\n# Wrap MyCube in a function so that it is clearer what the output is\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result\n\n```\n\nNote\nInputs tobackward, i.e.,grad_output, can also be tensors that\ntrack history. So ifbackwardis implemented with differentiable\noperations, (e.g., invocation of another customFunction), higher order derivatives will work.\nIn this case, the tensors saved withsave_for_backwardcan also be used\nin the backward and have gradients flowing back but tensors saved in thectxwon\u2019t have gradients flowing back for them.\nIf you need gradients to flow back for a Tensor saved in thectx, you should\nmake it an output of the customFunctionand save it withsave_for_backward.\nbackward\ngrad_output\nbackward\nFunction\nsave_for_backward\nctx\nctx\nFunction\nsave_for_backward\nYou probably want to check if the backward method you implemented actually\ncomputes the derivatives of your function. It is possible by comparing with\nnumerical approximations using small finite differences:\n\n```python\nfrom torch.autograd import gradcheck\n\n# gradcheck takes a tuple of tensors as input, check if your gradient\n# evaluated with these tensors are close enough to numerical\n# approximations and returns True if they all verify this condition.\ninput = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))\ntest = gradcheck(linear, input, eps=1e-6, atol=1e-4)\nprint(test)\n\n```\n\nSeeNumerical gradient checkingfor more details on finite-difference gradient comparisons.\nIf your function is used in higher order derivatives (differentiating the backward pass) you\ncan use thegradgradcheckfunction from the same package to check higher order derivatives.\ngradgradcheck\n\n## Combined or separateforward()andsetup_context()#\n\nforward()\nsetup_context()\nThere are two main ways to defineFunction. Either:\nFunction\ndefine aforward()that combines the forward compute logic withsetup_context()\nforward()\nsetup_context()\n(as of PyTorch 2.0) define a separateforward()andsetup_context()\nforward()\nsetup_context()\nWe recommend the second option (separateforward()andsetup_context())\nbecause that is closer to how PyTorch native operations are implemented and it composes\nwithtorch.functransforms. However, we plan to support both approaches going forward;\ncombiningforward()withsetup_context(): leads to more flexibility since\nyou are able to save intermediates without returning them as output.\nforward()\nsetup_context()\ntorch.func\nforward()\nsetup_context()\nPlease see the previous section for how to defineFunctionwith separateforward()andsetup_context().\nFunction\nforward()\nsetup_context()\nHere is an example of how to define aFunctionwith combinedforward()andsetup_context():\nFunction\nforward()\nsetup_context()\n\n```python\nclass LinearFunction(Function):\n    @staticmethod\n    # ctx is the first argument to forward\n    def forward(ctx, input, weight, bias=None):\n        # The forward pass can use ctx.\n        ctx.save_for_backward(input, weight, bias)\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0)\n\n        return grad_input, grad_weight, grad_bias\n\n```\n\n\n## Forward mode AD#\n\nOverriding the forward mode AD formula has a very similar API with some different subtleties.\nYou can implement thejvp()function.\njvp()\nIt will be given as manyTensorarguments as there were inputs, with each\nof them representing gradient w.r.t. that input. It should return as many tensors as there\nwere outputs, with each of them containing the gradient w.r.t. its corresponding output.\nThejvp()will be called just after theforward()method, before theapply()returns.\nTensor\njvp()\nforward()\napply()\njvp()has a few subtle differences with thebackward()function:\njvp()\nbackward()\nYou can use thectxto pass any data from theforward()to thejvp()function.\nIf that state will not be needed for thebackward(),\nyou can explicitly free it by doingdelctx.fooat the end of thejvp()function.\nforward()\njvp()\nbackward()\ndelctx.foo\njvp()\nThe implementation ofjvp()must be backward differentiable or explicitly check that\nnone of the given forward mode gradient hasrequires_gradset.\njvp()\nrequires_grad\nThejvp()function must match the view/inplace behavior offorward().\nFor example, if theith input is modified inplace, then theith gradient must be updated inplace.\nSimilarly, if thejth output is a view of thekth input. Then the returnedjth output gradient must be\na view of the givenkth input gradient.\njvp()\nforward()\ni\ni\nj\nk\nj\nk\nBecause the user cannot specify which gradient needs to be computed, thejvp()function should\nalways compute gradients for all the outputs.\njvp()\nThe forward mode gradients do respect the flag set byset_materialize_grads()and you can getNoneinput gradients when this is disabled.\nset_materialize_grads()\n\n## torch.functransforms and/ortorch.vmap()#\n\ntorch.func\ntorch.vmap()\nPlease seeExtending torch.func with autograd.Functionfor details.\n\n## Extendingtorch.nn#\n\ntorch.nn\nnnexports two kinds of interfaces - modules and their functional\nversions. You can extend it in both ways, but we recommend using modules for\nall kinds of layers, that hold any parameters or buffers, and recommend using\na functional form parameter-less operations like activation functions, pooling,\netc.\nnn\nAdding a functional version of an operation is already fully covered in the\nsection above.\n\n## Adding aModule#\n\nModule\nSincennheavily utilizesautograd, adding a newModulerequires implementing aFunctionthat performs the operation and can compute the gradient. From now on let\u2019s\nassume that we want to implement aLinearmodule and we have the function\nimplemented as in the listing above. There\u2019s very little code required to\nadd this. Now, there are two functions that need to be implemented:\nnn\nautograd\nModule\nFunction\nLinear\n__init__(optional) - takes in arguments such as kernel sizes, numbers\nof features, etc. and initializes parameters and buffers.\n__init__\nforward()- instantiates aFunctionand\nuses it to perform the operation. It\u2019s very similar to a functional wrapper\nshown above.\nforward()\nFunction\nThis is how aLinearmodule can be implemented:\nLinear\n\n```python\nclass Linear(nn.Module):\n    def __init__(self, input_features, output_features, bias=True):\n        super().__init__()\n        self.input_features = input_features\n        self.output_features = output_features\n\n        # nn.Parameter is a special kind of Tensor, that will get\n        # automatically registered as Module's parameter once it's assigned\n        # as an attribute. Parameters and buffers need to be registered, or\n        # they won't appear in .parameters() (doesn't apply to buffers), and\n        # won't be converted when e.g. .cuda() is called. You can use\n        # .register_buffer() to register buffers.\n        # nn.Parameters require gradients by default.\n        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n        if bias:\n            self.bias = nn.Parameter(torch.empty(output_features))\n        else:\n            # You should always register all possible parameters, but the\n            # optional ones can be None if you want.\n            self.register_parameter('bias', None)\n\n        # Not a very smart way to initialize weights\n        nn.init.uniform_(self.weight, -0.1, 0.1)\n        if self.bias is not None:\n            nn.init.uniform_(self.bias, -0.1, 0.1)\n\n    def forward(self, input):\n        # See the autograd section for explanation of what happens here.\n        return LinearFunction.apply(input, self.weight, self.bias)\n\n    def extra_repr(self):\n        # (Optional)Set the extra information about this module. You can test\n        # it by printing an object of this class.\n        return 'input_features={}, output_features={}, bias={}'.format(\n            self.input_features, self.output_features, self.bias is not None\n        )\n\n```\n\n\n## ExtendingtorchPython API#\n\ntorch\nYou can create custom types that emulateTensorby defining a custom\nclass with methods that matchTensor. But what if you want to be able\nto pass these types to functions liketorch.add()in the top-leveltorchnamespace that acceptTensoroperands?\nTensor\nTensor\ntorch.add()\ntorch\nTensor\nIf your custom Python type defines a method named__torch_function__, PyTorch\nwill invoke your__torch_function__implementation when an instance of your\ncustom class is passed to a function in thetorchnamespace. This makes\nit possible to define custom implementations for any of the functions in thetorchnamespace which your__torch_function__implementation can call,\nallowing your users to make use of your custom type with existing PyTorch\nworkflows that they have already written forTensor. This works with\n\u201cduck\u201d types that are unrelated toTensoras well as user-defined\nsubclasses ofTensor.\n__torch_function__\n__torch_function__\ntorch\ntorch\n__torch_function__\nTensor\nTensor\nTensor\n\n## Extendingtorchwith aTensor-like type#\n\ntorch\nTensor\nNote\nThis functionality is inspired by the NumPy__array_function__protocol. Seethe NumPy documentationandNEP-0018for\nmore details.\n__array_function__\nTo make this concrete, let\u2019s begin with a simple example that illustrates the\nAPI dispatch mechanism. We\u2019ll create a custom type that represents a 2D scalar\ntensor, parametrized by the orderNand value along the diagonal entries,value:\nN\nvalue\n\n```python\nclass ScalarTensor(object):\n   def __init__(self, N, value):\n       self._N = N\n       self._value = value\n\n   def __repr__(self):\n       return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n   def tensor(self):\n       return self._value * torch.eye(self._N)\n\n```\n\nThis first iteration of the design isn\u2019t very useful. The main functionality ofScalarTensoris to provide a more compact string representation of a scalar\ntensor than in the base tensor class:\nScalarTensor\n\n```python\n>>> d = ScalarTensor(5, 2)\n>>> d\nScalarTensor(N=5, value=2)\n>>> d.tensor()\ntensor([[2., 0., 0., 0., 0.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 0., 0.],\n        [0., 0., 0., 2., 0.],\n        [0., 0., 0., 0., 2.]])\n\n```\n\nIf we try to use this object with thetorchAPI, we will run\ninto issues:\ntorch\n\n```python\n>>> import torch\n>>> torch.mean(d)\nTypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor\n\n```\n\nAdding a__torch_function__implementation toScalarTensormakes it\npossible for the above operation to succeed. Let\u2019s re-do our implementation,\nthis time adding a__torch_function__implementation:\n__torch_function__\nScalarTensor\n__torch_function__\n\n```python\nHANDLED_FUNCTIONS = {}\nclass ScalarTensor(object):\n    def __init__(self, N, value):\n        self._N = N\n        self._value = value\n\n    def __repr__(self):\n        return \"ScalarTensor(N={}, value={})\".format(self._N, self._value)\n\n    def tensor(self):\n        return self._value * torch.eye(self._N)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n            return NotImplemented\n        return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\n```\n\nThe__torch_function__method takes four arguments:func, a reference\nto the torch API function that is being overridden,types, the list of\ntypes of Tensor-likes that implement__torch_function__,args, the\ntuple of arguments passed to the function, andkwargs, the dict of keyword\narguments passed to the function. It uses a global dispatch table namedHANDLED_FUNCTIONSto store custom implementations. The keys of this\ndictionary are functions in thetorchnamespace and the values are\nimplementations forScalarTensor.\n__torch_function__\nfunc\ntypes\n__torch_function__\nargs\nkwargs\nHANDLED_FUNCTIONS\ntorch\nScalarTensor\nNote\nUsing a global dispatch table is not a mandated part of the__torch_function__API, it is just a useful design pattern for\nstructuring your override implementations.\n__torch_function__\nThis class definition isn\u2019t quite enough to maketorch.meando the right\nthing when we pass it aScalarTensor\u2013 we also need to define an\nimplementation fortorch.meanforScalarTensoroperands and add the\nimplementation to theHANDLED_FUNCTIONSdispatch table dictionary. One way\nof doing this is to define a decorator:\ntorch.mean\nScalarTensor\ntorch.mean\nScalarTensor\nHANDLED_FUNCTIONS\n\n```python\nimport functools\ndef implements(torch_function):\n    \"\"\"Register a torch function override for ScalarTensor\"\"\"\n    def decorator(func):\n        functools.update_wrapper(func, torch_function)\n        HANDLED_FUNCTIONS[torch_function] = func\n        return func\n    return decorator\n\n```\n\nwhich can be applied to the implementation of our override:\n\n```python\n@implements(torch.mean)\ndef mean(input):\n    return float(input._value) / input._N\n\n```\n\nWith this change we can now usetorch.meanwithScalarTensor:\ntorch.mean\nScalarTensor\n\n```python\n>>> d = ScalarTensor(5, 2)\n>>> torch.mean(d)\n0.4\n\n```\n\nOf coursetorch.meanis an example of the simplest kind of function to\noverride since it only takes one operand. We can use the same machinery to\noverride a function that takes more than one operand, any one of which might be\na tensor or tensor-like that defines__torch_function__, for example fortorch.add():\ntorch.mean\n__torch_function__\ntorch.add()\n\n```python\ndef ensure_tensor(data):\n    if isinstance(data, ScalarTensor):\n        return data.tensor()\n    return torch.as_tensor(data)\n\n@implements(torch.add)\ndef add(input, other):\n   try:\n       if input._N == other._N:\n           return ScalarTensor(input._N, input._value + other._value)\n       else:\n           raise ValueError(\"Shape mismatch!\")\n   except AttributeError:\n       return torch.add(ensure_tensor(input), ensure_tensor(other))\n\n```\n\nThis version has a fast path for when both operands areScalarTensorinstances and also a slower path which degrades to converting the data to\ntensors when either operand is not aScalarTensor. That makes the override\nfunction correctly when either operand is aScalarTensoror a regularTensor:\nScalarTensor\nScalarTensor\nScalarTensor\nTensor\n\n```python\n>>> s = ScalarTensor(2, 2)\n>>> torch.add(s, s)\nScalarTensor(N=2, value=4)\n>>> t = torch.tensor([[1, 1,], [1, 1]])\n>>> torch.add(s, t)\ntensor([[3., 1.],\n        [1., 3.]])\n\n```\n\nNote that our implementation ofadddoes not takealphaoroutas\nkeyword arguments liketorch.add()does:\nadd\nalpha\nout\ntorch.add()\n\n```python\n>>> torch.add(s, s, alpha=2)\nTypeError: add() got an unexpected keyword argument 'alpha'\n\n```\n\nFor speed and flexibility the__torch_function__dispatch mechanism does not\ncheck that the signature of an override function matches the signature of the\nfunction being overridden in thetorchAPI. For some applications ignoring\noptional arguments would be fine but to ensure full compatibility withTensor, user implementations of torch API functions should take care to\nexactly emulate the API of the function that is being overridden.\n__torch_function__\ntorch\nTensor\nFunctions in thetorchAPI that do not have explicit overrides will\nreturnNotImplementedfrom__torch_function__. If all operands with__torch_function__defined on them returnNotImplemented, PyTorch will\nraise aTypeError. This means that most of the time operations that do not\nhave explicit overrides for a type will raise aTypeErrorwhen an instance\nof such a type is passed:\ntorch\nNotImplemented\n__torch_function__\n__torch_function__\nNotImplemented\nTypeError\nTypeError\n\n```python\n>>> torch.mul(s, 3)\nTypeError: no implementation found for 'torch.mul' on types that\nimplement __torch_function__: [ScalarTensor]\n\n```\n\nIn practice this means that if you would like to implement your overrides using\na__torch_function__implementation along these lines, you will need to\nexplicitly implement the fulltorchAPI or the entire subset of the API\nthat you care about for your use case. This may be a tall order as the fulltorchAPI is quite extensive.\n__torch_function__\ntorch\ntorch\nAnother option is to not returnNotImplementedfor operations that are not\nhandled but to instead pass aTensorto the originaltorchfunction when no override is available. For example, if we change our\nimplementation of__torch_function__forScalarTensorto the one below:\nNotImplemented\nTensor\ntorch\n__torch_function__\nScalarTensor\n\n```python\n@classmethod\ndef __torch_function__(cls, func, types, args=(), kwargs=None):\n    if kwargs is None:\n        kwargs = {}\n    if func not in HANDLED_FUNCTIONS or not all(\n            issubclass(t, (torch.Tensor, ScalarTensor))\n            for t in types\n        ):\n        args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]\n        return func(*args, **kwargs)\n    return HANDLED_FUNCTIONS[func](*args, **kwargs)\n\n```\n\nThentorch.mul()will work correctly, although the return type will always\nbe aTensorrather than aScalarTensor, even if both operands\nareScalarTensorinstances:\ntorch.mul()\nTensor\nScalarTensor\nScalarTensor\n\n```python\n>>> s = ScalarTensor(2, 2)\n>>> torch.mul(s, s)\ntensor([[4., 0.],\n        [0., 4.]])\n\n```\n\nAlso see theMetadataTensorexample below for another variation on this\npattern but instead always returns aMetadataTensorto propagate metadata\nthrough operations in thetorchAPI.\nMetadataTensor\nMetadataTensor\ntorch\nThe__torch_function__protocol is designed for full coverage of the API,\npartial coverage may lead to undesirable results, in particular, certain\nfunctions raising aTypeError. This is especially true for subclasses,\nwhere all three oftorch.add,torch.Tensor.__add__andtorch.Tensor.addmust be covered, even if they return exactly the same result. Failing to do\nthis may also lead to infinite recursion. If one requires the implementation\nof a function fromtorch.Tensorsubclasses, they must usesuper().__torch_function__inside their implementation.\n__torch_function__\nTypeError\ntorch.Tensor\nsuper().__torch_function__\n\n## Subclassingtorch.Tensor#\n\ntorch.Tensor\nAs of version 1.7.0, methods ontorch.Tensorand functions in publictorch.*namespaces applied ontorch.Tensorsubclasses\nwill return subclass instances instead oftorch.Tensorinstances:\ntorch.Tensor\ntorch.*\ntorch.Tensor\ntorch.Tensor\n\n```python\n>>> class SubTensor(torch.Tensor):\n...     pass\n>>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__\n'SubTensor'\n>>> type(torch.add(SubTensor([0]), torch.tensor([1]))).__name__\n'SubTensor'\n\n```\n\nIf multiple subclasses exist, the lowest one in the hierarchy will be chosen by\ndefault. If there is no unique way to determine such a case, then aTypeErroris raised:\nTypeError\n\n```python\n>>> type(torch.add(SubTensor2([0]), SubTensor([1]))).__name__\n'SubTensor2'\n>>> type(torch.add(SubTensor2([0]), torch.tensor([1]))).__name__\n'SubTensor2'\n>>> torch.add(SubTensor([0]), OtherSubTensor([1]))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor]\n\n```\n\nIf one wishes to have a global override for all tensor methods, one can use__torch_function__. Here is an example that logs all function/method\ncalls:\n__torch_function__\n\n```python\nclass LoggingTensor(torch.Tensor):\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion\n        if func is not torch.Tensor.__repr__:\n            logging.info(f\"func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}\")\n        if kwargs is None:\n            kwargs = {}\n        return super().__torch_function__(func, types, args, kwargs)\n\n```\n\nHowever, if one instead wishes to override a method on the Tensor subclass,\nthere one can do so either by directly overriding the method (by defining\nit for a subclass), or by using__torch_function__and matching withfunc.\n__torch_function__\nfunc\nOne should be careful within__torch_function__for subclasses to always\ncallsuper().__torch_function__(func,...)instead offuncdirectly,\nas was the case before version 1.7.0. Failing to do this may causefuncto recurse back into__torch_function__and therefore cause infinite\nrecursion.\n__torch_function__\nsuper().__torch_function__(func,...)\nfunc\nfunc\n__torch_function__\n\n## Extendingtorchwith aTensorwrapper type#\n\ntorch\nTensor\nAnother useful case is a type that wraps aTensor, either as an\nattribute or via subclassing. Below we implement a special case of this sort of\ntype, aMetadataTensorthat attaches a dictionary of metadata to aTensorthat is propagated throughtorchoperations. Since this\nis a generic sort of wrapping for the fulltorchAPI, we do not need to\nindividually implement each override so we can make the__torch_function__implementation more permissive about what operations are allowed:\nTensor\nMetadataTensor\nTensor\ntorch\ntorch\n__torch_function__\n\n```python\nclass MetadataTensor(object):\n    def __init__(self, data, metadata=None, **kwargs):\n        self._t = torch.as_tensor(data, **kwargs)\n        self._metadata = metadata\n\n    def __repr__(self):\n        return \"Metadata:\\n{}\\n\\ndata:\\n{}\".format(self._metadata, self._t)\n\n    @classmethod\n    def __torch_function__(cls, func, types, args=(), kwargs=None):\n        if kwargs is None:\n            kwargs = {}\n        metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))\n        args = [getattr(a, '_t', a) for a in args]\n        assert len(metadatas) > 0\n        ret = func(*args, **kwargs)\n        return MetadataTensor(ret, metadata=metadatas[0])\n\n```\n\nThis simple implementation won\u2019t necessarily work with every function in thetorchAPI but it is good enough to capture most common operations:\ntorch\n\n```python\n>>> metadata = {'owner': 'Ministry of Silly Walks'}\n>>> m = MetadataTensor([[1, 2], [3, 4]], metadata=metadata)\n>>> t = torch.tensor([[1, 2], [1, 2]])\n>>> torch.add(t, m)\nMetadata:\n{'owner': 'Ministry of Silly Walks'}\n\ndata:\ntensor([[2, 4],\n        [4, 6]])\n>>> torch.mul(t, m)\nMetadata:\n{'owner': 'Ministry of Silly Walks'}\n\ndata:\ntensor([[1, 4],\n        [3, 8]])\n\n```\n\n\n## Operations on multiple types that define__torch_function__#\n\n__torch_function__\nIt is possible to use the torch API with multiple distinct types that each have\na__torch_function__implementation, but special care must be taken. In such\na case the rules are:\n__torch_function__\nThe dispatch operation gathers all distinct implementations of__torch_function__for each operand and calls them in order: subclasses\nbefore superclasses, and otherwise left to right in the operator expression.\n__torch_function__\nIf any value other thanNotImplementedis returned, that value is\nreturned as the result. Implementations can register that they do not\nimplement an operation by returningNotImplemented.\nNotImplemented\nNotImplemented\nIf all of the__torch_function__implementations returnNotImplemented, PyTorch raises aTypeError.\n__torch_function__\nNotImplemented\nTypeError\n\n## Testing Coverage of Overrides for the PyTorch API#\n\nOne troublesome aspect of implementing__torch_function__is that if some\noperations do and others do not have overrides, users will at best see an\ninconsistent experience, or at worst will see errors raised at runtime when they\nuse a function that does not have an override. To ease this process, PyTorch\nprovides a developer-facing API for ensuring full support for__torch_function__overrides. This API is private and may be subject to\nchanges without warning in the future.\n__torch_function__\n__torch_function__\nFirst, to get a listing of all overridable functions, usetorch.overrides._get_overridable_functions. This returns a dictionary whose\nkeys are namespaces in thePyTorchPython API and whose values are a list of\nfunctions in that namespace that can be overridden. For example, let\u2019s print the\nnames of the first 5 functions intorch.nn.functionalthat can be\noverridden:\ntorch.overrides._get_overridable_functions\nPyTorch\ntorch.nn.functional\n\n```python\n>>> from torch.overrides import get_overridable_functions\n>>> func_dict = get_overridable_functions()\n>>> nn_funcs = func_dict[torch.nn.functional]\n>>> print([f.__name__ for f in nn_funcs[:5])\n['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d',\n 'adaptive_max_pool1d', 'adaptive_max_pool1d_with_indices']\n\n```\n\nThis listing of functions makes it possible to iterate over all overridable\nfunctions, however in practice this is not enough to write tests for all of\nthese functions without laboriously and manually copying the signature of each\nfunction for each test. To ease this process, thetorch.overrides._get_testing_overridesfunction returns a dictionary mapping\noverridable functions in thePyTorchAPI to dummy lambda functions that have\nthe same signature as the original function but unconditionally return -1. These\nfunctions are most useful to use withinspectto analyze the function\nsignature of the originalPyTorchfunction:\ntorch.overrides._get_testing_overrides\nPyTorch\ninspect\nPyTorch\n\n```python\n>>> import inspect\n>>> from torch.overrides import get_testing_overrides\n>>> override_dict = get_testing_overrides()\n>>> dummy_add = override_dict[torch.add]\n>>> inspect.signature(dummy_add)\n<Signature (input, other, out=None)>\n\n```\n\nFinally,torch.overrides.get_ignored_functionsreturns a tuple of functions\nthat explicitly cannot be overridden by__torch_function__. This list can be\nuseful to confirm that a function that isn\u2019t present in the dictionary returned\nbyget_overridable_functionscannot be overridden.\ntorch.overrides.get_ignored_functions\n__torch_function__\nget_overridable_functions\n\n## Extendingtorchnative API#\n\ntorch\nWhile__torch_function__allows one to effectively extend PyTorch\u2019s pure Python\ncomponents\u2019 behavior, it does not allow one to extend the parts of\nPyTorch implemented in C++. To that end, aTensorsubclass can also\ndefine__torch_dispatch__which will be able to override the behavior at the\nC++ level.\n__torch_function__\nTensor\n__torch_dispatch__\nTo effectively use this feature, it is important to know how the native part of\nPyTorch is implemented. The most important component there is what we call the\n\u201cdispatcher\u201d (the best description can be found in thisblog posteven though it is slightly outdated). As\nhinted by its name, it is responsible for calling the right backend\nfunction for a specific call of a function. For example, when callingtorch.add(a,b), the dispatcher will inspect both arguments, figure out which\n\u201cfeature\u201d (autograd, autocast, functionalization, etc) and which \u201cbackend\u201d (CPU,\nCUDA, MPS, etc) should be used for this specific call and finally call all the\nright kernels.\nA very common thing done by a kernel is to \u201credispatch\u201d. For example, when running your\nneural network on GPU with autocast, the first call will be the autocast kernel that\nwill handle any potential autocast logic and redispatch down. The next feature in line\nwill be autograd that will properly create the autograd graph and then redispatch down.\nFinally, we reach the backend kernel for CUDA which will launch the right CUDA kernel\nand return the final result. On the way out, autograd will attach the graph to the\noutput and, finally, autocast will have a chance to do any update it needs on exit.\ntorch.add(a,b)\nOne configuration of the dispatcher is the order in which all these feature and backend keys are called. The latest list and their order can be found inDispatchKey.hinside theDispatchKeyenum. For the purpose of extending torch, the important subset of the ordering for this discussion is:\nDispatchKey.h\nDispatchKey\nvmap -> Autocast -> Autograd -> ZeroTensor -> Neg/Conj -> Functionalize -> Python -> Backends\nThe most important key for the purpose of this discussion isPythonas every Tensor subclass with the__torch_dispatch__method defined will call into this feature. It is from there that the user-defined method is called and where the behavior can be overwritten arbitrarily. From there, calling the providedfuncagain will perform a \u201credispatch\u201d.\nPython\n__torch_dispatch__\nfunc\nSome important implications of this implementation are:\nThis code runs \u201cbelow all features\u201d. It is thus only responsible, like a regular backend, for generating the output value of each Tensor (and can, and should, ignore all advanced features like autograd, autocast, etc).\nIf any high level feature implements a given function without redispatching, it will never reach thePythonkey and so the__torch_dispatch__callback will never be triggered. This happens in particular for CompositeImplicitAutograd functions which are evaluated at the Autograd level without redispatching. This is because a CompositeImplicitAutograd function specifies its autograd formula by implicitly calling other native ops, so at the Autograd level, the function is decomposed into its native ops and those are evaluated instead.\nPython\n__torch_dispatch__\nWhen calling back to Python and when wrapping the results, the same conversions are used as the regular PyTorch Python/C++ binding. In particular, some objects cannot be represented in Python and need special handling (undefined Tensors for example become None).\nOur native functions are lazily populated astorch.ops.{namespace}.{func_name}.{overload_name}as callable Python objects to enable easily interacting with them from Python. Thefuncobject given to__torch_dispatch__is always an entry from this namespace. This namespace can be used to directly call native ops and bypass the usual Python API and binding code.\ntorch.ops.{namespace}.{func_name}.{overload_name}\nfunc\n__torch_dispatch__\nIn a similar way where__torch_function__is able to interpose on all of torch\u2019s Python API and Tensor methods,__torch_dispatch__is able to intercept all calls into the aten native API. Note that all methods on Tensors are converted into function calls before entering the dispatcher and thus will appear as function calls here:torch.add(a,2)anda+2will lead to exactly the same aten call.\nMost of these functions are defined innative_functions.yamlwhich specifies the properties of these functions as well as their backend implementation. Their implementation alongside specified features are then automatically registered via codegen.\nSome more exotic functions or features are also registered in other places in the C++ codebase or in user-defined C++ extensions.\n__torch_function__\n__torch_dispatch__\ntorch.add(a,2)\na+2\nnative_functions.yaml\nIt is also possible to addnewnative functions usingtorch.library. This Python feature allows defining and/or adding new implementations to native functions. This can be used to add missing kernels, replace existing ones or define brand new native functions.\ntorch.library\nYou can find many examples of__torch_dispatch__-based subclasses in thesubclass zoorepo.\n__torch_dispatch__\n\n## __torch_dispatch__calling convention#\n\n__torch_dispatch__\n\n```python\n@classmethod\ndef __torch_dispatch__(cls, func, types, args=(), kwargs=None):\n    pass\n\n```\n\nWhen a user calls an operator with inputs that have__torch_dispatch__, that call\nmay be forwarded to the__torch_dispatch__. args and kwargs get normalized before\nthe call to__torch_dispatch__, that is:\n__torch_dispatch__\n__torch_dispatch__\n__torch_dispatch__\nthekwargsconsist of keyword-only arguments in the operator\u2019s schema.\nIf a kwarg is equal to its default value (in the schema), it will not be passed.\nkwargs\ntheargsconsists of all other arguments, no matter how they were passed\nto the operator (positional vs keyword).\nIf an arg is equal to its default value, and\nit is the right-most positional arg or all the args to the right of it\nare not passed, it will not be passed.\nargs\n\n## Extending alltorchAPI with Modes#\n\ntorch\nUnfortunately, there are functions that do not take Tensor inputs. This means that the subclass approach described above cannot be used to override the behavior of all of PyTorch\u2019s functions. Also, if the use case requires to intercept every function call, changing every Tensor to be a subclass can be overly intrusive.\nTo address this use case, we introduced the concept of \u201cMode\u201d. These exist for__torch_function__and__torch_dispatch__overrides, are created by subclassing respectivelytorch.overrides.TorchFunctionModeandtorch.utils._python_dispatch.TorchDispatchMode, and are used as a context manager.\n__torch_function__\n__torch_dispatch__\ntorch.overrides.TorchFunctionMode\ntorch.utils._python_dispatch.TorchDispatchMode\nTo simplify the description of how it interacts with subclasses and other modes, whenever the context manager for a mode is entered, every function behaves as if there was an extra Tensor argument at the beginning of the argument list with the mode as a subclass.\nThis means in particular that all modes handlers will be called before any subclass handler and that modes corresponding to the inner context manager will always run first.\nIt is also important to note that within a given mode handler, this specific mode is disabled and can be re-enabled manually by doingwithself:.\nwithself:\nHere is an example that shows logging modes of each type:\n\n```python\nimport torch\nfrom torch.overrides import TorchFunctionMode, resolve_name\nfrom torch.utils._python_dispatch import TorchDispatchMode\n\nclass FunctionLog(TorchFunctionMode):\n    def __torch_function__(self, func, types, args, kwargs=None):\n        print(f\"Function Log: {resolve_name(func)}(*{args}, **{kwargs})\")\n        return func(*args, **(kwargs or {}))\n\nclass DispatchLog(TorchDispatchMode):\n    def __torch_dispatch__(self, func, types, args, kwargs=None):\n        print(f\"Dispatch Log: {func}(*{args}, **{kwargs})\")\n        return func(*args, **(kwargs or {}))\n\ndef f():\n    a = torch.rand(10, requires_grad=True)\n    b = a * 2\n    b.sum().backward()\n\nprint(\"TorchFunctionMode logging:\")\nwith FunctionLog():\n    f()\n\nprint(\"TorchDispatchMode logging:\")\nwith DispatchLog():\n    f()\n\n```\n\nWhich prints the following, with extra comments:\n\n```python\nTorchFunctionMode logging:\nFunction Log: torch.rand(*(10,), **{'requires_grad': True})\nFunction Log: torch.Tensor.mul(*(tensor([0.7164, 0.9897, 0.1745, 0.9336, 0.4287, 0.7989, 0.2169, 0.7474, 0.5624,\n        0.5970], requires_grad=True), 2), **None)\nFunction Log: torch.Tensor.sum(*(tensor([1.4328, 1.9794, 0.3490, 1.8671, 0.8573, 1.5977, 0.4338, 1.4948, 1.1249,\n        1.1939], grad_fn=<MulBackward0>),), **None)\n# Note that at the python level, we only see the call to backward but not what happens in the autograd engine.\nFunction Log: torch.Tensor.backward(*(tensor(12.3307, grad_fn=<SumBackward0>),), **{'gradient': None, 'retain_graph': None, 'create_graph': False, 'inputs': None})\n\nTorchDispatchMode logging:\n# Here the requires_grad flag from autograd is removed while default arguments were populated.\nDispatch Log: aten.rand.default(*([10],), **{'device': device(type='cpu'), 'pin_memory': False})\nDispatch Log: aten.mul.Tensor(*(tensor([0.2151, 0.6018, 0.8415, 0.9060, 0.2974, 0.7708, 0.6668, 0.0352, 0.7948,\n        0.6023], requires_grad=True), 2), **{})\nDispatch Log: aten.sum.default(*(tensor([0.4303, 1.2036, 1.6831, 1.8120, 0.5949, 1.5416, 1.3335, 0.0705, 1.5897,\n        1.2046], grad_fn=<MulBackward0>),), **{})\n# Here we don't see the call to backward itself, but its constituents. Starting here with the factory function that creates the initial gradient.\nDispatch Log: aten.ones_like.default(*(tensor(11.4637, grad_fn=<SumBackward0>),), **{'pin_memory': False, 'memory_format': torch.preserve_format})\n# This is the backward of the sum\nDispatch Log: aten.expand.default(*(tensor(1.), [10]), **{})\nDispatch Log: aten.mul.Tensor(*(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 2), **{})\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})\nDispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})\n\n```\n",
    "url": "https://pytorch.org/docs/stable/notes/extending.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e34a85898e901242b1ffb2e713baf139",
    "source": "pytorch_docs",
    "title": "torch.linalg \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.linalg#\n\nCreated On: Aug 07, 2020 | Last Updated On: Jun 17, 2025\nCommon linear algebra operations.\nSeeLinear algebra (torch.linalg)for some common numerical edge-cases.\n\n## Matrix Properties#\n\nnorm\n\nnorm\nComputes a vector or matrix norm.\nvector_norm\n\nvector_norm\nComputes a vector norm.\nmatrix_norm\n\nmatrix_norm\nComputes a matrix norm.\ndiagonal\n\ndiagonal\nAlias fortorch.diagonal()with defaultsdim1= -2,dim2= -1.\ntorch.diagonal()\ndim1\ndim2\ndet\n\ndet\nComputes the determinant of a square matrix.\nslogdet\n\nslogdet\nComputes the sign and natural logarithm of the absolute value of the determinant of a square matrix.\ncond\n\ncond\nComputes the condition number of a matrix with respect to a matrix norm.\nmatrix_rank\n\nmatrix_rank\nComputes the numerical rank of a matrix.\n\n## Decompositions#\n\ncholesky\n\ncholesky\nComputes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.\nqr\n\nqr\nComputes the QR decomposition of a matrix.\nlu\n\nlu\nComputes the LU decomposition with partial pivoting of a matrix.\nlu_factor\n\nlu_factor\nComputes a compact representation of the LU factorization with partial pivoting of a matrix.\neig\n\neig\nComputes the eigenvalue decomposition of a square matrix if it exists.\neigvals\n\neigvals\nComputes the eigenvalues of a square matrix.\neigh\n\neigh\nComputes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.\neigvalsh\n\neigvalsh\nComputes the eigenvalues of a complex Hermitian or real symmetric matrix.\nsvd\n\nsvd\nComputes the singular value decomposition (SVD) of a matrix.\nsvdvals\n\nsvdvals\nComputes the singular values of a matrix.\n\n## Solvers#\n\nsolve\n\nsolve\nComputes the solution of a square system of linear equations with a unique solution.\nsolve_triangular\n\nsolve_triangular\nComputes the solution of a triangular system of linear equations with a unique solution.\nlu_solve\n\nlu_solve\nComputes the solution of a square system of linear equations with a unique solution given an LU decomposition.\nlstsq\n\nlstsq\nComputes a solution to the least squares problem of a system of linear equations.\n\n## Inverses#\n\ninv\n\ninv\nComputes the inverse of a square matrix if it exists.\npinv\n\npinv\nComputes the pseudoinverse (Moore-Penrose inverse) of a matrix.\n\n## Matrix Functions#\n\nmatrix_exp\n\nmatrix_exp\nComputes the matrix exponential of a square matrix.\nmatrix_power\n\nmatrix_power\nComputes then-th power of a square matrix for an integern.\n\n## Matrix Products#\n\ncross\n\ncross\nComputes the cross product of two 3-dimensional vectors.\nmatmul\n\nmatmul\nAlias fortorch.matmul()\ntorch.matmul()\nvecdot\n\nvecdot\nComputes the dot product of two batches of vectors along a dimension.\nmulti_dot\n\nmulti_dot\nEfficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.\nhouseholder_product\n\nhouseholder_product\nComputes the firstncolumns of a product of Householder matrices.\n\n## Tensor Operations#\n\ntensorinv\n\ntensorinv\nComputes the multiplicative inverse oftorch.tensordot().\ntorch.tensordot()\ntensorsolve\n\ntensorsolve\nComputes the solutionXto the systemtorch.tensordot(A, X) = B.\n\n## Misc#\n\nvander\n\nvander\nGenerates a Vandermonde matrix.\n\n## Experimental Functions#\n\ncholesky_ex\n\ncholesky_ex\nComputes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.\ninv_ex\n\ninv_ex\nComputes the inverse of a square matrix if it is invertible.\nsolve_ex\n\nsolve_ex\nA version ofsolve()that does not perform error checks unlesscheck_errors= True.\nsolve()\ncheck_errors\nlu_factor_ex\n\nlu_factor_ex\nThis is a version oflu_factor()that does not perform error checks unlesscheck_errors= True.\nlu_factor()\ncheck_errors\nldl_factor\n\nldl_factor\nComputes a compact representation of the LDL factorization of a Hermitian or symmetric (possibly indefinite) matrix.\nldl_factor_ex\n\nldl_factor_ex\nThis is a version ofldl_factor()that does not perform error checks unlesscheck_errors= True.\nldl_factor()\ncheck_errors\nldl_solve\n\nldl_solve\nComputes the solution of a system of linear equations using the LDL factorization.",
    "url": "https://pytorch.org/docs/stable/linalg.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f6c9bb4b86deab941133cc21a3dffc2a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/random.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "fd2dd8830ab2bd0b8a47c949060f7880",
    "source": "pytorch_docs",
    "title": "torch.mps \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.mps#\n\nCreated On: Feb 10, 2023 | Last Updated On: Jun 08, 2025\nThis package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python.\nMetal is Apple\u2019s API for programming metal GPU (graphics processor unit). Using MPS means that increased\nperformance can be achieved, by running work on the metal GPU(s).\nSeehttps://developer.apple.com/documentation/metalperformanceshadersfor more details.\ndevice_count\n\ndevice_count\nReturns the number of available MPS devices.\nsynchronize\n\nsynchronize\nWaits for all kernels in all streams on a MPS device to complete.\nget_rng_state\n\nget_rng_state\nReturns the random number generator state as a ByteTensor.\nset_rng_state\n\nset_rng_state\nSets the random number generator state.\nmanual_seed\n\nmanual_seed\nSets the seed for generating random numbers.\nseed\n\nseed\nSets the seed for generating random numbers to a random number.\nempty_cache\n\nempty_cache\nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications.\nset_per_process_memory_fraction\n\nset_per_process_memory_fraction\nSet memory fraction for limiting process's memory allocation on MPS device.\ncurrent_allocated_memory\n\ncurrent_allocated_memory\nReturns the current GPU memory occupied by tensors in bytes.\ndriver_allocated_memory\n\ndriver_allocated_memory\nReturns total GPU memory allocated by Metal driver for the process in bytes.\nrecommended_max_memory\n\nrecommended_max_memory\nReturns recommended max Working set size for GPU memory in bytes.\ncompile_shader\n\ncompile_shader\nCompiles compute shader from source and allows one to invoke kernels defined there from the comfort of Python runtime Example.\n\n## MPS Profiler#\n\nprofiler.start\nprofiler.start\nStart OS Signpost tracing from MPS backend.\nprofiler.stop\nprofiler.stop\nStops generating OS Signpost tracing from MPS backend.\nprofiler.profile\nprofiler.profile\nContext Manager to enabling generating OS Signpost tracing from MPS backend.\nprofiler.is_capturing_metal\nprofiler.is_capturing_metal\nChecks if metal capture is in progress\nprofiler.is_metal_capture_enabled\nprofiler.is_metal_capture_enabled\nChecks ifmetal_capturecontext manager is usable To enable metal capture, set MTL_CAPTURE_ENABLED envvar\nprofiler.metal_capture\nprofiler.metal_capture\nContext manager that enables capturing of Metal calls into gputrace\n\n## MPS Event#\n\nevent.Event\nevent.Event\nWrapper around an MPS event.",
    "url": "https://pytorch.org/docs/stable/mps.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2e93ed16db8c47ef913bec7867edb268",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/quantization.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b44829d1ff36f087ea3b00acbad64288",
    "source": "pytorch_docs",
    "title": "Developer Notes \u2014 PyTorch 2.9 documentation",
    "text": "\n## Developer Notes#\n\nCreated On: Apr 16, 2025 | Last Updated On: Apr 16, 2025",
    "url": "https://pytorch.org/docs/stable/notes.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1d45959feadb705ac1ae978926b4ee73",
    "source": "pytorch_docs",
    "title": "Automatic differentiation package - torch.autograd \u2014 PyTorch 2.9 documentation",
    "text": "\n## Automatic differentiation package - torch.autograd#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jun 12, 2025\ntorch.autogradprovides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\ntorch.autograd\nIt requires minimal changes to the existing code - you only need to declareTensors\nfor which gradients should be computed with therequires_grad=Truekeyword.\nAs of now, we only support autograd for floating pointTensortypes (\nhalf, float, double and bfloat16) and complexTensortypes (cfloat, cdouble).\nTensor\nrequires_grad=True\nTensor\nTensor\nbackward\n\nbackward\nCompute the sum of gradients of given tensors with respect to graph leaves.\ngrad\n\ngrad\nCompute and return the sum of gradients of outputs with respect to the inputs.\n\n## Forward-mode Automatic Differentiation#\n\nWarning\nThis API is in beta. Even though the function signatures are very unlikely to change, improved\noperator coverage is planned before we consider this stable.\nPlease see theforward-mode AD tutorialfor detailed steps on how to use this API.\nforward_ad.dual_level\nforward_ad.dual_level\nContext-manager for forward AD, where all forward AD computation must occur within thedual_levelcontext.\ndual_level\nforward_ad.make_dual\nforward_ad.make_dual\nAssociate a tensor value with its tangent to create a \"dual tensor\" for forward AD gradient computation.\nforward_ad.unpack_dual\nforward_ad.unpack_dual\nUnpack a \"dual tensor\" to get both its Tensor value and its forward AD gradient.\nforward_ad.enter_dual_level\nforward_ad.enter_dual_level\nEnter a new forward grad level.\nforward_ad.exit_dual_level\nforward_ad.exit_dual_level\nExit a forward grad level.\nforward_ad.UnpackedDualTensor\nforward_ad.UnpackedDualTensor\nNamedtuple returned byunpack_dual()containing the primal and tangent components of the dual tensor.\nunpack_dual()\n\n## Functional higher level API#\n\nWarning\nThis API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable.\nThis section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.\nThis API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a functionfthat takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag asf(input,constant,flag=flag)you can use it asfunctional.jacobian(lambdax:f(x,constant,flag=flag),input).\nf\nf(input,constant,flag=flag)\nfunctional.jacobian(lambdax:f(x,constant,flag=flag),input)\nfunctional.jacobian\nfunctional.jacobian\nCompute the Jacobian of a given function.\nfunctional.hessian\nfunctional.hessian\nCompute the Hessian of a given scalar function.\nfunctional.vjp\nfunctional.vjp\nCompute the dot product between a vectorvand the Jacobian of the given function at the point given by the inputs.\nv\nfunctional.jvp\nfunctional.jvp\nCompute the dot product between the Jacobian of the given function at the point given by the inputs and a vectorv.\nv\nfunctional.vhp\nfunctional.vhp\nCompute the dot product between vectorvand Hessian of a  given scalar function at a specified point.\nv\nfunctional.hvp\nfunctional.hvp\nCompute the dot product between the scalar function's Hessian and a vectorvat a specified point.\nv\n\n## Locally disabling gradient computation#\n\nSeeLocally disabling gradient computationfor more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two. Also seeLocally disabling gradient computationfor a list of functions that can be used to locally disable gradients.\n\n## Default gradient layouts#\n\nWhen a non-sparseparamreceives a non-sparse gradient duringtorch.autograd.backward()ortorch.Tensor.backward()param.gradis accumulated as follows.\nparam\ntorch.autograd.backward()\ntorch.Tensor.backward()\nparam.grad\nIfparam.gradis initiallyNone:\nparam.grad\nNone\nIfparam\u2019s memory is non-overlapping and dense,.gradis\ncreated with strides matchingparam(thus matchingparam\u2019s\nlayout).\nparam\n.grad\nparam\nparam\nOtherwise,.gradis created with rowmajor-contiguous strides.\n.grad\nIfparamalready has a non-sparse.gradattribute:\nparam\n.grad\nIfcreate_graph=False,backward()accumulates into.gradin-place, which preserves its strides.\ncreate_graph=False\nbackward()\n.grad\nIfcreate_graph=True,backward()replaces.gradwith a\nnew tensor.grad+newgrad, which attempts (but does not guarantee)\nmatching the preexisting.grad\u2019s strides.\ncreate_graph=True\nbackward()\n.grad\n.grad+newgrad\n.grad\nThe default behavior (letting.grads beNonebefore the firstbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls tomodel.zero_grad()oroptimizer.zero_grad()will not affect.gradlayouts.\n.grad\nNone\nbackward()\nmodel.zero_grad()\noptimizer.zero_grad()\n.grad\nIn fact, resetting all.grads toNonebefore each\naccumulation phase, e.g.:\n.grad\nNone\n\n```python\nfor iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n\n```\n\nsuch that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative tomodel.zero_grad()oroptimizer.zero_grad()that may improve performance for some networks.\nmodel.zero_grad()\noptimizer.zero_grad()\n\n## Manual gradient layouts#\n\nIf you need manual control over.grad\u2019s strides,\nassignparam.grad=a zeroed tensor with desired strides\nbefore the firstbackward(), and never reset it toNone.\n3 guarantees your layout is preserved as long ascreate_graph=False.\n4 indicates your layout islikelypreserved even ifcreate_graph=True.\n.grad\nparam.grad=\nbackward()\nNone\ncreate_graph=False\ncreate_graph=True\n\n## In-place operations on Tensors#\n\nSupporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.\n\n## In-place correctness checks#\n\nAllTensors keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.\nTensor\n\n## Variable (deprecated)#\n\nWarning\nThe Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors withrequires_gradset toTrue. Below please find a quick guide on what\nhas changed:\nrequires_grad\nTrue\nVariable(tensor)andVariable(tensor,requires_grad)still work as expected,\nbut they return Tensors instead of Variables.\nVariable(tensor)\nVariable(tensor,requires_grad)\nvar.datais the same thing astensor.data.\nvar.data\ntensor.data\nMethods such asvar.backward(),var.detach(),var.register_hook()now work on tensors\nwith the same method names.\nvar.backward(),var.detach(),var.register_hook()\nIn addition, one can now create tensors withrequires_grad=Trueusing factory\nmethods such astorch.randn(),torch.zeros(),torch.ones(), and others\nlike the following:\nrequires_grad=True\ntorch.randn()\ntorch.zeros()\ntorch.ones()\nautograd_tensor=torch.randn((2,3,4),requires_grad=True)\nautograd_tensor=torch.randn((2,3,4),requires_grad=True)\n\n## Tensor autograd functions#\n\ntorch.Tensor.grad\ntorch.Tensor.grad\nThis attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself.\nNone\nbackward()\nself\ntorch.Tensor.requires_grad\ntorch.Tensor.requires_grad\nIsTrueif gradients need to be computed for this Tensor,Falseotherwise.\nTrue\nFalse\ntorch.Tensor.is_leaf\ntorch.Tensor.is_leaf\nAll Tensors that haverequires_gradwhich isFalsewill be leaf Tensors by convention.\nrequires_grad\nFalse\ntorch.Tensor.backward([gradient,\u00a0...])\ntorch.Tensor.backward\nComputes the gradient of current tensor wrt graph leaves.\ntorch.Tensor.detach\ntorch.Tensor.detach\nReturns a new Tensor, detached from the current graph.\ntorch.Tensor.detach_\ntorch.Tensor.detach_\nDetaches the Tensor from the graph that created it, making it a leaf.\ntorch.Tensor.register_hook(hook)\ntorch.Tensor.register_hook\nRegisters a backward hook.\ntorch.Tensor.register_post_accumulate_grad_hook(hook)\ntorch.Tensor.register_post_accumulate_grad_hook\nRegisters a backward hook that runs after grad accumulation.\ntorch.Tensor.retain_grad()\ntorch.Tensor.retain_grad\nEnables this Tensor to have theirgradpopulated duringbackward().\ngrad\nbackward()\n\n## Function#\n\nBase class to create customautograd.Function.\nTo create a customautograd.Function, subclass this class and implement\ntheforward()andbackward()static methods. Then, to use your custom\nop in the forward pass, call the class methodapply. Do not callforward()directly.\nforward()\nbackward()\napply\nforward()\nTo ensure correctness and best performance, make sure you are calling the\ncorrect methods onctxand validating your backward function usingtorch.autograd.gradcheck().\nctx\ntorch.autograd.gradcheck()\nSeeExtending torch.autogradfor more details on how to use this class.\nExamples:\n\n```python\n>>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)\n\n```\n\nFunction.forward\nFunction.forward\nDefine the forward of the custom autograd Function.\nFunction.backward\nFunction.backward\nDefine a formula for differentiating the operation with backward mode automatic differentiation.\nFunction.jvp\nFunction.jvp\nDefine a formula for differentiating the operation with forward mode automatic differentiation.\nFunction.vmap\nFunction.vmap\nDefine the behavior for this autograd.Function underneathtorch.vmap().\ntorch.vmap()\n\n## Context method mixins#\n\nWhen creating a newFunction, the following methods are available toctx.\nFunction\nctx\nfunction.FunctionCtx.mark_dirty\nfunction.FunctionCtx.mark_dirty\nMark given tensors as modified in an in-place operation.\nfunction.FunctionCtx.mark_non_differentiable\nfunction.FunctionCtx.mark_non_differentiable\nMark outputs as non-differentiable.\nfunction.FunctionCtx.save_for_backward\nfunction.FunctionCtx.save_for_backward\nSave given tensors for a future call tobackward().\nbackward()\nfunction.FunctionCtx.set_materialize_grads\nfunction.FunctionCtx.set_materialize_grads\nSet whether to materialize grad tensors.\n\n## Custom Function utilities#\n\nDecorator for backward method.\nfunction.once_differentiable\nfunction.once_differentiable\n\nBase customFunctionused to build PyTorch utilities\nFunction\nfunction.BackwardCFunction\nfunction.BackwardCFunction\nThis class is used for internal autograd work.\nfunction.InplaceFunction\nfunction.InplaceFunction\nThis class is here only for backward compatibility reasons.\nfunction.NestedIOFunction\nfunction.NestedIOFunction\nThis class is here only for backward compatibility reasons.\n\n## Numerical gradient checking#\n\ngradcheck\n\ngradcheck\nCheck gradients computed via small finite differences against analytical gradients wrt tensors ininputsthat are of floating point or complex type and withrequires_grad=True.\ninputs\nrequires_grad=True\ngradgradcheck\n\ngradgradcheck\nCheck gradients of gradients computed via small finite differences against analytical gradients wrt tensors ininputsandgrad_outputsthat are of floating point or complex type and withrequires_grad=True.\ninputs\ngrad_outputs\nrequires_grad=True\nGradcheckError\n\nGradcheckError\nError raised bygradcheck()andgradgradcheck().\ngradcheck()\ngradgradcheck()\n\n## Profiler#\n\nAutograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are three modes\nimplemented at the moment - CPU-only usingprofile.\nnvprof based (registers both CPU and GPU activity) usingemit_nvtx.\nand vtune profiler based usingemit_itt.\nprofile\nemit_nvtx\nemit_itt\nContext manager that manages autograd profiler state and holds a summary of results.\nNote\nThis is the backend, most people should usetorch.profilerinstead.\ntorch.profiler\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks\nenabled(bool,optional) \u2013 Setting this to False makes this context manager a no-op.\nuse_cuda(bool,optional) \u2013 Enables timing of CUDA events as well\nusing the cudaEvent API. (will be deprecated)\nuse_device(str,optional) \u2013 Enables timing of device events.\nAdds approximately 4us of overhead to each tensor operation when use cuda.\nThe valid devices options are \u2018cuda\u2019, \u2018xpu\u2019, \u2018mtia\u2019 and \u2018privateuseone\u2019.\nrecord_shapes(bool,optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection.\nwith_flops(bool,optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPs (floating point operations) value using the operator\u2019s input shape.\nThis allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators.\nprofile_memory(bool,optional) \u2013 track tensor memory allocation/deallocation.\nwith_stack(bool,optional) \u2013 record source information (file and line number) for the ops.\nwith_modules(bool) \u2013 record module hierarchy (including function names)\ncorresponding to the callstack of the op. e.g. If module A\u2019s forward call\u2019s\nmodule B\u2019s forward which contains an aten::add op,\nthen aten::add\u2019s module hierarchy is A.B\nNote that this support exist, at the moment, only for TorchScript models\nand not eager mode models.\nuse_kineto(bool,optional) \u2013 experimental, enable profiling with Kineto profiler.\nuse_cpu(bool,optional) \u2013 profile CPU events; setting toFalserequiresuse_kineto=Trueand can be used to lower the overhead for GPU-only profiling.\nFalse\nuse_kineto=True\nexperimental_config(_ExperimentalConfig) \u2013 A set of experimental options\nused by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.\nacc_events(bool) \u2013 Enable the accumulation of FunctionEvents across multiple profiling cycles\nWarning\nEnabling memory profiling or source attribution incurs additional profiler\noverhead\nWarning\nThis context managers should not be called recursively, i.e. no nested\ninstances are allowed\nWarning\nDue to some CUDA multiprocessing limitations (seeCUDA in multiprocessing),\none cannot use the profiler withuse_device='cuda'to benchmark\nDataLoaders withnum_workers>0. If you wish to benchmark data loading,\nplease useuse_device=Noneornum_workers=0.\nuse_device='cuda'\nnum_workers>0\nuse_device=None\nnum_workers=0\nExample\n\n```python\n>>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>>         y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n\n```\n\nprofiler.profile.export_chrome_trace\nprofiler.profile.export_chrome_trace\nExport an EventList as a Chrome tracing tools file.\nprofiler.profile.key_averages\nprofiler.profile.key_averages\nAverages all function events over their keys.\nprofiler.profile.self_cpu_time_total\nprofiler.profile.self_cpu_time_total\nReturns total time spent on CPU.\nprofiler.profile.total_average\nprofiler.profile.total_average\nAverages all events.\nprofiler.parse_nvprof_trace\nprofiler.parse_nvprof_trace\n\nprofiler.EnforceUnique\nprofiler.EnforceUnique\nRaises an error if a key is seen more than once.\nprofiler.KinetoStepTracker\nprofiler.KinetoStepTracker\nProvides an abstraction for incrementing the step count globally.\nprofiler.record_function\nprofiler.record_function\nContext manager/function decorator that adds a label to a code block/function when running autograd profiler.\nprofiler_util.Interval\nprofiler_util.Interval\n\nprofiler_util.Kernel\nprofiler_util.Kernel\n\nprofiler_util.MemRecordsAcc\nprofiler_util.MemRecordsAcc\nAcceleration structure for accessing mem_records in interval.\nprofiler_util.StringTable\nprofiler_util.StringTable\n\nContext manager that makes every autograd operation emit an NVTX range.\nIt is useful when running the program under nvprof:\n\n```python\nnvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n\n```\n\nUnfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, ortorch.autograd.profiler.load_nvprof()can load the results for inspection\ne.g. in Python REPL.\ntorch.autograd.profiler.load_nvprof()\nenabled(bool,optional) \u2013 Settingenabled=Falsemakes this context manager a no-op.\nDefault:True.\nenabled=False\nTrue\nrecord_shapes(bool,optional) \u2013 Ifrecord_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:[[arg0.size(0),arg0.size(1),...],[arg1.size(0),arg1.size(1),...],...]Non-tensor arguments will be represented by[].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.\nDefault:False\nrecord_shapes=True\n[[arg0.size(0),arg0.size(1),...],[arg1.size(0),arg1.size(1),...],...]\n[]\nFalse\nExample\n\n```python\n>>> with torch.cuda.profiler.profile():\n...     model(x)  # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n\n```\n\nForward-backward correlation\nWhen viewing a profile created usingemit_nvtxin the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task,emit_nvtxappends sequence number information to the ranges it\ngenerates.\nemit_nvtx\nemit_nvtx\nDuring the forward pass, each function range is decorated withseq=<N>.seqis a running\ncounter, incremented each time a new backward Function object is created and stashed for backward.\nThus, theseq=<N>annotation associated with each forward function range tells you that\nif a backward Function object is created by this forward function,\nthe backward object will receive sequence number N.\nDuring the backward pass, the top-level range wrapping each C++ backward Function\u2019sapply()call is decorated withstashedseq=<M>.Mis the sequence number that\nthe backward object was created with.  By comparingstashedseqnumbers in backward withseqnumbers in forward, you can track down which forward op created each backward Function.\nseq=<N>\nseq\nseq=<N>\napply()\nstashedseq=<M>\nM\nstashedseq\nseq\nAny functions executed during the backward pass are also decorated withseq=<N>.  During\ndefault backward (withcreate_graph=False) this information is irrelevant, and in fact,Nmay simply be 0 for all such functions.  Only the top-level ranges associated with\nbackward Function objects\u2019apply()methods are useful, as a way to correlate these Function\nobjects with the earlier forward pass.\nseq=<N>\ncreate_graph=False\nN\napply()\nDouble-backward\nIf, on the other hand, a backward pass withcreate_graph=Trueis underway (in other words,\nif you are setting up for a double-backward), each function\u2019s execution during backward\nis given a nonzero, usefulseq=<N>.  Those functions may themselves create Function objects\nto be executed later during double-backward, just as the original functions in the forward pass did.\nThe relationship between backward and double-backward is conceptually the same as the relationship\nbetween forward and backward: The functions still emit current-sequence-number-tagged ranges,\nthe Function objects they create still stash those sequence numbers, and during the eventual\ndouble-backward, the Function objects\u2019apply()ranges are still tagged withstashedseqnumbers, which can be compared toseqnumbers from the backward pass.\ncreate_graph=True\nseq=<N>\napply()\nstashedseq\nContext manager that makes every autograd operation emit an ITT range.\nIt is useful when running the program under Intel(R) VTune Profiler:\n\n```python\nvtune <--vtune-flags> <regular command here>\n\n```\n\nThe Instrumentation and Tracing Technology (ITT) API enables your application to generate and\ncontrol the collection of trace data during its execution across different Intel tools.\nThis context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager,\nyou will be able to see labeled ranges in Intel(R) VTune Profiler GUI.\nenabled(bool,optional) \u2013 Settingenabled=Falsemakes this context manager a no-op.\nDefault:True.\nenabled=False\nTrue\nrecord_shapes(bool,optional) \u2013 Ifrecord_shapes=True, the itt range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:[[arg0.size(0),arg0.size(1),...],[arg1.size(0),arg1.size(1),...],...]Non-tensor arguments will be represented by[].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of itt range creation.\nDefault:False\nrecord_shapes=True\n[[arg0.size(0),arg0.size(1),...],[arg1.size(0),arg1.size(1),...],...]\n[]\nFalse\nExample\n\n```python\n>>> with torch.autograd.profiler.emit_itt():\n...     model(x)\n\n```\n\nprofiler.load_nvprof\nprofiler.load_nvprof\nOpen an nvprof trace file and parses autograd annotations.\n\n## Debugging and anomaly detection#\n\nContext-manager that enable anomaly detection for the autograd engine.\nThis does two things:\nRunning the forward pass with detection enabled will allow the backward\npass to print the traceback of the forward operation that created the failing\nbackward function.\nIfcheck_nanisTrue, any backward computation that generate \u201cnan\u201d\nvalue will raise an error. DefaultTrue.\ncheck_nan\nTrue\nTrue\nWarning\nThis mode should be enabled only for debugging as the different tests\nwill slow down your program execution.\nExample\n\n```python\n>>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n\n```\n\nContext-manager that sets the anomaly detection for the autograd engine on or off.\nset_detect_anomalywill enable or disable the autograd anomaly detection\nbased on its argumentmode.\nIt can be used as a context-manager or as a function.\nset_detect_anomaly\nmode\nSeedetect_anomalyabove for details of the anomaly detection behaviour.\ndetect_anomaly\nmode(bool) \u2013 Flag whether to enable anomaly detection (True),\nor disable (False).\nTrue\nFalse\ncheck_nan(bool) \u2013 Flag whether to raise an error when the backward\ngenerate \u201cnan\u201d\ngrad_mode.set_multithreading_enabled\ngrad_mode.set_multithreading_enabled\nContext-manager that sets multithreaded backwards on or off.\n\n## Autograd graph#\n\nAutograd exposes methods that allow one to inspect the graph and interpose behavior during\nthe backward pass.\nThegrad_fnattribute of atorch.Tensorholds atorch.autograd.graph.Nodeif the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is\nenabled and at least one of the inputs required gradients), orNoneotherwise.\ngrad_fn\ntorch.Tensor\ntorch.autograd.graph.Node\nNone\ngraph.Node.name\ngraph.Node.name\nReturn the name.\ngraph.Node.metadata\ngraph.Node.metadata\nReturn the metadata.\ngraph.Node.next_functions\ngraph.Node.next_functions\n\ngraph.Node.register_hook\ngraph.Node.register_hook\nRegister a backward hook.\ngraph.Node.register_prehook\ngraph.Node.register_prehook\nRegister a backward pre-hook.\ngraph.increment_version\ngraph.increment_version\nUpdate autograd metadata tracking whether the given Tensor was modified in place.\nSome operations need intermediary results to be saved during the forward pass\nin order to execute the backward pass.\nThese intermediary results are saved as attributes on thegrad_fnand can be accessed.\nFor example:\ngrad_fn\n\n```python\n>>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> b = a.exp()\n>>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))\nTrue\n>>> print(dir(b.grad_fn))\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n>>> print(torch.allclose(b.grad_fn._saved_result, b))\nTrue\n\n```\n\nYou can also define how these saved tensors should be packed / unpacked using hooks.\nA common application is to trade compute for memory by saving those intermediary results\nto disk or to CPU instead of leaving them on the GPU. This is especially useful if you\nnotice your model fits on GPU during evaluation, but not training.\nAlso seeHooks for saved tensors.\nContext-manager that sets a pair of pack / unpack hooks for saved tensors.\nUse this context-manager to define how intermediary results of an operation\nshould be packed before saving, and unpacked on retrieval.\nIn that context, thepack_hookfunction will be called every time an\noperation saves a tensor for backward (this includes intermediary results\nsaved usingsave_for_backward()but\nalso those recorded by a PyTorch-defined operation). The output ofpack_hookis then stored in the computation graph instead of the\noriginal tensor.\npack_hook\nsave_for_backward()\npack_hook\nTheunpack_hookis called when the saved tensor needs to be accessed,\nnamely when executingtorch.Tensor.backward()ortorch.autograd.grad(). It takes as argument thepackedobject\nreturned bypack_hookand should return a tensor which has the same\ncontent as the original tensor (passed as input to the correspondingpack_hook).\nunpack_hook\ntorch.Tensor.backward()\ntorch.autograd.grad()\npack_hook\npack_hook\nThe hooks should have the following signatures:\npack_hook(tensor: Tensor) -> Any\nunpack_hook(Any) -> Tensor\nwhere the return value ofpack_hookis a valid input tounpack_hook.\npack_hook\nunpack_hook\nIn general, you wantunpack_hook(pack_hook(t))to be equal totin terms\nof value, size, dtype and device.\nunpack_hook(pack_hook(t))\nt\nExample:\n\n```python\n>>> def pack_hook(x):\n...     print(\"Packing\", x)\n...     return x.detach()\n>>>\n>>> def unpack_hook(x):\n...     print(\"Unpacking\", x)\n...     return x\n>>>\n>>> a = torch.ones(5, requires_grad=True)\n>>> b = torch.ones(5, requires_grad=True) * 2\n>>> with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n...     y = a * b\nPacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nPacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n>>> y.sum().backward()\nUnpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nUnpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n\n```\n\nWarning\nPerforming an inplace operation on the input to either hooks may lead\nto undefined behavior.\nWarning\nOnly one pair of hooks is allowed at a time. When recursively nesting this\ncontext-manager, only the inner-most pair of hooks will be applied.\nWarning\nTo avoid reference cycle, the return value ofpack_hookcannot hold a\nreference to the input tensor. For example, uselambda x: x.detach()instead oflambda x: xas the pack hook.\npack_hook\nContext manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.\nWhen performing operations within this context manager, intermediary\nresults saved in the graph during the forward pass will be moved to CPU,\nthen copied back to the original device when needed for the backward pass.\nIf the graph was already on CPU, no tensor copy is performed.\nUse this context-manager to trade compute for GPU memory usage (e.g.\nwhen your model doesn\u2019t fit in GPU memory during training).\npin_memory(bool) \u2013 IfTruetensors will be saved to CPU pinned memory\nduring packing and copied to GPU asynchronously during unpacking.\nDefaults toFalse.\nAlso seeUse pinned memory buffers.\nTrue\nFalse\nExample:\n\n```python\n>>> a = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> b = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> c = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>>\n>>> def f(a, b, c):\n...     prod_1 = a * b           # a and b are saved on GPU\n...     with torch.autograd.graph.save_on_cpu():\n...         prod_2 = prod_1 * c  # prod_1 and c are saved on CPU\n...     y = prod_2 * a           # prod_2 and a are saved on GPU\n...     return y\n>>>\n>>> y = f(a, b, c)\n>>> del a, b, c  # for illustration only\n>>> # the content of a, b, and prod_2 are still alive on GPU\n>>> # the content of prod_1 and c only live on CPU\n>>> y.sum().backward()  # all CPU tensors are moved back to GPU, for backward\n>>> # all intermediary tensors are released (deleted) after the call to backward\n\n```\n\nContext-manager that disables the saved tensors default hooks feature.\nUseful for if you are creating a feature that does not work with saved\ntensors default hooks.\nerror_message(str) \u2013 When saved tensors default hooks are used when they\nhave been are disabled, a RuntimeError with this\nerror message gets raised.\nGenerator[None, None, None]\nExample:\n\n```python\n>>> message = \"saved tensors default hooks are disabled\"\n>>> with torch.autograd.graph.disable_saved_tensors_hooks(message):\n...     # Raises RuntimeError: saved tensors default hooks are disabled\n...     with torch.autograd.graph.save_on_cpu():\n...         pass\n\n```\n\nRegister a multi-grad backward hook.\nThere are two supported modes:\"all\"and\"any\".\n\"all\"\n\"any\"\nUnder the\"all\"mode, the hook will be called after gradients with respect to every tensor intensorshave been computed. If a tensor is intensorsbut\nis not part of the graph, or if a tensor is not needed to compute the gradients\nfor anyinputsspecified for the current.backward()or.grad()call,\nthis tensor will be ignored and the hook will not wait for its gradient to be\ncomputed.\n\"all\"\ntensors\ntensors\ninputs\n.backward()\n.grad()\nAfter every non-ignored tensor\u2019s gradient has been computed,fnwill be\ncalled with those gradients.Nonewill be passed for tensors that did not\nhave their gradients computed.\nfn\nNone\nUnder the\"any\"mode, the hook will be called after the first gradient\nwith respect to a tensor intensorshas been computed. The hook\nwill be called with that gradient as its argument.\n\"any\"\ntensors\nThe hook should not modify its arguments.\nThis function returns a handle with a methodhandle.remove()that removes the hook.\nhandle.remove()\nNote\nSeeBackward Hooks executionfor more information on how when this hook\nis executed, and how its execution is ordered relative to other hooks.\nExample:\n\n```python\n>>> import torch\n>>>\n>>> a = torch.rand(2, 3, requires_grad=True)\n>>> b = torch.rand(2, 3, requires_grad=True)\n>>> c = a * b\n>>> d = a * b\n>>>\n>>> def fn(grads):\n...     print([g is not None for g in grads])\n...\n>>> torch.autograd.graph.register_multi_grad_hook((a, b, c, d), fn)\n>>>\n>>> c.sum().backward(retain_graph=True)\n[True, True, True, False]\n>>> c.sum().backward(inputs=(a,), retain_graph=True)\n[True, False, True, False]\n>>>\n\n```\n\nRemovableHandle\nContext manager under which mutating tensors saved for backward is allowed.\nUnder this context manager, tensors saved for backward are cloned on mutation,\nso the original version can still be used during backward. Normally, mutating a tensor\nsaved for backward will result in an error raised when it\u2019s used during backward.\nTo ensure the correct behavior, both the forward and backward should be run under\nthe same context manager.\nAn _AllowMutationOnSavedContext object storing the state managed by this\ncontext manager. This object can be useful for debugging purposes. The state\nmanaged by the context manager is automatically cleared upon exiting.\nGenerator[_AllowMutationOnSavedContext, None, None]\nExample:\n\n```python\n>>> import torch\n>>> with torch.autograd.graph.allow_mutation_on_saved_tensors():\n...     # forward\n...     a = torch.ones(2, 3, requires_grad=True)\n...     b = a.clone()\n...     out = (b**2).sum()\n...     b.sin_()\n...     # backward\n...     out.sum().backward()\n...\ntensor([[0.8415, 0.8415, 0.8415],\n        [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>)\n\n```\n\nObject representing a given gradient edge within the autograd graph.\nTo get the gradient edge where a given Tensor gradient will be computed,\nyou can doedge=autograd.graph.get_gradient_edge(tensor).\nedge=autograd.graph.get_gradient_edge(tensor)\nGet the gradient edge for computing the gradient of the given Tensor.\nIn particular, it is equivalent to callg=autograd.grad(loss,input)andg=autograd.grad(loss,get_gradient_edge(input)).\ng=autograd.grad(loss,input)\ng=autograd.grad(loss,get_gradient_edge(input))\nGradientEdge",
    "url": "https://pytorch.org/docs/stable/autograd.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5458d2bae300b3c248b9fff2b75a8f5c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/named_tensor.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b6f56963e0cb14af63d86ce55be69bab",
    "source": "pytorch_docs",
    "title": "Benchmark Utils - torch.utils.benchmark \u2014 PyTorch 2.9 documentation",
    "text": "\n## Benchmark Utils - torch.utils.benchmark#\n\nCreated On: Nov 02, 2020 | Last Updated On: Jun 12, 2025\nHelper class for measuring execution time of PyTorch statements.\nFor a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html\nThe PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences:\nTimer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous accelerator functions when\nnecessary.\nWhen measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) Thetimeitmethod is replicated for cases where an adaptive strategy is not\ndesired.\nWhen defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison.\nIn addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed.\nDirectly analogous totimeit.Timerconstructor arguments:\nstmt,setup,timer,globals\nPyTorch Timer specific constructor arguments:\nlabel,sub_label,description,env,num_threads\nstmt(str) \u2013 Code snippet to be run in a loop and timed.\nsetup(str) \u2013 Optional setup code. Used to define variables used instmt\nglobal_setup(str) \u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.\ntimer(Callable[[],float]) \u2013 Callable which returns the current time. If PyTorch was built\nwithout accelerators or there is no accelerator present, this defaults totimeit.default_timer; otherwise it will synchronize accelerators before\nmeasuring the time.\nglobals(Optional[dict[str,Any]]) \u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds.\nlabel(Optional[str]) \u2013 String which summarizesstmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability.\nsub_label(Optional[str]) \u2013Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d\u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare.\nProvide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d\n\u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare.\ndescription(Optional[str]) \u2013String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form:|n=1|n=4|...-------------...ReLU(x+1):(float)|...|...|...ReLU(x+1):(int)|...|...|...usingCompare. It is also included when printing a Measurement.\nString to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form:\n\n```python\n                        | n=1 | n=4 | ...\n                        ------------- ...\nReLU(x + 1): (float)    | ... | ... | ...\nReLU(x + 1): (int)      | ... | ... | ...\n\n```\n\nusingCompare. It is also included when printing a Measurement.\nenv(Optional[str]) \u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivalent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.\nnum_threads(int) \u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performance is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores.\nSimilar toblocked_autorangebut also checks for variablility in measurements\nand repeats until iqr/median is smaller thanthresholdormax_run_timeis reached.\nAt a high level, adaptive_autorange executes the following pseudo-code:\n\n```python\n`setup`\n\ntimes = []\nwhile times.sum < max_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    times.append(timer() - start)\n\n    enough_data = len(times)>3 and times.sum > min_run_time\n    small_iqr=times.iqr/times.mean<threshold\n\n    if enough_data and small_iqr:\n        break\n\n```\n\nthreshold(float) \u2013 value of iqr/median threshold for stopping\nmin_run_time(float) \u2013 total runtime needed before checkingthreshold\nmax_run_time(float) \u2013 total runtime  for all measurements regardless ofthreshold\nAMeasurementobject that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.)\nMeasurement\nMeasure many replicates while keeping timer overhead to a minimum.\nAt a high level, blocked_autorange executes the following pseudo-code:\n\n```python\n`setup`\n\ntotal_time = 0\nwhile total_time < min_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    total_time += (timer() - start)\n\n```\n\nNote the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives:\nA small block size results in more replicates and generally\nbetter statistics.\nA large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because accelerator synchronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement.\nblocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop.\nAMeasurementobject that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.)\nMeasurement\nCollect instruction counts using Callgrind.\nUnlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, however this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements.\nIn order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.\nBecause there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and TorchScripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly.\nBy default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt.\nACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results.\nMirrors the semantics of timeit.Timer.timeit().\nExecute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit\nMeasurement\nThe result of a Timer measurement.\nThis class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers.\nConvenience method for merging replicates.\nMerge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates)\nlist[\u2018Measurement\u2019]\nApproximate significant figure estimate.\nThis property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.\nThe significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare.\nTop level container for Callgrind results collected by Timer.\nManipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized().\nStrip library names and some prefixes from function strings.\nWhen comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:\n\n```python\n23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)\n\n```\n\nStripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivalent call sites\nwhen diffing.\nCallgrindStats\nReturns the total number of instructions executed.\nSeeFunctionCounts.denoise()for an explanation of thedenoisearg.\nint\nDiff two sets of counts.\nOne common reason to collect instruction counts is to determine the\nthe effect that a particular change will have on the number of instructions\nneeded to perform some unit of work. If a change increases that number, the\nnext logical question is \u201cwhy\u201d. This generally involves looking at what part\nif the code increased in instruction count. This function automates that\nprocess so that one can easily diff counts on both an inclusive and\nexclusive basis.\nFunctionCounts\nReturns detailed function counts.\nConceptually, the FunctionCounts returned can be thought of as a tuple\nof (count, path_and_function_name) tuples.\ninclusivematches the semantics of callgrind. If True, the counts\ninclude instructions executed by children.inclusive=Trueis useful\nfor identifying hot spots in code;inclusive=Falseis useful for\nreducing noise when diffing counts from two different runs. (See\nCallgrindStats.delta(\u2026) for more details)\nFunctionCounts\nContainer for manipulating Callgrind results.\nAddition and subtraction to combine or diff results.\nTuple-like indexing.\nAdenoisefunction which strips CPython calls which are known to\nbe non-deterministic and quite noisy.\nTwo higher order methods (filterandtransform) for custom\nmanipulation.\nRemove known noisy instructions.\nSeveral instructions in the CPython interpreter are rather noisy. These\ninstructions involve unicode to dictionary lookups which Python uses to\nmap variable names. FunctionCounts is generally a content agnostic\ncontainer, however this is sufficiently important for obtaining\nreliable results to warrant an exception.\nFunctionCounts\nKeep only the elements wherefilter_fnapplied to function name returns True.\nFunctionCounts\nApplymap_fnto all of the function names.\nThis can be used to regularize function names (e.g. stripping irrelevant\nparts of the file path), coalesce entries by mapping multiple functions\nto the same name (in which case the counts are added together), etc.\nFunctionCounts\nHelper class for displaying the results of many measurements in a\nformatted table.\nThe table format is based on the information fields provided intorch.utils.benchmark.Timer(description,label,sub_label,num_threads, etc).\ntorch.utils.benchmark.Timer\nThe table can be directly printed usingprint()or casted as astr.\nprint()\nFor a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html\nresults(list[torch.utils.benchmark.utils.common.Measurement]) \u2013 List of Measurement to display.\nColorize formatted table.\nColorize columnwise by default.\nAppend results to already stored ones.\nAll added results must be instances ofMeasurement.\nMeasurement\nEnables warning highlighting when building formatted table.\nPrint formatted table\nEnables trimming of significant figures when building the formatted table.",
    "url": "https://pytorch.org/docs/stable/benchmark_utils.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "601d4fdbc99c5fc1d9b0157c2571c374",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/optim.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "73113da12d8287ddbba29f0cef50e8d0",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/func.migrating.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "88d2c1ec6cda65eaa5e640b46d32a129",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/nn.functional.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "afced530a65cb66bcf3d194066e09735",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/torch.cuda.make_graphed_callables.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7920a566f2cd85b735d90c41f057f9fe",
    "source": "pytorch_docs",
    "title": "Index \u2014 PyTorch 2.9 documentation",
    "text": "\n## Index\n\n\n## _\n\n\n## A\n\n\n## B\n\n\n## C\n\n\n## D\n\n\n## E\n\n\n## F\n\n\n## G\n\n\n## H\n\n\n## I\n\n\n## J\n\n\n## K\n\n\n## L\n\n\n## M\n\n\n## N\n\n\n## O\n\n\n## P\n\n\n## Q\n\n\n## R\n\n\n## S\n\n\n## T\n\n\n## U\n\n\n## V\n\n\n## W\n\n\n## X\n\n\n## Z\n",
    "url": "https://pytorch.org/docs/stable/genindex.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4560c8ae58ab619fc6ca1ec640e777c9",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/func.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7aaece1b15db9606e4dc01a4fb08b3cf",
    "source": "pytorch_docs",
    "title": "torch.utils.model_zoo \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.model_zoo#\n\nCreated On: Jan 09, 2017 | Last Updated On: Jun 11, 2025\nMoved totorch.hub.\ntorch.hub\nLoads the Torch serialized object at the given URL.\nIf downloaded file is a zip file, it will be automatically\ndecompressed.\nIf the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir().\nmodel_dir\n<hub_dir>/checkpoints\nhub_dir\nget_dir()\nurl(str) \u2013 URL of the object to download\nmodel_dir(str,optional) \u2013 directory in which to save the object\nmap_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load)\nprogress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True\ncheck_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False\nfilename-<sha256>.ext\n<sha256>\nfile_name(str,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set.\nurl\nweights_only(bool,optional) \u2013 If True, only weights will be loaded and no complex pickled objects.\nRecommended for untrusted sources. Seeload()for more details.\nload()\ndict[str,Any]\nExample\n\n```python\n>>> state_dict = torch.hub.load_state_dict_from_url(\n...     \"https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth\"\n... )\n\n```\n",
    "url": "https://pytorch.org/docs/stable/model_zoo.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d61e934fe7a8da37573b297d68a30a89",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/logging.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8459d19fafcbeb7c79d734baf08a0bc5",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/dlpack.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c08e13fbe5a52d9569ca355c0dbe1065",
    "source": "pytorch_docs",
    "title": "Draft Export \u2014 PyTorch 2.9 documentation",
    "text": "\n## Draft Export#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jul 16, 2025\nWarning\nThis feature is not meant to be used in production and is designed to be\nused as a tool for debugging torch.export tracing errors.\nDraft-export is a new version of export, which is designed to consistently\nproduce a graph, even if there are potential soundness issues, and to generate a\nreport listing out all of the issues export encountered during\ntracing and providing additional debugging information. For custom operators that\ndon\u2019t have fake kernels, it will also generate a profile which you can register\nto automatically generate a fake kernel.\nHave you ever tried to export a model usingtorch.export.export(), only to\nencounter a data-dependent issue? You fix it, but then run into a missing fake\nkernel problem. And after resolving that, you get hit with another\ndata-dependent issue. You wonder to yourself, I wish there was a way I could\njust get a graph to play around with, and be able to view all the issues in one\nplace so that I can fix them later\u2026\ntorch.export.export()\ndraft_exportto the rescue!\ndraft_export\ndraft_exportis a version of export which will always successfully export a\ngraph, even if there are potential soundness issues. These issues will then be\ncompiled into a report for clearer visualization, which can be fixed later on.\ndraft_export\n\n## What sort of errors does it catch?#\n\nDraft-export helps to catch and debug the following errors:\nGuard on data-dependent errors\nConstraint violation errors\nMissing fake kernels\nIncorrectly written fake kernels\n\n## How does it work?#\n\nIn normal export, we will convert the sample inputs into FakeTensors and use\nthem to record operations and trace the program into a graph. Input tensor\nshapes that can change (which are marked throughdynamic_shapes), or values\nwithin tensors (typically from an.item()call) will be represented as a symbolic\nshape (SymInt) instead of a concrete integer. However some issues may occur\nwhile tracing - we may run into guards that we cannot evaluate, like if we want\nto check if some item in a tensor is greater than 0 (u0>=0). Since the tracer\ndoesn\u2019t know anything about the value ofu0, it will throw a data-dependent\nerror. If the model uses a custom operator but a fake kernel hasn\u2019t been\ndefined for it, then we will error withfake_tensor.UnsupportedOperatorExceptionbecause export doesn\u2019t know how to apply this onFakeTensors. If a custom\noperator has a fake kernel implemented incorrectly, export will silently produce\nan incorrect graph that doesn\u2019t match the eager behavior.\ndynamic_shapes\n.item()\nSymInt\nu0>=0\nu0\nfake_tensor.UnsupportedOperatorException\nFakeTensors\nTo fix the above errors, draft-export usesreal tensor tracingto guide us on\nhow to proceed when tracing. As we trace the model with fake tensors, for every\noperation that happens on a fake tensor, draft-export will also run the operator\non stored real tensors which come from the example inputs passed to export. This\nallows us to address the above errors: When we reach a guard that we cannot\nevaluate, likeu0>=0, we will use the stored real tensor values to\nevaluate this guard. Runtime asserts will be added into the graph to ensure that\nthe graph asserts the same guard that we assumed while tracing. If we run into\na custom operator without a fake kernel, we will run the operator\u2019s normal\nkernel with the stored real tensors, and return a fake tensor with the same rank\nbut unbacked shapes. Since we have the real tensor output for every operation,\nwe will compare this with the fake tensor output from the fake kernel. If the\nfake kernel is implemented incorrectly, we will then catch this behavior and\ngenerate a more correct fake kernel.\nu0>=0\n\n## How can I use draft export?#\n\nLet\u2019s say you\u2019re trying to export this piece of code:\n\n```python\nclass M(torch.nn.Module):\n    def forward(self, x, y, z):\n        res = torch.ops.mylib.foo2(x, y)\n\n        a = res.item()\n        a = -a\n        a = a // 3\n        a = a + 5\n\n        z = torch.cat([z, z])\n\n        torch._check_is_size(a)\n        torch._check(a < z.shape[0])\n\n        return z[:a]\n\ninp = (torch.tensor(3), torch.tensor(4), torch.ones(3, 3))\n\nep = torch.export.export(M(), inp)\n\n```\n\nThis runs into a \u201cmissing fake kernel\u201d error formylib.foo2and then aGuardOnDataDependentExpressionbecause of the slicing ofzwitha,\nan unbacked symint.\nmylib.foo2\nGuardOnDataDependentExpression\nz\na\nTo calldraft-export, we can replace thetorch.exportline with the following:\ndraft-export\ntorch.export\n\n```python\nep = torch.export.draft_export(M(), inp)\n\n```\n\nepis a valid ExportedProgram which can now be passed through further environments!\nep\n\n## Debugging with draft-export#\n\nIn the terminal output from draft-export, you should see the following message:\n\n```python\n#########################################################################################\nWARNING: 2 issue(s) found during export, and it was not able to soundly produce a graph.\nTo view the report of failures in an html page, please run the command:\n    `tlparse /tmp/export_angelayi/dedicated_log_torch_trace_axpofwe2.log --export`\nOr, you can view the errors in python by inspecting `print(ep._report)`.\n########################################################################################\n\n```\n\nDraft-export automatically dumps logs fortlparse. You can view the tracing\nerrors by usingprint(ep._report), or you can pass the logs intotlparseto generate an html report.\ntlparse\nprint(ep._report)\ntlparse\nRunning thetlparsecommand in the terminal will generate atlparseHTML report. Here is an example of thetlparsereport:\ntlparse\ntlparse\nClicking into the Data Dependent Error, we will see the following page which\ncontains information to help debug this error. Specifically, it contains:\nThe stacktrace at which this error occurs\nA list of local variables and their shapes\nInformation for how this guard was created\n\n## The returned Exported Program#\n\nBecause draft-export specializes on code paths based on the example inputs, the\nexported program resulting from draft-export is guaranteed to be runnable and\nreturn correct results forat leastthe given example inputs. Other inputs can\nwork, as long as they match the same guards that were taken when we were\ndraft-exporting.\nFor example, if we have a graph branching on if a value is greater than 5, if in\ndraft-export our example inputs were greater than 5, then the returnedExportedProgramwill specialize on that branch, and will assert that the value\nis greater than 5. This means that the program will succeed if you pass in\nanother value greater than 5, but will fail if you pass in a value less than 5.\nThis is more sound thantorch.jit.trace, which will silently specialize on the\nbranch. The proper way fortorch.exportto support both branches would be to\nrewrite the code usingtorch.cond, which will then capture both branches.\nExportedProgram\ntorch.jit.trace\ntorch.export\ntorch.cond\nBecause of the runtime assertions in the graph, the returned exported-program is\nalso retraceable withtorch.exportortorch.compile, with a minor addition in\nthe case where a custom operator is missing a fake kernel.\ntorch.export\ntorch.compile\n\n## Generating Fake Kernels#\n\nIf a custom operator does not contain a fake implementation, currently\ndraft-export will use the real-tensor propagation to get an output for the\noperator and continue tracing. However, if we run the exported program with fake\ntensors or retrace the exported model, we will still fail because there is still\nno fake kernel implementation.\nTo address this, after draft-export, we will generate an operator profile for\neach custom operator call that we encounter, and store this on the report\nattached to the exported program:ep._report.op_profiles. Users can then use the\ncontext managertorch._library.fake_profile.unsafe_generate_fake_kernelsto\ngenerate and register a fake implementation based on these operator profiles.\nThis way future fake tensor retracing will work.\nep._report.op_profiles\ntorch._library.fake_profile.unsafe_generate_fake_kernels\nThe workflow would look something like:\n\n```python\nclass M(torch.nn.Module):\n    def forward(self, a, b):\n        res = torch.ops.mylib.foo(a, b)  # no fake impl\n        return res\n\nep = draft_export(M(), (torch.ones(3, 4), torch.ones(3, 4)))\n\nwith torch._library.fake_profile.unsafe_generate_fake_kernels(ep._report.op_profiles):\n    decomp = ep.run_decompositions()\n\nnew_inp = (\n    torch.ones(2, 3, 4),\n    torch.ones(2, 3, 4),\n)\n\n# Save the profile to a yaml and check it into a codebase\nsave_op_profiles(ep._report.op_profiles, \"op_profile.yaml\")\n# Load the yaml\nloaded_op_profile = load_op_profiles(\"op_profile.yaml\")\n\n```\n\nThe operator profile is a dictionary mapping operator name to a set of profiles\nwhich describe the input and outputs of the operator, and could be manually\nwritten, saved into a yaml file, and checked into a codebase. Here\u2019s an example\nof a profile formylib.foo.default:\nmylib.foo.default\n\n```python\n\"mylib.foo.default\": {\n    OpProfile(\n        args_profile=(\n            TensorMetadata(\n                rank=2,\n                dtype=torch.float32,\n                device=torch.device(\"cpu\"),\n                layout=torch.strided,\n            ),\n            TensorMetadata(\n                rank=2,\n                dtype=torch.float32,\n                device=torch.device(\"cpu\"),\n                layout=torch.strided,\n            ),\n        ),\n        out_profile=TensorMetadata(\n            rank=2,\n            dtype=torch.float32,\n            device=torch.device(\"cpu\"),\n            layout=torch.strided,\n        ),\n    )\n}\n\n```\n\nmylib.foo.default\u2019s profile contains only one profile, which says that for 2\ninput tensors of rank 2, dtypetorch.float32, devicecpu, we will return\none tensor of rank 2, dtypetorch.float32, and devicecpu. Using the\ncontext manager, will then generate a fake kernel where given 2 input tensors of\nrank 2 (and the other tensor metadata), we will output one tensor of rank 2 (and\nthe other tensor metadata).\nmylib.foo.default\ntorch.float32\ncpu\ntorch.float32\ncpu\nIf the operator also supports other input ranks, then we can add the profile to\nthis list of profiles, either by manually adding it into the existing profile or\nrerunning draft-export with new inputs to get new profiles, so that the\ngenerated fake kernel will support more input types. Otherwise it will error.\n\n## Where to go from here?#\n\nNow that we have successfully created anExportedProgramusing draft-export,\nwe can use further compilers such asAOTInductorto optimize its performance\nand produce a runnable artifact. This optimized version can then be used for\ndeployment. In parallel, we can utilize the report generated by draft-export to\nidentify and fixtorch.exporterrors that were encountered so that the\noriginal model can be directly traceable withtorch.export.\nExportedProgram\nAOTInductor\ntorch.export\ntorch.export",
    "url": "https://pytorch.org/docs/stable/export/draft_export.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "743f87dddc2fa8aef40c021b798a1368",
    "source": "pytorch_docs",
    "title": "torch.fx.experimental \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.fx.experimental#\n\nCreated On: Feb 07, 2024 | Last Updated On: Jun 12, 2025\nWarning\nThese APIs are experimental and subject to change without notice.\n\n## torch.fx.experimental.symbolic_shapes#\n\nShapeEnv\n\nShapeEnv\n\nDimDynamic\n\nDimDynamic\nControls how to perform symbol allocation for a dimension.\nStrictMinMaxConstraint\n\nStrictMinMaxConstraint\nFor clients: the size at this dimension must be within 'vr' (which specifies a lower and upper bound, inclusive-inclusive) AND it must be non-negative and should not be 0 or 1 (but see NB below).\nRelaxedUnspecConstraint\n\nRelaxedUnspecConstraint\nFor clients: no explicit constraint; constraint is whatever is implicitly inferred by guards from tracing.\nEqualityConstraint\n\nEqualityConstraint\nRepresent and decide various kinds of equality constraints between input sources.\nSymbolicContext\n\nSymbolicContext\nData structure specifying how we should create symbols increate_symbolic_sizes_strides_storage_offset; e.g., should they be static or dynamic.\ncreate_symbolic_sizes_strides_storage_offset\nStatelessSymbolicContext\n\nStatelessSymbolicContext\nCreate symbols increate_symbolic_sizes_strides_storage_offsetvia a symbolic_context determination as given byDimDynamicandDimConstraint.\ncreate_symbolic_sizes_strides_storage_offset\nDimDynamic\nDimConstraint\nStatefulSymbolicContext\n\nStatefulSymbolicContext\nCreate symbols increate_symbolic_sizes_strides_storage_offsetvia a symbolic_context determination as given by a cache of Source:Symbol.\ncreate_symbolic_sizes_strides_storage_offset\nSubclassSymbolicContext\n\nSubclassSymbolicContext\nThe correct symbolic context for a given inner tensor of a traceable tensor subclass may differ from that of the outer symbolic context.\nDimConstraints\n\nDimConstraints\nCustom solver for a system of constraints on symbolic dimensions.\nShapeEnvSettings\n\nShapeEnvSettings\nEncapsulates all shape env settings that could potentially affect FakeTensor dispatch.\nConvertIntKey\n\nConvertIntKey\n\nCallMethodKey\n\nCallMethodKey\n\nPropagateUnbackedSymInts\n\nPropagateUnbackedSymInts\n\nDivideByKey\n\nDivideByKey\n\nInnerTensorKey\n\nInnerTensorKey\n\nSpecialization\n\nSpecialization\nThis class is used in multi-graph compilation contexts where we generate multiple specialized graphs and dispatch to the appropriate one at runtime.\nhint_int\n\nhint_int\nRetrieve the hint for an int (based on the underlying real values as observed at runtime).\nis_concrete_int\n\nis_concrete_int\nUtility to check if underlying object in SymInt is concrete value.\nis_concrete_bool\n\nis_concrete_bool\nUtility to check if underlying object in SymBool is concrete value.\nis_concrete_float\n\nis_concrete_float\nUtility to check if underlying object in SymInt is concrete value.\nhas_free_symbols\n\nhas_free_symbols\nFaster version of bool(free_symbols(val))\nhas_free_unbacked_symbols\n\nhas_free_unbacked_symbols\nFaster version of bool(free_unbacked_symbols(val))\nguard_or_true\n\nguard_or_true\nTry to guard a, if data dependent error encountered just return true.\nguard_or_false\n\nguard_or_false\nTry to guard a, if data dependent error encountered just return false.\nguard_size_oblivious\n\nguard_size_oblivious\nPerform a guard on a symbolic boolean expression in a size oblivious way.\nsym_and\n\nsym_and\nand, but for symbolic expressions, without bool casting.\nsym_eq\n\nsym_eq\nLike ==, but when run on list/tuple, it will recursively test equality and use sym_and to join the results together, without guarding.\nsym_or\n\nsym_or\nor, but for symbolic expressions, without bool casting.\nconstrain_range\n\nconstrain_range\nApplies a constraint that the passed in SymInt must lie between min-max inclusive-inclusive, WITHOUT introducing a guard on the SymInt (meaning that it can be used on unbacked SymInts).\nconstrain_unify\n\nconstrain_unify\nGiven two SymInts, constrain them so that they must be equal.\ncanonicalize_bool_expr\n\ncanonicalize_bool_expr\nCanonicalize a boolean expression by transforming it into a lt / le inequality and moving all the non-constant terms to the rhs.\nstatically_known_true\n\nstatically_known_true\nReturns True if x can be simplified to a constant and is true.\nstatically_known_false\n\nstatically_known_false\nReturns True if x can be simplified to a constant and is False.\nhas_static_value\n\nhas_static_value\nUser-code friendly utility to check if a value is static or dynamic.\nlru_cache\n\nlru_cache\n\ncheck_consistent\n\ncheck_consistent\nTest that two \"meta\" values (typically either Tensor or SymInt) have the same values, e.g., after retracing.\ncompute_unbacked_bindings\n\ncompute_unbacked_bindings\nAfter having run fake tensor propagation and producing example_value result, traverse example_value looking for freshly bound unbacked symbols and record their paths for later.\nrebind_unbacked\n\nrebind_unbacked\nSuppose we are retracing a pre-existing FX graph that previously had fake tensor propagation (and therefore unbacked SymInts).\nresolve_unbacked_bindings\n\nresolve_unbacked_bindings\nWhen we do fake tensor prop, we oftentimes will allocate new unbacked symints.\nis_accessor_node\n\nis_accessor_node\nHelper function to determine if a node is trying to access a symbolic integer such as size, stride, offset or item.\n\n## torch.fx.experimental.proxy_tensor#\n\nmake_fx\n\nmake_fx\nGiven a function f, return a new function which when executed with valid arguments to f, returns an FX GraphModule representing the set of operations that were executed during the course of execution.\nhandle_sym_dispatch\n\nhandle_sym_dispatch\nCall into the currently active proxy tracing mode to do a SymInt/SymFloat/SymBool dispatch trace on a function that operates on these arguments.\nget_proxy_mode\n\nget_proxy_mode\nCurrent the currently active proxy tracing mode, or None if we are not currently tracing.\nmaybe_enable_thunkify\n\nmaybe_enable_thunkify\nWithin this context manager, if you are doing make_fx tracing, we will thunkify all SymNode compute and avoid tracing it into the graph unless it is actually needed.\nmaybe_disable_thunkify\n\nmaybe_disable_thunkify\nWithin a context, disable thunkification.",
    "url": "https://pytorch.org/docs/stable/fx.experimental.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "31ac864ce2f997aaf994a30512fa137f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.common_graph_breaks.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9239fc26eaeec45fba009134f8515e0d",
    "source": "pytorch_docs",
    "title": "Control Flow - Cond \u2014 PyTorch 2.9 documentation",
    "text": "\n## Control Flow - Cond#\n\nCreated On: Oct 03, 2023 | Last Updated On: Jun 13, 2025\ntorch.condis a structured control flow operator. It can be used to specify if-else like control flow\nand can logically be seen as implemented as follows.\ntorch.cond\n\n```python\ndef cond(\n    pred: Union[bool, torch.Tensor],\n    true_fn: Callable,\n    false_fn: Callable,\n    operands: Tuple[torch.Tensor]\n):\n    if pred:\n        return true_fn(*operands)\n    else:\n        return false_fn(*operands)\n\n```\n\nIts unique power lies in its ability of expressingdata-dependent control flow: it lowers to a conditional\noperator (torch.ops.higher_order.cond), which preserves predicate, true function and false functions.\nThis unlocks great flexibility in writing and deploying models that change model architecture based on\nthevalueorshapeof inputs or intermediate outputs of tensor operations.\ntorch.ops.higher_order.cond\nWarning\ntorch.condis a prototype feature in PyTorch. It has limited support for input and output types and\ndoesn\u2019t support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\nRead more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\ntorch.cond\n\n## Examples#\n\nBelow is an example that uses cond to branch based on input shape:\n\n```python\n    import torch\n\n    def true_fn(x: torch.Tensor):\n        return x.cos() + x.sin()\n\n    def false_fn(x: torch.Tensor):\n        return x.sin()\n\n    class DynamicShapeCondPredicate(torch.nn.Module):\n        \"\"\"\n        A basic usage of cond based on dynamic shape predicate.\n        \"\"\"\n\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, x: torch.Tensor) -> torch.Tensor:\n            def true_fn(x: torch.Tensor):\n                return x.cos()\n\n            def false_fn(x: torch.Tensor):\n                return x.sin()\n\n            return torch.cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\n    dyn_shape_mod = DynamicShapeCondPredicate()\n\n```\n\nWe can eagerly run the model and expect the results vary based on input shape:\n\n```python\n    inp = torch.randn(3)\n    inp2 = torch.randn(5)\n    assert torch.equal(dyn_shape_mod(inp), false_fn(inp))\n    assert torch.equal(dyn_shape_mod(inp2), true_fn(inp2))\n\n```\n\nWe can export the model for further transformations and deployment:\n\n```python\n    inp = torch.randn(4, 3)\n    dim_batch = torch.export.Dim(\"batch\", min=2)\n    ep = torch.export.export(DynamicShapeCondPredicate(), (inp,), {}, dynamic_shapes={\"x\": {0: dim_batch}})\n    print(ep)\n\n```\n\nThis gives us an exported program as shown below:\n\n```python\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[s0, 3]):\n            sym_size: Sym(s0) = torch.ops.aten.sym_size.int(arg0_1, 0)\n            gt: Sym(s0 > 4) = sym_size > 4;  sym_size = None\n            true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            conditional: f32[s0, 3] = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n            return (conditional,)\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                cos: f32[s0, 3] = torch.ops.aten.cos.default(arg0_1)\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                add: f32[s0, 3] = torch.ops.aten.add.Tensor(cos, sin);  cos = sin = None\n                return add\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                return sin\n\n```\n\nNotice thattorch.condis lowered totorch.ops.higher_order.cond, its predicate becomes a Symbolic expression over the shape of input,\nand branch functions becomes two sub-graph attributes of the top level graph module.\ntorch.cond\ntorch.ops.higher_order.cond\nHere is another example that showcases how to express a data-dependent control flow:\n\n```python\n    class DataDependentCondPredicate(torch.nn.Module):\n        \"\"\"\n        A basic usage of cond based on data dependent predicate.\n        \"\"\"\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, x: torch.Tensor) -> torch.Tensor:\n            return torch.cond(x.sum() > 4.0, true_fn, false_fn, (x,))\n\n```\n\nThe exported program we get after export:\n\n```python\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[s0, 3]):\n            sum_1: f32[] = torch.ops.aten.sum.default(arg0_1)\n            gt: b8[] = torch.ops.aten.gt.Scalar(sum_1, 4.0);  sum_1 = None\n\n            true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            conditional: f32[s0, 3] = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n            return (conditional,)\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                cos: f32[s0, 3] = torch.ops.aten.cos.default(arg0_1)\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                add: f32[s0, 3] = torch.ops.aten.add.Tensor(cos, sin);  cos = sin = None\n                return add\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                return sin\n\n```\n\n\n## Invariants of torch.ops.higher_order.cond#\n\nThere are several useful invariants fortorch.ops.higher_order.cond:\ntorch.ops.higher_order.cond\nFor predicate:\nDynamicness of predicate is preserved (e.g.gtshown in the above example)\ngt\nIf the predicate in user-program is constant (e.g. a python bool constant), thepredof the operator will be a constant.\npred\nFor branches:\nThe input and output signature will be a flattened tuple.\nThey aretorch.fx.GraphModule.\ntorch.fx.GraphModule\nClosures in original function becomes explicit inputs. No closures.\nNo mutations on inputs or globals are allowed.\nFor operands:\nIt will also be a flat tuple.\nNesting oftorch.condin user program becomes nested graph modules.\ntorch.cond\n\n## API Reference#\n\nConditionally appliestrue_fnorfalse_fn.\nWarning\ntorch.condis a prototype feature in PyTorch. It has limited support for input and output types and\ndoesn\u2019t support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\nRead more about feature classification at:https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\ncondis structured control flow operator. That is, it is like a Python if-statement,\nbut has restrictions ontrue_fn,false_fn, andoperandsthat enable it to be\ncapturable using torch.compile and torch.export.\nAssuming the constraints oncond\u2019s arguments are met,condis equivalent to the following:\n\n```python\ndef cond(pred, true_branch, false_branch, operands):\n    if pred:\n        return true_branch(*operands)\n    else:\n        return false_branch(*operands)\n\n```\n\npred(Union[bool,torch.Tensor]) \u2013 A boolean expression or a tensor with one element,\nindicating which branch function to apply.\ntrue_fn(Callable) \u2013 A callable function (a -> b) that is within the\nscope that is being traced.\nfalse_fn(Callable) \u2013 A callable function (a -> b) that is within the\nscope that is being traced. The true branch and false branch must\nhave consistent input and outputs, meaning the inputs have to be\nthe same, and the outputs have to be the same type and shape. Int\noutput is also allowed. We\u2019ll make the output dynamic by turning it\ninto a symint.\noperands(Tupleofpossibly nested dict/list/tupleoftorch.Tensor) \u2013 A tuple of inputs to the\ntrue/false functions. It can be empty if true_fn/false_fn doesn\u2019t require input. Defaults to ().\nAny\nExample:\n\n```python\ndef true_fn(x: torch.Tensor):\n    return x.cos()\n\n\ndef false_fn(x: torch.Tensor):\n    return x.sin()\n\n\nreturn cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\n```\n\nThe conditional statement (akapred) must meet one of the following constraints:\nIt\u2019s atorch.Tensorwith only one element, and torch.bool dtype\nIt\u2019s a boolean expression, e.g.x.shape[0] > 10orx.dim() > 1 and x.shape[1] > 10\nThe branch function (akatrue_fn/false_fn) must meet all of the following constraints:\nThe function signature must match with operands.\nThe function must return a tensor with the same metadata, e.g. shape,\ndtype, etc.\nThe function cannot have in-place mutations on inputs or global variables.\n(Note: in-place tensor operations such asadd_for intermediate results\nare allowed in a branch)",
    "url": "https://pytorch.org/docs/stable/cond.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0fa1e5dc10e0108f482c834d1b2a3970",
    "source": "pytorch_docs",
    "title": "Profiling to understand torch.compile performance \u2014 PyTorch 2.9 documentation",
    "text": "\n## Profiling to understand torch.compile performance#\n\nCreated On: Jun 06, 2023 | Last Updated On: Jul 11, 2025\n\n## What to use torch.profiler for:#\n\ntorch.profiler is helpful for understanding the performance of your program at a kernel-level granularity - for example, it can show graph breaks and resources utilization at the level of the program. The data provided by the profiler can often help users understand where to investigate further to understand model performance.\nTo understand kernel-level performance, other tools exist, such asNvidia Nsight compute tool,AMD Omnitrace,  Intel\u00ae VTune\u2122 Profiler orinductor\u2019s profiling toolscan be used.\nSee also thegeneral pytorch profiler guide.\n\n## Basics of using torch.profiler and viewing traces#\n\nExample program: We\u2019ll use this example of profiling resnet18. Notice the following parts of this example program:\nInclude a warm-up run to wait for compilation to complete (this will warm up systems like the CUDA caching allocator)\nUsetorch.profiler.profile()context for profiling the section we are interested in\ntorch.profiler.profile()\nUseprof.export_chrome_trace(\"trace.json\")to export the profiling artifact.\nprof.export_chrome_trace(\"trace.json\")\n\n```python\n\n    import torch\n    from torchvision.models import resnet18\n\n    device = 'cuda'      # or 'cpu', 'xpu', etc.\n    model = resnet18().to(device)\n\n    inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]\n\n    model_c = torch.compile(model)\n\n    def fwd_bwd(inp):\n        out = model_c(inp)\n        out.sum().backward()\n\n    # warm up\n    fwd_bwd(inputs[0])\n\n    with torch.profiler.profile() as prof:\n        for i in range(1, 4):\n            fwd_bwd(inputs[i])\n            prof.step()\n\n    prof.export_chrome_trace(\"trace.json\")\n\n```\n\nViewing chrome traces: In the Chrome browser, open chrome://tracing and load the json file. Use the \u201cw\u201d and \u201cs\u201d keys to zoom in and out, and use \u201ca\u201d and \u201cd\u201d to scroll left and right. \u201c?\u201d will show a \u201chelp\u201d screen with a list of shortcuts.\nHere, we observe:\nCompiledFunction and CompiledFunctionBackward events, which correspond to the dynamo-compiled regions.\nCPU events at the top, and GPU events at the bottom.\nFlows between CPU and accelerator events\nEvery kernel on the accelerator occurs after being launched by code running on the CPU. The profiler can draw connections (i.e. \u201cflows\u201d) between the accelerator and CPU events to show which CPU event launched a accelerator kernel. This is particularly helpful because, with a few exceptions, accelerator kernels are launched asynchronously.\nTo view a flow connection, click on a GPU kernel and click \u201cac2g\u201d:\nAlternatively, turn onallflows with the \u201cFlow events\u201d dropdown at the top.\n\n## Working around CUDA Graph profiling issues#\n\nWhen CUDA graphs are enabled, some CUDA configurations (driver version under 525.85.12 or CUDA < 12)  can encounter issues between the profiling tools and CUDA graphs. To fix these issues, add an empty profiling context at the top of your program:\n\n```python\n\n    import torch\n\n    torch.profiler._utils._init_for_cuda_graphs()\n\n    # ... rest of program\n\n```\n\n\n## Understanding compilation time#\n\nTo understand why compilation is taking a long time, you can profile the first invocation of a torch.compile-ed program. Keep in mind that profile traces of compilations can be distorted more than typical profiling, because compilation workloads can be quite different from typical PyTorch workloads. In some cases, trace files may also be quite large. Traces > 1GB can be difficult to open with the chrome tracing tool.\nNote: roughly the same information can also be obtained in non-graphical format with :code:torch._dynamo.utils.compile_times(). This utility won\u2019t show when the compilation steps occur, but it will show the amount of time spent on each step - and times will not be affected by any profiling overhead.\ntorch._dynamo.utils.compile_times()\nSee an example below:\n\n```python\n\n    import torch\n    from torchvision.models import resnet18\n\n    # user can switch between cuda and xpu\n    device = 'cuda'\n    model = resnet18().to(device)\n    inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]\n\n    model_c = torch.compile(model)\n\n    def fwd_bwd(inp):\n        out = model_c(inp)\n        out.sum().backward()\n\n    def warmup_compile():\n        def fn(x):\n            return x.sin().relu()\n\n        x = torch.rand((2, 2), device=device, requires_grad=True)\n        fn_c = torch.compile(fn)\n        out = fn_c(x)\n        out.sum().backward()\n\n    with torch.profiler.profile() as prof:\n        with torch.profiler.record_function(\"warmup compile\"):\n            warmup_compile()\n\n        with torch.profiler.record_function(\"resnet18 compile\"):\n            fwd_bwd(inputs[0])\n\n    prof.export_chrome_trace(\"trace_compile.json\")\n\n```\n\nNote a few things:\nThe first invocation should occurduringprofiling in order to capture compilation\nAdd a warm-up compilation in order to initialize any systems that need to be lazily initialized.\n\n## Finding graph breaks: \u201cTorch-Compiled Region\u201d and \u201cCompiledFunction\u201d#\n\nAlthough there are logging tools for identifying graph breaks, the profiler provides a quick visual method of identifying :ref:graphbreaks<torch.compiler_graph_breaks>. There are two profiler events to look for:Torch-Compiled RegionandCompiledFunction.\ngraphbreaks<torch.compiler_graph_breaks>\nTorch-Compiled Region- which was introduced in PyTorch 2.2 - is a profiler event that covers the entire compiled region. Graph breaks almost always look the same: nested \u201cTorch-Compiled Region\u201d events. Starting in PyTorch 2.5, the profiler event will also contain the frame ID and the frame compile ID. The frame ID is a unique identifier for the frame, and the frame compile ID denotes how many times the frame has been compiled.\nIf you run two separate functions with torch.compile() applied independently on each of them, you should generally expect to see two adjacent (i.e NOT stacked/nested) Torch-Compiled regions. Meanwhile, if you encounter graph breaks (or disable()\u2019ed/skipped regions), expect nested \u201cTorch-Compiled Region\u201d events.\nCompiledFunction- introduced in PyTorch 2.0 - is a profiler event that appears when gradients are required for any inputs.  Each graph break will interrupt a CompiledFunction block, splitting it in two. CompiledFunction events only appear when Autograd is involved, i.e. some of the input tensors to the graph have requires_grad=True.\nWhen a CompiledFunction appears in a trace, it is typically paired with a CompiledFunctionBackward event in the backward pass. A \u201cfwd-bwd link\u201d should appear in the trace connecting the two, if the backward function is called.\nIf your use case includes a graph that doesn\u2019t require grad and doesn\u2019t include \u201cTorch-Compiled Region\u201d events, it can be more difficult to identify whether torch.compile is being applied correctly. One clue can be the existence of Inductor-generated Triton kernels.\nSee the synthetic example below for a demonstration:\n\n```python\n\n    import torch\n    import torch._dynamo\n    # user can switch between cuda and xpu\n    device = 'cuda'\n\n    class ModelWithBreaks(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            def create_sequential():\n                return torch.nn.Sequential(\n                    torch.nn.Linear(128, 128),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(128, 128),\n                    torch.nn.ReLU(),\n                )\n            self.mod1 = create_sequential()\n            self.mod2 = create_sequential()\n            self.mod3 = create_sequential()\n            self.mod4 = create_sequential()\n\n        def forward(self, inp):\n            mod1 = self.mod1(inp)\n            torch._dynamo.graph_break()\n            mod2 = self.mod2(mod1)\n            torch._dynamo.graph_break()\n            mod3 = self.mod3(mod2)\n            torch._dynamo.graph_break()\n            mod4 = self.mod4(mod3)\n            return mod4\n\n    model = ModelWithBreaks().to(device)\n    inputs = [torch.randn((128, 128), device=device) for _ in range(10)]\n\n    model_c = torch.compile(model)\n\n    def fwd_bwd(inp):\n        out = model_c(inp)\n        out.sum().backward()\n\n    # warm up\n    fwd_bwd(inputs[0])\n\n    with torch.profiler.profile() as prof:\n        for i in range(1, 4):\n            fwd_bwd(inputs[i])\n            prof.step()\n\n    prof.export_chrome_trace(\"trace_break.json\")\n\n```\n\n\n## Operator Kernels#\n\nWhen an operator is launched, we expect to see a few events:\nCPU-side event\nKernel launch (if dealing with a GPU kernel)\nGPU-side event\nInductor-generated Triton kernels:\nTheCPU-side eventshould appear as an event prefixed with \u201ctriton_\u201d. The events currently have minimal information - the kernel name and a launch, but less information than typical aten kernel launches (which contain input shapes, types, etc.).\nThekernel launchshould appear as cuLaunchKernel instead of cudaLaunchKernel (cudaLaunchKernel is typical for aten ops)\nTheGPU-side eventshould appear, and how descriptive the name will be depends on the inductor config for unique_kernel_names\nNon-Inductor generated Triton kernels:\nTheCPU-sideevent may not appear in traces; the machinery for automatically inserting a profiler event is currently implemented at the Inductor level, so Triton kernels that bypass Inductor may not appear in traces, unless users have annotated them manually\nThekernel launchshould appear s cuLaunchKernel instead of cudaLaunchKernel (cudaLaunchKernel is typical for aten ops)\nTheGPU-sideevent should appear, named similarly to the triton kernel that was authored.\nInductor-generated CPU kernels:\nTheCPU-side eventwill not appear in traces; we haven\u2019t added profiling for this yet.\nThekernel launchandGPU-side eventsdon\u2019t exist\nNon-Triton kernels(i.e. aten kernels or custom ops) should also be expected to sometimes appear in traces. Sometimes, Inductor will fall back to the original op implementation, in which case you will see a call to the aten op.\n\n## Launch overhead#\n\nOne common issue is bad GPU utilization. A quick way to identify this is if there are large gaps between kernels on the GPU:\nThis is often the result of CPU overhead, e.g. if the amount of time spent on the CPU between kernel launches is larger than the amount of time spent by the GPU to process the kernels. The issue is more common for small batch sizes.\nWhen using inductor, enabling CUDA graphs can often help improve performance when launch overhead is a concern.",
    "url": "https://pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e3f913b7aeeed038dc582aa87b2faf33",
    "source": "pytorch_docs",
    "title": "Use torch._dynamo.nonstrict_trace \u2014 PyTorch 2.9 documentation",
    "text": "\n## Usetorch._dynamo.nonstrict_trace#\n\ntorch._dynamo.nonstrict_trace\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nSummary:\nUsenonstrict_traceto trace a function with non-strict tracing inside of atorch.compile\u2019d region.\nYou may wish to do this because the Dynamo graph breaks on something inside of the function\nand you are sure that the function is non-strict traceable.\nnonstrict_trace\ntorch.compile\nConsider the following scenario:\n\n```python\ndef get_magic_num():\n    # This explicit graph break call is meant to emulate any kind of Dynamo\n    # graph break, e.g., the function is implemented in C, or uses some python\n    # language feature Dynamo doesn't yet support.\n    torch._dynamo.graph_break()\n    return torch.tensor([42])\n@torch.compile(fullgraph=True)\ndef func(x):\n    n = get_magic_num()\n    return x + n\ntry:\n    func(torch.rand(10))\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nCall to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_316/2253748958.py\", line 9, in func\n    n = get_magic_num()\n  File \"/tmp/ipykernel_316/2253748958.py\", line 5, in get_magic_num\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\nIf we run the code above, we\u2019ll get an error from Dynamo, because it sees a graph break while the user specifiedfullgraph=True.\nfullgraph=True\nIn these situations, if a user still wants to keepfullgraph=True, they typically have several options:\nfullgraph=True\nThe graph break is due to a language feature Dynamo doesn\u2019t yet support.\nIn this case, the user either rewrites their code, or files an issue on GitHub.\nThe graph break is due to a call to a function implemented in C.\nIn this case, the user can try to use a custom op.\nThe user could also try providing a polyfill (a reference implementation in Python)\nso that Dynamo can trace through it.\nWorst case scenario \u2013 an internal compiler error. In this case, the user likely has to file an issue on GitHub.\nIn addition to all these options, PyTorch does provide an alternativetorch._dynamo.nonstrict_trace, if the function call that induced the graph break satisfies certain requirements:\ntorch._dynamo.nonstrict_trace\nThe requirements ofgeneral non-strict tracing.\nThe inputs and outputs must contain either basic types (e.g.,int,float,list,dict,torch.Tensor),\nor user-defined types that are registered totorch.utils._pytree.\nint\nfloat\nlist\ndict\ntorch.Tensor\ntorch.utils._pytree\nThe function must be defined outside thetorch.compile\u2019d region.\ntorch.compile\nAny non-input values read by the function will be treated as a constant\n(e.g., a global tensor), and will not be guarded on.\nWhen tracing through a call to atorch._dynamo.nonstrict_trace\u2019d function,torch.compileswitches tonon-strict tracing,\nand the FX graph will eventually contain all the relevant tensor operations which happened inside that function.\ntorch._dynamo.nonstrict_trace\ntorch.compile\nFor the example above, we can usetorch._dynamo.nonstrict_tracetoeliminatethe graph break:\ntorch._dynamo.nonstrict_tracetoeliminate\n\n```python\n@torch._dynamo.nonstrict_trace\ndef get_magic_num():\n    # This explicit graph break call is meant to emulate any kind of Dynamo\n    # graph break, e.g., the function is implemented in C, or uses some python\n    # language feature Dynamo doesn't yet support.\n    torch._dynamo.graph_break()\n    return torch.tensor([42])\n@torch.compile(fullgraph=True)\ndef func(x):\n    n = get_magic_num()\n    return x + n\nprint(func(torch.rand(10)))\n# No graph break and no error.\n\n```\n\n\n```python\ntensor([42.6048, 42.6620, 42.3730, 42.1969, 42.7525, 42.4637, 42.1544, 42.3485,\n        42.8316, 42.8360])\n\n```\n\nNote that one can use it inside atorch.compile\u2019d region as well:\ntorch.compile\n\n```python\ndef get_magic_num():\n    # This explicit graph break call is meant to emulate any kind of Dynamo\n    # graph break, e.g., the function is implemented in C, or uses some python\n    # language feature Dynamo doesn't yet support.\n    torch._dynamo.graph_break()\n    return torch.tensor([42])\n@torch.compile(fullgraph=True)\ndef func(x):\n    n = torch._dynamo.nonstrict_trace(get_magic_num)()\n    return x + n\nprint(func(torch.rand(10)))\n# No graph break and no error.\n\n```\n\n\n```python\ntensor([42.8445, 42.3965, 42.4669, 42.6729, 42.7607, 42.7630, 42.2933, 42.7238,\n        42.9972, 42.4853])\n\n```\n",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.dynamo_nonstrict_trace.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2a35c0aa3e77e8dae65572e06d7a77f0",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.dynamo_nonstrict_trace.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "12b3f8ef620671acf843b1788399e1da",
    "source": "pytorch_docs",
    "title": "MPS Environment Variables \u2014 PyTorch 2.9 documentation",
    "text": "\n## MPS Environment Variables#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\nPyTorch Environment Variables\nVariable\nDescription\nPYTORCH_DEBUG_MPS_ALLOCATOR\nPYTORCH_DEBUG_MPS_ALLOCATOR\nIf set to1, set allocator logging level to verbose.\n1\nPYTORCH_MPS_LOG_PROFILE_INFO\nPYTORCH_MPS_LOG_PROFILE_INFO\nSet log options bitmask toMPSProfiler. SeeLogOptionsenum inaten/src/ATen/mps/MPSProfiler.h.\nMPSProfiler\nLogOptions\naten/src/ATen/mps/MPSProfiler.h\nPYTORCH_MPS_TRACE_SIGNPOSTS\nPYTORCH_MPS_TRACE_SIGNPOSTS\nSet profile and signpost bitmasks toMPSProfiler. SeeProfileOptionsandSignpostTypes.\nMPSProfiler\nProfileOptions\nSignpostTypes\nPYTORCH_MPS_HIGH_WATERMARK_RATIO\nPYTORCH_MPS_HIGH_WATERMARK_RATIO\nHigh watermark ratio for MPS allocator. Default is 1.7.\nPYTORCH_MPS_LOW_WATERMARK_RATIO\nPYTORCH_MPS_LOW_WATERMARK_RATIO\nLow watermark ratio for MPS allocator. Default is 1.4 (unified) or 1.0 (discrete).\nPYTORCH_MPS_FAST_MATH\nPYTORCH_MPS_FAST_MATH\nIf1, enables fast math for MPS kernels. See section 1.6.3 in theMetal Shading Language Spec.\n1\nPYTORCH_MPS_PREFER_METAL\nPYTORCH_MPS_PREFER_METAL\nIf1, uses metal kernels instead of MPS Graph APIs. Used for matmul.\n1\nPYTORCH_ENABLE_MPS_FALLBACK\nPYTORCH_ENABLE_MPS_FALLBACK\nIf1, falls back to CPU when MPS ops aren\u2019t supported.\n1\nNote\nhigh watermark ratiois a hard limit for the total allowed allocations\n0.0: disables high watermark limit (may cause system failure if system-wide OOM occurs)\n0.0\n1.0: recommended maximum allocation size (i.e., device.recommendedMaxWorkingSetSize)\n1.0\n>1.0: allows limits beyond the device.recommendedMaxWorkingSetSize\n>1.0\ne.g., value 0.95 means we allocate up to 95% of recommended maximum\nallocation size; beyond that, the allocations would fail with OOM error.\nlow watermark ratiois a soft limit to attempt limiting memory allocations up to the lower watermark\nlevel by garbage collection or committing command buffers more frequently (a.k.a, adaptive commit).\nValue between 0 to m_high_watermark_ratio (setting 0.0 disables adaptive commit and garbage collection)\ne.g., value 0.9 means we \u2018attempt\u2019 to limit allocations up to 90% of recommended maximum\nallocation size.",
    "url": "https://pytorch.org/docs/stable/mps_environment_variables.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "04a641bd6fe56015047d43c15c95cdea",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.non_strict_tracing_model.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1f3ff0af843fae261f571f7b81445bbe",
    "source": "pytorch_docs",
    "title": "Elastic Agent \u2014 PyTorch 2.9 documentation",
    "text": "\n## Elastic Agent#\n\nCreated On: May 04, 2021 | Last Updated On: Jun 07, 2025\n\n## Server#\n\nThe elastic agent is the control plane of torchelastic.\nIt is a process that launches and manages underlying worker processes.\nThe agent is responsible for:\nWorking with distributed torch: the workers are started with all the\nnecessary information to successfully and trivially calltorch.distributed.init_process_group().\ntorch.distributed.init_process_group()\nFault tolerance: monitors workers and upon detecting worker failures\nor unhealthiness, tears down all workers and restarts everyone.\nElasticity: Reacts to membership changes and restarts workers with the new\nmembers.\nThe simplest agents are deployed per node and works with local processes.\nA more advanced agent can launch and manage workers remotely. Agents can\nbe completely decentralized, making decisions based on the workers it manages.\nOr can be coordinated, communicating to other agents (that manage workers\nin the same job) to make a collective decision.\nBelow is a diagram of an agent that manages a local group of workers.\n\n## Concepts#\n\nThis section describes the high-level classes and concepts that\nare relevant to understanding the role of theagentin torchelastic.\nagent\nAn agent process responsible for managing one or more worker processes.\nThe worker processes are assumed to be regular distributed PyTorch scripts.\nWhen the worker process is created by the agent, the agent provides the\nnecessary information for the worker processes to properly initialize\na torch process group.\nThe exact deployment topology and ratio of agent-to-worker is dependent\non the specific implementation of the agent and the user\u2019s job placement\npreferences. For instance, to run a distributed training job on GPU with\n8 trainers (one per GPU) one can:\nUse 8 x single GPU instances, place an agent per instance, managing\n1 worker per agent.\nUse 4 x double GPU instances, place an agent per instance, managing\n2 workers per agent.\nUse 2 x quad GPU instances, place an agent per instance, managing\n4 workers per agent.\nUse 1 x 8 GPU instance, place an agent per instance, managing\n8 workers per agent.\nUsage\n\n```python\ngroup_result = agent.run()\n if group_result.is_failed():\n   # workers failed\n   failure = group_result.failures[0]\n   logger.exception(\"worker 0 failed with exit code : %s\", failure.exit_code)\n else:\n   return group_result.return_values[0] # return rank 0's results\n\n```\n\nReturn theWorkerGroupfor the givenrole.\nWorkerGroup\nrole\nNote that the worker group is a mutable object and hence in a\nmulti-threaded/process environment it may change state.\nImplementers are encouraged (but not required) to return\na defensive read-only copy.\nWorkerGroup\nRun the agent.\nSupports retrying the worker group on failures up tomax_restarts.\nmax_restarts\nThe result of the execution, containing the return values or\nfailure details for each worker mapped by the worker\u2019s global rank.\nException - any other failures NOT related to worker process\u2013\nRunResult\nBlueprint information about a particular type of worker.\nFor a given role, there must only exist a single worker spec.\nWorker spec is expected to be homogeneous across all nodes (machine),\nthat is each node runs the same number of workers for a particular spec.\nrole(str) \u2013 user-defined role for the workers with this spec\nlocal_world_size(int) \u2013 number local workers to run\nfn(Optional[Callable]) \u2013 (deprecated use entrypoint instead)\nentrypoint(Optional[Union[Callable,str]]) \u2013 worker function or command\nargs(tuple) \u2013 arguments to pass toentrypoint\nentrypoint\nrdzv_handler(RendezvousHandler) \u2013 handles rdzv for this set of workers\nmax_restarts(int) \u2013 number of max retries for the workers\nmonitor_interval(float) \u2013 monitor status of workers everynseconds\nn\nmaster_port(Optional[int]) \u2013 fixed port to run the c10d store on rank 0\nif not specified then will chose a random free port\nmaster_addr(Optional[str]) \u2013 fixed master_addr to run the c10d store on rank 0\nif not specified then will chose hostname on agent rank 0\nredirects\u2013 redirect std streams to a file,\nselectively redirect for a particular\nlocal rank by passing a map\ntee\u2013 tees the specified std stream(s) to console + file,\nselectively tee for a particular local rank by passing a map,\ntakes precedence overredirectssettings.\nredirects\nevent_log_handler(str) \u2013 name of the event logging handler as registered inelastic/events/handlers.py.\nGet the entry point name.\nIf the entrypoint is a function (e.g.Callable) returns its__qualname__else if the entrypoint is a binary (e.g.str), returns the binary name.\nCallable\n__qualname__\nstr\nA state of theWorkerGroup.\nWorkerGroup\nWorkers in a worker group change state as a unit. If a single worker\nin a worker group fails the entire set is considered failed:\n\n```python\nUNKNOWN - agent lost track of worker group state, unrecoverable\nINIT - worker group object created not yet started\nHEALTHY - workers running and healthy\nUNHEALTHY - workers running and unhealthy\nSTOPPED - workers stopped (interrupted) by the agent\nSUCCEEDED - workers finished running (exit 0)\nFAILED - workers failed to successfully finish (exit !0)\n\n```\n\nA worker group starts from an initialINITstate,\nthen progresses toHEALTHYorUNHEALTHYstates,\nand finally reaches a terminalSUCCEEDEDorFAILEDstate.\nINIT\nHEALTHY\nUNHEALTHY\nSUCCEEDED\nFAILED\nWorker groups can be interrupted and temporarily put intoSTOPPEDstate\nby the agent. Workers inSTOPPEDstate are scheduled to be restarted\nin the near future by the agent. Some examples of workers being put intoSTOPPEDstate are:\nSTOPPED\nSTOPPED\nSTOPPED\nWorker group failure|unhealthy observed\nMembership change detected\nWhen actions (start, stop, rdzv, retry, etc) on worker group fails\nand results in the action being partially applied to the worker group\nthe state will beUNKNOWN. Typically this happens on uncaught/unhandled\nexceptions during state change events on the agent. The agent is not\nexpected to recover worker groups inUNKNOWNstate and is better off\nself terminating and allowing the job manager to retry the node.\nUNKNOWN\nUNKNOWN\nReturn the state of the Worker.\nTrue if the worker state represents workers still running\n(e.g. that the process exists but not necessarily healthy).\nbool\nA worker instance.\nContrast this withWorkerSpecthat represents the specifications of a\nworker. AWorkeris created from aWorkerSpec. AWorkeris to\naWorkerSpecas an object is to a class.\nWorkerSpec\nWorker\nWorkerSpec\nWorker\nWorkerSpec\nTheidof the worker is interpreted\nby the specific implementation ofElasticAgent. For a local\nagent, it could be thepid(int)of the worker, for a remote\nagent it could be encoded ashost:port(string).\nid\nElasticAgent\npid(int)\nhost:port(string)\nid(Any) \u2013 uniquely identifies a worker (interpreted by the agent)\nlocal_rank(int) \u2013 local rank of the worker\nglobal_rank(int) \u2013 global rank of the worker\nrole_rank(int) \u2013 rank of the worker across all workers that have the same role\nworld_size(int) \u2013 number of workers (globally)\nrole_world_size(int) \u2013 number of workers that have the same role\nA set ofWorkerinstances.\nWorker\nThe class defines a set ofWorkerinstances for the givenWorkerSpecmanaged byElasticAgent. Whether the worker\ngroup contains cross instance workers or not depends on the implementation of the agent.\nWorker\nWorkerSpec\nElasticAgent\n\n## Implementations#\n\nBelow are the agent implementations provided by torchelastic.\nAn implementation oftorchelastic.agent.server.ElasticAgentthat handles host-local workers.\ntorchelastic.agent.server.ElasticAgent\nThis agent is deployed per host and is configured to spawnnworkers.\nWhen using GPUs,nmaps to the number of GPUs available on the host.\nn\nn\nThe local agent does not communicate to other local agents deployed on\nother hosts, even if the workers may communicate inter-host. The worker id\nis interpreted to be a local process. The agent starts and stops all worker\nprocesses as a single unit.\nThe worker function and argument passed to the worker function must be\npython multiprocessing compatible. To pass multiprocessing data structures\nto the workers you may create the data structure in the same multiprocessing\ncontext as the specifiedstart_methodand pass it as a function argument.\nstart_method\nTheexit_barrier_timeoutspecifies the amount of time (in seconds) to wait\nfor other agents to finish. This acts as a safety net to handle cases where\nworkers finish at different times, to prevent agents from viewing workers\nthat finished early as a scale-down event. It is strongly advised that the\nuser code deal with ensuring that workers are terminated in a synchronous\nmanner rather than relying on the exit_barrier_timeout.\nexit_barrier_timeout\nA named pipe based watchdog can be enabled in`LocalElasticAgent`if an\nenvironment variableTORCHELASTIC_ENABLE_FILE_TIMERwith value 1 has\nbeen defined in the`LocalElasticAgent`process.\nOptionally, another environment variable`TORCHELASTIC_TIMER_FILE`can be set with a unique file name for the named pipe. If the environment\nvariable`TORCHELASTIC_TIMER_FILE`is not set,`LocalElasticAgent`will internally create a unique file name and set it to the environment\nvariable`TORCHELASTIC_TIMER_FILE`, and this environment variable will\nbe propagated to the worker processes to allow them to connect to the same\nnamed pipe that`LocalElasticAgent`uses.\n`LocalElasticAgent`\nTORCHELASTIC_ENABLE_FILE_TIMER\n`LocalElasticAgent`\n`TORCHELASTIC_TIMER_FILE`\n`TORCHELASTIC_TIMER_FILE`\n`LocalElasticAgent`\n`TORCHELASTIC_TIMER_FILE`\n`LocalElasticAgent`\nLogs are written to the specified log directory. Each log line will be by default\nprefixed by[${role_name}${local_rank}]:(e.g.[trainer0]:foobar).\nLog prefixes can be customized by passing atemplate stringas thelog_line_prefix_templateargument.\nThe following macros (identifiers) are substituted at runtime:${role_name},${local_rank},${rank}. For example, to prefix each log line with\nglobal rank instead of the local rank, setlog_line_prefix_template=\"[${rank}]:.\n[${role_name}${local_rank}]:\n[trainer0]:foobar\nlog_line_prefix_template\n${role_name},${local_rank},${rank}\nlog_line_prefix_template=\"[${rank}]:\nExample launching function\n\n```python\ndef trainer(args) -> str:\n    return \"do train\"\n\ndef main():\n    start_method=\"spawn\"\n    shared_queue= multiprocessing.get_context(start_method).Queue()\n    spec = WorkerSpec(\n                role=\"trainer\",\n                local_world_size=nproc_per_process,\n                entrypoint=trainer,\n                args=(\"foobar\",),\n                ...<OTHER_PARAMS...>)\n    agent = LocalElasticAgent(spec, start_method)\n    results = agent.run()\n\n    if results.is_failed():\n        print(\"trainer failed\")\n    else:\n        print(f\"rank 0 return value: {results.return_values[0]}\")\n        # prints -> rank 0 return value: do train\n\n```\n\nExample launching binary\n\n```python\ndef main():\n    spec = WorkerSpec(\n                role=\"trainer\",\n                local_world_size=nproc_per_process,\n                entrypoint=\"/usr/local/bin/trainer\",\n                args=(\"--trainer-args\", \"foobar\"),\n                ...<OTHER_PARAMS...>)\n    agent = LocalElasticAgent(spec)\n    results = agent.run()\n\n    if not results.is_failed():\n        print(\"binary launches do not have return values\")\n\n```\n\n\n## Extending the Agent#\n\nTo extend the agent you can implementElasticAgentdirectly, however\nwe recommend you extendSimpleElasticAgentinstead, which provides\nmost of the scaffolding and leaves you with a few specific abstract methods\nto implement.\nElasticAgent\nSimpleElasticAgent\nAnElasticAgentthat manages one particular type of worker role.\nElasticAgent\nAnElasticAgentthat manages workers (WorkerGroup) for a singleWorkerSpecsuch as one particular type of worker role.\nElasticAgent\nWorkerGroup\nWorkerSpec\nDetermine proper ranks for worker processes.\nFast Path: when all workers have the same role and world size. We calculate\nthe global rank to be group_rank * group_world_size + local_rank. And therole_world_sizeis the same asglobal_world_size. No TCP store is used in\nthis case. This is only enabled when users set the environment variableTORCH_ELASTIC_WORKER_IDENTICALto 1.\nTime complexity: each worker O(1), overall O(1)\nSlow Path: when workers have different roles and world sizes. We use the\nthe following algorithm:\nEach agent writes its configuration(group_rank, group_world_size\n, num_workers) to the common store.\nThe rank 0 agent reads all the role_info from the store and\ndetermines each agents worker ranks.\nDetermine the global rank: the global rank of the workers is computed\nby cumulative sum of the local_world_size for all workers in front of it.\nFor efficiency reasons each worker is assigned a base global rank\nsuch that it\u2019s workers are in the range [base_global_rank,\nbase_global_rank + local_world_size).\nDetermine the role rank: The role rank is determined using the algorithms\nin the point 3 with the exception that the ranks are calculated with\nrespect to the role name.\nThe rank 0 agent writes the assigned ranks to the store.\nEach agent reads the assigned ranks from the store.\nTime complexity: each worker O(1), rank0 O(n), overall O(n)\nlist[torch.distributed.elastic.agent.server.api.Worker]\nDefine a barrier that keeps the agent process alive until all workers finish.\nWait forexit_barrier_timeoutseconds for all agents to finish\nexecuting their local workers (either successfully or not). This\nacts as a safety guard against user scripts that terminate at different\ntimes.\nexit_barrier_timeout\nStart a fresh set of workers for the worker_group.\nEssentially, a rendezvous followed by astart_workers.\nThe caller should first call_stop_workers()to stop running workers\nprior to calling this method.\nstart_workers\n_stop_workers()\nOptimistically sets the state of the worker group that\njust started asHEALTHYand delegates the actual monitoring\nof state to_monitor_workers()method\nHEALTHY\n_monitor_workers()\nCheck on the workers for theworker_group.\nworker_group\nThis function also returns the new state of the worker group.\nRunResult\nRun rendezvous for the workers specified by the worker spec.\nAssigns workers a new global rank and world size.\nUpdates the rendezvous store for the worker group.\nRestart (stops, rendezvous, starts) all local workers in the group.\nClean up any resources that were allocated during the agent\u2019s work.\ndeath_sig(Signals) \u2013 Signal to send to the child process, SIGTERM is default\nStartworker_group.spec.local_world_sizenumber of workers.\nworker_group.spec.local_world_size\nThis is according to worker spec for the worker group .\nReturns a map oflocal_rankto workerid.\nlocal_rank\nid\ndict[int,Any]\nStop all workers in the given worker group.\nImplementers must deal with workers in all states defined byWorkerState. That is, it must gracefully handle stopping\nnon-existent workers, unhealthy (stuck) workers, etc.\nWorkerState\nReturn results of the worker executions.\nRun results follow an \u201call-or-nothing\u201d policy where the run is successful if and\nonly if ALL local workers managed by this agent complete successfully.\nIf the result is successful (e.g.is_failed()=False) then thereturn_valuesfield contains the outputs (return values) of the workers managed by THIS agent mapped\nby their GLOBAL ranks. That isresult.return_values[0]is the return value of\nglobal rank 0.\nis_failed()=False\nreturn_values\nresult.return_values[0]\nNote\nreturn_valuesare only meaningful for when the worker entrypoint\nis a function. Workers specified as a binary entrypoint do not canonically\nhave a return value and thereturn_valuesfield is meaningless and\nmay be empty.\nreturn_values\nreturn_values\nIfis_failed()returnsTruethen thefailuresfield contains the\nfailure information, again, mapped by the GLOBAL rank of the worker that failed.\nis_failed()\nTrue\nfailures\nThe keys inreturn_valuesandfailuresare mutually exclusive, that is,\na worker\u2019s final state can only be one of: succeeded, failed. Workers intentionally\nterminated by the agent according to the agent\u2019s restart policy, are not represented\nin eitherreturn_valuesnorfailures.\nreturn_values\nfailures\nreturn_values\nfailures\n\n## Watchdog in the Agent#\n\nA named pipe based watchdog can be enabled inLocalElasticAgentif an\nenvironment variableTORCHELASTIC_ENABLE_FILE_TIMERwith value 1 has\nbeen defined in theLocalElasticAgentprocess.\nOptionally, another environment variableTORCHELASTIC_TIMER_FILEcan be set with a unique file name for the named pipe. If the environment\nvariableTORCHELASTIC_TIMER_FILEis not set,LocalElasticAgentwill internally create a unique file name and set it to the environment\nvariableTORCHELASTIC_TIMER_FILE, and this environment variable will\nbe propagated to the worker processes to allow them to connect to the same\nnamed pipe thatLocalElasticAgentuses.\nLocalElasticAgent\nTORCHELASTIC_ENABLE_FILE_TIMER\nLocalElasticAgent\nTORCHELASTIC_TIMER_FILE\nTORCHELASTIC_TIMER_FILE\nLocalElasticAgent\nTORCHELASTIC_TIMER_FILE\nLocalElasticAgent\n\n## Health Check Server#\n\nA health check monitoring server can be enabled inLocalElasticAgentif an environment variableTORCHELASTIC_HEALTH_CHECK_PORThas been defined\nin theLocalElasticAgentprocess.\nAdding interface for health check server which can be extended by starting tcp/http\nserver on the specified port number.\nAdditionally, health check server will have callback to check watchdog is alive.\nLocalElasticAgent\nTORCHELASTIC_HEALTH_CHECK_PORT\nLocalElasticAgent\nInterface for health check monitoring server, which can be extended\nby starting tcp/http server on the specified port.\nalive_callback(Callable[[],int]) \u2013 Callable[[], int], callback to last progress time of agent\nport(int) \u2013 int, port number to start tcp/http server\ntimeout(int) \u2013 int, timeout seconds to decide agent is alive/dead\nUnsupported functionality for Pytorch, doesn\u2019t start any health check server\nFunction to stop health check server\ncreates health check server object\nHealthCheckServer",
    "url": "https://pytorch.org/docs/stable/elastic/agent.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "588ababb0194b51bda04af65c54a4a53",
    "source": "pytorch_docs",
    "title": "torch.cuda \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.cuda#\n\nCreated On: Dec 23, 2016 | Last Updated On: Aug 19, 2025\nThis package adds support for CUDA tensor types.\nIt implements the same function as CPU tensors, but they utilize\nGPUs for computation.\nIt is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA.\nis_available()\nCUDA semanticshas more details about working with CUDA.\nStreamContext\n\nStreamContext\nContext-manager that selects a given stream.\ncan_device_access_peer\n\ncan_device_access_peer\nCheck if peer access between two devices is possible.\ncurrent_blas_handle\n\ncurrent_blas_handle\nReturn cublasHandle_t pointer to current cuBLAS handle\ncurrent_device\n\ncurrent_device\nReturn the index of a currently selected device.\ncurrent_stream\n\ncurrent_stream\nReturn the currently selectedStreamfor a given device.\nStream\ncudart\n\ncudart\nRetrieves the CUDA runtime API module.\ndefault_stream\n\ndefault_stream\nReturn the defaultStreamfor a given device.\nStream\ndevice\n\ndevice\nContext-manager that changes the selected device.\ndevice_count\n\ndevice_count\nReturn the number of GPUs available.\ndevice_memory_used\n\ndevice_memory_used\nReturn used global (device) memory in bytes as given bynvidia-smioramd-smi.\ndevice_of\n\ndevice_of\nContext-manager that changes the current device to that of given object.\nget_arch_list\n\nget_arch_list\nReturn list CUDA architectures this library was compiled for.\nget_device_capability\n\nget_device_capability\nGet the cuda capability of a device.\nget_device_name\n\nget_device_name\nGet the name of a device.\nget_device_properties\n\nget_device_properties\nGet the properties of a device.\nget_gencode_flags\n\nget_gencode_flags\nReturn NVCC gencode flags this library was compiled with.\nget_stream_from_external\n\nget_stream_from_external\nReturn aStreamfrom an externally allocated CUDA stream.\nStream\nget_sync_debug_mode\n\nget_sync_debug_mode\nReturn current value of debug mode for cuda synchronizing operations.\ninit\n\ninit\nInitialize PyTorch's CUDA state.\nipc_collect\n\nipc_collect\nForce collects GPU memory after it has been released by CUDA IPC.\nis_available\n\nis_available\nReturn a bool indicating if CUDA is currently available.\nis_initialized\n\nis_initialized\nReturn whether PyTorch's CUDA state has been initialized.\nis_tf32_supported\n\nis_tf32_supported\nReturn a bool indicating if the current CUDA/ROCm device supports dtype tf32.\nmemory_usage\n\nmemory_usage\nReturn the percent of time over the past sample period during which global (device) memory was being read or written as given bynvidia-smi.\nset_device\n\nset_device\nSet the current device.\nset_stream\n\nset_stream\nSet the current stream.This is a wrapper API to set the stream.\nset_sync_debug_mode\n\nset_sync_debug_mode\nSet the debug mode for cuda synchronizing operations.\nstream\n\nstream\nWrap around the Context-manager StreamContext that selects a given stream.\nsynchronize\n\nsynchronize\nWait for all kernels in all streams on a CUDA device to complete.\nutilization\n\nutilization\nReturn the percent of time over the past sample period during which one or more kernels was executing on the GPU as given bynvidia-smi.\ntemperature\n\ntemperature\nReturn the average temperature of the GPU sensor in Degrees C (Centigrades).\npower_draw\n\npower_draw\nReturn the average power draw of the GPU sensor in mW (MilliWatts)\nclock_rate\n\nclock_rate\nReturn the clock speed of the GPU SM in MHz (megahertz) over the past sample period as given bynvidia-smi.\nAcceleratorError\n\nAcceleratorError\nException raised while executing on device\nOutOfMemoryError\n\nOutOfMemoryError\nException raised when device is out of memory\n\n## Random Number Generator#\n\nget_rng_state\n\nget_rng_state\nReturn the random number generator state of the specified GPU as a ByteTensor.\nget_rng_state_all\n\nget_rng_state_all\nReturn a list of ByteTensor representing the random number states of all devices.\nset_rng_state\n\nset_rng_state\nSet the random number generator state of the specified GPU.\nset_rng_state_all\n\nset_rng_state_all\nSet the random number generator state of all devices.\nmanual_seed\n\nmanual_seed\nSet the seed for generating random numbers for the current GPU.\nmanual_seed_all\n\nmanual_seed_all\nSet the seed for generating random numbers on all GPUs.\nseed\n\nseed\nSet the seed for generating random numbers to a random number for the current GPU.\nseed_all\n\nseed_all\nSet the seed for generating random numbers to a random number on all GPUs.\ninitial_seed\n\ninitial_seed\nReturn the current random seed of the current GPU.\n\n## Communication collectives#\n\ncomm.broadcast\ncomm.broadcast\nBroadcasts a tensor to specified GPU devices.\ncomm.broadcast_coalesced\ncomm.broadcast_coalesced\nBroadcast a sequence of tensors to the specified GPUs.\ncomm.reduce_add\ncomm.reduce_add\nSum tensors from multiple GPUs.\ncomm.reduce_add_coalesced\ncomm.reduce_add_coalesced\nSum tensors from multiple GPUs.\ncomm.scatter\ncomm.scatter\nScatters tensor across multiple GPUs.\ncomm.gather\ncomm.gather\nGathers tensors from multiple GPU devices.\n\n## Streams and events#\n\nStream\n\nStream\nWrapper around a CUDA stream.\nExternalStream\n\nExternalStream\nWrapper around an externally allocated CUDA stream.\nEvent\n\nEvent\nWrapper around a CUDA event.\n\n## Graphs (beta)#\n\nis_current_stream_capturing\n\nis_current_stream_capturing\nReturn True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\ngraph_pool_handle\n\ngraph_pool_handle\nReturn an opaque token representing the id of a graph memory pool.\nCUDAGraph\n\nCUDAGraph\nWrapper around a CUDA graph.\ngraph\n\ngraph\nContext-manager that captures CUDA work into atorch.cuda.CUDAGraphobject for later replay.\ntorch.cuda.CUDAGraph\nmake_graphed_callables\n\nmake_graphed_callables\nAccept callables (functions ornn.Modules) and returns graphed versions.\nnn.Module\nThis package adds support for device memory management implemented in CUDA.\n\n## Memory management#\n\nempty_cache\n\nempty_cache\nRelease all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.\nget_per_process_memory_fraction\n\nget_per_process_memory_fraction\nGet memory fraction for a process.\nlist_gpu_processes\n\nlist_gpu_processes\nReturn a human-readable printout of the running processes and their GPU memory use for a given device.\nmem_get_info\n\nmem_get_info\nReturn the global free and total GPU memory for a given device using cudaMemGetInfo.\nmemory_stats\n\nmemory_stats\nReturn a dictionary of CUDA memory allocator statistics for a given device.\nmemory_stats_as_nested_dict\n\nmemory_stats_as_nested_dict\nReturn the result ofmemory_stats()as a nested dictionary.\nmemory_stats()\nreset_accumulated_memory_stats\n\nreset_accumulated_memory_stats\nReset the \"accumulated\" (historical) stats tracked by the CUDA memory allocator.\nhost_memory_stats\n\nhost_memory_stats\nReturn a dictionary of CUDA memory allocator statistics for a given device.\nhost_memory_stats_as_nested_dict\n\nhost_memory_stats_as_nested_dict\nReturn the result ofhost_memory_stats()as a nested dictionary.\nhost_memory_stats()\nreset_accumulated_host_memory_stats\n\nreset_accumulated_host_memory_stats\nReset the \"accumulated\" (historical) stats tracked by the host memory allocator.\nmemory_summary\n\nmemory_summary\nReturn a human-readable printout of the current memory allocator statistics for a given device.\nmemory_snapshot\n\nmemory_snapshot\nReturn a snapshot of the CUDA memory allocator state across all devices.\nmemory_allocated\n\nmemory_allocated\nReturn the current GPU memory occupied by tensors in bytes for a given device.\nmax_memory_allocated\n\nmax_memory_allocated\nReturn the maximum GPU memory occupied by tensors in bytes for a given device.\nreset_max_memory_allocated\n\nreset_max_memory_allocated\nReset the starting point in tracking maximum GPU memory occupied by tensors for a given device.\nmemory_reserved\n\nmemory_reserved\nReturn the current GPU memory managed by the caching allocator in bytes for a given device.\nmax_memory_reserved\n\nmax_memory_reserved\nReturn the maximum GPU memory managed by the caching allocator in bytes for a given device.\nset_per_process_memory_fraction\n\nset_per_process_memory_fraction\nSet memory fraction for a process.\nmemory_cached\n\nmemory_cached\nDeprecated; seememory_reserved().\nmemory_reserved()\nmax_memory_cached\n\nmax_memory_cached\nDeprecated; seemax_memory_reserved().\nmax_memory_reserved()\nreset_max_memory_cached\n\nreset_max_memory_cached\nReset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.\nreset_peak_memory_stats\n\nreset_peak_memory_stats\nReset the \"peak\" stats tracked by the CUDA memory allocator.\nreset_peak_host_memory_stats\n\nreset_peak_host_memory_stats\nReset the \"peak\" stats tracked by the host memory allocator.\ncaching_allocator_alloc\n\ncaching_allocator_alloc\nPerform a memory allocation using the CUDA memory allocator.\ncaching_allocator_delete\n\ncaching_allocator_delete\nDelete memory allocated using the CUDA memory allocator.\nget_allocator_backend\n\nget_allocator_backend\nReturn a string describing the active allocator backend as set byPYTORCH_CUDA_ALLOC_CONF.\nPYTORCH_CUDA_ALLOC_CONF\nCUDAPluggableAllocator\n\nCUDAPluggableAllocator\nCUDA memory allocator loaded from a so file.\nchange_current_allocator\n\nchange_current_allocator\nChange the currently used memory allocator to be the one provided.\nMemPool\n\nMemPool\nMemPool represents a pool of memory in a caching allocator.\ncaching_allocator_enable\n\ncaching_allocator_enable\nEnable or disable the CUDA memory allocator.\nA context manager that routes allocations to a given pool.\npool(torch.cuda.MemPool) \u2013 a MemPool object to be made active so that\nallocations route to this pool.\ndevice(torch.deviceorint,optional) \u2013 selected device. Uses MemPool on\nthe current device, given bycurrent_device(),\nifdeviceisNone(default).\ncurrent_device()\ndevice\nNone\nNote\nThis context manager makes only current thread\u2019s allocations route to\nthe given pool. If a new thread is spawned inside the context manager\n(e.g. by calling backward) the allocations in that thread will not\nroute to the given pool.\n\n## NVIDIA Tools Extension (NVTX)#\n\nnvtx.mark\nnvtx.mark\nDescribe an instantaneous event that occurred at some point.\nnvtx.range_push\nnvtx.range_push\nPush a range onto a stack of nested range span.\nnvtx.range_pop\nnvtx.range_pop\nPop a range off of a stack of nested range spans.\nnvtx.range\nnvtx.range\nContext manager / decorator that pushes an NVTX range at the beginning of its scope, and pops it at the end.\n\n## Jiterator (beta)#\n\njiterator._create_jit_fn\njiterator._create_jit_fn\nCreate a jiterator-generated cuda kernel for an elementwise op.\njiterator._create_multi_output_jit_fn\njiterator._create_multi_output_jit_fn\nCreate a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.\n\n## TunableOp#\n\nSome operations could be implemented using more than one library or more than\none technique. For example, a GEMM could be implemented for CUDA or ROCm using\neither the cublas/cublasLt libraries or hipblas/hipblasLt libraries,\nrespectively. How does one know which implementation is the fastest and should\nbe chosen? That\u2019s what TunableOp provides. Certain operators have been\nimplemented using multiple strategies as Tunable Operators. At runtime, all\nstrategies are profiled and the fastest is selected for all subsequent\noperations.\nSee thedocumentationfor information on how to use it.\n\n## Stream Sanitizer (prototype)#\n\nCUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch.\nSee thedocumentationfor information on how to use it.\n\n## GPUDirect Storage (prototype)#\n\nThe APIs intorch.cuda.gdsprovide thin wrappers around certain cuFile APIs that allow\ndirect memory access transfers between GPU memory and storage, avoiding a bounce buffer in the CPU. See thecufile api documentationfor more details.\ntorch.cuda.gds\nThese APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs, one must\nensure that their system is appropriately configured to use GPUDirect Storage per theGPUDirect Storage documentation.\nSee the docs forGdsFilefor an example of how to use these.\nGdsFile\ngds_register_buffer\n\ngds_register_buffer\nRegisters a storage on a CUDA device as a cufile buffer.\ngds_deregister_buffer\n\ngds_deregister_buffer\nDeregisters a previously registered storage on a CUDA device as a cufile buffer.\nGdsFile\n\nGdsFile\nWrapper around cuFile.",
    "url": "https://pytorch.org/docs/stable/cuda.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5c3f3a515a51072cac5abfe3a5a5052c",
    "source": "pytorch_docs",
    "title": "torch.library \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.library#\n\nCreated On: Jun 13, 2022 | Last Updated On: Aug 13, 2025\ntorch.library is a collection of APIs for extending PyTorch\u2019s core library\nof operators. It contains utilities for testing custom operators, creating new\ncustom operators, and extending operators defined with PyTorch\u2019s C++ operator\nregistration APIs (e.g. aten operators).\nFor a detailed guide on effectively using these APIs, please seePyTorch Custom Operators Landing Pagefor more details on how to effectively use these APIs.\n\n## Testing custom ops#\n\nUsetorch.library.opcheck()to test custom ops for incorrect usage of the\nPython torch.library and/or C++ TORCH_LIBRARY APIs. Also, if your operator supports\ntraining, usetorch.autograd.gradcheck()to test that the gradients are\nmathematically correct.\ntorch.library.opcheck()\ntorch.autograd.gradcheck()\nGiven an operator and some sample arguments, tests if the operator is\nregistered correctly.\nThat is, when you use the torch.library/TORCH_LIBRARY APIs to create a\ncustom op, you specified metadata (e.g. mutability info) about the custom op\nand these APIs require that the functions you pass them satisfy certain\nproperties (e.g. no data pointer access in the fake/meta/abstract kernel)opchecktests these metadata and properties.\nopcheck\nConcretely, we test the following:\ntest_schema: If the schema matches the implementation of\nthe operator. For example: if the schema specifies a Tensor is mutated,\nthen we check the implementation mutates the Tensor. If the schema\nspecifies that we return a new Tensor, then we check that the\nimplementation returns a new Tensor (instead of an existing one or\na view of an existing one).\ntest_autograd_registration: If the operator supports training\n(autograd): we check that its autograd formula is registered via\ntorch.library.register_autograd or a manual registration to one\nor more DispatchKey::Autograd keys. Any other DispatchKey-based\nregistrations may lead to undefined behavior.\ntest_faketensor: If the operator has a FakeTensor kernel\n(and if it is correct). The FakeTensor kernel is necessary (\nbut not sufficient) for the operator to work with PyTorch compilation\nAPIs (torch.compile/export/FX). We check that a FakeTensor kernel\n(also sometimes known as a meta kernel) was registered for the\noperator and that it is correct. This test takes the result of\nrunning the operator on real tensors and the result of running\nthe operator on FakeTensors and checks that they have the same\nTensor metadata (sizes/strides/dtype/device/etc).\ntest_aot_dispatch_dynamic: If the operator has correct behavior\nwith PyTorch compilation APIs (torch.compile/export/FX).\nThis checks that the outputs (and gradients, if applicable) are the\nsame under eager-mode PyTorch and torch.compile.\nThis test is a superset oftest_faketensorand is an e2e test;\nother things it tests are that the operator supports\nfunctionalization and that the backward pass (if it exists) also\nsupports FakeTensor and functionalization.\ntest_faketensor\nFor best results, please callopcheckmultiple times with a\nrepresentative set of inputs. If your operator supports\nautograd, please useopcheckwith inputs withrequires_grad=True;\nif your operator supports multiple devices (e.g. CPU and CUDA), please\nuseopcheckwith inputs on all supported devices.\nopcheck\nopcheck\nrequires_grad=True\nopcheck\nop(Union[OpOverload,OpOverloadPacket,CustomOpDef]) \u2013 The operator. Must either be a function decorated withtorch.library.custom_op()or an OpOverload/OpOverloadPacket\nfound in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)\ntorch.library.custom_op()\nargs(tuple[Any,...]) \u2013 The args to the operator\nkwargs(Optional[dict[str,Any]]) \u2013 The kwargs to the operator\ntest_utils(Union[str,Sequence[str]]) \u2013 Tests that we should run. Default: all of them.\nExample: (\u201ctest_schema\u201d, \u201ctest_faketensor\u201d)\nraise_exception(bool) \u2013 If we should raise an exception on the first\nerror. If False, we will return a dict with information\non if each test passed or not.\nrtol(Optional[float]) \u2013 Relative tolerance for floating point comparisons.\nIf specifiedatolmust also be specified.\nIf omitted, default values based on thedtypeare selected\n(see the table intorch.testing.assert_close()).\natol\ndtype\ntorch.testing.assert_close()\natol(Optional[float]) \u2013 Absolute tolerance for floating point comparisons.\nIf specifiedrtolmust also be specified.\nIf omitted, default values based on thedtypeare selected\n(see the table intorch.testing.assert_close()).\nrtol\ndtype\ntorch.testing.assert_close()\ndict[str,str]\nWarning\nopcheck andtorch.autograd.gradcheck()test different things;\nopcheck tests if your usage of torch.library APIs is correct whiletorch.autograd.gradcheck()tests if your autograd formula is\nmathematically correct. Use both to test custom ops that support\ngradient computation.\ntorch.autograd.gradcheck()\ntorch.autograd.gradcheck()\nExample\n\n```python\n>>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\n>>> def numpy_mul(x: Tensor, y: float) -> Tensor:\n>>>     x_np = x.numpy(force=True)\n>>>     z_np = x_np * y\n>>>     return torch.from_numpy(z_np).to(x.device)\n>>>\n>>> @numpy_mul.register_fake\n>>> def _(x, y):\n>>>     return torch.empty_like(x)\n>>>\n>>> def setup_context(ctx, inputs, output):\n>>>     y, = inputs\n>>>     ctx.y = y\n>>>\n>>> def backward(ctx, grad):\n>>>     return grad * ctx.y, None\n>>>\n>>> numpy_mul.register_autograd(backward, setup_context=setup_context)\n>>>\n>>> sample_inputs = [\n>>>     (torch.randn(3), 3.14),\n>>>     (torch.randn(2, 3, device='cuda'), 2.718),\n>>>     (torch.randn(1, 10, requires_grad=True), 1.234),\n>>>     (torch.randn(64, 64, device='cuda', requires_grad=True), 90.18),\n>>> ]\n>>>\n>>> for args in sample_inputs:\n>>>     torch.library.opcheck(numpy_mul, args)\n\n```\n\n\n## Creating new custom ops in Python#\n\nUsetorch.library.custom_op()to create new custom ops.\ntorch.library.custom_op()\nWraps a function into custom operator.\nReasons why you may want to create a custom op include:\n- Wrapping a third-party library or custom kernel to work with PyTorch\nsubsystems like Autograd.\n- Preventing torch.compile/export/FX tracing from peeking inside your function.\nThis API is used as a decorator around a function (please see examples).\nThe provided function must have type hints; these are needed to interface\nwith PyTorch\u2019s various subsystems.\nname(str) \u2013 A name for the custom op that looks like \u201c{namespace}::{name}\u201d,\ne.g. \u201cmylib::my_linear\u201d. The name is used as the op\u2019s stable identifier\nin PyTorch subsystems (e.g. torch.export, FX graphs).\nTo avoid name collisions, please use your project name as the namespace;\ne.g. all custom ops in pytorch/fbgemm use \u201cfbgemm\u201d as the namespace.\nmutates_args(Iterable[str] or\"unknown\") \u2013 The names of args that the function mutates.\nThis MUST be accurate, otherwise, the behavior is undefined. If \u201cunknown\u201d,\nit pessimistically assumes that all inputs to the operator are being mutated.\ndevice_types(None|str|Sequence[str]) \u2013 The device type(s) the function\nis valid for. If no device type is provided, then the function\nis used as the default implementation for all device types.\nExamples: \u201ccpu\u201d, \u201ccuda\u201d.\nWhen registering a device-specific implementation for an operator that accepts no Tensors,\nwe require the operator to have a \u201cdevice: torch.device argument\u201d.\nschema(None|str) \u2013 A schema string for the operator. If None\n(recommended) we\u2019ll infer a schema for the operator from its type\nannotations. We recommend letting us infer a schema unless you\nhave a specific reason not to.\nExample: \u201c(Tensor x, int y) -> (Tensor, Tensor)\u201d.\nUnion[Callable[[Callable[[\u2026],object]],CustomOpDef],CustomOpDef]\nNote\nWe recommend not passing in aschemaarg and instead letting us infer\nit from the type annotations. It is error-prone to write your own schema.\nYou may wish to provide your own schema if our interpretation of\nthe type annotation is not what you want.\nFor more info on how to write a schema string, seehere\nschema\n\n```python\n>>> import torch\n>>> from torch import Tensor\n>>> from torch.library import custom_op\n>>> import numpy as np\n>>>\n>>> @custom_op(\"mylib::numpy_sin\", mutates_args=())\n>>> def numpy_sin(x: Tensor) -> Tensor:\n>>>     x_np = x.cpu().numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np).to(device=x.device)\n>>>\n>>> x = torch.randn(3)\n>>> y = numpy_sin(x)\n>>> assert torch.allclose(y, x.sin())\n>>>\n>>> # Example of a custom op that only works for one device type.\n>>> @custom_op(\"mylib::numpy_sin_cpu\", mutates_args=(), device_types=\"cpu\")\n>>> def numpy_sin_cpu(x: Tensor) -> Tensor:\n>>>     x_np = x.numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np)\n>>>\n>>> x = torch.randn(3)\n>>> y = numpy_sin_cpu(x)\n>>> assert torch.allclose(y, x.sin())\n>>>\n>>> # Example of a custom op that mutates an input\n>>> @custom_op(\"mylib::numpy_sin_inplace\", mutates_args={\"x\"}, device_types=\"cpu\")\n>>> def numpy_sin_inplace(x: Tensor) -> None:\n>>>     x_np = x.numpy()\n>>>     np.sin(x_np, out=x_np)\n>>>\n>>> x = torch.randn(3)\n>>> expected = x.sin()\n>>> numpy_sin_inplace(x)\n>>> assert torch.allclose(x, expected)\n>>>\n>>> # Example of a factory function\n>>> @torch.library.custom_op(\"mylib::bar\", mutates_args={}, device_types=\"cpu\")\n>>> def bar(device: torch.device) -> Tensor:\n>>>     return torch.ones(3)\n>>>\n>>> bar(\"cpu\")\n\n```\n\nCreate a custom operator whose implementation is backed by 1+ triton kernels.\nThis is a more structured way of using triton kernels with PyTorch.\nPrefer using triton kernels with notorch.librarycustom operator wrappers\n(liketorch.library.custom_op(),torch.library.triton_op()) because\nthat is simpler;\nonly usetorch.library.custom_op()/torch.library.triton_op()if you\nwant to create an operator that behaves like PyTorch built-in operators.\nFor example, you may use atorch.librarywrapper API to define the\nbehavior of the triton kernel when passed a tensor subclass or under\na TorchDispatchMode.\ntorch.library\ntorch.library.custom_op()\ntorch.library.triton_op()\ntorch.library.custom_op()\ntorch.library.triton_op()\ntorch.library\nUsetorch.library.triton_op()instead oftorch.library.custom_op()when the implementation\nconsists of 1+ triton kernels.torch.library.custom_op()treats\ncustom operators as opaque (torch.compile()andtorch.export.export()will never trace into them), buttriton_opmakes the implementation visible to these subsystems, allowing them\nto optimize the triton kernel(s).\ntorch.library.triton_op()\ntorch.library.custom_op()\ntorch.library.custom_op()\ntorch.compile()\ntorch.export.export()\ntriton_op\nNote thatfnmust only consist of calls to PyTorch-understood\noperators and triton kernels. Any triton kernels called insidefnmust be wrapped in a call totorch.library.wrap_triton().\nfn\nfn\ntorch.library.wrap_triton()\nname(str) \u2013 A name for the custom op that looks like \u201c{namespace}::{name}\u201d,\ne.g. \u201cmylib::my_linear\u201d. The name is used as the op\u2019s stable identifier\nin PyTorch subsystems (e.g. torch.export, FX graphs).\nTo avoid name collisions, please use your project name as the namespace;\ne.g. all custom ops in pytorch/fbgemm use \u201cfbgemm\u201d as the namespace.\nmutates_args(Iterable[str] or\"unknown\") \u2013 The names of args that the function mutates.\nThis MUST be accurate, otherwise, the behavior is undefined. If \u201cunknown\u201d,\nit pessimistically assumes that all inputs to the operator are being mutated.\nschema(None|str) \u2013 A schema string for the operator. If None\n(recommended) we\u2019ll infer a schema for the operator from its type\nannotations. We recommend letting us infer a schema unless you\nhave a specific reason not to.\nExample: \u201c(Tensor x, int y) -> (Tensor, Tensor)\u201d.\nCallable\nExample:\n\n```python\n>>> import torch\n>>> from torch.library import triton_op, wrap_triton\n>>>\n>>> import triton\n>>> from triton import language as tl\n>>>\n>>> @triton.jit\n>>> def add_kernel(\n>>>     in_ptr0,\n>>>     in_ptr1,\n>>>     out_ptr,\n>>>     n_elements,\n>>>     BLOCK_SIZE: \"tl.constexpr\",\n>>> ):\n>>>     pid = tl.program_id(axis=0)\n>>>     block_start = pid * BLOCK_SIZE\n>>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n>>>     mask = offsets < n_elements\n>>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n>>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n>>>     output = x + y\n>>>     tl.store(out_ptr + offsets, output, mask=mask)\n>>>\n>>> @triton_op(\"mylib::add\", mutates_args={})\n>>> def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n>>>     output = torch.empty_like(x)\n>>>     n_elements = output.numel()\n>>>\n>>>     def grid(meta):\n>>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n>>>\n>>>     # NB: we need to wrap the triton kernel in a call to wrap_triton\n>>>     wrap_triton(add_kernel)[grid](x, y, output, n_elements, 16)\n>>>     return output\n>>>\n>>> @torch.compile\n>>> def f(x, y):\n>>>     return add(x, y)\n>>>\n>>> x = torch.randn(3, device=\"cuda\")\n>>> y = torch.randn(3, device=\"cuda\")\n>>>\n>>> z = f(x, y)\n>>> assert torch.allclose(z, x + y)\n\n```\n\nAllows capture of a triton kernel into a graph via make_fx or\nnon-stricttorch.export.\ntorch.export\nThese technologies perform Dispatcher-based tracing (via__torch_dispatch__) and cannot see calls to raw triton kernels.\nThewrap_tritonAPI wraps a triton kernel into a callable that\ncan actually be traced into a graph.\n__torch_dispatch__\nwrap_triton\nPlease use this API together withtorch.library.triton_op().\ntorch.library.triton_op()\nExamples\n\n```python\n>>> import torch\n>>> import triton\n>>> from triton import language as tl\n>>> from torch.fx.experimental.proxy_tensor import make_fx\n>>> from torch.library import wrap_triton\n>>>\n>>> @triton.jit\n>>> def add_kernel(\n>>>     in_ptr0,\n>>>     in_ptr1,\n>>>     out_ptr,\n>>>     n_elements,\n>>>     BLOCK_SIZE: \"tl.constexpr\",\n>>> ):\n>>>     pid = tl.program_id(axis=0)\n>>>     block_start = pid * BLOCK_SIZE\n>>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)\n>>>     mask = offsets < n_elements\n>>>     x = tl.load(in_ptr0 + offsets, mask=mask)\n>>>     y = tl.load(in_ptr1 + offsets, mask=mask)\n>>>     output = x + y\n>>>     tl.store(out_ptr + offsets, output, mask=mask)\n>>>\n>>> def add(x, y):\n>>>     output = torch.empty_like(x)\n>>>     n_elements = output.numel()\n>>>\n>>>     def grid_fn(meta):\n>>>         return (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]),)\n>>>\n>>>     wrap_triton(add_kernel)[grid_fn](x, y, output, n_elements, 16)\n>>>     return output\n>>>\n>>> x = torch.randn(3, device=\"cuda\")\n>>> y = torch.randn(3, device=\"cuda\")\n>>> gm = make_fx(add)(x, y)\n>>> print(gm.code)\n>>> # def forward(self, x_1, y_1):\n>>> #     empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False)\n>>> #     triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation(\n>>> #         kernel_idx = 0, constant_args_idx = 0,\n>>> #         grid = [(1, 1, 1)], kwargs = {\n>>> #             'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like,\n>>> #             'n_elements': 3, 'BLOCK_SIZE': 16\n>>> #         })\n>>> #     return empty_like\n\n```\n\nAny\n\n## Extending custom ops (created from Python or C++)#\n\nUse theregister.*methods, such astorch.library.register_kernel()andtorch.library.register_fake(), to add implementations\nfor any operators (they may have been created usingtorch.library.custom_op()or\nvia PyTorch\u2019s C++ operator registration APIs).\nregister.*\ntorch.library.register_kernel()\ntorch.library.register_fake()\ntorch.library.custom_op()\nRegister an implementation for a device type for this operator.\nSome valid device_types are: \u201ccpu\u201d, \u201ccuda\u201d, \u201cxla\u201d, \u201cmps\u201d, \u201cipu\u201d, \u201cxpu\u201d.\nThis API may be used as a decorator.\nop(str|OpOverload) \u2013 The operator to register an impl to.\ndevice_types(None|str|Sequence[str]) \u2013 The device_types to register an impl to.\nIf None, we will register to all device types \u2013 please only use\nthis option if your implementation is truly device-type-agnostic.\nfunc(Callable) \u2013 The function to register as the implementation for\nthe given device types.\nlib(Optional[Library]) \u2013 If provided, the lifetime of this registration\n\n```python\n>>> import torch\n>>> from torch import Tensor\n>>> from torch.library import custom_op\n>>> import numpy as np\n>>>\n>>> # Create a custom op that works on cpu\n>>> @custom_op(\"mylib::numpy_sin\", mutates_args=(), device_types=\"cpu\")\n>>> def numpy_sin(x: Tensor) -> Tensor:\n>>>     x_np = x.numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np)\n>>>\n>>> # Add implementations for the cuda device\n>>> @torch.library.register_kernel(\"mylib::numpy_sin\", \"cuda\")\n>>> def _(x):\n>>>     x_np = x.cpu().numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np).to(device=x.device)\n>>>\n>>> x_cpu = torch.randn(3)\n>>> x_cuda = x_cpu.cuda()\n>>> assert torch.allclose(numpy_sin(x_cpu), x_cpu.sin())\n>>> assert torch.allclose(numpy_sin(x_cuda), x_cuda.sin())\n\n```\n\nRegister an autocast dispatch rule for this custom op.\nValiddevice_typeinclude: \u201ccpu\u201d and \u201ccuda\u201d.\nop(str|OpOverload) \u2013 The operator to register an autocast dispatch rule to.\ndevice_type(str) \u2013 Device type to use. \u2018cuda\u2019 or \u2018cpu\u2019.\nThe type is the same as thetypeattribute of atorch.device.\nThus, you may obtain the device type of a tensor usingTensor.device.type.\ntorch.device\ncast_inputs(torch.dtype) \u2013 When custom op runs in an autocast-enabled region,\ncasts incoming floating-point Tensors to the target dtype (non-floating-point Tensors\nare not affected), then executes custom op with autocast disabled.\ntorch.dtype\nlib(Optional[Library]) \u2013 If provided, the lifetime of this registration\n\n```python\n>>> import torch\n>>> from torch import Tensor\n>>> from torch.library import custom_op\n>>>\n>>> # Create a custom op that works on cuda\n>>> @torch.library.custom_op(\"mylib::my_sin\", mutates_args=())\n>>> def my_sin(x: Tensor) -> Tensor:\n>>>     return torch.sin(x)\n>>>\n>>> # Register autocast dispatch rule for the cuda device\n>>> torch.library.register_autocast(\"mylib::my_sin\", \"cuda\", torch.float16)\n>>>\n>>> x = torch.randn(3, dtype=torch.float32, device=\"cuda\")\n>>> with torch.autocast(\"cuda\", dtype=torch.float16):\n>>>     y = torch.ops.mylib.my_sin(x)\n>>> assert y.dtype == torch.float16\n\n```\n\nRegister a backward formula for this custom op.\nIn order for an operator to work with autograd, you need to register\na backward formula:\n1. You must tell us how to compute gradients during the backward pass\nby providing us a \u201cbackward\u201d function.\n2. If you need any values from the forward to compute gradients, you can\nusesetup_contextto save values for backward.\nbackwardruns during the backward pass. It accepts(ctx,*grads):\n-gradsis one or more gradients. The number of gradients matches\nthe number of outputs of the operator.\nThectxobject isthe same ctx objectused bytorch.autograd.Function. The semantics ofbackward_fnare the\nsame astorch.autograd.Function.backward().\nbackward\n(ctx,*grads)\ngrads\nctx\ntorch.autograd.Function\nbackward_fn\ntorch.autograd.Function.backward()\nsetup_context(ctx,inputs,output)runs during the forward pass.\nPlease save quantities needed for backward onto thectxobject via\neithertorch.autograd.function.FunctionCtx.save_for_backward()or assigning them as attributes ofctx. If your custom op has\nkwarg-only arguments, we expect the signature ofsetup_contextto besetup_context(ctx,inputs,keyword_only_inputs,output).\nsetup_context(ctx,inputs,output)\nctx\ntorch.autograd.function.FunctionCtx.save_for_backward()\nctx\nsetup_context\nsetup_context(ctx,inputs,keyword_only_inputs,output)\nBothsetup_context_fnandbackward_fnmust be traceable. That is,\nthey may not directly accesstorch.Tensor.data_ptr()and they must\nnot depend on or mutate global state. If you need a non-traceable backward,\nyou can make it a separate custom_op that you call insidebackward_fn.\nsetup_context_fn\nbackward_fn\ntorch.Tensor.data_ptr()\nbackward_fn\nIf you need different autograd behavior on different devices, then we\nrecommend creating two different custom operators, one for each device\nthat needs different behavior, and switching between them at runtime.\nExamples\n\n```python\n>>> import torch\n>>> import numpy as np\n>>> from torch import Tensor\n>>>\n>>> @torch.library.custom_op(\"mylib::numpy_sin\", mutates_args=())\n>>> def numpy_sin(x: Tensor) -> Tensor:\n>>>     x_np = x.cpu().numpy()\n>>>     y_np = np.sin(x_np)\n>>>     return torch.from_numpy(y_np).to(device=x.device)\n>>>\n>>> def setup_context(ctx, inputs, output) -> Tensor:\n>>>     x, = inputs\n>>>     ctx.save_for_backward(x)\n>>>\n>>> def backward(ctx, grad):\n>>>     x, = ctx.saved_tensors\n>>>     return grad * x.cos()\n>>>\n>>> torch.library.register_autograd(\n...     \"mylib::numpy_sin\", backward, setup_context=setup_context\n... )\n>>>\n>>> x = torch.randn(3, requires_grad=True)\n>>> y = numpy_sin(x)\n>>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))\n>>> assert torch.allclose(grad_x, x.cos())\n>>>\n>>> # Example with a keyword-only arg\n>>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\n>>> def numpy_mul(x: Tensor, *, val: float) -> Tensor:\n>>>     x_np = x.cpu().numpy()\n>>>     y_np = x_np * val\n>>>     return torch.from_numpy(y_np).to(device=x.device)\n>>>\n>>> def setup_context(ctx, inputs, keyword_only_inputs, output) -> Tensor:\n>>>     ctx.val = keyword_only_inputs[\"val\"]\n>>>\n>>> def backward(ctx, grad):\n>>>     return grad * ctx.val\n>>>\n>>> torch.library.register_autograd(\n...     \"mylib::numpy_mul\", backward, setup_context=setup_context\n... )\n>>>\n>>> x = torch.randn(3, requires_grad=True)\n>>> y = numpy_mul(x, val=3.14)\n>>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))\n>>> assert torch.allclose(grad_x, torch.full_like(x, 3.14))\n\n```\n\nRegister a FakeTensor implementation (\u201cfake impl\u201d) for this operator.\nAlso sometimes known as a \u201cmeta kernel\u201d, \u201cabstract impl\u201d.\nAn \u201cFakeTensor implementation\u201d specifies the behavior of this operator on\nTensors that carry no data (\u201cFakeTensor\u201d). Given some input Tensors with\ncertain properties (sizes/strides/storage_offset/device), it specifies\nwhat the properties of the output Tensors are.\nThe FakeTensor implementation has the same signature as the operator.\nIt is run for both FakeTensors and meta tensors. To write a FakeTensor\nimplementation, assume that all Tensor inputs to the operator are\nregular CPU/CUDA/Meta tensors, but they do not have storage, and\nyou are trying to return regular CPU/CUDA/Meta tensor(s) as output.\nThe FakeTensor implementation must consist of only PyTorch operations\n(and may not directly access the storage or data of any input or\nintermediate Tensors).\nThis API may be used as a decorator (see examples).\nFor a detailed guide on custom ops, please seehttps://pytorch.org/tutorials/advanced/custom_ops_landing_page.html\nop_name\u2013 Operator name (along with the overload) or OpOverload object.\nfunc(Optional[Callable]) \u2013 Fake tensor implementation.\nlib(Optional[Library]) \u2013 Library to register the fake tensor to.\nallow_override(bool) \u2013 Flag controlling if we want to override an\nexisting registered fake impl. This is by default off,\nand will error you\u2019re trying to register a fake impl to\nan operator that already has a fake impl. This also only\napplies if the custom operator was not created via\ntorch.library.custom_op, as overriding and existing fake\nimpl is already allowed.\nExamples\n\n```python\n>>> import torch\n>>> import numpy as np\n>>> from torch import Tensor\n>>>\n>>> # Example 1: an operator without data-dependent output shape\n>>> @torch.library.custom_op(\"mylib::custom_linear\", mutates_args=())\n>>> def custom_linear(x: Tensor, weight: Tensor, bias: Tensor) -> Tensor:\n>>>     raise NotImplementedError(\"Implementation goes here\")\n>>>\n>>> @torch.library.register_fake(\"mylib::custom_linear\")\n>>> def _(x, weight, bias):\n>>>     assert x.dim() == 2\n>>>     assert weight.dim() == 2\n>>>     assert bias.dim() == 1\n>>>     assert x.shape[1] == weight.shape[1]\n>>>     assert weight.shape[0] == bias.shape[0]\n>>>     assert x.device == weight.device\n>>>\n>>>     return (x @ weight.t()) + bias\n>>>\n>>> with torch._subclasses.fake_tensor.FakeTensorMode():\n>>>     x = torch.randn(2, 3)\n>>>     w = torch.randn(3, 3)\n>>>     b = torch.randn(3)\n>>>     y = torch.ops.mylib.custom_linear(x, w, b)\n>>>\n>>> assert y.shape == (2, 3)\n>>>\n>>> # Example 2: an operator with data-dependent output shape\n>>> @torch.library.custom_op(\"mylib::custom_nonzero\", mutates_args=())\n>>> def custom_nonzero(x: Tensor) -> Tensor:\n>>>     x_np = x.numpy(force=True)\n>>>     res = np.stack(np.nonzero(x_np), axis=1)\n>>>     return torch.tensor(res, device=x.device)\n>>>\n>>> @torch.library.register_fake(\"mylib::custom_nonzero\")\n>>> def _(x):\n>>> # Number of nonzero-elements is data-dependent.\n>>> # Since we cannot peek at the data in an fake impl,\n>>> # we use the ctx object to construct a new symint that\n>>> # represents the data-dependent size.\n>>>     ctx = torch.library.get_ctx()\n>>>     nnz = ctx.new_dynamic_size()\n>>>     shape = [nnz, x.dim()]\n>>>     result = x.new_empty(shape, dtype=torch.int64)\n>>>     return result\n>>>\n>>> from torch.fx.experimental.proxy_tensor import make_fx\n>>>\n>>> x = torch.tensor([0, 1, 2, 3, 4, 0])\n>>> trace = make_fx(torch.ops.mylib.custom_nonzero, tracing_mode=\"symbolic\")(x)\n>>> trace.print_readable()\n>>>\n>>> assert torch.allclose(trace(x), torch.ops.mylib.custom_nonzero(x))\n\n```\n\nRegister a vmap implementation to supporttorch.vmap()for this custom op.\ntorch.vmap()\nThis API may be used as a decorator (see examples).\nIn order for an operator to work withtorch.vmap(), you may need to register a\nvmap implementation in the following signature:\ntorch.vmap()\nvmap_func(info,in_dims:Tuple[Optional[int]],*args,**kwargs),\nvmap_func(info,in_dims:Tuple[Optional[int]],*args,**kwargs)\nwhere*argsand**kwargsare the arguments and kwargs forop.\nWe do not support kwarg-only Tensor args.\n*args\n**kwargs\nop\nIt specifies how do we compute the batched version ofopgiven inputs with an additional\ndimension (specified byin_dims).\nop\nin_dims\nFor each arg inargs,in_dimshas a correspondingOptional[int]. It isNoneif the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer\nspecifying what dimension of the Tensor is being vmapped over.\nargs\nin_dims\nOptional[int]\nNone\ninfois a collection of additional metadata that may be helpful:info.batch_sizespecifies the size of the dimension being vmapped over, whileinfo.randomnessis therandomnessoption that was passed totorch.vmap().\ninfo\ninfo.batch_size\ninfo.randomness\nrandomness\ntorch.vmap()\nThe return of the functionfuncis a tuple of(output,out_dims). Similar toin_dims,out_dimsshould be of the same structure asoutputand contain oneout_dimper output that specifies if the output has the vmapped dimension and what index it is in.\nfunc\n(output,out_dims)\nin_dims\nout_dims\noutput\nout_dim\nExamples\n\n```python\n>>> import torch\n>>> import numpy as np\n>>> from torch import Tensor\n>>> from typing import Tuple\n>>>\n>>> def to_numpy(tensor):\n>>>     return tensor.cpu().numpy()\n>>>\n>>> lib = torch.library.Library(\"mylib\", \"FRAGMENT\")\n>>> @torch.library.custom_op(\"mylib::numpy_cube\", mutates_args=())\n>>> def numpy_cube(x: Tensor) -> Tuple[Tensor, Tensor]:\n>>>     x_np = to_numpy(x)\n>>>     dx = torch.tensor(3 * x_np ** 2, device=x.device)\n>>>     return torch.tensor(x_np ** 3, device=x.device), dx\n>>>\n>>> def numpy_cube_vmap(info, in_dims, x):\n>>>     result = numpy_cube(x)\n>>>     return result, (in_dims[0], in_dims[0])\n>>>\n>>> torch.library.register_vmap(numpy_cube, numpy_cube_vmap)\n>>>\n>>> x = torch.randn(3)\n>>> torch.vmap(numpy_cube)(x)\n>>>\n>>> @torch.library.custom_op(\"mylib::numpy_mul\", mutates_args=())\n>>> def numpy_mul(x: Tensor, y: Tensor) -> Tensor:\n>>>     return torch.tensor(to_numpy(x) * to_numpy(y), device=x.device)\n>>>\n>>> @torch.library.register_vmap(\"mylib::numpy_mul\")\n>>> def numpy_mul_vmap(info, in_dims, x, y):\n>>>     x_bdim, y_bdim = in_dims\n>>>     x = x.movedim(x_bdim, -1) if x_bdim is not None else x.unsqueeze(-1)\n>>>     y = y.movedim(y_bdim, -1) if y_bdim is not None else y.unsqueeze(-1)\n>>>     result = x * y\n>>>     result = result.movedim(-1, 0)\n>>>     return result, 0\n>>>\n>>>\n>>> x = torch.randn(3)\n>>> y = torch.randn(3)\n>>> torch.vmap(numpy_mul)(x, y)\n\n```\n\nNote\nThe vmap function should aim to preserve the semantics of the entire custom operator.\nThat is,grad(vmap(op))should be replaceable with agrad(map(op)).\ngrad(vmap(op))\ngrad(map(op))\nIf your custom operator has any custom behavior in the backward pass, please\nkeep this in mind.\nThis API was renamed totorch.library.register_fake()in PyTorch 2.4.\nPlease use that instead.\ntorch.library.register_fake()\nget_ctx() returns the current AbstractImplCtx object.\nCallingget_ctx()is only valid inside of an fake impl\n(seetorch.library.register_fake()for more usage details.\nget_ctx()\ntorch.library.register_fake()\nFakeImplCtx\nRegisters a torch_dispatch rule for the given operator andtorch_dispatch_class.\ntorch_dispatch_class\nThis allows for open registration to specify the behavior between the operator\nand thetorch_dispatch_classwithout needing to modify thetorch_dispatch_classor the operator directly.\ntorch_dispatch_class\ntorch_dispatch_class\nThetorch_dispatch_classis either a Tensor subclass with__torch_dispatch__or a\nTorchDispatchMode.\ntorch_dispatch_class\n__torch_dispatch__\nIf it is a Tensor subclass, we expectfuncto have the following signature:(cls,func:OpOverload,types:Tuple[type,...],args,kwargs)->Any\nfunc\n(cls,func:OpOverload,types:Tuple[type,...],args,kwargs)->Any\nIf it is a TorchDispatchMode, we expectfuncto have the following signature:(mode,func:OpOverload,types:Tuple[type,...],args,kwargs)->Any\nfunc\n(mode,func:OpOverload,types:Tuple[type,...],args,kwargs)->Any\nargsandkwargswill have been normalized the same way they are\nin__torch_dispatch__(see__torch_dispatch__ calling convention).\nargs\nkwargs\n__torch_dispatch__\nExamples\n\n```python\n>>> import torch\n>>>\n>>> @torch.library.custom_op(\"mylib::foo\", mutates_args={})\n>>> def foo(x: torch.Tensor) -> torch.Tensor:\n>>>     return x.clone()\n>>>\n>>> class MyMode(torch.utils._python_dispatch.TorchDispatchMode):\n>>>     def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n>>>         return func(*args, **kwargs)\n>>>\n>>> @torch.library.register_torch_dispatch(\"mylib::foo\", MyMode)\n>>> def _(mode, func, types, args, kwargs):\n>>>     x, = args\n>>>     return x + 1\n>>>\n>>> x = torch.randn(3)\n>>> y = foo(x)\n>>> assert torch.allclose(y, x)\n>>>\n>>> with MyMode():\n>>>     y = foo(x)\n>>> assert torch.allclose(y, x + 1)\n\n```\n\nParses the schema of a given function with type hints. The schema is inferred from the\nfunction\u2019s type hints, and can be used to define a new operator.\nWe make the following assumptions:\nNone of the outputs alias any of the inputs or each other.\nmutates_args\nmutates_args\nCallers (e.g. the custom ops API) are responsible for checking these assumptions.\nprototype_function(Callable) \u2013 The function from which to infer a schema for from its type annotations.\nop_name(Optional[str]) \u2013 The name of the operator in the schema. Ifnameis None, then the\nname is not included in the inferred schema. Note that the input schema totorch.library.Library.definerequires a operator name.\nname\ntorch.library.Library.define\nmutates_args(\"unknown\"|Iterable[str]) \u2013 The arguments that are mutated in the function.\nThe inferred schema.\nstr\nExample\n\n```python\n>>> def foo_impl(x: torch.Tensor) -> torch.Tensor:\n>>>     return x.sin()\n>>>\n>>> infer_schema(foo_impl, op_name=\"foo\", mutates_args={})\nfoo(Tensor x) -> Tensor\n>>>\n>>> infer_schema(foo_impl, mutates_args={})\n(Tensor x) -> Tensor\n\n```\n\nCustomOpDef is a wrapper around a function that turns it into a custom op.\nIt has various methods for registering additional behavior for this\ncustom op.\nYou should not instantiate CustomOpDef directly; instead, use thetorch.library.custom_op()API.\ntorch.library.custom_op()\nDisable or re-enable an already registered kernel for this custom operator.\nIf the kernel is already disabled/enabled, this is a no-op.\nNote\nIf a kernel is first disabled and then registered, it is disabled until enabled again.\ndevice_type(str) \u2013 The device type to disable/enable the kernel for.\ndisable(bool) \u2013 Whether to disable or enable the kernel.\nExample\n\n```python\n>>> inp = torch.randn(1)\n>>>\n>>> # define custom op `f`.\n>>> @custom_op(\"mylib::f\", mutates_args=())\n>>> def f(x: Tensor) -> Tensor:\n>>>     return torch.zeros(1)\n>>>\n>>> print(f(inp))  # tensor([0.]), default kernel\n>>>\n>>> @f.register_kernel(\"cpu\")\n>>> def _(x):\n>>>     return torch.ones(1)\n>>>\n>>> print(f(inp))  # tensor([1.]), CPU kernel\n>>>\n>>> # temporarily disable the CPU kernel\n>>> with f.set_kernel_enabled(\"cpu\", enabled = False):\n>>>     print(f(inp))  # tensor([0.]) with CPU kernel disabled\n\n```\n\nReturns the computed kernel for a given operator and dispatch key.\nThis function retrieves the kernel that would be executed for a given\noperator and dispatch key combination. The returned SafeKernelFunction\ncan be used to call the kernel in a boxed fashion. The intended use\ncase for this function is to retrieve the original kernel for a given\ndispatch key and then register another kernel to the same dispatch key\nthat calls into the original kernel for certain cases.\nop(Union[str,OpOverload,CustomOpDef]) \u2013 Operator name (along with the overload) or OpOverload object\nCan be a string (e.g., \u201caten::add.Tensor\u201d), an OpOverload, or a CustomOpDef.\ndispatch_key(str|torch.DispatchKey) \u2013 The dispatch key to get the kernel for.\nCan be a string (e.g., \u201cCPU\u201d, \u201cCUDA\u201d) or a DispatchKey enum value.\nA safe kernel function that can be used tocall the kernel.\ncall the kernel.\ntorch._C._SafeKernelFunction\nRuntimeError\u2013 If the operator does not exist.\nExample\n\n```python\n>>> # Get the CPU kernel for torch.add\n>>> kernel = torch.library.get_kernel(\"aten::add.Tensor\", \"CPU\")\n>>>\n>>> # You can also use DispatchKey enum\n>>> kernel = torch.library.get_kernel(\"aten::add.Tensor\", torch.DispatchKey.CPU)\n>>>\n>>> # Or use an OpOverload directly\n>>> kernel = torch.library.get_kernel(torch.ops.aten.add.Tensor, \"CPU\")\n>>>\n>>> # Example: Using get_kernel in a custom op with conditional dispatch\n>>> # Get the original kernel for torch.sin\n>>> original_sin_kernel = torch.library.get_kernel(\"aten::sin\", \"CPU\")\n>>>\n>>> # If input has negative values, use original sin, otherwise return zeros\n>>> def conditional_sin_impl(dispatch_keys, x):\n>>>     if (x < 0).any():\n>>>         return original_sin_kernel.call_boxed(dispatch_keys, x)\n>>>     else:\n>>>         return torch.zeros_like(x)\n>>>\n>>> lib = torch.library.Library(\"aten\", \"IMPL\")\n>>> # with_keyset=True so the first argument to the impl is the current DispatchKeySet\n>>> which needs to be the first argument to ``kernel.call_boxed``\n>>> lib.impl(\"sin\", conditional_sin_impl, \"CPU\", with_keyset=True)\n>>>\n>>> # Test the conditional behavior\n>>> x_positive = torch.tensor([1.0, 2.0])\n>>> x_mixed = torch.tensor([-1.0, 2.0])\n>>> torch.sin(x_positive)\ntensor([0., 0.])\n>>> torch.sin(x_mixed)\ntensor([-0.8415, 0.9093])\n\n```\n\n\n## Low-level APIs#\n\nThe following APIs are direct bindings to PyTorch\u2019s C++ low-level\noperator registration APIs.\nWarning\nThe low-level operator registration APIs and the PyTorch Dispatcher are a complicated PyTorch concept. We recommend you use the higher level APIs above (that do not require a torch.library.Library object) when possible.This blog postis a good starting point to learn about the PyTorch Dispatcher.\nA tutorial that walks you through some examples on how to use this API is available onGoogle Colab.\nA class to create libraries that can be used to register new operators or\noverride operators in existing libraries from Python.\nA user can optionally pass in a dispatch keyname if they only want to register\nkernels corresponding to only one specific dispatch key.\nTo create a library to override operators in an existing library (with name ns), set the kind to \u201cIMPL\u201d.\nTo create a new library (with name ns) to register new operators, set the kind to \u201cDEF\u201d.\nTo create a fragment of a possibly existing library to register operators (and bypass\nthe limitation that there is only one library for a given namespace), set the kind to\n\u201cFRAGMENT\u201d.\nns\u2013 library name\nkind\u2013 \u201cDEF\u201d, \u201cIMPL\u201d, \u201cFRAGMENT\u201d\ndispatch_key\u2013 PyTorch dispatch key (default: \u201c\u201d)\nDefines a new operator and its semantics in the ns namespace.\nschema\u2013 function schema to define a new operator.\nalias_analysis(optional) \u2013 Indicates if the aliasing properties of the operator arguments can be\ninferred from the schema (default behavior) or not (\u201cCONSERVATIVE\u201d).\ntags(Tag|Sequence[Tag]) \u2013 one or more torch.Tag to apply to this\noperator. Tagging an operator changes the operator\u2019s behavior\nunder various PyTorch subsystems; please read the docs for the\ntorch.Tag carefully before applying it.\nname of the operator as inferred from the schema.\nExample:\n\n```python\n>>> my_lib = Library(\"mylib\", \"DEF\")\n>>> my_lib.define(\"sum(Tensor self) -> Tensor\")\n\n```\n\nRegisters the function implementation as the fallback for the given key.\nThis function only works for a library with global namespace (\u201c_\u201d).\nfn\u2013 function used as fallback for the given dispatch key orfallthrough_kernel()to register a fallthrough.\nfallthrough_kernel()\ndispatch_key\u2013 dispatch key that the input function should be registered for. By default, it uses\nthe dispatch key that the library was created with.\nwith_keyset\u2013 flag controlling if the current dispatcher call keyset should be passed as the first argument\ntofnwhen calling. This should be used to create the appropriate keyset for redispatch calls.\nfn\nExample:\n\n```python\n>>> my_lib = Library(\"_\", \"IMPL\")\n>>> def fallback_kernel(op, *args, **kwargs):\n>>>     # Handle all autocast ops generically\n>>>     # ...\n>>> my_lib.fallback(fallback_kernel, \"Autocast\")\n\n```\n\nRegisters the function implementation for an operator defined in the library.\nop_name\u2013 operator name (along with the overload) or OpOverload object.\nfn\u2013 function that\u2019s the operator implementation for the input dispatch key orfallthrough_kernel()to register a fallthrough.\nfallthrough_kernel()\ndispatch_key\u2013 dispatch key that the input function should be registered for. By default, it uses\nthe dispatch key that the library was created with.\nwith_keyset\u2013 flag controlling if the current dispatcher call keyset should be passed as the first argument\ntofnwhen calling. This should be used to create the appropriate keyset for redispatch calls.\nfn\nallow_override\u2013 Flag controlling if we want to override an\nexisting registered kernel implementation. This is by\ndefault off, and will error you\u2019re trying to register a\nkernel to a dispatch key with a kernel already\nregistered.\nExample:\n\n```python\n>>> my_lib = Library(\"aten\", \"IMPL\")\n>>> def div_cpu(self, other):\n>>>     return self * (1 / other)\n>>> my_lib.impl(\"div.Tensor\", div_cpu, \"CPU\")\n\n```\n\nA dummy function to pass toLibrary.implin order to register a fallthrough.\nLibrary.impl\nDefines a new operator.\nIn PyTorch, defining an op (short for \u201coperator\u201d) is a two step-process:\n- we need to define the op (by providing an operator name and schema)\n- we need to implement behavior for how the operator interacts with\nvarious PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc.\nThis entrypoint defines the custom operator (the first step)\nyou must then perform the second step by calling variousimpl_*APIs, liketorch.library.impl()ortorch.library.register_fake().\nimpl_*\ntorch.library.impl()\ntorch.library.register_fake()\nqualname(str) \u2013 The qualified name for the operator. Should be\na string that looks like \u201cnamespace::name\u201d, e.g. \u201caten::sin\u201d.\nOperators in PyTorch need a namespace to\navoid name collisions; a given operator may only be created once.\nIf you are writing a Python library, we recommend the namespace to\nbe the name of your top-level module.\nschema(str) \u2013 The schema of the operator. E.g. \u201c(Tensor x) -> Tensor\u201d\nfor an op that accepts one Tensor and returns one Tensor. It does\nnot contain the operator name (that is passed inqualname).\nqualname\nlib(Optional[Library]) \u2013 If provided, the lifetime of this operator\nwill be tied to the lifetime of the Library object.\ntags(Tag|Sequence[Tag]) \u2013 one or more torch.Tag to apply to this\noperator. Tagging an operator changes the operator\u2019s behavior\nunder various PyTorch subsystems; please read the docs for the\ntorch.Tag carefully before applying it.\n\n```python\n>>> import torch\n>>> import numpy as np\n>>>\n>>> # Define the operator\n>>> torch.library.define(\"mylib::sin\", \"(Tensor x) -> Tensor\")\n>>>\n>>> # Add implementations for the operator\n>>> @torch.library.impl(\"mylib::sin\", \"cpu\")\n>>> def f(x):\n>>>     return torch.from_numpy(np.sin(x.numpy()))\n>>>\n>>> # Call the new operator from torch.ops.\n>>> x = torch.randn(3)\n>>> y = torch.ops.mylib.sin(x)\n>>> assert torch.allclose(y, x.sin())\n\n```\n\nRegister an implementation for a device type for this operator.\nYou may pass \u201cdefault\u201d fortypesto register this implementation as the\ndefault implementation for ALL device types.\nPlease only use this if the implementation truly supports all device types;\nfor example, this is true if it is a composition of built-in PyTorch operators.\ntypes\nThis API may be used as a decorator. You can use nested decorators\nwith this API provided they return a function and are placed inside\nthis API (see Example 2).\nSome valid types are: \u201ccpu\u201d, \u201ccuda\u201d, \u201cxla\u201d, \u201cmps\u201d, \u201cipu\u201d, \u201cxpu\u201d.\nqualname(str) \u2013 Should be a string that looks like \u201cnamespace::operator_name\u201d.\ntypes(str|Sequence[str]) \u2013 The device types to register an impl to.\nlib(Optional[Library]) \u2013 If provided, the lifetime of this registration\nwill be tied to the lifetime of the Library object.\nExamples\n\n```python\n>>> import torch\n>>> import numpy as np\n>>> # Example 1: Register function.\n>>> # Define the operator\n>>> torch.library.define(\"mylib::mysin\", \"(Tensor x) -> Tensor\")\n>>>\n>>> # Add implementations for the cpu device\n>>> @torch.library.impl(\"mylib::mysin\", \"cpu\")\n>>> def f(x):\n>>>     return torch.from_numpy(np.sin(x.numpy()))\n>>>\n>>> x = torch.randn(3)\n>>> y = torch.ops.mylib.mysin(x)\n>>> assert torch.allclose(y, x.sin())\n>>>\n>>> # Example 2: Register function with decorator.\n>>> def custom_decorator(func):\n>>>     def wrapper(*args, **kwargs):\n>>>         return func(*args, **kwargs) + 1\n>>>     return wrapper\n>>>\n>>> # Define the operator\n>>> torch.library.define(\"mylib::sin_plus_one\", \"(Tensor x) -> Tensor\")\n>>>\n>>> # Add implementations for the operator\n>>> @torch.library.impl(\"mylib::sin_plus_one\", \"cpu\")\n>>> @custom_decorator\n>>> def f(x):\n>>>     return torch.from_numpy(np.sin(x.numpy()))\n>>>\n>>> # Call the new operator from torch.ops.\n>>> x = torch.randn(3)\n>>>\n>>> y1 = torch.ops.mylib.sin_plus_one(x)\n>>> y2 = torch.sin(x) + 1\n>>> assert torch.allclose(y1, y2)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/library.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "250cf2a258f429e1d227f6803036f9b7",
    "source": "pytorch_docs",
    "title": "Debugging Environment Variables \u2014 PyTorch 2.9 documentation",
    "text": "\n## Debugging Environment Variables#\n\nCreated On: Feb 15, 2024 | Last Updated On: Jun 06, 2025\nVariable\nDescription\nTORCH_SHOW_CPP_STACKTRACES\nTORCH_SHOW_CPP_STACKTRACES\nIf set to1, makes PyTorch print out a stack trace when it detects a C++ error.\n1\nTORCH_CPP_LOG_LEVEL\nTORCH_CPP_LOG_LEVEL\nSet the log level of c10 logging facility (supports both GLOG and c10 loggers). Valid values areINFO,WARNING,ERROR, andFATALor their numerical equivalents0,1,2, and3.\nINFO\nWARNING\nERROR\nFATAL\n0\n1\n2\n3\nTORCH_LOGS\nTORCH_LOGS\nFor a more in depth explanation of this environment variable, seetorch._logging.",
    "url": "https://pytorch.org/docs/stable/debugging_environment_variables.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2d748e1eb180400a0189d9fb3bf4f830",
    "source": "pytorch_docs",
    "title": "TorchDynamo APIs for fine-grained tracing \u2014 PyTorch 2.9 documentation",
    "text": "\n## TorchDynamo APIs for fine-grained tracing#\n\nCreated On: Jul 28, 2023 | Last Updated On: Jun 16, 2025\nNote\nIn this documenttorch.compiler.compileandtorch.compileare used interchangeably.\nBoth versions will work in your code.\ntorch.compiler.compile\ntorch.compile\ntorch.compileperforms TorchDynamo tracing on the whole user model.\nHowever, it is possible that a small part of the model code cannot be\nhandled bytorch.compiler. In this case, you might want to disable\nthe compiler on that particular portion, while running compilation on\nthe rest of the model. This section describe the existing APIs that\nuse to define parts of your code in which you want to skip compilation\nand the relevant use cases.\ntorch.compile\ntorch.compiler\nThe API that you can use to define portions of the code on which you can\ndisable compilation are listed in the following table:\nAPI\nDescription\nWhen to use?\ntorch.compiler.disable\ntorch.compiler.disable\nDisables Dynamo on the decorated function as well as recursively invoked functions.\nExcellent for unblocking a user, if a small portion of the model cannot be handled withtorch.compile.\ntorch.compile\ntorch._dynamo.disallow_in_graph\ntorch._dynamo.disallow_in_graph\nDisallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.nnThis is suitable for the ops, whiletorch.compiler.disableis suitable for decorating functions.\ntorch.compiler.disable\nThis API is excellent for both debugging and unblocking if a custom op liketorch.ops.fbgemm.*is causing issues with thetorch.compilefunction.\ntorch.ops.fbgemm.*\ntorch.compile\ntorch.compile.allow_in_graph\ntorch.compile.allow_in_graph\nThe annotated callable goes as is in the TorchDynamo graph. For example, a black-box for TorchDynamo Dynamo.nnNote that AOT Autograd will trace through it, so theallow_in_graphis only a Dynamo-level concept.\nallow_in_graph\nThis API is useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks orautograd.Function. However, each usage ofallow_in_graphmust be carefully screened(no graph breaks, no closures).\nautograd.Function\nallow_in_graph\ntorch._dynamo.graph_break\ntorch._dynamo.graph_break\nAdds a graph break. The code before and after the graph break goes through TorchDynamo.\nRarely useful for deployment- If you think you need this, most probably you need eitherdisableordisallow_in_graph.\ndisable\ndisallow_in_graph\ntorch.compiler.is_compiling\ntorch.compiler.is_compiling\nIndicates whether a graph is executed/traced as part of torch.compile() or torch.export().\ntorch.compiler.is_dynamo_compiling\ntorch.compiler.is_dynamo_compiling\nIndicates whether a graph is traced via TorchDynamo. It\u2019s stricter than torch.compiler.is_compiling() flag, as it would only be set to True when TorchDynamo is used.\ntorch.compiler.is_exporting\ntorch.compiler.is_exporting\nIndicates whether a graph is traced via export. It\u2019s stricter than torch.compiler.is_compiling() flag, as it would only be set to True when torch.export is used.\n\n## torch.compiler.disable#\n\ntorch.compiler.disable\ntorch.compiler.disabledisables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame.\ntorch.compiler.disable\nTorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the functionfncalls functionsa_fnandb_fn. Anda_fncallsaa_fnandab_fn. When you use the PyTorch eager mode rather thantorch.compile, these function frames run as is. Withtorch.compile, TorchDynamo intercepts each of these function frames (indicated by the green color):\nfn\na_fn\nb_fn\na_fn\naa_fn\nab_fn\ntorch.compile\ntorch.compile\nLet\u2019s imagine, that functiona_fnis causing troubles withtorch.compile.\nAnd this is a non-critical portion of the model. You can usecompiler.disableon functiona_fn. As shown above, TorchDynamo will stop looking at frames\noriginating from thea_fncall (white color indicates original Python behavior).\na_fn\ntorch.compile\ncompiler.disable\na_fn\na_fn\nTo skip compilation, you can decorate the offending function with@torch.compiler.disable.\n@torch.compiler.disable\nYou can also use the non-decorator syntax if you don\u2019t want to change the source\ncode\nHowever, we recommend that you avoid this style if possible. Here, you have to\ntake care that all users of the original function are now using the patched\nversion.\n\n## torch._dynamo.disallow_in_graph#\n\ntorch._dynamo.disallow_in_graph\ntorch._dynamo.disallow_in_graphdisallows an operator but not the function\nto be present in the TorchDynamo extracted graph. Note that this is suitable\nfor operators and not general functions as in the case of_dynamo.disable.\ntorch._dynamo.disallow_in_graph\n_dynamo.disable\nLet\u2019s imagine you compile your model with PyTorch. TorchDynamo is able to\nextract a graph, but then you see the downstream compiler failing. For example,\nthe meta kernel is missing, or some Autograd dispatch key is set incorrectly\nfor a particular operator. Then you can mark that operator asdisallow_in_graph, and TorchDynamo will cause a graph break and run that\noperator by using the PyTorch eager mode.\ndisallow_in_graph\nThe catch is that you will have to find the corresponding Dynamo level operator,\nand not the ATen level operator. See more in the Limitations section of the doc.\nWarning\ntorch._dynamo.disallow_in_graphis a global flag. If you are comparing\ndifferent backend compilers, you might have to callallow_in_graphfor\nthe disallowed operator when switching to the other compiler.\ntorch._dynamo.disallow_in_graph\nallow_in_graph\n\n## torch.compiler.allow_in_graph#\n\ntorch.compiler.allow_in_graph\ntorch.compiler.allow_in_graphis useful when the relevant function frame\nhas some known hard-to-support TorchDynamo feature, such as hooks andautograd.Function, and you are confident that downstream PyTorch components\nsuch as AOTAutograd can safely trace through the decorated function. When a\nfunction is decorated withallow_in_graph, TorchDynamo treats it as a\nblack-box and puts it as is in the generated graph.\ntorch.compiler.allow_in_graph\nautograd.Function\nallow_in_graph\nWarning\nallow_in_graphskips TorchDynamo completely on the decorated function\nomitting all TorchDynamo safety checks, including graph breaks, handling\nclosures, and others. Useallow_in_graphwith caution. PyTorch downstream\ncomponents, such as AOTAutograd rely on TorchDynamo to handle complex Python\nfeatures, butallow_in_graphbypasses TorchDynamo. Usingallow_in_graphcould lead to soundness and hard-to-debug issues.\nallow_in_graph\nallow_in_graph\nallow_in_graph\nallow_in_graph\n\n## Limitations#\n\nAll the existing APIs are applied at the TorchDynamo level. Therefore, these\nAPIs have visibility to only what TorchDynamo sees. This can lead to confusing\nscenarios.\nFor example,torch._dynamo.disallow_in_graphwill not work for ATen operators\nbecause they are visible to AOT Autograd. For example,torch._dynamo.disallow_in_graph(torch.ops.aten.add)will not work in the\nabove example.\ntorch._dynamo.disallow_in_graph\ntorch._dynamo.disallow_in_graph(torch.ops.aten.add)",
    "url": "https://pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3d06971425274ef80fd37dd6742f280b",
    "source": "pytorch_docs",
    "title": "Non-strict Tracing Programming Model \u2014 PyTorch 2.9 documentation",
    "text": "\n## Non-strict Tracing Programming Model#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nSummary:\nNon-strict tracingis a way to trace Python code that is less strict than Dynamo, but may result in silent incorrectness.\nNon-strict tracing runs a Python function and uses Python and PyTorch\u2019s operator overloading capabilities to record what Tensor operations occurred during execution into a trace.\nA function isnon-strict traceableif it complies with some constraints, namely, that the function ispureand does not directly manipulate Tensor.data_ptr().\nNon-strict tracing mayspecializeon certain variables and treat them asconstants, baking the values of the variables into the trace.\ntorch.compileinternals (make_fx, AOTDispatcher) usenon-strict tracing.torch._dynamo.nonstrict_tracecan also be used intorch.compiled code to mark sections of code to be traced with non-strict tracing.\nNon-strict tracing runs a Python function and uses Python and PyTorch\u2019s operator overloading capabilities to record what Tensor operations occurred during execution into a trace.\ntorch.compile\nmake_fx\ntorch._dynamo.nonstrict_trace\ntorch.compile\nmake_fxis the main entrypoint for non-strict tracing. For the following function, only the top branch is taken during execution of the inputs, so it captures a graph with only that branch.\nmake_fx\n\n```python\nfrom torch.fx.experimental.proxy_tensor import make_fx\ndef f(x):\n    if x.shape[0] > 2:\n        return x ** 2 / 6\n    else:\n        return x * 3\nx = torch.randn(3)\ngm = make_fx(f, tracing_mode=\"fake\")(x)\ngm.print_readable()\n\n```\n\n\n```python\nclass f(torch.nn.Module):\n    def forward(self, x_1: \"f32[3]\"):\n        # No stacktrace found for following nodes\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\n        return div\n        \n\n```\n\n\n```python\n'class f(torch.nn.Module):\\n    def forward(self, x_1: \"f32[3]\"):\\n        # No stacktrace found for following nodes\\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\\n        return div\\n        '\n\n```\n\nNon-strict tracing differs from Dynamo (strict) tracing in thatit is unsafe, that is, given a function, it captures a graph of Tensor operations that may have different semantics than the original function.\nGiven a Python function, Dynamo Tracing captures a graph of Tensor operations and residual bytecode that when combined give the same semantics as the Python function.\n\n## Pure Functions#\n\nNon-strict tracing is sound only onpure functions, and thus only pure functions should be non-strict traced.\nA pure function is a function with the following properties:\nDeterminism.Given the same inputs, the pure function will always return the same output.\nNo side effects.A pure function does not have any side effects such as modifying external state or performing I/O operations.\nExplicit input/output.All the input data must be passed through the function parameters and all of the outputs are returned from the function.\nHere are some examples of impure functions for which the captured graph behaves differently from the original function.\n\n## Example 1: No explicit input (e.g. accesses global tensor)#\n\n\n```python\nvar = torch.tensor(1)\ndef function_with_global_access(y):\n    return y + var\nx = torch.tensor([0, 1, 2])\n# _allow_non_fake_inputs=True is needed to capture the global variable\n# for demonstration purposes.\ngm = make_fx(\n    function_with_global_access, tracing_mode=\"fake\", _allow_non_fake_inputs=True\n)(x)\n# Non-strict Tracing captures the value of the global (1.)\nprint(\"1. call function\", function_with_global_access(x))\nprint(\"1. call graph\", gm(x))\n# However, after changing the global, the captured graph\n# produces a different result from the original function\nvar = torch.tensor(2)\nprint(\"2. call function\", function_with_global_access(x))\nprint(\"2. call graph\", gm(x))\n# To capture a graph that can have a varying `var` tensor,\n# it must be an explicit input:\ndef function_fixed(y, var):\n    return y + var\nvar = torch.tensor(3)\ngm = make_fx(function_fixed, tracing_mode=\"fake\")(x, var)\nprint(\"3. call function\", function_fixed(x, var))\nprint(\"3. call graph\", gm(x, var))\nvar = torch.tensor(4)\nprint(\"4. call function\", function_fixed(x, var))\nprint(\"4. call graph\", gm(x, var))\n\n```\n\n\n```python\n1. call function tensor([1, 2, 3])\n1. call graph tensor([1, 2, 3])\n2. call function tensor([2, 3, 4])\n2. call graph tensor([1, 2, 3])\n3. call function tensor([3, 4, 5])\n3. call graph tensor([3, 4, 5])\n4. call function tensor([4, 5, 6])\n4. call graph tensor([4, 5, 6])\n\n```\n\nSeeSpecialization and Constantsfor an explanation of why.\n\n## Example 2: Side effect (printing)#\n\n\n```python\ndef function_with_side_effect(y):\n    print(y)\nx = torch.tensor([0, 1, 2])\n_ = function_with_side_effect(x)\n\n```\n\n\n```python\ntensor([0, 1, 2])\n\n```\n\nRunningfin Python prints a Tensor as a side effect.\nf\n\n```python\ngm = make_fx(function_with_side_effect, tracing_mode=\"fake\")(x)\n\n```\n\n\n```python\nFakeTensor(..., size=(3,), dtype=torch.int64)\n\n```\n\nDuring non-strict tracing, this print occurs during the graph capture.\n\n```python\n_ = gm(x)\n\n```\n\nThe graph does not store a call to theprintstatement, so executing the graph doesn\u2019t print anything.\nprint\n\n## Example 3: Side effect (input list mutation)#\n\n\n```python\nlst = []\ndef function_with_input_list_mutation(lst):\n    val = lst.pop()\n    return val\nx = torch.tensor([0, 1, 2])\ny = torch.tensor([0, 1, 2])\n# Each time the function is executed, the list shrinks in size\nlst = [x, y]\nfunction_with_input_list_mutation(lst)\nprint(\"len(lst) after one call\", len(lst))\nfunction_with_input_list_mutation(lst)\nprint(\"len(lst) after two calls\", len(lst))\n# With Non-strict Tracing, the length of the list shrinks during\n# the graph capture but not in invocations of the graph.\nlst = [x, y]\ngm = make_fx(function_with_input_list_mutation, tracing_mode=\"fake\")(lst)\nprint(\"len(lst) after graph capture\", len(lst))\ngm(lst)\nprint(\"len(lst) after one call to graph\", len(lst))\ngm(lst)\nprint(\"len(lst) after two calls to graph\", len(lst))\n\n```\n\n\n```python\nlen(lst) after one call 1\nlen(lst) after two calls 0\nlen(lst) after graph capture 2\nlen(lst) after one call to graph 2\nlen(lst) after two calls to graph 2\n\n```\n\n\n## No direct data_ptr manipulation#\n\nDirectly manipulatingTensor.data_ptris not non-strict traceable. The intuition behind this is that PyTorch is unable to tellhowyou manipulated thedata_ptr.\nTensor.data_ptr\ndata_ptr\n\n```python\nimport ctypes\n# Create a tensor with a single element\ntensor = torch.tensor([42], dtype=torch.int32)  # Using int32 for simplicity\ndef function_with_data_ptr(tensor):\n    # Get the data pointer\n    ptr = tensor.data_ptr()\n    # Cast the pointer to a ctypes pointer\n    ctypes_ptr = ctypes.cast(ptr, ctypes.POINTER(ctypes.c_int32))\n    # Increment the value at the pointer\n    ctypes_ptr.contents.value += 1\n    return tensor\ntry:\n    make_fx(function_with_data_ptr, tracing_mode=\"fake\")(tensor)\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nCannot access data pointer of Tensor (e.g. FakeTensor, FunctionalTensor). If you're using torch.compile/export/fx, it is likely that we are erroneously tracing into a custom kernel. To fix this, please wrap the custom kernel into an opaque custom op. Please see the following for details: https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html\n\n```\n\n\n## Specialization and Constants#\n\nNon-strict tracing captures a graph that may be specialized on some values. What this means is the captured graph is only valid for these values. We say the graph treats those values asconstant.\nAll non-Tensor variables are treated as constant during Non-strict Tracing:\n\n```python\ndef f(x, y):\n    return x + y\nx = torch.tensor([0, 1, 2])\ny = 3.14\ngm = make_fx(f, tracing_mode=\"fake\")(x, y)\ngm.print_readable()\n\n```\n\n\n```python\nclass f(torch.nn.Module):\n    def forward(self, x_1: \"i64[3]\", y_1):\n        # No stacktrace found for following nodes\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\n        return add\n        \n\n```\n\n\n```python\n'class f(torch.nn.Module):\\n    def forward(self, x_1: \"i64[3]\", y_1):\\n        # No stacktrace found for following nodes\\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\\n        return add\\n        '\n\n```\n\n3.14 is a constant in the graph.\nNon-strict tracing will also specialize on properties of the input Tensors.\n\n```python\ndef f(x):\n    if x.shape[0] > 2:\n        return x ** 2 / 6\n    else:\n        return x * 3\nx = torch.randn(3)\ngm = make_fx(f, tracing_mode=\"fake\")(x)\ngm.print_readable()\n\n```\n\n\n```python\nclass f(torch.nn.Module):\n    def forward(self, x_1: \"f32[3]\"):\n        # No stacktrace found for following nodes\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\n        return div\n        \n\n```\n\n\n```python\n'class f(torch.nn.Module):\\n    def forward(self, x_1: \"f32[3]\"):\\n        # No stacktrace found for following nodes\\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\\n        return div\\n        '\n\n```\n\nAnd it will also specialize on any variables not directly passed into the function:\n\n```python\nvar = torch.tensor(1)\ndef f(x):\n    return x + y\nx = torch.randn(3)\ngm = make_fx(f, tracing_mode=\"fake\")(x)\ngm.print_readable()\n\n```\n\n\n```python\nclass f(torch.nn.Module):\n    def forward(self, x_1: \"f32[3]\"):\n        # No stacktrace found for following nodes\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\n        return add\n        \n\n```\n\n\n```python\n'class f(torch.nn.Module):\\n    def forward(self, x_1: \"f32[3]\"):\\n        # No stacktrace found for following nodes\\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\\n        return add\\n        '\n\n```\n",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.non_strict_tracing_model.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e4d4ff07289a8f4bcca11ed6d99e14e0",
    "source": "pytorch_docs",
    "title": "Dealing with Recompilations \u2014 PyTorch 2.9 documentation",
    "text": "\n## Dealing with Recompilations#\n\nCreated On: Jul 29, 2025 | Last Updated On: Jul 29, 2025\nRecompilations are necessary fortorch.compilesoundness, but can result in significantly increased compile time.\nThus, minimizing recompilations while preserving soundness is essential for reducing compile time.\ntorch.compile\nYou can view recompilations and their reasons using tlparse orTORCH_LOGS=recompiles.\nTORCH_LOGS=recompiles\n\n## Is Dynamic Shapes Enabled?#\n\nIn the below example, we recompile due to mismatched shapes:\n\n```python\n@torch.compile\ndef fn(x):\n    return x + 1\nfn(torch.ones(3))\nfn(torch.ones(4))\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/2479206322.py:1\n    triggered by the following guard failure(s):\n    - 0/0: tensor 'x' size mismatch at index 0. expected 3, actual 4\n\n```\n\n\n```python\ntensor([2., 2., 2., 2.])\n\n```\n\nMake sure that the dynamic option oftorch.compileis not set toFalse.\nThe default option,dynamic=None, will only attempt dynamic shapes after the first compilation.\nYou can setdynamic=Trueto upfront compile as dynamic as possible:\ntorch.compile\nFalse\ndynamic=None\ndynamic=True\n\n```python\n@torch.compile(dynamic=True)\ndef gn(x):\n    return x + 1\ngn(torch.ones(3))\ngn(torch.ones(4))\n\n```\n\n\n```python\ntensor([2., 2., 2., 2.])\n\n```\n\nFor more information on dynamic shapes, including dealing with errors/recompilations due to\ndynamic shapes, seethe dynamic shapes manual.\n\n## Wrapping Constants with Tensors#\n\nBy default,int/floatvariables are treated as constants and are guarded on their exact value.\nIn the below example, we have a recompilation for each function call.\nint\nfloat\n\n```python\n@torch.compile\ndef fn(x, c):\n    return x + c\nfor i in range(5):\n    fn(torch.ones(i), 0.5 + i)\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/3647755280.py:1\n    triggered by the following guard failure(s):\n    - 2/0: c == 0.5                                                 # return x + c  # mp/ipykernel_507/3647755280.py:3 in fn\nRecompiling function fn in /tmp/ipykernel_507/3647755280.py:1\n    triggered by the following guard failure(s):\n    - 2/1: tensor 'x' size mismatch at index 0. expected 1, actual 2\n    - 2/0: c == 0.5                                                 # return x + c  # mp/ipykernel_507/3647755280.py:3 in fn\n\n```\n\nIn particular, for LR schedulers, initializing with a constant can lead to recompilations:\n\n```python\nmod = torch.nn.Linear(3, 3)\nopt = torch.optim.Adam(mod.parameters(), lr=0.01)\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, 0.9)\n@torch.compile\ndef gn(inp):\n    opt.zero_grad(True)\n    out = mod(inp).sum()\n    out.backward()\n    opt.step()\n    sched.step()\nfor i in range(5):\n    gn(torch.ones(3, 3))\n\n```\n\n\n```python\nProfiler function <class 'torch.autograd.profiler.record_function'> will be ignored\nRecompiling function step in /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/optim/adam.py:213\n    triggered by the following guard failure(s):\n    - 7/0: self.param_groups[0]['lr'] == 0.01                       # for group in self.param_groups:  # optim/adam.py:228 in step\n\n```\n\nIn both examples, we can wrapfloatvariables in tensors in order to prevent recompilations.\nfloat\n\n```python\n# first example\nfor i in range(5):\n    fn(torch.ones(i), torch.tensor(0.5 + i))\n# second example\nopt = torch.optim.Adam(mod.parameters(), lr=torch.tensor(0.01))\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, torch.tensor(0.9))\nfor i in range(5):\n    gn(torch.ones(3, 3))\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/3647755280.py:1\n    triggered by the following guard failure(s):\n    - 0/0: tensor 'x' size mismatch at index 0. expected 0, actual 1\nRecompiling function fn in /tmp/ipykernel_507/3647755280.py:1\n    triggered by the following guard failure(s):\n    - 0/1: tensor 'x' size mismatch at index 0. expected 1, actual 2\n    - 0/0: tensor 'x' size mismatch at index 0. expected 0, actual 2\n\n```\n\n\n## Changing the Cache Size Limit#\n\nThere is a limit to how many times a function can be recompiled,\ndetermined bytorch._dynamo.config.cache_size_limitandtorch._dynamo.config.accumulated_cache_size_limit(The exact difference between these 2 values is detailed intorch/_dynamo/cache_size.py).\nIf the Dynamo cache limit is hit, then all future compilation attemptswill result in the function being skipped (run eagerly).\nDynamo will still attempt to use previously compiled bytecode for future function calls, if the guards pass.\nNote that in the case of a recompilation limit hit,all nested function calls WILL be skipped(Dynamo will try to use previously compiled bytecode for the nested functions).\nDynamo will also issue a warning containing the affected function and which limit was hit.\nIn the example below, each function call results in a recompile attempt.\nWhen we hit the cache size limit (by default, 8), we stop attempting to recompile.\n(Note that we setdynamic=Falsefor demonstration purposes to force recompilation every time).\ntorch._dynamo.config.cache_size_limit\ntorch._dynamo.config.accumulated_cache_size_limit\ntorch/_dynamo/cache_size.py\ndynamic=False\n\n```python\n@torch.compile(dynamic=False)\ndef fn(x):\n    return x + 1\nfor i in range(1, 10):\n    # recompile every time due to dynamic=False\n    fn(torch.ones(i))\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 2\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 3\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 3\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 4\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 4\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 4\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 5\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 5\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 5\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 5\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/4: tensor 'x' size mismatch at index 0. expected 5, actual 6\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 6\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 6\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 6\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 6\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/5: tensor 'x' size mismatch at index 0. expected 6, actual 7\n    - 8/4: tensor 'x' size mismatch at index 0. expected 5, actual 7\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 7\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 7\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 7\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 7\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/6: tensor 'x' size mismatch at index 0. expected 7, actual 8\n    - 8/5: tensor 'x' size mismatch at index 0. expected 6, actual 8\n    - 8/4: tensor 'x' size mismatch at index 0. expected 5, actual 8\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 8\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 8\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 8\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 8\nRecompiling function fn in /tmp/ipykernel_507/3054308037.py:1\n    triggered by the following guard failure(s):\n    - 8/7: tensor 'x' size mismatch at index 0. expected 8, actual 9\n    - 8/6: tensor 'x' size mismatch at index 0. expected 7, actual 9\n    - 8/5: tensor 'x' size mismatch at index 0. expected 6, actual 9\n    - 8/4: tensor 'x' size mismatch at index 0. expected 5, actual 9\n    - 8/3: tensor 'x' size mismatch at index 0. expected 4, actual 9\n    - 8/2: tensor 'x' size mismatch at index 0. expected 3, actual 9\n    - 8/1: tensor 'x' size mismatch at index 0. expected 2, actual 9\n    - 8/0: tensor 'x' size mismatch at index 0. expected 1, actual 9\ntorch._dynamo hit config.recompile_limit (8)\n   function: 'fn' (/tmp/ipykernel_507/3054308037.py:1)\n   last reason: 8/7: tensor 'x' size mismatch at index 0. expected 8, actual 9\nTo log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\nTo diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n\n```\n\nIf you know that the number of recompilations has a reasonable constant upper bound, you can raise the cache size limit.\nIf the cost of recompilation outweighs the benefit of compilation, then you can consider lowering the cache size limit.\n\n```python\ntorch._dynamo.config.cache_size_limit = 16\n@torch.compile(dynamic=False)\ndef gn(x):\n    return x + 1\nfor i in range(1, 10):\n    gn(torch.ones(i))\n\n```\n\n\n```python\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 2\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 3\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 3\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 4\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 4\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 4\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 5\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 5\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 5\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 5\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/4: tensor 'x' size mismatch at index 0. expected 5, actual 6\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 6\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 6\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 6\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 6\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/5: tensor 'x' size mismatch at index 0. expected 6, actual 7\n    - 9/4: tensor 'x' size mismatch at index 0. expected 5, actual 7\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 7\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 7\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 7\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 7\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/6: tensor 'x' size mismatch at index 0. expected 7, actual 8\n    - 9/5: tensor 'x' size mismatch at index 0. expected 6, actual 8\n    - 9/4: tensor 'x' size mismatch at index 0. expected 5, actual 8\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 8\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 8\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 8\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 8\nRecompiling function gn in /tmp/ipykernel_507/887097224.py:2\n    triggered by the following guard failure(s):\n    - 9/7: tensor 'x' size mismatch at index 0. expected 8, actual 9\n    - 9/6: tensor 'x' size mismatch at index 0. expected 7, actual 9\n    - 9/5: tensor 'x' size mismatch at index 0. expected 6, actual 9\n    - 9/4: tensor 'x' size mismatch at index 0. expected 5, actual 9\n    - 9/3: tensor 'x' size mismatch at index 0. expected 4, actual 9\n    - 9/2: tensor 'x' size mismatch at index 0. expected 3, actual 9\n    - 9/1: tensor 'x' size mismatch at index 0. expected 2, actual 9\n    - 9/0: tensor 'x' size mismatch at index 0. expected 1, actual 9\n\n```\n\n\n## Graph Breaking to Reduce Recompilation Costs#\n\nIf a large graph is recompiling and causing high compile time, you can intentionally introduce\na graph break in order to reduce recompilation costs, at the expense of introducing a performance hit.\n\n```python\ndef very_large_function(x):\n    return x + 1\n\n@torch.compile(dynamic=False)\ndef fn(x, c):\n    y = very_large_function(x)  # recompiled every time\n    return y + c\n\nfor i in range(1, 5):\n    fn(torch.ones(3), i)\n\n@torch.compile(dynamic=False)\ndef gn(x, c):\n    y = very_large_function(x)  # compiled only once\n    torch._dynamo.graph_break()\n    return y + c  # recompiled every time\n\nfor i in range(1, 5):\n    gn(torch.ones(3), i)\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_507/2876112129.py:4\n    triggered by the following guard failure(s):\n    - 10/0: c == 1                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\nRecompiling function fn in /tmp/ipykernel_507/2876112129.py:4\n    triggered by the following guard failure(s):\n    - 10/1: c == 2                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\n    - 10/0: c == 1                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\nRecompiling function fn in /tmp/ipykernel_507/2876112129.py:4\n    triggered by the following guard failure(s):\n    - 10/2: c == 3                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\n    - 10/1: c == 2                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\n    - 10/0: c == 1                                                   # return y + c  # mp/ipykernel_507/2876112129.py:7 in fn\nRecompiling function torch_dynamo_resume_in_gn_at_15 in /tmp/ipykernel_507/2876112129.py:15\n    triggered by the following guard failure(s):\n    - 12/0: c == 1                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\nRecompiling function torch_dynamo_resume_in_gn_at_15 in /tmp/ipykernel_507/2876112129.py:15\n    triggered by the following guard failure(s):\n    - 12/1: c == 2                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\n    - 12/0: c == 1                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\nRecompiling function torch_dynamo_resume_in_gn_at_15 in /tmp/ipykernel_507/2876112129.py:15\n    triggered by the following guard failure(s):\n    - 12/2: c == 3                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\n    - 12/1: c == 2                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\n    - 12/0: c == 1                                                   # return y + c  # recompiled every time  # mp/ipykernel_507/2876112129.py:16 in torch_dynamo_resume_in_gn_at_15\n\n```\n",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.recompilation.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3c35a6c577a3a70358db1aed1ec0a60f",
    "source": "pytorch_docs",
    "title": "torch.package \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.package#\n\nCreated On: Jun 10, 2025 | Last Updated On: Jul 15, 2025\ntorch.packageadds support for creating packages containing both artifacts and arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production usingtorch::deploy.\ntorch.package\ntorch::deploy\nThis document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it.\ntorch.package\nWarning\nThis module depends on thepicklemodule which is not secure. Only unpackage data you trust.\npickle\nIt is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with.\nFor more information, review thedocumentationfor thepicklemodule.\npickle\nTutorials\nPackaging your first model\nHow do I\u2026\nSee what is inside a package?\nSee why a given module was included as a dependency?\nInclude arbitrary resources with my package and access them later?\nCustomize how a class is packaged?\nTest in my source code whether or not it is executing inside a package?\nPatch code into a package?\nAccess package contents from packaged code?\nDistinguish between packaged code and non-packaged code?\nRe-export an imported object?\nExplanation\ntorch.packageFormat Overview\ntorch.package\nHowtorch.packagefinds your code\u2019s dependencies\ntorch.package\nDependency Management\ntorch.packagesharp edges\ntorch.package\nHowtorch.packagekeeps packages isolated from each other\ntorch.package\nAPI Reference\n\n## Tutorials#\n\n\n## Packaging your first model#\n\nA tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.\n\n## How do I\u2026#\n\n\n## See what is inside a package?#\n\nThe container format for atorch.packageis ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:\ntorch.package\nunzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.\nunzipmy_package.pt\ntorch.package\n\n```python\n$ unzip my_package.pt && tree my_package\nmy_package\n\u251c\u2500\u2500 .data\n\u2502   \u251c\u2500\u2500 94304870911616.storage\n\u2502   \u251c\u2500\u2500 94304900784016.storage\n\u2502   \u251c\u2500\u2500 extern_modules\n\u2502   \u2514\u2500\u2500 version\n\u251c\u2500\u2500 models\n\u2502   \u2514\u2500\u2500 model_1.pkl\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py\n~ cd my_package && cat torchvision/models/resnet.py\n...\n\n```\n\nThe Pythonzipfilemodule provides a standard way to read and write ZIP archive contents.\nzipfile\n\n```python\nfrom zipfile import ZipFile\nwith ZipFile(\"my_package.pt\") as myzip:\n    file_bytes = myzip.read(\"torchvision/models/resnet.py\")\n    # edit file_bytes in some way\n    myzip.writestr(\"torchvision/models/resnet.py\", new_file_bytes)\n\n```\n\nvim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!\nwrite\n\n```python\n# add this to your .vimrc to treat `*.pt` files as zip files\nau BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\"))\n\n~ vi my_package.pt\n\n```\n\nfile_structure()\nPackageImporterprovides afile_structure()method, which will return a printable\nand queryableDirectoryobject. TheDirectoryobject is a simple directory structure that you can use to explore the\ncurrent contents of atorch.package.\nPackageImporter\nfile_structure()\nDirectory\nDirectory\ntorch.package\nTheDirectoryobject itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments.\nDirectory\ninclude\nexclude\n\n```python\nwith PackageExporter('my_package.pt') as pe:\n    pe.save_pickle('models', 'model_1.pkl', mod)\n\nimporter = PackageImporter('my_package.pt')\n# can limit printed items with include/exclude args\nprint(importer.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storage\"))\nprint(importer.file_structure()) # will print out all files\n\n```\n\nOutput:\n\n```python\n# filtered with glob pattern:\n#    include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storage\"\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u2514\u2500\u2500 utils.py\n\n# all files\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 .data\n    \u2502   \u251c\u2500\u2500 94304870911616.storage\n    \u2502   \u251c\u2500\u2500 94304900784016.storage\n    \u2502   \u251c\u2500\u2500 extern_modules\n    \u2502   \u2514\u2500\u2500 version\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u251c\u2500\u2500 resnet.py\n            \u2514\u2500\u2500 utils.py\n\n```\n\nYou can also queryDirectoryobjects with thehas_file()method.\nDirectory\nhas_file()\n\n```python\nimporter_file_structure = importer.file_structure()\nfound: bool = importer_file_structure.has_file(\"package_a/subpackage.py\")\n\n```\n\n\n## See why a given module was included as a dependency?#\n\nSay there is a given modulefoo, and you want to know why yourPackageExporteris pulling infooas a dependency.\nfoo\nPackageExporter\nfoo\nPackageExporter.get_rdeps()will return all modules that directly depend onfoo.\nPackageExporter.get_rdeps()\nfoo\nIf you would like to see how a given modulesrcdepends onfoo, thePackageExporter.all_paths()method will\nreturn a DOT-formatted graph showing all the dependency paths betweensrcandfoo.\nsrc\nfoo\nPackageExporter.all_paths()\nsrc\nfoo\nIf you would just like to see the whole dependency graph of your :class:PackageExporter, you can usePackageExporter.dependency_graph_string().\nPackageExporter\nPackageExporter.dependency_graph_string()\n\n## Include arbitrary resources with my package and access them later?#\n\nPackageExporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.\nPackageExporter\nsave_pickle\nsave_text\nsave_binary\n\n```python\nwith torch.PackageExporter(\"package.pt\") as exporter:\n    # Pickles the object and saves to `my_resources/tensor.pkl` in the archive.\n    exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4))\n    exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\")\n    exporter.save_binary(\"raw_data\", \"binary\", my_bytes)\n\n\n```\n\nPackageImporterexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.\nPackageImporter\nload_pickle\nload_text\nload_binary\n\n```python\nimporter = torch.PackageImporter(\"package.pt\")\nmy_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\")\ntext = importer.load_text(\"config_stuff\", \"words.txt\")\nbinary = importer.load_binary(\"raw_data\", \"binary\")\n\n```\n\n\n## Customize how a class is packaged?#\n\ntorch.packageallows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process.\ntorch.package\n__reduce_package__\n__reduce__\nSteps:\nDefine the method__reduce_package__(self,exporter:PackageExporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by thePackageExporterwhen it encounters an instance of the target class.\n__reduce_package__(self,exporter:PackageExporter)\nPackageExporter\nDefine a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be aPackageImporterinstance, and the rest of the parameters are user defined.\nPackageImporter\n\n```python\n# foo.py [Example of customizing how class Foo is packaged]\nfrom torch.package import PackageExporter, PackageImporter\nimport time\n\n\nclass Foo:\n    def __init__(self, my_string: str):\n        super().__init__()\n        self.my_string = my_string\n        self.time_imported = 0\n        self.time_exported = 0\n\n    def __reduce_package__(self, exporter: PackageExporter):\n        \"\"\"\n        Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when\n        saving an instance of this object. This method should do the work to save this\n        object inside of the ``torch.package`` archive.\n\n        Returns function w/ arguments to load the object from a\n        ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.\n        \"\"\"\n\n        # use this pattern to ensure no naming conflicts with normal dependencies,\n        # anything saved under this module name shouldn't conflict with other\n        # items in the package\n        generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\"\n        exporter.save_text(\n            generated_module_name,\n            \"foo.txt\",\n            self.my_string + \", with exporter modification!\",\n        )\n        time_exported = time.clock_gettime(1)\n\n        # returns de-packaging function w/ arguments to invoke with\n        return (unpackage_foo, (generated_module_name, time_exported,))\n\n\ndef unpackage_foo(\n    importer: PackageImporter, generated_module_name: str, time_exported: float\n) -> Foo:\n    \"\"\"\n    Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function\n    when depickling a Foo object.\n    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\n    \"\"\"\n    time_imported = time.clock_gettime(1)\n    foo = Foo(importer.load_text(generated_module_name, \"foo.txt\"))\n    foo.time_imported = time_imported\n    foo.time_exported = time_exported\n    return foo\n\n\n```\n\n\n```python\n# example of saving instances of class Foo\n\nimport torch\nfrom torch.package import PackageImporter, PackageExporter\nimport foo\n\nfoo_1 = foo.Foo(\"foo_1 initial string\")\nfoo_2 = foo.Foo(\"foo_2 initial string\")\nwith PackageExporter('foo_package.pt') as pe:\n    # save as normal, no extra work necessary\n    pe.save_pickle('foo_collection', 'foo1.pkl', foo_1)\n    pe.save_pickle('foo_collection', 'foo2.pkl', foo_2)\n\npi = PackageImporter('foo_package.pt')\nprint(pi.file_structure())\nimported_foo = pi.load_pickle('foo_collection', 'foo1.pkl')\nprint(f\"foo_1 string: '{imported_foo.my_string}'\")\nprint(f\"foo_1 export time: {imported_foo.time_exported}\")\nprint(f\"foo_1 import time: {imported_foo.time_imported}\")\n\n```\n\n\n```python\n# output of running above script\n\u2500\u2500\u2500 foo_package\n    \u251c\u2500\u2500 foo-generated\n    \u2502   \u251c\u2500\u2500 _0\n    \u2502   \u2502   \u2514\u2500\u2500 foo.txt\n    \u2502   \u2514\u2500\u2500 _1\n    \u2502       \u2514\u2500\u2500 foo.txt\n    \u251c\u2500\u2500 foo_collection\n    \u2502   \u251c\u2500\u2500 foo1.pkl\n    \u2502   \u2514\u2500\u2500 foo2.pkl\n    \u2514\u2500\u2500 foo.py\n\nfoo_1 string: 'foo_1 initial string, with reduction modification!'\nfoo_1 export time: 9857706.650140837\nfoo_1 import time: 9857706.652698385\n\n```\n\n\n## Test in my source code whether or not it is executing inside a package?#\n\nAPackageImporterwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not.\nPackageImporter\n__torch_package__\n\n```python\n# In foo/bar.py:\n\nif \"__torch_package__\" in dir():  # true if the code is being loaded from a package\n    def is_in_package():\n        return True\n\n    UserException = Exception\nelse:\n    def is_in_package():\n        return False\n\n    UserException = UnpackageableException\n\n```\n\nNow, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from atorch.package.\ntorch.package\n\n```python\nfrom foo.bar import is_in_package\n\nprint(is_in_package())  # False\n\nloaded_module = PackageImporter(my_package).import_module(\"foo.bar\")\nloaded_module.is_in_package()  # True\n\n```\n\nWarning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.\n\n## Patch code into a package?#\n\nPackageExporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing.\nPackageExporter\nsave_source_string()\n\n```python\nwith PackageExporter(f) as exporter:\n    # Save the my_module.foo available in your current Python environment.\n    exporter.save_module(\"my_module.foo\")\n\n    # This saves the provided string to my_module/foo.py in the package archive.\n    # It will override the my_module.foo that was previously saved.\n    exporter.save_source_string(\"my_module.foo\", textwrap.dedent(\n        \"\"\"\\\n        def my_function():\n            print('hello world')\n        \"\"\"\n    ))\n\n    # If you want to treat my_module.bar as a package\n    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)\n    # pass is_package=True,\n    exporter.save_source_string(\"my_module.bar\",\n                                \"def foo(): print('hello')\\n\",\n                                is_package=True)\n\nimporter = PackageImporter(f)\nimporter.import_module(\"my_module.foo\").my_function()  # prints 'hello world'\n\n```\n\n\n## Access package contents from packaged code?#\n\nPackageImporterimplements theimportlib.resourcesAPI for accessing resources from inside a package.\nPackageImporter\nimportlib.resources\n\n```python\nwith PackageExporter(f) as exporter:\n    # saves text to my_resource/a.txt in the archive\n    exporter.save_text(\"my_resource\", \"a.txt\", \"hello world!\")\n    # saves the tensor to my_pickle/obj.pkl\n    exporter.save_pickle(\"my_pickle\", \"obj.pkl\", torch.ones(2, 2))\n\n    # see below for module contents\n    exporter.save_module(\"foo\")\n    exporter.save_module(\"bar\")\n\n```\n\nTheimportlib.resourcesAPI allows access to resources from within packaged code.\nimportlib.resources\n\n```python\n# foo.py:\nimport importlib.resources\nimport my_resource\n\n# returns \"hello world!\"\ndef get_my_resource():\n    return importlib.resources.read_text(my_resource, \"a.txt\")\n\n```\n\nUsingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent :class:PackageImporterinstance itself from within\npackaged code.\nimportlib.resources\nPackageImporter\n\n```python\n# bar.py:\nimport torch_package_importer # this is the PackageImporter that imported this module.\n\n# Prints \"hello world!\", equivalent to importlib.resources.read_text\ndef get_my_resource():\n    return torch_package_importer.load_text(\"my_resource\", \"a.txt\")\n\n# You also do things that the importlib.resources API does not support, like loading\n# a pickled object from the package.\ndef get_my_pickle():\n    return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")\n\n```\n\n\n## Distinguish between packaged code and non-packaged code?#\n\nTo tell if an object\u2019s code is from atorch.package, use thetorch.package.is_from_package()function.\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\nthis check will returnFalse.\ntorch.package\ntorch.package.is_from_package()\nextern\nstdlib\nFalse\n\n```python\nimporter = PackageImporter(f)\nmod = importer.import_module('foo')\nobj = importer.load_pickle('model', 'model.pkl')\ntxt = importer.load_text('text', 'my_test.txt')\n\nassert is_from_package(mod)\nassert is_from_package(obj)\nassert not is_from_package(txt) # str is from stdlib, so this will return False\n\n```\n\n\n## Re-export an imported object?#\n\nTo re-export an object that was previously imported by aPackageImporter, you must make the newPackageExporteraware of the originalPackageImporterso that it can find source code for your object\u2019s dependencies.\nPackageImporter\nPackageExporter\nPackageImporter\n\n```python\nimporter = PackageImporter(f)\nobj = importer.load_pickle(\"model\", \"model.pkl\")\n\n# re-export obj in a new package\nwith PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\n    exporter.save_pickle(\"model\", \"model.pkl\", obj)\n\n```\n\n\n## Explanation#\n\n\n## torch.packageFormat Overview#\n\ntorch.package\nAtorch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files:\ntorch.package\n.pt\nFramework files, which are placed in the.data/.\n.data/\nUser files, which is everything else.\nAs an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:\ntorchvision\n\n```python\nresnet\n\u251c\u2500\u2500 .data  # All framework-specific data is stored here.\n\u2502   \u2502      # It's named to avoid conflicts with user-serialized code.\n\u2502   \u251c\u2500\u2500 94286146172688.storage  # tensor data\n\u2502   \u251c\u2500\u2500 94286146172784.storage\n\u2502   \u251c\u2500\u2500 extern_modules  # text file with names of extern modules (e.g. 'torch')\n\u2502   \u251c\u2500\u2500 version         # version metadata\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u2514\u2500\u2500 torchvision  # all code dependencies are captured as source files\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py\n\n```\n\nThe.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages).\n.data/\ntorch.package\n.data/\ntorch.packages\nCurrently, the.data/directory contains the following items:\n.data/\nversion: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package.\nversion\ntorch.package\nextern_modules: a list of modules that are consideredextern.externmodules will be imported using the loading environment\u2019s system importer.\nextern_modules\nextern\nextern\n*.storage: serialized tensor data.\n*.storage\n\n```python\n.data\n\u251c\u2500\u2500 94286146172688.storage\n\u251c\u2500\u2500 94286146172784.storage\n\u251c\u2500\u2500 extern_modules\n\u251c\u2500\u2500 version\n\u251c\u2500\u2500 ...\n\n```\n\nAll other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation.\n\n```python\n<package root>\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u251c\u2500\u2500 another_package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 foo.txt         # a resource file , see importlib.resources\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py   # torchvision.models.resnet\n        \u2514\u2500\u2500 utils.py    # torchvision.models.utils\n\n```\n\n\n## Howtorch.packagefinds your code\u2019s dependencies#\n\ntorch.package\nWhen you issue asave_pickle(obj,...)call,PackageExporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode.\nsave_pickle(obj,...)\nPackageExporter\npickletools\nIn a pickle, an object is saved along with aGLOBALopcode that describes where to find the implementation of the object\u2019s type, like:\nGLOBAL\n\n```python\nGLOBAL 'torchvision.models.resnet Resnet`\n\n```\n\nThe dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.\nGLOBAL\nWhen a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.\ntorch.package\nfromximporty\nimportz\nfromwimportvasu\ntorch.package\nNote: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.\n__import__(...)\nimportlib.import_module\ntorch.package\n\n## Dependency Management#\n\ntorch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify anactionto take.\ntorch.package\nThe allowed actions are:\nintern: put this module into the package.\nintern\nextern: declare this module as an external dependency of the package.\nextern\nmock: stub out this module.\nmock\ndeny: depending on this module will raise an error during package export.\ndeny\nFinally, there is one more important action that is not technically part oftorch.package:\ntorch.package\nRefactoring: remove or change the dependencies in your code.\nNote that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from a module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.\ntorch.package\nActions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackageExporter, e.g.\n\"foo.bar\"\n\"foo.**\"\nPackageExporter\n\n```python\nmy_exporter.intern(\"torchvision.**\")\nmy_exporter.extern(\"numpy\")\n\n```\n\nIf a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken.\nintern\nIf a module isintern-ed, it will be placed into the package.\nintern\nThis action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need tointernthe module torchvision.models.resnet.\ntorchvision\nintern\nOn package import, when your packaged code tries to import anintern-ed module, PackageImporter will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackageImporteris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,PackageImporterwill only use the version in your\npackage.\nintern\nPackageImporter\nmy_interned_module\nPackageImporter\nNote: Only Python source modules can beintern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt tointernthem. These kinds of modules need to bemock-ed orextern-ed.\nintern\nintern\nmock\nextern\nextern\nIf a module isextern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist onpackage_exporter.extern_modules.\nextern\npackage_exporter.extern_modules\nOn package import, when the packaged code tries to import anextern-ed module,PackageImporterwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised.\nextern\nPackageImporter\nimportlib.import_module(\"my_externed_module\")\nIn this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.\nnumpy\nscipy\nWarning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use ofextern.\nextern\nmock\nIf a module ismock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise aNotImplementedError.\nmock\nfrommy_mocked_moduleimportfoo\nNotImplementedError\nmockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.\nmock\nWarning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.\nmock\nThe best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):\nInclude only what you use. Do not leave unused imports in your code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.\nQualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.\nfoo.bar.baz\nfromfoo.barimportbaz\nfoo.bar\nfoo\nSplit up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.\nutils\nutils\nPatterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob().\nA module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz.\nfoo.bar.baz\nA pattern contains one or more segments. Segments can be:\nA literal string (e.g.foo), which matches exactly.\nfoo\nA string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string.\ntorch\nfoo*baz*\nA double wildcard (**). This matches against zero or more complete segments.\n**\nExamples:\ntorch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.\ntorch.**\ntorch\ntorch.nn\ntorch.nn.functional\ntorch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch\ntorch.*\ntorch.nn\ntorch.functional\ntorch.nn.functional\ntorch\ntorch*.**: matchestorch,torchvision, and all of their submodules\ntorch*.**\ntorch\ntorchvision\nWhen specifying actions, you can pass multiple patterns, e.g.\n\n```python\nexporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"])\n\n```\n\nA module will match against this action if it matches any of the patterns.\nYou can also specify patterns to exclude, e.g.\n\n```python\nexporter.mock(\"**\", exclude=[\"torchvision.**\"])\n\n```\n\nA module will not match against this action if it matches any of the exclude patterns. In this example, we are mocking all modules excepttorchvisionand its submodules.\ntorchvision\nWhen a module could potentially match against multiple actions, the first action defined will be taken.\n\n## torch.packagesharp edges#\n\ntorch.package\nPython makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state.\nMutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package.\ntorch.package\nEveryPackageImportercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.\nPackageImporter\nAny class that you import from aPackageImporterwill be a version of the class specific to that importer. For example:\nPackageImporter\n\n```python\nfrom foo import MyClass\n\nmy_class_instance = MyClass()\n\nwith PackageExporter(f) as exporter:\n    exporter.save_module(\"foo\")\n\nimporter = PackageImporter(f)\nimported_MyClass = importer.import_module(\"foo\").MyClass\n\nassert isinstance(my_class_instance, MyClass)  # works\nassert isinstance(my_class_instance, imported_MyClass)  # ERROR!\n\n```\n\nIn this example,MyClassandimported_MyClassarenot the same type. In this specific example,MyClassandimported_MyClasshave exactly the\nsame implementation, so you might think it\u2019s okay to consider them the same class. But consider the situation whereimported_MyClassis coming from an\nolder package with an entirely different implementation ofMyClass\u2014 in that case, it\u2019s unsafe to consider them the same class.\nMyClass\nimported_MyClass\nMyClass\nimported_MyClass\nimported_MyClass\nMyClass\nUnder the hood, each importer has a prefix that allows it to uniquely identify classes:\n\n```python\nprint(MyClass.__name__)  # prints \"foo.MyClass\"\nprint(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass\n\n```\n\nThat means you should not expectisinstancechecks to work when one of the arguments is from a package and the other is not. If you need this\nfunctionality, consider the following options:\nisinstance\nDoing duck typing (just using the class instead of explicitly checking that it is of a given type).\nMake the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.\nself.handler=\"handle_me_this_way\"\nhandler\n\n## Howtorch.packagekeeps packages isolated from each other#\n\ntorch.package\nEachPackageImporterinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiplePackageImporterinstances to load a single package, you will get\nmultiple independent environments that do not interact.\nPackageImporter\nextern\nPackageImporter\nThis is achieved by extending Python\u2019s import infrastructure with a custom importer.PackageImporterprovides the same core API as theimportlibimporter; namely, it implements theimport_moduleand__import__methods.\nPackageImporter\nimportlib\nimport_module\n__import__\nWhen you invokePackageImporter.import_module(),PackageImporterwill construct and return a new module, much as the system importer does.\nHowever,PackageImporterpatches the returned module to useself(i.e. thatPackageImporterinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment.\nPackageImporter.import_module()\nPackageImporter\nPackageImporter\nself\nPackageImporter\nTo avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),PackageImportermangles the__name__and__file__of all imported modules, by adding amangle prefixto them.\nfoo.bar\nPackageImporter\n__name__\n__file__\nFor__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18.\n__name__\ntorchvision.models.resnet18\n<torch_package_0>.torchvision.models.resnet18\nFor__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.\n__file__\ntorchvision/models/resnet18.py\n<torch_package_0>.torchvision/modules/resnet18.py\nName mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.\nmangling.md\ntorch/package/\n\n## API Reference#\n\nThis exception is raised when there is an issue with exporting a package.PackageExporterwill attempt to gather up all the errors and present\nthem to you at once.\nPackageExporter\nThis is an exception that is thrown when a mock or extern is marked asallow_empty=False, and is not matched with any module during packaging.\nallow_empty=False\nExporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package.\nImports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.\nThe code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.\nThe importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine.\nextern()\nextern_modules\nWhen source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()).\ndependencies=True\nextern()\nmock()\nintern()\nCreate an exporter.\nf(Union[str,PathLike[str],IO[bytes]]) \u2013 The location to export to. Can be astring/Pathobject containing a filename\nor a binary I/O object.\nstring\nPath\nimporter(Union[Importer,Sequence[Importer]]) \u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passed, anOrderedImporterwill be constructed out of them.\nOrderedImporter\ndebug(bool) \u2013 If set to True, add path of broken modules to PackagingErrors.\nGiven a module, add it to the dependency graph according to patterns\nspecified by the user.\nthat has all paths from src to dst.\nA dot representation containing all paths from src to dst.\n(https://graphviz.org/doc/info/lang.html)\nstr\nWrite the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead:\nclose()\n\n```python\nwith PackageExporter(\"file.zip\") as e:\n    ...\n\n```\n\nReturn all modules that are currently denied.\nA list containing the names of modules which will be\ndenied in this package.\nlist[str]\nBlocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.\nPackagingError\ninclude(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().\n\"my_package.my_subpackage\"\nmock()\nexclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.\nReturns digraph string representation of dependencies in package.\nA string representation of dependencies in package.\nstr\nIncludemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package.\nmodule\ninclude(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock().\n\"my_package.my_subpackage\"\nmock()\nexclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string.\nallow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added withallow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. Ifallow_empty=True,\nno such exception is thrown.\nextern\nallow_empty=False\nclose()\n__exit__\nallow_empty=True\nReturn all modules that are currently externed.\nA list containing the names of modules which will be\nexterned in this package.\nlist[str]\nReturn a list of all modules which depend on the modulemodule_name.\nmodule_name\nA list containing the names of modules which depend onmodule_name.\nmodule_name\nlist[str]\nGet an id. This id is guaranteed to only be handed out once for this package.\nstr\nSpecify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively.\nintern\ninclude(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().\nmock()\nexclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.\nallow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If aninternmodule glob\npattern is added withallow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. Ifallow_empty=True, no such exception is thrown.\nintern\nintern\nallow_empty=False\nclose()\n__exit__\nallow_empty=True\nReturn all modules that are currently interned.\nA list containing the names of modules which will be\ninterned in this package.\nlist[str]\nReplace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code.\ninclude(Union[List[str],str]) \u2013A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be mocked out. Strings can also be a glob-style pattern\nstring that may match multiple modules. Any required dependencies that match this pattern\nstring will be mocked out automatically.Examples :'torch.**'\u2013 matchestorchand all submodules of torch, e.g.'torch.nn'and'torch.nn.functional''torch.*'\u2013 matches'torch.nn'or'torch.functional', but not'torch.nn.functional'\nA string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be mocked out. Strings can also be a glob-style pattern\nstring that may match multiple modules. Any required dependencies that match this pattern\nstring will be mocked out automatically.\n\"my_package.my_subpackage\"\n'torch.**'\u2013 matchestorchand all submodules of torch, e.g.'torch.nn'and'torch.nn.functional'\n'torch.**'\ntorch\n'torch.nn'\n'torch.nn.functional'\n'torch.*'\u2013 matches'torch.nn'or'torch.functional', but not'torch.nn.functional'\n'torch.*'\n'torch.nn'\n'torch.functional'\n'torch.nn.functional'\nexclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.\ne.g.include='torch.**',exclude='torch.foo'will mock all torch packages except'torch.foo',\nDefault: is[].\ninclude='torch.**',exclude='torch.foo'\n'torch.foo'\n[]\nallow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added withallow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIfallow_empty=True, no such exception is thrown.\nmock()\nallow_empty=False\nclose()\n__exit__\nallow_empty=True\nReturn all modules that are currently mocked.\nA list containing the names of modules which will be\nmocked in this package.\nlist[str]\nRegisters an extern hook on the exporter.\nThe hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature:\nextern()\n\n```python\nhook(exporter: PackageExporter, module_name: str) -> None\n\n```\n\nHooks will be called in order of registration.\nA handle that can be used to remove the added hook by callinghandle.remove().\nhandle.remove()\ntorch.utils.hooks.RemovableHandle\ntorch.utils.hooks.RemovableHandle\nRegisters an intern hook on the exporter.\nThe hook will be called each time a module matches against anintern()pattern.\nIt should have the following signature:\nintern()\n\n```python\nhook(exporter: PackageExporter, module_name: str) -> None\n\n```\n\nHooks will be called in order of registration.\nA handle that can be used to remove the added hook by callinghandle.remove().\nhandle.remove()\ntorch.utils.hooks.RemovableHandle\ntorch.utils.hooks.RemovableHandle\nRegisters a mock hook on the exporter.\nThe hook will be called each time a module matches against amock()pattern.\nIt should have the following signature:\nmock()\n\n```python\nhook(exporter: PackageExporter, module_name: str) -> None\n\n```\n\nHooks will be called in order of registration.\nA handle that can be used to remove the added hook by callinghandle.remove().\nhandle.remove()\ntorch.utils.hooks.RemovableHandle\ntorch.utils.hooks.RemovableHandle\nSave raw bytes to the package.\npackage(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\").\n\"my_package.my_subpackage\"\nresource(str) \u2013 A unique name for the resource, used to identify it to load.\nbinary(str) \u2013 The data to save.\nSave the code formoduleinto the package. Code for the module is resolved using theimporterspath to find the\nmodule object, and then using its__file__attribute to find the source code.\nmodule\nimporters\n__file__\nmodule_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package.\nmy_package.my_subpackage\ndependencies(bool,optional) \u2013 IfTrue, we scan the source for dependencies.\nTrue\nSave a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Standard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.\ntorch.save()\ndependencies\nTo be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work.\ntype(obj).__name__\nmy_module.MyObject\nmy_module.MyObject\nimporter\nimport_module\nimporter\npackage(str) \u2013 The name of module package this resource should go in (e.g.\"my_package.my_subpackage\").\n\"my_package.my_subpackage\"\nresource(str) \u2013 A unique name for the resource, used to identify it to load.\nobj(Any) \u2013 The object to save, must be picklable.\ndependencies(bool,optional) \u2013 IfTrue, we scan the source for dependencies.\nTrue\nAdds the local file systemfile_or_directoryto the source package to provide the code\nformodule_name.\nfile_or_directory\nmodule_name\nmodule_name(str) \u2013 e.g.\"my_package.my_subpackage\", code will be saved to provide code for this package.\n\"my_package.my_subpackage\"\nfile_or_directory(str) \u2013 the path to a file or directory of code. When a directory, all python files in the directory\nare recursively copied usingsave_source_file(). If a file is named\"/__init__.py\"the code is treated\nas a package.\nsave_source_file()\n\"/__init__.py\"\ndependencies(bool,optional) \u2013 IfTrue, we scan the source for dependencies.\nTrue\nAddssrcas the source code formodule_namein the exported package.\nsrc\nmodule_name\nmodule_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code for this package.\nmy_package.my_subpackage\nsrc(str) \u2013 The Python source code to save for this package.\nis_package(bool,optional) \u2013 IfTrue, this module is treated as a package. Packages are allowed to have submodules\n(e.g.my_package.my_subpackage.my_subsubpackage), and resources can be saved inside them. Defaults toFalse.\nTrue\nmy_package.my_subpackage.my_subsubpackage\nFalse\ndependencies(bool,optional) \u2013 IfTrue, we scan the source for dependencies.\nTrue\nSave text data to the package.\npackage(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\").\n\"my_package.my_subpackage\"\nresource(str) \u2013 A unique name for the resource, used to identify it to load.\ntext(str) \u2013 The contents to save.\nImporters allow you to load code written to packages byPackageExporter.\nCode is loaded in a hermetic way, using files from the package\nrather than the normal python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.\nPackageExporter\nThe importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external during export.\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine.\nextern_modules\nOpenfile_or_bufferfor importing. This checks that the imported package only requires modules\nallowed bymodule_allowed\nfile_or_buffer\nmodule_allowed\nfile_or_buffer(Union[str,PathLike[str],IO[bytes],PyTorchFileReader]) \u2013 a file-like object (has to implementread(),readline(),tell(), andseek()),\na string, or anos.PathLikeobject containing a filename.\nread()\nreadline()\ntell()\nseek()\nos.PathLike\nmodule_allowed(Callable[[str],bool],optional) \u2013 A method to determine if a externally provided module\nshould be allowed. Can be used to ensure packages loaded do not depend on modules that the server\ndoes not support. Defaults to allowing anything.\nImportError\u2013 If the package will use a disallowed module.\nReturns a file structure representation of package\u2019s zipfile.\ninclude(Union[List[str],str]) \u2013 An optional string e.g.\"my_package.my_subpackage\", or optional list of strings\nfor the names of the files to be included in the zipfile representation. This can also be\na glob-style pattern, as described inPackageExporter.mock()\n\"my_package.my_subpackage\"\nPackageExporter.mock()\nexclude(Union[List[str],str]) \u2013 An optional pattern that excludes files whose name match the pattern.\nDirectory\nDirectory\nDirectory\nReturns internal identifier that torch.package uses to distinguishPackageImporterinstances.\nLooks like:\nPackageImporter\n\n```python\n<torch_package_0>\n\n```\n\nLoad a module from the package if it hasn\u2019t already been loaded, and then return\nthe module. Modules are loaded locally\nto the importer and will appear inself.modulesrather thansys.modules.\nself.modules\nsys.modules\nname(str) \u2013 Fully qualified name of the module to load.\npackage([type],optional) \u2013 Unused, but present to match the signature of importlib.import_module. Defaults toNone.\nNone\nThe (possibly already) loaded module.\ntypes.ModuleType\nLoad raw bytes.\npackage(str) \u2013 The name of module package (e.g.\"my_package.my_subpackage\").\n\"my_package.my_subpackage\"\nresource(str) \u2013 The unique name for the resource.\nThe loaded data.\nbytes\nUnpickles the resource from the package, loading any modules that are needed to construct the objects\nusingimport_module().\nimport_module()\npackage(str) \u2013 The name of module package (e.g.\"my_package.my_subpackage\").\n\"my_package.my_subpackage\"\nresource(str) \u2013 The unique name for the resource.\nmap_location\u2013 Passed totorch.loadto determine how tensors are mapped to devices. Defaults toNone.\nNone\nThe unpickled object.\nAny\nLoad a string.\npackage(str) \u2013 The name of module package (e.g.\"my_package.my_subpackage\").\n\"my_package.my_subpackage\"\nresource(str) \u2013 The unique name for the resource.\nencoding(str,optional) \u2013 Passed todecode. Defaults to'utf-8'.\ndecode\n'utf-8'\nerrors(str,optional) \u2013 Passed todecode. Defaults to'strict'.\ndecode\n'strict'\nThe loaded text.\nstr\nReturns the version of python that was used to create this package.\nNote: this function is experimental and not Forward Compatible. The plan is to move this into a lock\nfile later on.\nOptional[str]a python version e.g. 3.8.9 or None if no version was stored with this package\nOptional[str]\nA file structure representation. Organized as Directory nodes that have lists of\ntheir Directory children. Directories for a package are created by callingPackageImporter.file_structure().\nPackageImporter.file_structure()\nChecks if a file is present in aDirectory.\nDirectory\nfilename(str) \u2013 Path of file to search for.\nIf aDirectorycontains the specified file.\nDirectory\nbool",
    "url": "https://pytorch.org/docs/stable/package.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "687319926c581bb24b4c994af595577e",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/tensorboard.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ada520a3815860512e15c6a6f9de2982",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/large_scale_deployments.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9fb414b7b10cde3353ff4793c31f403b",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.where_to_apply_compile.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "57548f38b046bd20bbcb4c5b1516b70a",
    "source": "pytorch_docs",
    "title": "torch.cuda.make_graphed_callables \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.cuda.make_graphed_callables#\n\nAccept callables (functions ornn.Modules) and returns graphed versions.\nnn.Module\nEach graphed callable\u2019s forward pass runs its source callable\u2019s\nforward CUDA work as a CUDA graph inside a single autograd node.\nThe graphed callable\u2019s forward pass also appends\na backward node to the autograd graph. During backward, this node runs the\ncallable\u2019s backward work as a CUDA graph.\nTherefore, each graphed callable should be a drop-in replacement for its source callable\nin an autograd-enabled training loop.\nSeePartial-network capturefor detailed use and constraints.\nIf you pass a tuple of several callables, their captures will use the same memory pool.\nSeeGraph memory managementfor when this is appropriate.\ncallables(torch.nn.ModuleorPython function, ortupleofthese) \u2013 Callable or callables to graph.\nSeeGraph memory managementfor when passing a tuple of callables\nis appropriate.  If you pass a tuple of callables, their order in the tuple must be the same order\nthey\u2019ll run in the live workload.\nsample_args(tupleofTensors, ortupleoftuplesofTensors) \u2013 Samples args for each callable.\nIf a single callable was passed,sample_argsmust be a single tuple of argument Tensors.\nIf a tuple of callables was passed,sample_argsmust be tuple of tuples of argument Tensors.\nsample_args\nsample_args\nnum_warmup_iters(int) \u2013 The number of warmup iterations. Currently,DataDistributedParallelneeds\n11 iterations for warm up. Default:3.\nDataDistributedParallel\n3\nallow_unused_input(bool) \u2013 If False, specifying inputs that were not used when computing outputs\n(and therefore their grad is always zero) is an error. Defaults to False.\npool(optional) \u2013 Token (returned bygraph_pool_handle()orother_Graph_instance.pool()) that hints this graph may share memory\nwith the indicated pool.  SeeGraph memory management.\ngraph_pool_handle()\nother_Graph_instance.pool()\nNote\nTherequires_gradstate of each Tensor insample_argsmust match the state\nthat\u2019s expected for the corresponding real input in the training loop.\nrequires_grad\nsample_args\nWarning\nThis API is in beta and may change in future releases.\nWarning\nsample_argsfor each callable must contain only Tensors. Other types are not allowed.\nsample_args\nWarning\nReturned callables do not support higher order differentiation (e.g., double backward).\nWarning\nIn anyModulepassed tomake_graphed_callables(), only parameters\nmay be trainable. Buffers must haverequires_grad=False.\nModule\nmake_graphed_callables()\nrequires_grad=False\nWarning\nAfter you pass atorch.nn.Modulethroughmake_graphed_callables(),\nyou may not add or remove any of that Module\u2019s parameters or buffers.\ntorch.nn.Module\nmake_graphed_callables()\nWarning\ntorch.nn.Modules passed tomake_graphed_callables()must not have module hooks\nregistered on them at the time they are passed. However, registering hooks on modulesafterpassing them\nthroughmake_graphed_callables()is allowed.\ntorch.nn.Module\nmake_graphed_callables()\nmake_graphed_callables()\nWarning\nWhen running a graphed callable, you must pass its arguments in the same order and format\nthey appeared in that callable\u2019ssample_args.\nsample_args\nWarning\nThe automatic mixed precision is supported inmake_graphed_callables()only with disabled\ncaching. The context managertorch.cuda.amp.autocast()must havecache_enabled=False.\nmake_graphed_callables()",
    "url": "https://pytorch.org/docs/stable/generated/torch.cuda.make_graphed_callables.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "20ee1f5552bccc0c63ed2496fe27bceb",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/control_plane.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "be5d70a5630b5449f7c89df13e1305be",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/pytorch-api.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c7339e59575396c378d99c98db47d76c",
    "source": "pytorch_docs",
    "title": "Torch Environment Variables \u2014 PyTorch 2.9 documentation",
    "text": "\n## Torch Environment Variables#\n\nCreated On: Feb 15, 2024 | Last Updated On: Jun 10, 2025\nPyTorch leverages environment variables for adjusting various settings that influence its runtime behavior.\nThese variables offer control over key functionalities, such as displaying the C++ stack trace upon encountering errors, synchronizing the execution of CUDA kernels,\nspecifying the number of threads for parallel processing tasks and many more.\nMoreover, PyTorch leverages several high-performance libraries, such as MKL and cuDNN,\nwhich also utilize environment variables to modify their functionality.\nThis interplay of settings allows for a highly customizable development environment that can be\noptimized for efficiency, debugging, and computational resource management.\nPlease note that while this documentation covers a broad spectrum of environment variables relevant to PyTorch and its associated libraries, it is not exhaustive.\nIf you find anything in this documentation that is missing, incorrect, or could be improved, please let us know by filing an issue or opening a pull request.",
    "url": "https://pytorch.org/docs/stable/torch_environment_variables.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7292666c29006e7e84a43ec11d67243c",
    "source": "pytorch_docs",
    "title": "torch.utils.mobile_optimizer \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.mobile_optimizer#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\nPyTorch Mobile is no longer actively supported. Redirecting toExecuTorch documentation.\nWarning\nPyTorch Mobile is no longer actively supported. Please check outExecuTorch, PyTorch\u2019s\nall-new on-device inference library. You can also review\ndocumentation onXNNPACKandVulkandelegates.\nOptimize a torch script module for mobile deployment.\nscript_module(ScriptModule) \u2013 An instance of torch script module with type of ScriptModule.\noptimization_blocklist(Optional[set[torch._C._MobileOptimizerType]]) \u2013 A set with type of MobileOptimizerType. When set is not passed,\noptimization method will run all the optimizer pass; otherwise, optimizer\nmethod will run the optimization pass that is not included inside optimization_blocklist.\npreserved_methods(Optional[list[~AnyStr]]) \u2013 A list of methods that needed to be preserved when freeze_module pass is invoked\nbackend(str) \u2013 Device type to use for running the result model (\u2018CPU\u2019(default), \u2018Vulkan\u2019 or \u2018Metal\u2019).\nA new optimized torch script module\nRecursiveScriptModule",
    "url": "https://pytorch.org/docs/stable/mobile_optimizer.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "99445727568f74e4faff6718ead60948",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/ddp_comm_hooks.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2d17388495b0993a2c90735d37c4802a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/cuda.tunable.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6d7aed1767c33ffe3142b8306401b2f9",
    "source": "pytorch_docs",
    "title": "Understanding CUDA Memory Usage \u2014 PyTorch 2.9 documentation",
    "text": "\n## Understanding CUDA Memory Usage#\n\nCreated On: Aug 23, 2023 | Last Updated On: Sep 02, 2025\nTo debug CUDA memory use, PyTorch provides a way to generate memory snapshots that record the state of allocated CUDA memory\nat any point in time, and optionally record the history of allocation events that led up to that snapshot.\nThe generated snapshots can then be drag and dropped onto the interactiver viewer hosted atpytorch.org/memory_vizwhich\ncan be used to explore the snapshot.\nNote\nThe memory profiler and visualizer described in this document only have visibility into the CUDA memory that is\nallocated and managed through the PyTorch allocator.  Any memory allocated directly from CUDA APIs will not be\nvisible in the PyTorch memory profiler.\nNCCL (used for distributed communication on CUDA devices) is a common example of a library that allocates some\nGPU memory that is invisible to the PyTorch memory profiler.  SeeIdentifying Non-PyTorch allocationsfor more info.\n\n## Generating a Snapshot#\n\nThe common pattern for recording a snapshot is to enable memory history, run the code to be observed, and then save a file with a pickled snapshot:\n\n```python\n# enable memory history, which will\n# add tracebacks and event history to snapshots\ntorch.cuda.memory._record_memory_history()\n\nrun_your_code()\ntorch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n\n```\n\n\n## Using the visualizer#\n\nOpenhttps://pytorch.org/memory_vizand drag/drop the pickled snapshot file into the visualizer.\nThe visualizer is a javascript application that runs locally on your computer. It does not upload any snapshot data.\n\n## Active Memory Timeline#\n\nThe Active Memory Timeline shows all the live tensors over time in the snapshot on a particular GPU. Pan/Zoom over the plot to look at smaller allocations.\nMouse over allocated blocks to see a stack trace for when that block was allocated, and details like its address. The detail slider can be adjusted to\nrender fewer allocations and improve performance when there is a lot of data.\n\n## Allocator State History#\n\nThe Allocator State History shows individual allocator events in a timeline on the left. Select an event in the timeline to see a visual summary of the\nallocator state at that event. This summary shows each individual segment returned from cudaMalloc and how it is split up into blocks of individual allocations\nor free space. Mouse over segments and blocks to see the stack trace when the memory was allocated. Mouse over events to see the stack trace when the event occurred,\nsuch as when a tensor was freed. Out of memory errors are reported as OOM events. Looking at the state of memory during an OOM may provide insight into why\nan allocation failed even though reserved memory still exists.\nThe stack trace information also reports the address at which an allocation occurred.\nThe address b7f064c000000_0 refers to the (b)lock at address 7f064c000000 which is the \u201c_0\u201dth time this address was allocated.\nThis unique string can be looked up in the Active Memory Timeline and searched\nin the Active State History to examine the memory state when a tensor was allocated or freed.\n\n## Identifying Non-PyTorch allocations#\n\nIf you suspect CUDA memory is being allocated outside of PyTorch, you can collect the raw CUDA allocation info using\nthe pynvml package, and compare that to the allocation reported by pytorch.\nTo collect raw memory usage outside pytorch, usedevice_memory_used()\ndevice_memory_used()\n\n```python\nimport torch\ndevice_idx = ...\nprint(torch.cuda.device_memory_used(device_idx))\n\n```\n\n\n## Snapshot API Reference#\n\nEnable recording of stack traces associated with memory\nallocations, so you can tell what allocated any piece of memory intorch.cuda.memory._snapshot().\ntorch.cuda.memory._snapshot()\nIn addition to keeping stack traces with each current allocation and free,\nthis will also enable recording of a history of all alloc/free events.\nUsetorch.cuda.memory._snapshot()to retrieve this information,\nand the tools in_memory_viz.pyto visualize snapshots.\ntorch.cuda.memory._snapshot()\n\n## Buffer behavior#\n\nThis will store up tomax_entriesinstances ofTraceEntrywhen enabled.\nPython trace collection defaults tosys.maxsize, meaning long-running\nor indefinitely running jobs should set a reasonable limit to avoid excessive\nmemory use. Expect each entry to be several KB.\nLonger running workflows or those with smallermax_entriesvalues will only\nstore the last accumulatedmax_entriesentries, meaning new entries overwrite\nolder entries.\nC++ implementation for reference to ring buffer implementation:\n\n```python\nif (record_history) {\n  if (alloc_trace->size() < alloc_trace_max_entries_) {\n    alloc_trace->emplace_back(te);\n  } else {\n    (*alloc_trace)[alloc_trace_next++] = te;\n    if (alloc_trace_next == alloc_trace_max_entries_) {\n      alloc_trace_next = 0;\n    }\n  }\n}\n\n```\n\n\n## Latency impact#\n\nThe Python trace collection is fast (2us per trace), so you may consider\nenabling this on production jobs if you anticipate ever having to debug\nmemory issues.\nC++ trace collection is also fast (~50ns/frame), which for many typical programs\nworks out to ~2us per trace, but can vary depending on stack depth.\nNone, disable recording memory history.\u201cstate\u201d, keep information for currently allocated memory.\u201call\u201d, additionally keep a history of all alloc/free calls.\nDefaults to \u201call\u201d.\nLiteral[None, \u201cstate\u201d, \u201call\u201d], optional\nNone, Do not record any tracebacks.\u201cstate\u201d, Record tracebacks for currently allocated memory.\u201calloc\u201d, additionally keep tracebacks for alloc calls.\u201call\u201d, additionally keep tracebacks for free calls.\nDefaults to \u201call\u201d.\nLiteral[None, \u201cstate\u201d, \u201calloc\u201d, \u201call\u201d], optional\n\u201cpython\u201d, include Python, TorchScript, and inductor frames in tracebacks\u201call\u201d, additionally include C++ frames\nDefaults to \u201call\u201d.\nLiteral[\u201cpython\u201d, \u201call\u201d], optional\nKeep a maximum ofmax_entriesalloc/free events in the recorded history recorded.\nint, optional\nSave a snapshot of CUDA memory state at the time it was called.\nThe state is represented as a dictionary with the following structure.\n\n```python\nclass Snapshot(TypedDict):\n    segments: List[Segment]\n    device_traces: List[List[TraceEntry]]\n\n\nclass Segment(TypedDict):\n    # Segments are memory returned from a cudaMalloc call.\n    # The size of reserved memory is the sum of all Segments.\n    # Segments are cached and reused for future allocations.\n    # If the reuse is smaller than the segment, the segment\n    # is split into more then one Block.\n    # empty_cache() frees Segments that are entirely inactive.\n    address: int\n    total_size: int  #  cudaMalloc'd size of segment\n    stream: int\n    segment_type: Literal[\"small\", \"large\"]  # 'large' (>1MB)\n    allocated_size: int  # size of memory in use\n    active_size: int  # size of memory in use or in active_awaiting_free state\n    blocks: List[Block]\n\n\nclass Block(TypedDict):\n    # A piece of memory returned from the allocator, or\n    # current cached but inactive.\n    size: int\n    requested_size: int  # size requested during malloc, may be smaller than\n    # size due to rounding\n    address: int\n    state: Literal[\n        \"active_allocated\",  # used by a tensor\n        \"active_awaiting_free\",  # waiting for another stream to finish using\n        # this, then it will become free\n        \"inactive\",\n    ]  # free for reuse\n    frames: List[Frame]  # stack trace from where the allocation occurred\n\n\nclass Frame(TypedDict):\n    filename: str\n    line: int\n    name: str\n\n\nclass TraceEntry(TypedDict):\n    # When `torch.cuda.memory._record_memory_history()` is enabled,\n    # the snapshot will contain TraceEntry objects that record each\n    # action the allocator took.\n    action: Literal[\n        \"alloc\"  # memory allocated\n        \"free_requested\",  # the allocated received a call to free memory\n        \"free_completed\",  # the memory that was requested to be freed is now\n        # able to be used in future allocation calls\n        \"segment_alloc\",  # the caching allocator ask cudaMalloc for more memory\n        # and added it as a segment in its cache\n        \"segment_free\",  # the caching allocator called cudaFree to return memory\n        # to cuda possibly trying free up memory to\n        # allocate more segments or because empty_caches was called\n        \"oom\",  # the allocator threw an OOM exception. 'size' is\n        # the requested number of bytes that did not succeed\n        \"snapshot\",  # the allocator generated a memory snapshot\n        # useful to coorelate a previously taken\n        # snapshot with this trace\n    ]\n    addr: int  # not present for OOM\n    frames: List[Frame]\n    size: int\n    stream: int\n    device_free: int  # only present for OOM, the amount of\n    # memory cuda still reports to be free\n\n```\n\nThe Snapshot dictionary object\nSave a pickled version of thetorch.memory._snapshot()dictionary to a file.\nThis file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz\nSnapshot file sizes scale withmax_entriesand stack trace depth per entry,\nwith several KB per entry. These can easily be in the GB range for longer running\nworkflows with largemax_entries.\nfilename(str,optional) \u2013 Name of the file to create. Defaults to \u201cdump_snapshot.pickle\u201d.",
    "url": "https://pytorch.org/docs/stable/torch_cuda_memory.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ab04f4cdc0ca9122c72f658fbfb0ca2b",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/deterministic.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2bf12e654a57b421bba7499e3a8359c1",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/rpc/distributed_autograd.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "545e5d8e731934d044ffb960656d4bab",
    "source": "pytorch_docs",
    "title": "Named Tensors \u2014 PyTorch 2.9 documentation",
    "text": "\n## Named Tensors#\n\nCreated On: Oct 08, 2019 | Last Updated On: Jun 14, 2025\nNamed Tensors allow users to give explicit names to tensor dimensions.\nIn most cases, operations that take dimension parameters will accept\ndimension names, avoiding the need to track dimensions by position.\nIn addition, named tensors use names to automatically check that APIs\nare being used correctly at runtime, providing extra safety. Names can\nalso be used to rearrange dimensions, for example, to support\n\u201cbroadcasting by name\u201d rather than \u201cbroadcasting by position\u201d.\nWarning\n\n```python\nThe named tensor API is a prototype feature and subject to change.\n\n```\n\n\n## Creating named tensors#\n\nFactory functions now take a newnamesargument that associates a name\nwith each dimension.\nnames\n\n```python\n    >>> torch.zeros(2, 3, names=('N', 'C'))\n    tensor([[0., 0., 0.],\n            [0., 0., 0.]], names=('N', 'C'))\n\n```\n\nNamed dimensions, like regular Tensor dimensions, are ordered.tensor.names[i]is the name of dimensionioftensor.\ntensor.names[i]\ni\ntensor\nThe following factory functions support named tensors:\ntorch.empty()\ntorch.empty()\ntorch.rand()\ntorch.rand()\ntorch.randn()\ntorch.randn()\ntorch.ones()\ntorch.ones()\ntorch.tensor()\ntorch.tensor()\ntorch.zeros()\ntorch.zeros()\n\n## Named dimensions#\n\nSeenamesfor restrictions on tensor names.\nnames\nUsenamesto access the dimension names of a tensor andrename()to rename named dimensions.\nnames\nrename()\n\n```python\n    >>> imgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W'))\n    >>> imgs.names\n    ('N', 'C', 'H', 'W')\n\n    >>> renamed_imgs = imgs.rename(H='height', W='width')\n    >>> renamed_imgs.names\n    ('N', 'C', 'height', 'width)\n\n```\n\nNamed tensors can coexist with unnamed tensors; named tensors are instances oftorch.Tensor. Unnamed tensors haveNone-named dimensions. Named\ntensors do not require all dimensions to be named.\ntorch.Tensor\nNone\n\n```python\n    >>> imgs = torch.randn(1, 2, 2, 3 , names=(None, 'C', 'H', 'W'))\n    >>> imgs.names\n    (None, 'C', 'H', 'W')\n\n```\n\n\n## Name propagation semantics#\n\nNamed tensors use names to automatically check that APIs are being called\ncorrectly at runtime. This occurs in a process calledname inference.\nMore formally, name inference consists of the following two steps:\nCheck names: an operator may perform automatic checks at runtime that\ncheck that certain dimension names must match.\nPropagate names: name inference propagates names to output tensors.\nAll operations that support named tensors propagate names.\n\n```python\n    >>> x = torch.randn(3, 3, names=('N', 'C'))\n    >>> x.abs().names\n    ('N', 'C')\n\n```\n\n\n## match semantics#\n\nTwo namesmatchif they are equal (string equality) or if at least one isNone.\nNones are essentially a special \u201cwildcard\u201d name.\nNone\nunify(A,B)determines which of the namesAandBto propagate to the outputs.\nIt returns the morespecificof the two names, if they match. If the names do not match,\nthen it errors.\nunify(A,B)\nA\nB\nNote\nIn practice, when working with named tensors, one should avoid having unnamed\ndimensions because their handling can be complicated. It is recommended to lift\nall unnamed dimensions to be named dimensions by usingrefine_names().\nrefine_names()\n\n## Basic name inference rules#\n\nLet\u2019s see howmatchandunifyare used in name inference in the case of\nadding two one-dim tensors with no broadcasting.\nmatch\nunify\n\n```python\n    x = torch.randn(3, names=('X',))\n    y = torch.randn(3)\n    z = torch.randn(3, names=('Z',))\n\n```\n\nCheck names: check that the names of the two tensorsmatch.\nFor the following examples:\n\n```python\n    >>> # x + y  # match('X', None) is True\n    >>> # x + z  # match('X', 'Z') is False\n    >>> # x + x  # match('X', 'X') is True\n\n    >>> x + z\n    Error when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match.\n\n```\n\nPropagate names:unifythe names to select which one to propagate.\nIn the case ofx+y,unify('X',None)='X'because'X'is more\nspecific thanNone.\nx+y\nunify('X',None)='X'\n'X'\nNone\n\n```python\n    >>> (x + y).names\n    ('X',)\n    >>> (x + x).names\n    ('X',)\n\n```\n\nFor a comprehensive list of name inference rules, seeNamed Tensors operator coverage.\nHere are two common operations that may be useful to go over:\nBinary arithmetic ops:Unifies names from inputs\nMatrix multiplication ops:Contracts away dims\n\n## Explicit alignment by names#\n\nUsealign_as()oralign_to()to align tensor dimensions\nby name to a specified ordering. This is useful for performing \u201cbroadcasting by names\u201d.\nalign_as()\nalign_to()\n\n```python\n    # This function is agnostic to the dimension ordering of `input`,\n    # as long as it has a `C` dimension somewhere.\n    def scale_channels(input, scale):\n        scale = scale.refine_names('C')\n        return input * scale.align_as(input)\n\n    >>> num_channels = 3\n    >>> scale = torch.randn(num_channels, names=('C',))\n    >>> imgs = torch.rand(3, 3, 3, num_channels, names=('N', 'H', 'W', 'C'))\n    >>> more_imgs = torch.rand(3, num_channels, 3, 3, names=('N', 'C', 'H', 'W'))\n    >>> videos = torch.randn(3, num_channels, 3, 3, 3, names=('N', 'C', 'H', 'W', 'D')\n\n    >>> scale_channels(imgs, scale)\n    >>> scale_channels(more_imgs, scale)\n    >>> scale_channels(videos, scale)\n\n```\n\n\n## Manipulating dimensions#\n\nUsealign_to()to permute large amounts of dimensions without\nmentioning all of them as in required bypermute().\nalign_to()\npermute()\n\n```python\n    >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n    >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n    # Move the F (dim 5) and E dimension (dim 4) to the front while keeping\n    # the rest in the same order\n    >>> tensor.permute(5, 4, 0, 1, 2, 3)\n    >>> named_tensor.align_to('F', 'E', ...)\n\n```\n\nUseflatten()andunflatten()to flatten and unflatten\ndimensions, respectively. These methods are more verbose thanview()andreshape(), but have more semantic meaning to someone reading the code.\nflatten()\nunflatten()\nview()\nreshape()\n\n```python\n    >>> imgs = torch.randn(32, 3, 128, 128)\n    >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n\n    >>> flat_imgs = imgs.view(32, -1)\n    >>> named_flat_imgs = named_imgs.flatten(['C', 'H', 'W'], 'features')\n    >>> named_flat_imgs.names\n    ('N', 'features')\n\n    >>> unflattened_named_imgs = named_flat_imgs.unflatten('features', [('C', 3), ('H', 128), ('W', 128)])\n    >>> unflattened_named_imgs.names\n    ('N', 'C', 'H', 'W')\n\n```\n\n\n## Autograd support#\n\nAutograd currently supports named tensors in a limited manner: autograd ignores\nnames on all tensors. Gradient computation is still correct but we lose the\nsafety that names give us.\n\n```python\n    >>> x = torch.randn(3, names=('D',))\n    >>> weight = torch.randn(3, names=('D',), requires_grad=True)\n    >>> loss = (x - weight).abs()\n    >>> grad_loss = torch.randn(3)\n    >>> loss.backward(grad_loss)\n    >>> weight.grad  # Unnamed for now. Will be named in the future\n    tensor([-1.8107, -0.6357,  0.0783])\n\n    >>> weight.grad.zero_()\n    >>> grad_loss = grad_loss.refine_names('C')\n    >>> loss = (x - weight).abs()\n    # Ideally we'd check that the names of loss and grad_loss match but we don't yet.\n    >>> loss.backward(grad_loss)\n    >>> weight.grad\n    tensor([-1.8107, -0.6357,  0.0783])\n\n```\n\n\n## Currently supported operations and subsystems#\n\n\n## Operators#\n\nSeeNamed Tensors operator coveragefor a full list of the supported torch and\ntensor operations. We do not yet support the following that is not covered by the link:\nindexing, advanced indexing.\nFortorch.nn.functionaloperators, we support the following:\ntorch.nn.functional\ntorch.nn.functional.relu()\ntorch.nn.functional.relu()\ntorch.nn.functional.softmax()\ntorch.nn.functional.softmax()\ntorch.nn.functional.log_softmax()\ntorch.nn.functional.log_softmax()\ntorch.nn.functional.tanh()\ntorch.nn.functional.tanh()\ntorch.nn.functional.sigmoid()\ntorch.nn.functional.sigmoid()\ntorch.nn.functional.dropout()\ntorch.nn.functional.dropout()\n\n## Subsystems#\n\nAutograd is supported, seeAutograd support.\nBecause gradients are currently unnamed, optimizers may work but are untested.\nNN modules are currently unsupported. This can lead to the following when calling\nmodules with named tensor inputs:\nNN module parameters are unnamed, so outputs may be partially named.\nNN module forward passes have code that don\u2019t support named tensors and will\nerror out appropriately.\nWe also do not support the following subsystems, though some may work out\nof the box:\ndistributions\nserialization (torch.load(),torch.save())\ntorch.load()\ntorch.save()\nmultiprocessing\nJIT\ndistributed\nONNX\nIf any of these would help your use case, pleasesearch if an issue has already been filedand if not,file one.\n\n## Named tensor API reference#\n\nIn this section please find the documentation for named tensor specific APIs.\nFor a comprehensive reference for how names are propagated through other PyTorch\noperators, seeNamed Tensors operator coverage.\nStores names for each of this tensor\u2019s dimensions.\nnames[idx]corresponds to the name of tensor dimensionidx.\nNames are either a string if the dimension is named orNoneif the\ndimension is unnamed.\nnames[idx]\nidx\nNone\nDimension names may contain characters or underscore. Furthermore, a dimension\nname must be a valid Python variable name (i.e., does not start with underscore).\nTensors may not have two named dimensions with the same name.\nWarning\nThe named tensor API is experimental and subject to change.\nRenames dimension names ofself.\nself\nThere are two main usages:\nself.rename(**rename_map)returns a view on tensor that has dims\nrenamed as specified in the mappingrename_map.\nself.rename(**rename_map)\nrename_map\nself.rename(*names)returns a view on tensor, renaming all\ndimensions positionally usingnames.\nUseself.rename(None)to drop names on a tensor.\nself.rename(*names)\nnames\nself.rename(None)\nOne cannot specify both positional argsnamesand keyword argsrename_map.\nnames\nrename_map\nExamples:\n\n```python\n>>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n>>> renamed_imgs = imgs.rename(N='batch', C='channels')\n>>> renamed_imgs.names\n('batch', 'channels', 'H', 'W')\n\n>>> renamed_imgs = imgs.rename(None)\n>>> renamed_imgs.names\n(None, None, None, None)\n\n>>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n>>> renamed_imgs.names\n('batch', 'channel', 'height', 'width')\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.\nIn-place version ofrename().\nrename()\nRefines the dimension names ofselfaccording tonames.\nself\nnames\nRefining is a special case of renaming that \u201clifts\u201d unnamed dimensions.\nANonedim can be refined to have any name; a named dim can only be\nrefined to have the same name.\nNone\nBecause named tensors can coexist with unnamed tensors, refining names\ngives a nice way to write named-tensor-aware code that works with both\nnamed and unnamed tensors.\nnamesmay contain up to one Ellipsis (...).\nThe Ellipsis is expanded greedily; it is expanded in-place to fillnamesto the same length asself.dim()using names from the\ncorresponding indices ofself.names.\nnames\n...\nnames\nself.dim()\nself.names\nPython 2 does not support Ellipsis but one may use a string literal\ninstead ('...').\n'...'\nnames(iterableofstr) \u2013 The desired names of the output tensor. May\ncontain up to one Ellipsis.\nExamples:\n\n```python\n>>> imgs = torch.randn(32, 3, 128, 128)\n>>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n>>> named_imgs.names\n('N', 'C', 'H', 'W')\n\n>>> tensor = torch.randn(2, 3, 5, 7, 11)\n>>> tensor = tensor.refine_names('A', ..., 'B', 'C')\n>>> tensor.names\n('A', None, None, 'B', 'C')\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.\nPermutes the dimensions of theselftensor to match the dimension order\nin theothertensor, adding size-one dims for any new names.\nself\nother\nThis operation is useful for explicit broadcasting by names (see examples).\nAll of the dims ofselfmust be named in order to use this method.\nThe resulting tensor is a view on the original tensor.\nself\nAll dimension names ofselfmust be present inother.names.othermay contain named dimensions that are not inself.names;\nthe output tensor has a size-one dimension for each of those new names.\nself\nother.names\nother\nself.names\nTo align a tensor to a specific order, usealign_to().\nalign_to()\nExamples:\n\n```python\n# Example 1: Applying a mask\n>>> mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')\n>>> imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))\n>>> imgs.masked_fill_(mask.align_as(imgs), 0)\n\n\n# Example 2: Applying a per-channel-scale\n>>> def scale_channels(input, scale):\n>>>    scale = scale.refine_names('C')\n>>>    return input * scale.align_as(input)\n\n>>> num_channels = 3\n>>> scale = torch.randn(num_channels, names=('C',))\n>>> imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))\n>>> more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))\n>>> videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))\n\n# scale_channels is agnostic to the dimension order of the input\n>>> scale_channels(imgs, scale)\n>>> scale_channels(more_imgs, scale)\n>>> scale_channels(videos, scale)\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.\nPermutes the dimensions of theselftensor to match the order\nspecified innames, adding size-one dims for any new names.\nself\nnames\nAll of the dims ofselfmust be named in order to use this method.\nThe resulting tensor is a view on the original tensor.\nself\nAll dimension names ofselfmust be present innames.namesmay contain additional names that are not inself.names;\nthe output tensor has a size-one dimension for each of those new names.\nself\nnames\nnames\nself.names\nnamesmay contain up to one Ellipsis (...).\nThe Ellipsis is expanded to be equal to all dimension names ofselfthat are not mentioned innames, in the order that they appear\ninself.\nnames\n...\nself\nnames\nself\nPython 2 does not support Ellipsis but one may use a string literal\ninstead ('...').\n'...'\nnames(iterableofstr) \u2013 The desired dimension ordering of the\noutput tensor. May contain up to one Ellipsis that is expanded\nto all unmentioned dim names ofself.\nself\nExamples:\n\n```python\n>>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n>>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n# Move the F and E dims to the front while keeping the rest in order\n>>> named_tensor.align_to('F', 'E', ...)\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.\nFlattensdimsinto a single dimension with nameout_dim.\ndims\nout_dim\nAll ofdimsmust be consecutive in order in theselftensor,\nbut not necessary contiguous in memory.\nself\nExamples:\n\n```python\n>>> imgs = torch.randn(32, 3, 128, 128, names=('N', 'C', 'H', 'W'))\n>>> flat_imgs = imgs.flatten(['C', 'H', 'W'], 'features')\n>>> flat_imgs.names, flat_imgs.shape\n(('N', 'features'), torch.Size([32, 49152]))\n\n```\n\nWarning\nThe named tensor API is experimental and subject to change.",
    "url": "https://pytorch.org/docs/stable/named_tensor.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "54a7f488adf7754186dee7d1e0f272f9",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/torch.dynamic-value.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "51e610475014018d4a7a7adfa960c1cb",
    "source": "pytorch_docs",
    "title": "PyTorch Design Philosophy \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch Design Philosophy#\n\nCreated On: Jun 10, 2022 | Last Updated On: Apr 16, 2025\nThis document is designed to help contributors and module maintainers\nunderstand the high-level design principles that have developed over\ntime in PyTorch. These are not meant to be hard-and-fast rules, but to\nserve as a guide to help trade off different concerns and to resolve\ndisagreements that may come up while developing PyTorch. For more\ninformation on contributing, module maintainership, and how to escalate a\ndisagreement to the Core Maintainers, please seePyTorch\nGovernance.\n\n## Design Principles#\n\n\n## Principle 1: Usability over Performance#\n\nThis principle may be surprising! As one Hacker News poster wrote:PyTorch is amazing! [\u2026] Although I\u2019m confused. How can a ML framework be\nnot obsessed with speed/performance?SeeHacker News discussion on\nPyTorch.\nSoumith\u2019s blog post onGrowing the PyTorch\nCommunitygoes into this in some depth, but at a high-level:\nPyTorch\u2019s primary goal is usability\nA secondary goal is to havereasonableperformance\nWe believe the ability to maintain our flexibility to support\nresearchers who are building on top of our abstractions remains\ncritical. We can\u2019t see what the future of what workloads will be, but we\nknow we want them to be built first on PyTorch and that requires\nflexibility.\nIn more concrete terms, we operate in ausability-firstmanner and try\nto avoid jumping torestriction-firstregimes (for example, static shapes,\ngraph-mode only) without a clear-eyed view of the tradeoffs. Often there\nis a temptation to impose strict user restrictions upfront because it\ncan simplify implementation, but this comes with risks:\nThe performance may not be worth the user friction, either because\nthe performance benefit is not compelling enough or it only applies to\na relatively narrow set of subproblems.\nEven if the performance benefit is compelling, the restrictions can\nfragment the ecosystem into different sets of limitations that can\nquickly become incomprehensible to users.\nWe want users to be able to seamlessly move their PyTorch code to\ndifferent hardware and software platforms, to interoperate with\ndifferent libraries and frameworks, and to experience the full richness\nof the PyTorch user experience, not a least common denominator subset.\n\n## Principle 2: Simple Over Easy#\n\nHere, we borrow fromThe Zen of\nPython:\nExplicit is better than implicit\nSimple is better than complex\nA more concise way of describing these two goals isSimple Over\nEasy. Let\u2019s start with an example becausesimpleandeasyare\noften used interchangeably in everyday English. Consider how one may\nmodeldevicesin PyTorch:\nSimple / Explicit (to understand, debug):every tensor is associated\nwith a device. The user explicitly specifies tensor device movement.\nOperations that require cross-device movement result in an error.\nEasy / Implicit (to use):the user does not have to worry about\ndevices; the system figures out the globally optimal device\nplacement.\nIn this specific case, and as a general design philosophy, PyTorch\nfavors exposing simple and explicit building blocks rather than APIs\nthat are easy-to-use by practitioners. The simple version is immediately\nunderstandable and debuggable by a new PyTorch user: you get a clear\nerror if you call an operator requiring cross-device movement at the\npoint in the program where the operator is actually invoked. The easy\nsolution may let a new user move faster initially, but debugging such a\nsystem can be complex: How did the system make its determination? What\nis the API for plugging into such a system and how are objects\nrepresented in its IR?\nSome classic arguments in favor of this sort of design come fromA\nNote on Distributed\nComputation(TLDR: Do not\nmodel resources with very different performance characteristics\nuniformly, the details will leak) and theEnd-to-End\nPrinciple(TLDR: building smarts into the lower-layers of the stack can prevent\nbuilding performant features at higher layers in the stack, and often\ndoesn\u2019t work anyway). For example, we could build operator-level or\nglobal device movement rules, but the precise choices aren\u2019t obvious and\nbuilding an extensible mechanism has unavoidable complexity and latency\ncosts.\nA caveat here is that this does not mean that higher-level \u201ceasy\u201d APIs\nare not valuable; certainly there is a value in, for example,\nhigher-levels in the stack to support efficient tensor computations\nacross heterogeneous compute in a large cluster. Instead, what we mean\nis that focusing on simple lower-level building blocks helps inform the\neasy API while still maintaining a good experience when users need to\nleave the beaten path. It also allows space for innovation and the\ngrowth of more opinionated tools at a rate we cannot support in the\nPyTorch core library, but ultimately benefit from, as evidenced by\nourrich ecosystem. In other\nwords, not automating at the start allows us to potentially reach levels\nof good automation faster.\n\n## Principle 3: Python First with Best In Class Language Interoperability#\n\nThis principle began asPython First:\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python. You can use it\nnaturally like you would useNumPy,SciPy,scikit-learn,\nor other Python libraries. You can write your new neural network\nlayers in Python itself, using your favorite libraries and use\npackages such asCythonandNumba. Our goal is to not reinvent\nthe wheel where appropriate.\nOne thing PyTorch has needed to deal with over the years is Python\noverhead: we first rewrote theautogradengine in C++, then the majority\nof operator definitions, then developed TorchScript and the C++\nfrontend.\nStill, working in Python provides easily the best experience for our\nusers: it is flexible, familiar, and perhaps most importantly, has a\nhuge ecosystem of scientific computing libraries and extensions\navailable for use. This fact motivates a few of our most recent\ncontributions, which attempt to hit a Pareto optimal point close to the\nPython usability end of the curve:\nTorchDynamo,\na Python frame evaluation tool capable of speeding up existing\neager-mode PyTorch programs with minimal user intervention.\ntorch_functionandtorch_dispatchextension points, which have enabled Python-first functionality to be\nbuilt on-top of C++ internals, such as thetorch.fx\ntracerandfunctorchrespectively.\nThese design principles are not hard-and-fast rules, but hard won\nchoices and anchor how we built PyTorch to be the debuggable, hackable\nand flexible framework it is today. As we have more contributors and\nmaintainers, we look forward to applying these core principles with you\nacross our libraries and ecosystem. We are also open to evolving them as\nwe learn new things and the AI space evolves, as we know it will.",
    "url": "https://pytorch.org/docs/stable/community/design.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a4452cb439969ca647f3741252f336e2",
    "source": "pytorch_docs",
    "title": "Multiprocessing \u2014 PyTorch 2.9 documentation",
    "text": "\n## Multiprocessing#\n\nCreated On: May 04, 2021 | Last Updated On: Feb 29, 2024\nLibrary that launches and managesncopies of worker subprocesses either specified by a function or a binary.\nn\nFor functions, it usestorch.multiprocessing(and therefore pythonmultiprocessing) to spawn/fork worker processes. For binaries it uses pythonsubprocessing.Popento create worker processes.\ntorch.multiprocessing\nmultiprocessing\nsubprocessing.Popen\nUsage 1: Launching two trainers as a function\n\n```python\nfrom torch.distributed.elastic.multiprocessing import Std, start_processes\n\n\ndef trainer(a, b, c):\n    pass  # train\n\n\n# runs two trainers\n# LOCAL_RANK=0 trainer(1,2,3)\n# LOCAL_RANK=1 trainer(4,5,6)\nctx = start_processes(\n    name=\"trainer\",\n    entrypoint=trainer,\n    args={0: (1, 2, 3), 1: (4, 5, 6)},\n    envs={0: {\"LOCAL_RANK\": 0}, 1: {\"LOCAL_RANK\": 1}},\n    log_dir=\"/tmp/foobar\",\n    redirects=Std.ALL,  # write all worker stdout/stderr to a log file\n    tee={0: Std.ERR},  # tee only local rank 0's stderr to console\n)\n\n# waits for all copies of trainer to finish\nctx.wait()\n\n```\n\nUsage 2: Launching 2 echo workers as a binary\n\n```python\n# same as invoking\n# echo hello\n# echo world > stdout.log\nctx = start_processes(\n        name=\"echo\"\n        entrypoint=\"echo\",\n        log_dir=\"/tmp/foobar\",\n        args={0: \"hello\", 1: \"world\"},\n        redirects={1: Std.OUT},\n       )\n\n```\n\nJust liketorch.multiprocessing, the return value of the functionstart_processes()is a process context (api.PContext). If a function\nwas launched, aapi.MultiprocessContextis returned and if a binary\nwas launched aapi.SubprocessContextis returned. Both are specific\nimplementations of the parentapi.PContextclass.\ntorch.multiprocessing\nstart_processes()\napi.PContext\napi.MultiprocessContext\napi.SubprocessContext\napi.PContext\n\n## Starting Multiple Workers#\n\nStartncopies ofentrypointprocesses with the provided options.\nn\nentrypoint\nentrypointis either aCallable(function) or astr(binary).\nThe number of copies is determined by the number of entries forargsandenvsarguments, which need to have the same key set.\nentrypoint\nCallable\nstr\nargs\nenvs\nargsandenvparameters are the arguments and environment variables\nto pass down to the entrypoint mapped by the replica index (local rank).\nAll local ranks must be accounted for.\nThat is, the keyset should be{0,1,...,(nprocs-1)}.\nargs\nenv\n{0,1,...,(nprocs-1)}\nNote\nWhen theentrypointis a binary (str),argscan only be strings.\nIf any other type is given, then it is casted to a string representation\n(e.g.str(arg1)). Furthermore, a binary failure will only write\nanerror.jsonerror file if the main function is annotated withtorch.distributed.elastic.multiprocessing.errors.record. For function launches,\nthis is done by default and there is no need to manually annotate\nwith the@recordannotation.\nentrypoint\nstr\nargs\nstr(arg1)\nerror.json\ntorch.distributed.elastic.multiprocessing.errors.record\n@record\nredirectsandteeare bitmasks specifying which std stream(s) to redirect\nto a log file in thelog_dir. Valid mask values are defined inStd.\nTo redirect/tee only certain local ranks, passredirectsas a map with the key as\nthe local rank to specify the redirect behavior for.\nAny missing local ranks will default toStd.NONE.\nredirects\ntee\nlog_dir\nStd\nredirects\nStd.NONE\nteeacts like the unix \u201ctee\u201d command in that it redirects + prints to console.\nTo avoid worker stdout/stderr from printing to console, use theredirectsparameter.\ntee\nredirects\nFor each process, thelog_dirwill contain:\nlog_dir\n{local_rank}/error.json: if the process failed, a file with the error info\n{local_rank}/error.json\n{local_rank}/stdout.log: ifredirect&STDOUT==STDOUT\n{local_rank}/stdout.log\nredirect&STDOUT==STDOUT\n{local_rank}/stderr.log: ifredirect&STDERR==STDERR\n{local_rank}/stderr.log\nredirect&STDERR==STDERR\nNote\nIt is expected that thelog_direxists, is empty, and is a directory.\nlog_dir\nExample:\n\n```python\nlog_dir = \"/tmp/test\"\n\n# ok; two copies of foo: foo(\"bar0\"), foo(\"bar1\")\nstart_processes(\n   name=\"trainer\",\n   entrypoint=foo,\n   args:{0:(\"bar0\",), 1:(\"bar1\",),\n   envs:{0:{}, 1:{}},\n   log_dir=log_dir\n)\n\n# invalid; envs missing for local rank 1\nstart_processes(\n   name=\"trainer\",\n   entrypoint=foo,\n   args:{0:(\"bar0\",), 1:(\"bar1\",),\n   envs:{0:{}},\n   log_dir=log_dir\n)\n\n# ok; two copies of /usr/bin/touch: touch file1, touch file2\nstart_processes(\n   name=\"trainer\",\n   entrypoint=\"/usr/bin/touch\",\n   args:{0:(\"file1\",), 1:(\"file2\",),\n   envs:{0:{}, 1:{}},\n   log_dir=log_dir\n )\n\n# caution; arguments casted to string, runs:\n# echo \"1\" \"2\" \"3\" and echo \"[1, 2, 3]\"\nstart_processes(\n   name=\"trainer\",\n   entrypoint=\"/usr/bin/echo\",\n   args:{0:(1,2,3), 1:([1,2,3],),\n   envs:{0:{}, 1:{}},\n   log_dir=log_dir\n )\n\n```\n\nname(str) \u2013 a human readable short name that describes what the processes are\n(used as header when tee\u2019ing stdout/stderr outputs)\nentrypoint(Union[Callable,str]) \u2013 either aCallable(function) orcmd(binary)\nCallable\ncmd\nargs(dict[int,tuple]) \u2013 arguments to each replica\nenvs(dict[int,dict[str,str]]) \u2013 env vars to each replica\nlog_dir\u2013 directory used to write log files\nstart_method(str) \u2013 multiprocessing start method (spawn, fork, forkserver)\nignored for binaries\nredirects\u2013 which std streams to redirect to a log file\ntee\u2013 which std streams to redirect + print to console\nlocal_ranks_filter\u2013 which ranks\u2019 logs to print to console\nPContext\n\n## Process Context#\n\nThe base class that standardizes operations over a set of processes that are launched via different mechanisms.\nThe namePContextis intentional to disambiguate withtorch.multiprocessing.ProcessContext.\nPContext\ntorch.multiprocessing.ProcessContext\nWarning\nstdouts and stderrs should ALWAYS be a superset of\ntee_stdouts and tee_stderrs (respectively) this is b/c\ntee is implemented as a redirect + tail -f <stdout/stderr.log>\nPContextholding worker processes invoked as a function.\nPContext\nPContextholding worker processes invoked as a binary.\nPContext\nResults of a completed run of processes started withstart_processes(). Returned byPContext.\nstart_processes()\nPContext\nNote the following:\nAll fields are mapped by local rank\nreturn_values- only populated for functions (not the binaries).\nreturn_values\nstdouts- path to stdout.log (empty string if no redirect)\nstdouts\nstderrs- path to stderr.log (empty string if no redirect)\nstderrs\nDefault LogsSpecs implementation:\nlog_dirwill be created if it doesn\u2019t exist\nGenerates nested folders for each attempt and rank.\nUses following scheme to build log destination paths:\n<log_dir>/<rdzv_run_id>/attempt_<attempt>/<rank>/stdout.log\n<log_dir>/<rdzv_run_id>/attempt_<attempt>/<rank>/stderr.log\n<log_dir>/<rdzv_run_id>/attempt_<attempt>/<rank>/error.json\nLogsDest\nFor each log type, holds mapping of local rank ids to file paths.\nDefines logs processing and redirection for each worker process.\nlog_dir(Optional[str]) \u2013 Base directory where logs will be written.\nredirects(Union[Std,dict[int,torch.distributed.elastic.multiprocessing.api.Std]]) \u2013 Streams to redirect to files. Pass a singleStdenum to redirect for all workers, or a mapping keyed\nby local_rank to selectively redirect.\nStd\ntee(Union[Std,dict[int,torch.distributed.elastic.multiprocessing.api.Std]]) \u2013 Streams to duplicate to stdout/stderr.\nPass a singleStdenum to duplicate streams for all workers,\nor a mapping keyed by local_rank to selectively duplicate.\nStd\nGiven the environment variables, builds destination of log files for each of the local ranks.\nEnvs parameter contains env variables dict for each of the local ranks, where entries are defined in:_start_workers().\n_start_workers()\nLogsDest",
    "url": "https://pytorch.org/docs/stable/elastic/multiprocessing.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0e19ad2e9f1303125d35c66086b2bbe3",
    "source": "pytorch_docs",
    "title": "torch.utils.dlpack \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.dlpack#\n\nCreated On: Jul 11, 2018 | Last Updated On: Jun 13, 2025\nConverts a tensor from an external library into atorch.Tensor.\ntorch.Tensor\nThe returned PyTorch tensor will share the memory with the input tensor\n(which may have come from another library). Note that in-place operations\nwill therefore also affect the data of the input tensor. This may lead to\nunexpected issues (e.g., other libraries may have read-only flags or\nimmutable data structures), so the user should only do this if they know\nfor sure that this is fine.\next_tensor(object with__dlpack__attribute, or a DLPack capsule) \u2013The tensor or DLPack capsule to convert.Ifext_tensoris a tensor (or ndarray) object, it must support\nthe__dlpack__protocol (i.e., have aext_tensor.__dlpack__method). Otherwiseext_tensormay be a DLPack capsule, which is\nan opaquePyCapsuleinstance, typically produced by ato_dlpackfunction or method.\n__dlpack__\nThe tensor or DLPack capsule to convert.\nIfext_tensoris a tensor (or ndarray) object, it must support\nthe__dlpack__protocol (i.e., have aext_tensor.__dlpack__method). Otherwiseext_tensormay be a DLPack capsule, which is\nan opaquePyCapsuleinstance, typically produced by ato_dlpackfunction or method.\next_tensor\n__dlpack__\next_tensor.__dlpack__\next_tensor\nPyCapsule\nto_dlpack\ndevice(torch.deviceorstrorNone) \u2013 An optional PyTorch device\nspecifying where to place the new tensor. If None (default), the\nnew tensor will be on the same device asext_tensor.\next_tensor\ncopy(boolorNone) \u2013 An optional boolean indicating whether or not to copyself. If None, PyTorch will copy only if necessary.\nself\nTensor\nExamples:\n\n```python\n>>> import torch.utils.dlpack\n>>> t = torch.arange(4)\n\n# Convert a tensor directly (supported in PyTorch >= 1.10)\n>>> t2 = torch.from_dlpack(t)\n>>> t2[:2] = -1  # show that memory is shared\n>>> t2\ntensor([-1, -1,  2,  3])\n>>> t\ntensor([-1, -1,  2,  3])\n\n# The old-style DLPack usage, with an intermediate capsule object\n>>> capsule = torch.utils.dlpack.to_dlpack(t)\n>>> capsule\n<capsule object \"dltensor\" at ...>\n>>> t3 = torch.from_dlpack(capsule)\n>>> t3\ntensor([-1, -1,  2,  3])\n>>> t3[0] = -9  # now we're sharing memory between 3 tensors\n>>> t3\ntensor([-9, -1,  2,  3])\n>>> t2\ntensor([-9, -1,  2,  3])\n>>> t\ntensor([-9, -1,  2,  3])\n\n```\n\nReturns an opaque object (a \u201cDLPack capsule\u201d) representing the tensor.\nNote\nto_dlpackis a legacy DLPack interface. The capsule it returns\ncannot be used for anything in Python other than use it as input tofrom_dlpack. The more idiomatic use of DLPack is to callfrom_dlpackdirectly on the tensor object - this works when that\nobject has a__dlpack__method, which PyTorch and most other\nlibraries indeed have now.\nto_dlpack\nfrom_dlpack\nfrom_dlpack\n__dlpack__\nWarning\nOnly callfrom_dlpackonce per capsule produced withto_dlpack.\nBehavior when a capsule is consumed multiple times is undefined.\nfrom_dlpack\nto_dlpack\ntensor\u2013 a tensor to be exported\nThe DLPack capsule shares the tensor\u2019s memory.",
    "url": "https://pytorch.org/docs/stable/dlpack.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9535486e09c4b7860c8249ac19b09c5b",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_nn_module.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b08e74f61d7df93be68966dce9114f86",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/mps.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b9bf34d5371d09bf8b3a62b692c1ec85",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_profiling_torch_compile.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "71c620852c92f0a1412590bfd3b5b76f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/extending.func.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "25c8a30b8ceec5378139d3562213f69c",
    "source": "pytorch_docs",
    "title": "Complex Numbers \u2014 PyTorch 2.9 documentation",
    "text": "\n## Complex Numbers#\n\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\nComplex numbers are numbers that can be expressed in the forma+bja + bja+bj, where a and b are real numbers,\nandjis called the imaginary unit, which satisfies the equationj2=\u22121j^2 = -1j2=\u22121. Complex numbers frequently occur in mathematics and\nengineering, especially in topics like signal processing. Traditionally many users and libraries (e.g., TorchAudio) have\nhandled complex numbers by representing the data in float tensors with shape(...,2)(..., 2)(...,2)where the last\ndimension contains the real and imaginary values.\nTensors of complex dtypes provide a more natural user experience while working with complex numbers. Operations on\ncomplex tensors (e.g.,torch.mv(),torch.matmul()) are likely to be faster and more memory efficient\nthan operations on float tensors mimicking them. Operations involving complex numbers in PyTorch are optimized\nto use vectorized assembly instructions and specialized kernels (e.g. LAPACK, cuBlas).\ntorch.mv()\ntorch.matmul()\nNote\nSpectral operations in thetorch.fft modulesupport\nnative complex tensors.\nWarning\nComplex tensors is a beta feature and subject to change.\n\n## Creating Complex Tensors#\n\nWe support two complex dtypes:torch.cfloatandtorch.cdouble\ntorch.cfloat\ntorch.cdouble\n\n```python\n>>> x = torch.randn(2,2, dtype=torch.cfloat)\n>>> x\ntensor([[-0.4621-0.0303j, -0.2438-0.5874j],\n     [ 0.7706+0.1421j,  1.2110+0.1918j]])\n\n```\n\nNote\nThe default dtype for complex tensors is determined by the default floating point dtype.\nIf the default floating point dtype istorch.float64then complex numbers are inferred to\nhave a dtype oftorch.complex128, otherwise they are assumed to have a dtype oftorch.complex64.\ntorch.float64\ntorch.complex128\ntorch.complex64\nAll factory functions apart fromtorch.linspace(),torch.logspace(), andtorch.arange()are\nsupported for complex tensors.\ntorch.linspace()\ntorch.logspace()\ntorch.arange()\n\n## Transition from the old representation#\n\nUsers who currently worked around the lack of complex tensors with real tensors of shape(...,2)(..., 2)(...,2)can easily to switch using the complex tensors in their code usingtorch.view_as_complex()andtorch.view_as_real(). Note that these functions don\u2019t perform any copy and return a\nview of the input tensor.\ntorch.view_as_complex()\ntorch.view_as_real()\n\n```python\n>>> x = torch.randn(3, 2)\n>>> x\ntensor([[ 0.6125, -0.1681],\n     [-0.3773,  1.3487],\n     [-0.0861, -0.7981]])\n>>> y = torch.view_as_complex(x)\n>>> y\ntensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])\n>>> torch.view_as_real(y)\ntensor([[ 0.6125, -0.1681],\n     [-0.3773,  1.3487],\n     [-0.0861, -0.7981]])\n\n```\n\n\n## Accessing real and imag#\n\nThe real and imaginary values of a complex tensor can be accessed using therealandimag.\nreal\nimag\nNote\nAccessingrealandimagattributes doesn\u2019t allocate any memory, and in-place updates on therealandimagtensors will update the original complex tensor. Also, the\nreturnedrealandimagtensors are not contiguous.\nreal\nimag\nreal\nimag\nreal\nimag\n\n```python\n>>> y.real\ntensor([ 0.6125, -0.3773, -0.0861])\n>>> y.imag\ntensor([-0.1681,  1.3487, -0.7981])\n\n>>> y.real.mul_(2)\ntensor([ 1.2250, -0.7546, -0.1722])\n>>> y\ntensor([ 1.2250-0.1681j, -0.7546+1.3487j, -0.1722-0.7981j])\n>>> y.real.stride()\n(2,)\n\n```\n\n\n## Angle and abs#\n\nThe angle and absolute values of a complex tensor can be computed usingtorch.angle()andtorch.abs().\ntorch.angle()\ntorch.abs()\n\n```python\n>>> x1=torch.tensor([3j, 4+4j])\n>>> x1.abs()\ntensor([3.0000, 5.6569])\n>>> x1.angle()\ntensor([1.5708, 0.7854])\n\n```\n\n\n## Linear Algebra#\n\nMany linear algebra operations, liketorch.matmul(),torch.linalg.svd(),torch.linalg.solve()etc., support complex numbers.\nIf you\u2019d like to request an operation we don\u2019t currently support, pleasesearchif an issue has already been filed and if not,file one.\ntorch.matmul()\ntorch.linalg.svd()\ntorch.linalg.solve()\n\n## Serialization#\n\nComplex tensors can be serialized, allowing data to be saved as complex values.\n\n```python\n>>> torch.save(y, 'complex_tensor.pt')\n>>> torch.load('complex_tensor.pt')\ntensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])\n\n```\n\n\n## Autograd#\n\nPyTorch supports autograd for complex tensors. The gradient computed is the Conjugate Wirtinger derivative,\nthe negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus,\nall the existing optimizers can be implemented to work out of the box with complex parameters. For more details,\ncheck out the noteAutograd for Complex Numbers.\n\n## Optimizers#\n\nSemantically, we define stepping through a PyTorch optimizer with complex parameters as being equivalent to stepping\nthrough the same optimizer on thetorch.view_as_real()equivalent of the complex params. More concretely:\ntorch.view_as_real()\n\n```python\n>>> params = [torch.rand(2, 3, dtype=torch.complex64) for _ in range(5)]\n>>> real_params = [torch.view_as_real(p) for p in params]\n\n>>> complex_optim = torch.optim.AdamW(params)\n>>> real_optim = torch.optim.AdamW(real_params)\n\n```\n\nreal_optimandcomplex_optimwill compute the same updates on the parameters, though there may be slight numerical\ndiscrepancies between the two optimizers, similar to numerical discrepancies between foreach vs forloop optimizers\nand capturable vs default optimizers. For more details, seenumbercial accuracy.\nreal_optim\ncomplex_optim\nSpecifically, while you can think of our optimizer\u2019s handling of complex tensors as the same as optimizing over theirp.realandp.imagpieces separately, the implementation details are not precisely that. Note that thetorch.view_as_real()equivalent will convert a complex tensor to a real tensor with shape(...,2)(..., 2)(...,2),\nwhereas splitting a complex tensor into two tensors is 2 tensors of size(...)(...)(...). This distinction has no impact on\npointwise optimizers (like AdamW) but will cause slight discrepancy in optimizers that do global reductions (like LBFGS).\nWe currently do not have optimizers that do per-Tensor reductions and thus do not yet define this behavior. Open an issue\nif you have a use case that requires precisely defining this behavior.\np.real\np.imag\ntorch.view_as_real()\nWe do not fully support the following subsystems:\nQuantization\nJIT\nSparse Tensors\nDistributed\nIf any of these would help your use case, pleasesearchif an issue has already been filed and if not,file one.",
    "url": "https://pytorch.org/docs/stable/complex_numbers.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "038da29cd5dfeec84f50a338bac4c11c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_custom_backends.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "27d09f53e44c2c7202fdd919d833be9a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/torch.jit.optimize_for_inference.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a4b71f88a8cfcfb40fdcbfdae1e4b924",
    "source": "pytorch_docs",
    "title": "User Guide \u2014 PyTorch 2.9 documentation",
    "text": "\n## User Guide#\n\nCreated On: Aug 13, 2025 | Last Updated On: Sep 02, 2025\nPyTorch provides a flexible and efficient platform for building deep\nlearning models, offering dynamic computation graphs and a rich\necosystem of tools and libraries. This guide will help you harness the power\nof PyTorch to create and deploy machine learning models effectively.\nNote\nThis guide is a work in progress.\nIntroduction\nCore Concepts\nDeveloper Notes\nAccelerator Integration",
    "url": "https://pytorch.org/docs/stable/user_guide/index.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c189e78cf3fbeaa5820eb65d1f9f4a66",
    "source": "pytorch_docs",
    "title": "torch.testing \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.testing#\n\nCreated On: May 07, 2021 | Last Updated On: Jun 10, 2025\nAsserts thatactualandexpectedare close.\nactual\nexpected\nIfactualandexpectedare strided, non-quantized, real-valued, and finite, they are considered close if\nactual\nexpected\nNon-finite values (-infandinf) are only considered close if and only if they are equal.NaN\u2019s are\nonly considered equal to each other ifequal_nanisTrue.\n-inf\ninf\nNaN\nequal_nan\nTrue\nIn addition, they are only considered close if they have the same\ndevice(ifcheck_deviceisTrue),\ndevice\ncheck_device\nTrue\ndtype(ifcheck_dtypeisTrue),\ndtype\ncheck_dtype\nTrue\nlayout(ifcheck_layoutisTrue), and\nlayout\ncheck_layout\nTrue\nstride (ifcheck_strideisTrue).\ncheck_stride\nTrue\nIf eitheractualorexpectedis a meta tensor, only the attribute checks will be performed.\nactual\nexpected\nIfactualandexpectedare sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are\nchecked individually. Indices, namelyindicesfor COO,crow_indicesandcol_indicesfor CSR and BSR,\norccol_indicesandrow_indicesfor CSC and BSC layouts, respectively,\nare always checked for equality whereas the values are checked for closeness according to the definition above.\nactual\nexpected\nindices\ncrow_indices\ncol_indices\nccol_indices\nrow_indices\nIfactualandexpectedare quantized, they are considered close if they have the sameqscheme()and the result ofdequantize()is close according to the\ndefinition above.\nactual\nexpected\nqscheme()\ndequantize()\nactualandexpectedcan beTensor\u2019s or any tensor-or-scalar-likes from whichtorch.Tensor\u2019s can be constructed withtorch.as_tensor(). Except for Python scalars the input types\nhave to be directly related. In addition,actualandexpectedcan beSequence\u2019s\norMapping\u2019s in which case they are considered close if their structure matches and all\ntheir elements are considered close according to the above definition.\nactual\nexpected\nTensor\ntorch.Tensor\ntorch.as_tensor()\nactual\nexpected\nSequence\nMapping\nNote\nPython scalars are an exception to the type relation requirement, because theirtype(), i.e.int,float, andcomplex, is equivalent to thedtypeof a tensor-like. Thus,\nPython scalars of different types can be checked, but requirecheck_dtype=False.\ntype()\nint\nfloat\ncomplex\ndtype\ncheck_dtype=False\nactual(Any) \u2013 Actual input.\nexpected(Any) \u2013 Expected input.\nallow_subclasses(bool) \u2013 IfTrue(default) and except for Python scalars, inputs of directly related types\nare allowed. Otherwise type equality is required.\nTrue\nrtol(Optional[float]) \u2013 Relative tolerance. If specifiedatolmust also be specified. If omitted, default\nvalues based on thedtypeare selected with the below table.\natol\ndtype\natol(Optional[float]) \u2013 Absolute tolerance. If specifiedrtolmust also be specified. If omitted, default\nvalues based on thedtypeare selected with the below table.\nrtol\ndtype\nequal_nan(Union[bool,str]) \u2013 IfTrue, twoNaNvalues will be considered equal.\nTrue\nNaN\ncheck_device(bool) \u2013 IfTrue(default), asserts that corresponding tensors are on the samedevice. If this check is disabled, tensors on differentdevice\u2019s are moved to the CPU before being compared.\nTrue\ndevice\ndevice\ncheck_dtype(bool) \u2013 IfTrue(default), asserts that corresponding tensors have the samedtype. If this\ncheck is disabled, tensors with differentdtype\u2019s are promoted  to a commondtype(according totorch.promote_types()) before being compared.\nTrue\ndtype\ndtype\ndtype\ntorch.promote_types()\ncheck_layout(bool) \u2013 IfTrue(default), asserts that corresponding tensors have the samelayout. If this\ncheck is disabled, tensors with differentlayout\u2019s are converted to strided tensors before being\ncompared.\nTrue\nlayout\nlayout\ncheck_stride(bool) \u2013 IfTrueand corresponding tensors are strided, asserts that they have the same stride.\nTrue\nmsg(Optional[Union[str,Callable[[str],str]]]) \u2013 Optional error message to use in case a failure occurs during\nthe comparison. Can also passed as callable in which case it will be called with the generated message and\nshould return the new message.\nValueError\u2013 If notorch.Tensorcan be constructed from an input.\ntorch.Tensor\nValueError\u2013 If onlyrtoloratolis specified.\nrtol\natol\nAssertionError\u2013 If corresponding inputs are not Python scalars and are not directly related.\nAssertionError\u2013 Ifallow_subclassesisFalse, but corresponding inputs are not Python scalars and have\n    different types.\nallow_subclasses\nFalse\nAssertionError\u2013 If the inputs areSequence\u2019s, but their length does not match.\nSequence\nAssertionError\u2013 If the inputs areMapping\u2019s, but their set of keys do not match.\nMapping\nAssertionError\u2013 If corresponding tensors do not have the sameshape.\nshape\nAssertionError\u2013 Ifcheck_layoutisTrue, but corresponding tensors do not have the samelayout.\ncheck_layout\nTrue\nlayout\nAssertionError\u2013 If only one of corresponding tensors is quantized.\nAssertionError\u2013 If corresponding tensors are quantized, but have differentqscheme()\u2019s.\nqscheme()\nAssertionError\u2013 Ifcheck_deviceisTrue, but corresponding tensors are not on the samedevice.\ncheck_device\nTrue\ndevice\nAssertionError\u2013 Ifcheck_dtypeisTrue, but corresponding tensors do not have the samedtype.\ncheck_dtype\nTrue\ndtype\nAssertionError\u2013 Ifcheck_strideisTrue, but corresponding strided tensors do not have the same stride.\ncheck_stride\nTrue\nAssertionError\u2013 If the values of corresponding tensors are not close according to the definition above.\nThe following table displays the defaultrtolandatolfor differentdtype\u2019s. In case of mismatchingdtype\u2019s, the maximum of both tolerances is used.\nrtol\natol\ndtype\ndtype\ndtype\ndtype\nrtol\nrtol\natol\natol\nfloat16\nfloat16\n1e-3\n1e-3\n1e-5\n1e-5\nbfloat16\nbfloat16\n1.6e-2\n1.6e-2\n1e-5\n1e-5\nfloat32\nfloat32\n1.3e-6\n1.3e-6\n1e-5\n1e-5\nfloat64\nfloat64\n1e-7\n1e-7\n1e-7\n1e-7\ncomplex32\ncomplex32\n1e-3\n1e-3\n1e-5\n1e-5\ncomplex64\ncomplex64\n1.3e-6\n1.3e-6\n1e-5\n1e-5\ncomplex128\ncomplex128\n1e-7\n1e-7\n1e-7\n1e-7\nquint8\nquint8\n1.3e-6\n1.3e-6\n1e-5\n1e-5\nquint2x4\nquint2x4\n1.3e-6\n1.3e-6\n1e-5\n1e-5\nquint4x2\nquint4x2\n1.3e-6\n1.3e-6\n1e-5\n1e-5\nqint8\nqint8\n1.3e-6\n1.3e-6\n1e-5\n1e-5\nqint32\nqint32\n1.3e-6\n1.3e-6\n1e-5\n1e-5\nother\n0.0\n0.0\n0.0\n0.0\nNote\nassert_close()is highly configurable with strict default settings. Users are encouraged\ntopartial()it to fit their use case. For example, if an equality check is needed, one might\ndefine anassert_equalthat uses zero tolerances for everydtypeby default:\nassert_close()\npartial()\nassert_equal\ndtype\n\n```python\n>>> import functools\n>>> assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n>>> assert_equal(1e-9, 1e-10)\nTraceback (most recent call last):\n...\nAssertionError: Scalars are not equal!\n\nExpected 1e-10 but got 1e-09.\nAbsolute difference: 9.000000000000001e-10\nRelative difference: 9.0\n\n```\n\nExamples\n\n```python\n>>> # tensor to tensor comparison\n>>> expected = torch.tensor([1e0, 1e-1, 1e-2])\n>>> actual = torch.acos(torch.cos(expected))\n>>> torch.testing.assert_close(actual, expected)\n\n```\n\n\n```python\n>>> # scalar to scalar comparison\n>>> import math\n>>> expected = math.sqrt(2.0)\n>>> actual = 2.0 / math.sqrt(2.0)\n>>> torch.testing.assert_close(actual, expected)\n\n```\n\n\n```python\n>>> # numpy array to numpy array comparison\n>>> import numpy as np\n>>> expected = np.array([1e0, 1e-1, 1e-2])\n>>> actual = np.arccos(np.cos(expected))\n>>> torch.testing.assert_close(actual, expected)\n\n```\n\n\n```python\n>>> # sequence to sequence comparison\n>>> import numpy as np\n>>> # The types of the sequences do not have to match. They only have to have the same\n>>> # length and their elements have to match.\n>>> expected = [torch.tensor([1.0]), 2.0, np.array(3.0)]\n>>> actual = tuple(expected)\n>>> torch.testing.assert_close(actual, expected)\n\n```\n\n\n```python\n>>> # mapping to mapping comparison\n>>> from collections import OrderedDict\n>>> import numpy as np\n>>> foo = torch.tensor(1.0)\n>>> bar = 2.0\n>>> baz = np.array(3.0)\n>>> # The types and a possible ordering of mappings do not have to match. They only\n>>> # have to have the same set of keys and their elements have to match.\n>>> expected = OrderedDict([(\"foo\", foo), (\"bar\", bar), (\"baz\", baz)])\n>>> actual = {\"baz\": baz, \"bar\": bar, \"foo\": foo}\n>>> torch.testing.assert_close(actual, expected)\n\n```\n\n\n```python\n>>> expected = torch.tensor([1.0, 2.0, 3.0])\n>>> actual = expected.clone()\n>>> # By default, directly related instances can be compared\n>>> torch.testing.assert_close(torch.nn.Parameter(actual), expected)\n>>> # This check can be made more strict with allow_subclasses=False\n>>> torch.testing.assert_close(\n...     torch.nn.Parameter(actual), expected, allow_subclasses=False\n... )\nTraceback (most recent call last):\n...\nTypeError: No comparison pair was able to handle inputs of type\n<class 'torch.nn.parameter.Parameter'> and <class 'torch.Tensor'>.\n>>> # If the inputs are not directly related, they are never considered close\n>>> torch.testing.assert_close(actual.numpy(), expected)\nTraceback (most recent call last):\n...\nTypeError: No comparison pair was able to handle inputs of type <class 'numpy.ndarray'>\nand <class 'torch.Tensor'>.\n>>> # Exceptions to these rules are Python scalars. They can be checked regardless of\n>>> # their type if check_dtype=False.\n>>> torch.testing.assert_close(1.0, 1, check_dtype=False)\n\n```\n\n\n```python\n>>> # NaN != NaN by default.\n>>> expected = torch.tensor(float(\"Nan\"))\n>>> actual = expected.clone()\n>>> torch.testing.assert_close(actual, expected)\nTraceback (most recent call last):\n...\nAssertionError: Scalars are not close!\n\nExpected nan but got nan.\nAbsolute difference: nan (up to 1e-05 allowed)\nRelative difference: nan (up to 1.3e-06 allowed)\n>>> torch.testing.assert_close(actual, expected, equal_nan=True)\n\n```\n\n\n```python\n>>> expected = torch.tensor([1.0, 2.0, 3.0])\n>>> actual = torch.tensor([1.0, 4.0, 5.0])\n>>> # The default error message can be overwritten.\n>>> torch.testing.assert_close(\n...     actual, expected, msg=\"Argh, the tensors are not close!\"\n... )\nTraceback (most recent call last):\n...\nAssertionError: Argh, the tensors are not close!\n>>> # If msg is a callable, it can be used to augment the generated message with\n>>> # extra information\n>>> torch.testing.assert_close(\n...     actual, expected, msg=lambda msg: f\"Header\\n\\n{msg}\\n\\nFooter\"\n... )\nTraceback (most recent call last):\n...\nAssertionError: Header\n\nTensor-likes are not close!\n\nMismatched elements: 2 / 3 (66.7%)\nGreatest absolute difference: 2.0 at index (1,) (up to 1e-05 allowed)\nGreatest relative difference: 1.0 at index (1,) (up to 1.3e-06 allowed)\n\nFooter\n\n```\n\nCreates a tensor with the givenshape,device, anddtype, and filled with\nvalues uniformly drawn from[low,high).\nshape\ndevice\ndtype\n[low,high)\nIfloworhighare specified and are outside the range of thedtype\u2019s representable\nfinite values then they are clamped to the lowest or highest representable finite value, respectively.\nIfNone, then the following table describes the default values forlowandhigh,\nwhich depend ondtype.\nlow\nhigh\ndtype\nNone\nlow\nhigh\ndtype\ndtype\ndtype\nlow\nlow\nhigh\nhigh\nboolean type\n0\n0\n2\n2\nunsigned integral type\n0\n0\n10\n10\nsigned integral types\n-9\n-9\n10\n10\nfloating types\n-9\n-9\n9\n9\ncomplex types\n-9\n-9\n9\n9\nshape(Tuple[int,...]) \u2013 Single integer or a sequence of integers defining the shape of the output tensor.\ndtype(torch.dtype) \u2013 The data type of the returned tensor.\ntorch.dtype\ndevice(Union[str,torch.device]) \u2013 The device of the returned tensor.\nlow(Optional[Number]) \u2013 Sets the lower limit (inclusive) of the given range. If a number is provided it is\nclamped to the least representable finite value of the given dtype. WhenNone(default),\nthis value is determined based on thedtype(see the table above). Default:None.\nNone\ndtype\nNone\nhigh(Optional[Number]) \u2013Sets the upper limit (exclusive) of the given range. If a number is provided it is\nclamped to the greatest representable finite value of the given dtype. WhenNone(default) this value\nis determined based on thedtype(see the table above). Default:None.Deprecated since version 2.1:Passinglow==hightomake_tensor()for floating or complex types is deprecated\nsince 2.1 and will be removed in 2.3. Usetorch.full()instead.\nSets the upper limit (exclusive) of the given range. If a number is provided it is\nclamped to the greatest representable finite value of the given dtype. WhenNone(default) this value\nis determined based on thedtype(see the table above). Default:None.\nNone\ndtype\nNone\nDeprecated since version 2.1:Passinglow==hightomake_tensor()for floating or complex types is deprecated\nsince 2.1 and will be removed in 2.3. Usetorch.full()instead.\nlow==high\nmake_tensor()\ntorch.full()\nrequires_grad(Optional[bool]) \u2013 If autograd should record operations on the returned tensor. Default:False.\nFalse\nnoncontiguous(Optional[bool]) \u2013 IfTrue, the returned tensor will be noncontiguous. This argument is\nignored if the constructed tensor has fewer than two elements. Mutually exclusive withmemory_format.\nmemory_format\nexclude_zero(Optional[bool]) \u2013 IfTruethen zeros are replaced with the dtype\u2019s small positive value\ndepending on thedtype. For bool and integer types zero is replaced with one. For floating\npoint types it is replaced with the dtype\u2019s smallest positive normal number (the \u201ctiny\u201d value of thedtype\u2019sfinfo()object), and for complex types it is replaced with a complex number\nwhose real and imaginary parts are both the smallest positive normal number representable by the complex\ntype. DefaultFalse.\nTrue\ndtype\ndtype\nfinfo()\nFalse\nmemory_format(Optional[torch.memory_format]) \u2013 The memory format of the returned tensor. Mutually exclusive\nwithnoncontiguous.\nnoncontiguous\nValueError\u2013 Ifrequires_grad=Trueis passed for integraldtype\nrequires_grad=True\nValueError\u2013 Iflow>=high.\nlow>=high\nValueError\u2013 If eitherloworhighisnan.\nlow\nhigh\nnan\nValueError\u2013 If bothnoncontiguousandmemory_formatare passed.\nnoncontiguous\nmemory_format\nTypeError\u2013 Ifdtypeisn\u2019t supported by this function.\ndtype\nTensor\nExamples\n\n```python\n>>> from torch.testing import make_tensor\n>>> # Creates a float tensor with values in [-1, 1)\n>>> make_tensor((3,), device=\"cpu\", dtype=torch.float32, low=-1, high=1)\ntensor([ 0.1205, 0.2282, -0.6380])\n>>> # Creates a bool tensor on CUDA\n>>> make_tensor((2, 2), device=\"cuda\", dtype=torch.bool)\ntensor([[False, False],\n        [False, True]], device='cuda:0')\n\n```\n\nWarning\ntorch.testing.assert_allclose()is deprecated since1.12and will be removed in a future release.\nPlease usetorch.testing.assert_close()instead. You can find detailed upgrade instructionshere.\ntorch.testing.assert_allclose()\n1.12\ntorch.testing.assert_close()",
    "url": "https://pytorch.org/docs/stable/testing.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d94451546a64c308d3423e80cf998293",
    "source": "pytorch_docs",
    "title": "NUMA Binding Utilities \u2014 PyTorch 2.9 documentation",
    "text": "\n## NUMA Binding Utilities#\n\nCreated On: Jul 25, 2025 | Last Updated On: Aug 12, 2025\nSee behavior description for each affinity mode\nin torch.distributed.run.\nIf true, we will fall back to using the original command/entrypoint if we fail to compute\nor apply NUMA bindings.\nYou should avoid using this option! It is only intended as a safety mechanism for facilitating\nmass rollouts of numa binding.\n1. Applies NUMA binding to the current thread, suitable for the thread\nwhich will be interacting with GPU gpu_index.\n2. Resets to the original CPU affinity before exiting the context manager.\nIterator[None]",
    "url": "https://pytorch.org/docs/stable/elastic/numa.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f148de5b868158bf8a24a961b6a78ef0",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/onnx.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8b5b197f2592bf79474ece4062e8e703",
    "source": "pytorch_docs",
    "title": "torch.overrides \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.overrides#\n\nCreated On: Nov 30, 2020 | Last Updated On: Jun 06, 2025\nThis module exposes various helper functions for the__torch_function__protocol. SeeExtending torch Python APIfor more details on the__torch_function__protocol.\n__torch_function__\n__torch_function__\n\n## Functions#\n\nReturn public functions that cannot be overridden by__torch_function__.\n__torch_function__\nA tuple of functions that are publicly available in the torch API but cannot\nbe overridden with__torch_function__. Mostly this is because none of the\narguments of these functions are tensors or tensor-likes.\n__torch_function__\nset[Callable]\nExamples\n\n```python\n>>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()\nTrue\n>>> torch.add in torch.overrides.get_ignored_functions()\nFalse\n\n```\n\nList functions that are overridable via __torch_function__\nA dictionary that maps namespaces that contain overridable functions\nto functions in that namespace that can be overridden.\nDict[Any, List[Callable]]\nGet a human readable string name for a function passed to\n__torch_function__\nf(Callable) \u2013 Function to resolve the name of.\nName of the function; if eval\u2019ed it should give back the input\nfunction.\nstr\nReturn a dict containing dummy overrides for all overridable functions\nA dictionary that maps overridable functions in the PyTorch API to\nlambda functions that have the same signature as the real function\nand unconditionally return -1. These lambda functions are useful\nfor testing API coverage for a type that defines__torch_function__.\n__torch_function__\nDict[Callable, Callable]\nExamples\n\n```python\n>>> import inspect\n>>> my_add = torch.overrides.get_testing_overrides()[torch.add]\n>>> inspect.signature(my_add)\n<Signature (input, other, out=None)>\n\n```\n\nImplement a function with checks for__torch_function__overrides.\n__torch_function__\nSee torch::autograd::handle_torch_function for the equivalent of this\nfunction in the C++ implementation.\npublic_api(function) \u2013 Function exposed by the public torch API originally called likepublic_api(*args,**kwargs)on which arguments are now being\nchecked.\npublic_api(*args,**kwargs)\nrelevant_args(iterable) \u2013 Iterable of arguments to check for __torch_function__ methods.\nargs(tuple) \u2013 Arbitrary positional arguments originally passed intopublic_api.\npublic_api\nkwargs(tuple) \u2013 Arbitrary keyword arguments originally passed intopublic_api.\npublic_api\nResult from callingimplementationor an__torch_function__method, as appropriate.\nimplementation\n__torch_function__\nobject\n:raises TypeError : if no implementation is found.:\nExample\n\n```python\n>>> def func(a):\n...     if has_torch_function_unary(a):\n...         return handle_torch_function(func, (a,), a)\n...     return a + 0\n\n```\n\nCheck for __torch_function__ implementations in the elements of an iterable\nor if a __torch_function__ mode is enabled.  Considers exactTensors\nandParameters non-dispatchable.  Use this to guard a call tohandle_torch_function(); don\u2019t use it to test if something\nis Tensor-like, useis_tensor_like()instead.\n:param relevant_args: Iterable or arguments to check for __torch_function__ methods.\n:type relevant_args: iterable\nTensor\nParameter\nhandle_torch_function()\nis_tensor_like()\nTrue if any of the elements of relevant_args have __torch_function__\nimplementations, False otherwise.\nbool\nSee also\ntorch.is_tensor_like\nChecks if something is a Tensor-like, including an exactTensor.\nTensor\nReturnsTrueif the passed-in input is a Tensor-like.\nTrue\nCurrently, this occurs whenever there\u2019s a__torch_function__attribute on the type of the input.\n__torch_function__\nExamples\nA subclass of tensor is generally a Tensor-like.\n\n```python\n>>> class SubTensor(torch.Tensor): ...\n>>> is_tensor_like(SubTensor([0]))\nTrue\n\n```\n\nBuilt-in or user types aren\u2019t usually Tensor-like.\n\n```python\n>>> is_tensor_like(6)\nFalse\n>>> is_tensor_like(None)\nFalse\n>>> class NotATensor: ...\n>>> is_tensor_like(NotATensor())\nFalse\n\n```\n\nBut, they can be made Tensor-like by implementing __torch_function__.\n\n```python\n>>> class TensorLike:\n...     @classmethod\n...     def __torch_function__(cls, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue\n\n```\n\nReturns True if the function passed in is a handler for a\nmethod or property belonging totorch.Tensor, as passed\ninto__torch_function__.\ntorch.Tensor\n__torch_function__\nNote\nFor properties, their__get__method must be passed in.\n__get__\nThis may be needed, in particular, for the following reasons:\nMethods/properties sometimes don\u2019t contain a__module__slot.\nThey require that the first passed-in argument is an instance\noftorch.Tensor.\ntorch.Tensor\nExamples\n\n```python\n>>> is_tensor_method_or_property(torch.Tensor.add)\nTrue\n>>> is_tensor_method_or_property(torch.add)\nFalse\n\n```\n\nbool\nWraps a given function with__torch_function__-related functionality.\n__torch_function__\ndispatcher(Callable) \u2013 A callable that returns an iterable of Tensor-likes passed into the function.\nNote\nThis decorator may reduce the performance of your code. Generally, it\u2019s enough to express\nyour code as a series of functions that, themselves, support __torch_function__. If you\nfind yourself in the rare situation where this is not the case, e.g. if you\u2019re wrapping a\nlow-level library and you also need it to work for Tensor-likes, then this function is available.\nExamples\n\n```python\n>>> def dispatcher(a):  # Must have the same signature as func\n...     return (a,)\n>>> @torch.overrides.wrap_torch_function(dispatcher)\n>>> def func(a):  # This will make func dispatchable by __torch_function__\n...     return a + 0\n\n```\n",
    "url": "https://pytorch.org/docs/stable/torch.overrides.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "64f77bc37a54c17820499e555f0dc9fc",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_aot_inductor_debugging_guide.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9f1e22e39f6c6d4b8d26d19c239a973a",
    "source": "pytorch_docs",
    "title": "torch \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jul 22, 2025\nThe torch package contains data structures for multi-dimensional\ntensors and defines mathematical operations over these tensors.\nAdditionally, it provides many utilities for efficient serialization of\nTensors and arbitrary types, and other useful utilities.\nIt has a CUDA counterpart, that enables you to run your tensor computations\non an NVIDIA GPU with compute capability >= 3.0.\n\n## Tensors#\n\nis_tensor\n\nis_tensor\nReturns True ifobjis a PyTorch tensor.\nis_storage\n\nis_storage\nReturns True ifobjis a PyTorch storage object.\nis_complex\n\nis_complex\nReturns True if the data type ofinputis a complex data type i.e., one oftorch.complex64, andtorch.complex128.\ninput\ntorch.complex64\ntorch.complex128\nis_conj\n\nis_conj\nReturns True if theinputis a conjugated tensor, i.e. its conjugate bit is set toTrue.\ninput\nis_floating_point\n\nis_floating_point\nReturns True if the data type ofinputis a floating point data type i.e., one oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16.\ninput\ntorch.float64\ntorch.float32\ntorch.float16\ntorch.bfloat16\nis_nonzero\n\nis_nonzero\nReturns True if theinputis a single element tensor which is not equal to zero after type conversions.\ninput\nset_default_dtype\n\nset_default_dtype\nSets the default floating point dtype tod.\nd\nget_default_dtype\n\nget_default_dtype\nGet the current default floating pointtorch.dtype.\ntorch.dtype\nset_default_device\n\nset_default_device\nSets the defaulttorch.Tensorto be allocated ondevice.\ntorch.Tensor\ndevice\nget_default_device\n\nget_default_device\nGets the defaulttorch.Tensorto be allocated ondevice\ntorch.Tensor\ndevice\nset_default_tensor_type\n\nset_default_tensor_type\n\nnumel\n\nnumel\nReturns the total number of elements in theinputtensor.\ninput\nset_printoptions\n\nset_printoptions\nSet options for printing.\nset_flush_denormal\n\nset_flush_denormal\nDisables denormal floating numbers on CPU.\n\n## Creation Ops#\n\nNote\nRandom sampling creation ops are listed underRandom samplingand\ninclude:torch.rand()torch.rand_like()torch.randn()torch.randn_like()torch.randint()torch.randint_like()torch.randperm()You may also usetorch.empty()with theIn-place random samplingmethods to createtorch.Tensors with values sampled from a broader\nrange of distributions.\ntorch.rand()\ntorch.rand_like()\ntorch.randn()\ntorch.randn_like()\ntorch.randint()\ntorch.randint_like()\ntorch.randperm()\ntorch.empty()\ntorch.Tensor\ntensor\n\ntensor\nConstructs a tensor with no autograd history (also known as a \"leaf tensor\", seeAutograd mechanics) by copyingdata.\ndata\nsparse_coo_tensor\n\nsparse_coo_tensor\nConstructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.\nindices\nsparse_csr_tensor\n\nsparse_csr_tensor\nConstructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices.\ncrow_indices\ncol_indices\nsparse_csc_tensor\n\nsparse_csc_tensor\nConstructs asparse tensor in CSC (Compressed Sparse Column)with specified values at the givenccol_indicesandrow_indices.\nccol_indices\nrow_indices\nsparse_bsr_tensor\n\nsparse_bsr_tensor\nConstructs asparse tensor in BSR (Block Compressed Sparse Row))with specified 2-dimensional blocks at the givencrow_indicesandcol_indices.\ncrow_indices\ncol_indices\nsparse_bsc_tensor\n\nsparse_bsc_tensor\nConstructs asparse tensor in BSC (Block Compressed Sparse Column))with specified 2-dimensional blocks at the givenccol_indicesandrow_indices.\nccol_indices\nrow_indices\nasarray\n\nasarray\nConvertsobjto a tensor.\nobj\nas_tensor\n\nas_tensor\nConvertsdatainto a tensor, sharing data and preserving autograd history if possible.\ndata\nas_strided\n\nas_strided\nCreate a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.\ninput\nsize\nstride\nstorage_offset\nfrom_file\n\nfrom_file\nCreates a CPU tensor with a storage backed by a memory-mapped file.\nfrom_numpy\n\nfrom_numpy\nCreates aTensorfrom anumpy.ndarray.\nTensor\nnumpy.ndarray\nfrom_dlpack\n\nfrom_dlpack\nConverts a tensor from an external library into atorch.Tensor.\ntorch.Tensor\nfrombuffer\n\nfrombuffer\nCreates a 1-dimensionalTensorfrom an object that implements the Python buffer protocol.\nTensor\nzeros\n\nzeros\nReturns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.\nsize\nzeros_like\n\nzeros_like\nReturns a tensor filled with the scalar value0, with the same size asinput.\ninput\nones\n\nones\nReturns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.\nsize\nones_like\n\nones_like\nReturns a tensor filled with the scalar value1, with the same size asinput.\ninput\narange\n\narange\nReturns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.\n[start,end)\nstep\nrange\n\nrange\nReturns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.\nstart\nend\nstep\nlinspace\n\nlinspace\nCreates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.\nsteps\nstart\nend\nlogspace\n\nlogspace\nCreates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.\nsteps\nbase\neye\n\neye\nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.\nempty\n\nempty\nReturns a tensor filled with uninitialized data.\nempty_like\n\nempty_like\nReturns an uninitialized tensor with the same size asinput.\ninput\nempty_strided\n\nempty_strided\nCreates a tensor with the specifiedsizeandstrideand filled with undefined data.\nsize\nstride\nfull\n\nfull\nCreates a tensor of sizesizefilled withfill_value.\nsize\nfill_value\nfull_like\n\nfull_like\nReturns a tensor with the same size asinputfilled withfill_value.\ninput\nfill_value\nquantize_per_tensor\n\nquantize_per_tensor\nConverts a float tensor to a quantized tensor with given scale and zero point.\nquantize_per_channel\n\nquantize_per_channel\nConverts a float tensor to a per-channel quantized tensor with given scales and zero points.\ndequantize\n\ndequantize\nReturns an fp32 Tensor by dequantizing a quantized Tensor\ncomplex\n\ncomplex\nConstructs a complex tensor with its real part equal torealand its imaginary part equal toimag.\nreal\nimag\npolar\n\npolar\nConstructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute valueabsand angleangle.\nabs\nangle\nheaviside\n\nheaviside\nComputes the Heaviside step function for each element ininput.\ninput\n\n## Indexing, Slicing, Joining, Mutating Ops#\n\nadjoint\n\nadjoint\nReturns a view of the tensor conjugated and with the last two dimensions transposed.\nargwhere\n\nargwhere\nReturns a tensor containing the indices of all non-zero elements ofinput.\ninput\ncat\n\ncat\nConcatenates the given sequence of tensors intensorsin the given dimension.\ntensors\nconcat\n\nconcat\nAlias oftorch.cat().\ntorch.cat()\nconcatenate\n\nconcatenate\nAlias oftorch.cat().\ntorch.cat()\nconj\n\nconj\nReturns a view ofinputwith a flipped conjugate bit.\ninput\nchunk\n\nchunk\nAttempts to split a tensor into the specified number of chunks.\ndsplit\n\ndsplit\nSplitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.\ninput\nindices_or_sections\ncolumn_stack\n\ncolumn_stack\nCreates a new tensor by horizontally stacking the tensors intensors.\ntensors\ndstack\n\ndstack\nStack tensors in sequence depthwise (along third axis).\ngather\n\ngather\nGathers values along an axis specified bydim.\nhsplit\n\nhsplit\nSplitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.\ninput\nindices_or_sections\nhstack\n\nhstack\nStack tensors in sequence horizontally (column wise).\nindex_add\n\nindex_add\nSeeindex_add_()for function description.\nindex_add_()\nindex_copy\n\nindex_copy\nSeeindex_add_()for function description.\nindex_add_()\nindex_reduce\n\nindex_reduce\nSeeindex_reduce_()for function description.\nindex_reduce_()\nindex_select\n\nindex_select\nReturns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.\ninput\ndim\nindex\nmasked_select\n\nmasked_select\nReturns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.\ninput\nmask\nmovedim\n\nmovedim\nMoves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.\ninput\nsource\ndestination\nmoveaxis\n\nmoveaxis\nAlias fortorch.movedim().\ntorch.movedim()\nnarrow\n\nnarrow\nReturns a new tensor that is a narrowed version ofinputtensor.\ninput\nnarrow_copy\n\nnarrow_copy\nSame asTensor.narrow()except this returns a copy rather than shared storage.\nTensor.narrow()\nnonzero\n\nnonzero\n\npermute\n\npermute\nReturns a view of the original tensorinputwith its dimensions permuted.\ninput\nreshape\n\nreshape\nReturns a tensor with the same data and number of elements asinput, but with the specified shape.\ninput\nrow_stack\n\nrow_stack\nAlias oftorch.vstack().\ntorch.vstack()\nselect\n\nselect\nSlices theinputtensor along the selected dimension at the given index.\ninput\nscatter\n\nscatter\nOut-of-place version oftorch.Tensor.scatter_()\ntorch.Tensor.scatter_()\ndiagonal_scatter\n\ndiagonal_scatter\nEmbeds the values of thesrctensor intoinputalong the diagonal elements ofinput, with respect todim1anddim2.\nsrc\ninput\ninput\ndim1\ndim2\nselect_scatter\n\nselect_scatter\nEmbeds the values of thesrctensor intoinputat the given index.\nsrc\ninput\nslice_scatter\n\nslice_scatter\nEmbeds the values of thesrctensor intoinputat the given dimension.\nsrc\ninput\nscatter_add\n\nscatter_add\nOut-of-place version oftorch.Tensor.scatter_add_()\ntorch.Tensor.scatter_add_()\nscatter_reduce\n\nscatter_reduce\nOut-of-place version oftorch.Tensor.scatter_reduce_()\ntorch.Tensor.scatter_reduce_()\nsegment_reduce\n\nsegment_reduce\nPerform a segment reduction operation on the input tensor along the specified axis.\nsplit\n\nsplit\nSplits the tensor into chunks.\nsqueeze\n\nsqueeze\nReturns a tensor with all specified dimensions ofinputof size1removed.\ninput\nstack\n\nstack\nConcatenates a sequence of tensors along a new dimension.\nswapaxes\n\nswapaxes\nAlias fortorch.transpose().\ntorch.transpose()\nswapdims\n\nswapdims\nAlias fortorch.transpose().\ntorch.transpose()\nt\n\nt\nExpectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.\ninput\ntake\n\ntake\nReturns a new tensor with the elements ofinputat the given indices.\ninput\ntake_along_dim\n\ntake_along_dim\nSelects values frominputat the 1-dimensional indices fromindicesalong the givendim.\ninput\nindices\ndim\ntensor_split\n\ntensor_split\nSplits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.\ninput\ndim\nindices_or_sections\ntile\n\ntile\nConstructs a tensor by repeating the elements ofinput.\ninput\ntranspose\n\ntranspose\nReturns a tensor that is a transposed version ofinput.\ninput\nunbind\n\nunbind\nRemoves a tensor dimension.\nunravel_index\n\nunravel_index\nConverts a tensor of flat indices into a tuple of coordinate tensors that index into an arbitrary tensor of the specified shape.\nunsqueeze\n\nunsqueeze\nReturns a new tensor with a dimension of size one inserted at the specified position.\nvsplit\n\nvsplit\nSplitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.\ninput\nindices_or_sections\nvstack\n\nvstack\nStack tensors in sequence vertically (row wise).\nwhere\n\nwhere\nReturn a tensor of elements selected from eitherinputorother, depending oncondition.\ninput\nother\ncondition\n\n## Accelerators#\n\nWithin the PyTorch repo, we define an \u201cAccelerator\u201d as atorch.devicethat is being used\nalongside a CPU to speed up computation. These device use an asynchronous execution scheme,\nusingtorch.Streamandtorch.Eventas their main way to perform synchronization.\nWe also assume that only one such accelerator can be available at once on a given host. This allows\nus to use the current accelerator as the default device for relevant concepts such as pinned memory,\nStream device_type, FSDP, etc.\ntorch.device\ntorch.Stream\ntorch.Event\nAs of today, accelerator devices are (in no particular order)\u201cCUDA\u201d,\u201cMTIA\u201d,\u201cXPU\u201d,\u201cMPS\u201d, \u201cHPU\u201d, and PrivateUse1 (many device not in the PyTorch repo itself).\nMany tools in the PyTorch Ecosystem use fork to create subprocesses (for example dataloading\nor intra-op parallelism), it is thus important to delay as much as possible any\noperation that would prevent further forks. This is especially important here as most accelerator\u2019s initialization has such effect.\nIn practice, you should keep in mind that checkingtorch.accelerator.current_accelerator()is a compile-time check by default, it is thus always fork-safe.\nOn the contrary, passing thecheck_available=Trueflag to this function or callingtorch.accelerator.is_available()will usually prevent later fork.\ntorch.accelerator.current_accelerator()\ncheck_available=True\ntorch.accelerator.is_available()\nSome backends provide an experimental opt-in option to make the runtime availability\ncheck fork-safe. When using the CUDA devicePYTORCH_NVML_BASED_CUDA_CHECK=1can be\nused for example.\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nStream\n\nStream\nAn in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.\nEvent\n\nEvent\nQuery and record Stream status to identify or control dependencies across Stream and measure timing.\n\n## Generators#\n\nGenerator\n\nGenerator\nCreates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.\n\n## Random sampling#\n\nseed\n\nseed\nSets the seed for generating random numbers to a non-deterministic random number on all devices.\nmanual_seed\n\nmanual_seed\nSets the seed for generating random numbers on all devices.\ninitial_seed\n\ninitial_seed\nReturns the initial seed for generating random numbers as a Pythonlong.\nget_rng_state\n\nget_rng_state\nReturns the random number generator state as atorch.ByteTensor.\nset_rng_state\n\nset_rng_state\nSets the random number generator state.\nbernoulli\n\nbernoulli\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\nmultinomial\n\nmultinomial\nReturns a tensor where each row containsnum_samplesindices sampled from the multinomial (a stricter definition would be multivariate, refer totorch.distributions.multinomial.Multinomialfor more details) probability distribution located in the corresponding row of tensorinput.\nnum_samples\ntorch.distributions.multinomial.Multinomial\ninput\nnormal\n\nnormal\nReturns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.\npoisson\n\npoisson\nReturns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,\ninput\ninput\nrand\n\nrand\nReturns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)\nrand_like\n\nrand_like\nReturns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).\ninput\nrandint\n\nrandint\nReturns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).\nlow\nhigh\nrandint_like\n\nrandint_like\nReturns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).\ninput\nlow\nhigh\nrandn\n\nrandn\nReturns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).\nrandn_like\n\nrandn_like\nReturns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.\ninput\nrandperm\n\nrandperm\nReturns a random permutation of integers from0ton-1.\n0\nn-1\n\n## In-place random sampling#\n\nThere are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:\ntorch.Tensor.bernoulli_()- in-place version oftorch.bernoulli()\ntorch.Tensor.bernoulli_()\ntorch.bernoulli()\ntorch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution\ntorch.Tensor.cauchy_()\ntorch.Tensor.exponential_()- numbers drawn from the exponential distribution\ntorch.Tensor.exponential_()\ntorch.Tensor.geometric_()- elements drawn from the geometric distribution\ntorch.Tensor.geometric_()\ntorch.Tensor.log_normal_()- samples from the log-normal distribution\ntorch.Tensor.log_normal_()\ntorch.Tensor.normal_()- in-place version oftorch.normal()\ntorch.Tensor.normal_()\ntorch.normal()\ntorch.Tensor.random_()- numbers sampled from the discrete uniform distribution\ntorch.Tensor.random_()\ntorch.Tensor.uniform_()- numbers sampled from the continuous uniform distribution\ntorch.Tensor.uniform_()\n\n## Quasi-random sampling#\n\nquasirandom.SobolEngine\nquasirandom.SobolEngine\nThetorch.quasirandom.SobolEngineis an engine for generating (scrambled) Sobol sequences.\ntorch.quasirandom.SobolEngine\n\n## Serialization#\n\nsave\n\nsave\nSaves an object to a disk file.\nload\n\nload\nLoads an object saved withtorch.save()from a file.\ntorch.save()\n\n## Parallelism#\n\nget_num_threads\n\nget_num_threads\nReturns the number of threads used for parallelizing CPU operations\nset_num_threads\n\nset_num_threads\nSets the number of threads used for intraop parallelism on CPU.\nget_num_interop_threads\n\nget_num_interop_threads\nReturns the number of threads used for inter-op parallelism on CPU (e.g.\nset_num_interop_threads\n\nset_num_interop_threads\nSets the number of threads used for interop parallelism (e.g.\n\n## Locally disabling gradient computation#\n\nThe context managerstorch.no_grad(),torch.enable_grad(), andtorch.set_grad_enabled()are helpful for locally disabling and enabling\ngradient computation. SeeLocally disabling gradient computationfor more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using thethreadingmodule, etc.\ntorch.no_grad()\ntorch.enable_grad()\ntorch.set_grad_enabled()\nthreading\nExamples:\n\n```python\n>>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n\n```\n\nno_grad\n\nno_grad\nContext-manager that disables gradient calculation.\nenable_grad\n\nenable_grad\nContext-manager that enables gradient calculation.\nautograd.grad_mode.set_grad_enabled\nautograd.grad_mode.set_grad_enabled\nContext-manager that sets gradient calculation on or off.\nis_grad_enabled\n\nis_grad_enabled\nReturns True if grad mode is currently enabled.\nautograd.grad_mode.inference_mode\nautograd.grad_mode.inference_mode\nContext manager that enables or disables inference mode.\nis_inference_mode_enabled\n\nis_inference_mode_enabled\nReturns True if inference mode is currently enabled.\n\n## Math operations#\n\n\n## Constants#\n\ninf\ninf\nA floating-point positive infinity. Alias formath.inf.\nmath.inf\nnan\nnan\nA floating-point \u201cnot a number\u201d value. This value is not a legal number. Alias formath.nan.\nmath.nan\n\n## Pointwise Ops#\n\nabs\n\nabs\nComputes the absolute value of each element ininput.\ninput\nabsolute\n\nabsolute\nAlias fortorch.abs()\ntorch.abs()\nacos\n\nacos\nComputes the inverse cosine of each element ininput.\ninput\narccos\n\narccos\nAlias fortorch.acos().\ntorch.acos()\nacosh\n\nacosh\nReturns a new tensor with the inverse hyperbolic cosine of the elements ofinput.\ninput\narccosh\n\narccosh\nAlias fortorch.acosh().\ntorch.acosh()\nadd\n\nadd\nAddsother, scaled byalpha, toinput.\nother\nalpha\ninput\naddcdiv\n\naddcdiv\nPerforms the element-wise division oftensor1bytensor2, multiplies the result by the scalarvalueand adds it toinput.\ntensor1\ntensor2\nvalue\ninput\naddcmul\n\naddcmul\nPerforms the element-wise multiplication oftensor1bytensor2, multiplies the result by the scalarvalueand adds it toinput.\ntensor1\ntensor2\nvalue\ninput\nangle\n\nangle\nComputes the element-wise angle (in radians) of the giveninputtensor.\ninput\nasin\n\nasin\nReturns a new tensor with the arcsine of the elements ofinput.\ninput\narcsin\n\narcsin\nAlias fortorch.asin().\ntorch.asin()\nasinh\n\nasinh\nReturns a new tensor with the inverse hyperbolic sine of the elements ofinput.\ninput\narcsinh\n\narcsinh\nAlias fortorch.asinh().\ntorch.asinh()\natan\n\natan\nReturns a new tensor with the arctangent of the elements ofinput.\ninput\narctan\n\narctan\nAlias fortorch.atan().\ntorch.atan()\natanh\n\natanh\nReturns a new tensor with the inverse hyperbolic tangent of the elements ofinput.\ninput\narctanh\n\narctanh\nAlias fortorch.atanh().\ntorch.atanh()\natan2\n\natan2\nElement-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.\narctan2\n\narctan2\nAlias fortorch.atan2().\ntorch.atan2()\nbitwise_not\n\nbitwise_not\nComputes the bitwise NOT of the given input tensor.\nbitwise_and\n\nbitwise_and\nComputes the bitwise AND ofinputandother.\ninput\nother\nbitwise_or\n\nbitwise_or\nComputes the bitwise OR ofinputandother.\ninput\nother\nbitwise_xor\n\nbitwise_xor\nComputes the bitwise XOR ofinputandother.\ninput\nother\nbitwise_left_shift\n\nbitwise_left_shift\nComputes the left arithmetic shift ofinputbyotherbits.\ninput\nother\nbitwise_right_shift\n\nbitwise_right_shift\nComputes the right arithmetic shift ofinputbyotherbits.\ninput\nother\nceil\n\nceil\nReturns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.\ninput\nclamp\n\nclamp\nClamps all elements ininputinto the range[min,max].\ninput\nmin\nmax\nclip\n\nclip\nAlias fortorch.clamp().\ntorch.clamp()\nconj_physical\n\nconj_physical\nComputes the element-wise conjugate of the giveninputtensor.\ninput\ncopysign\n\ncopysign\nCreate a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.\ninput\nother\ncos\n\ncos\nReturns a new tensor with the cosine  of the elements ofinput.\ninput\ncosh\n\ncosh\nReturns a new tensor with the hyperbolic cosine  of the elements ofinput.\ninput\ndeg2rad\n\ndeg2rad\nReturns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.\ninput\ndiv\n\ndiv\nDivides each element of the inputinputby the corresponding element ofother.\ninput\nother\ndivide\n\ndivide\nAlias fortorch.div().\ntorch.div()\ndigamma\n\ndigamma\nAlias fortorch.special.digamma().\ntorch.special.digamma()\nerf\n\nerf\nAlias fortorch.special.erf().\ntorch.special.erf()\nerfc\n\nerfc\nAlias fortorch.special.erfc().\ntorch.special.erfc()\nerfinv\n\nerfinv\nAlias fortorch.special.erfinv().\ntorch.special.erfinv()\nexp\n\nexp\nReturns a new tensor with the exponential of the elements of the input tensorinput.\ninput\nexp2\n\nexp2\nAlias fortorch.special.exp2().\ntorch.special.exp2()\nexpm1\n\nexpm1\nAlias fortorch.special.expm1().\ntorch.special.expm1()\nfake_quantize_per_channel_affine\n\nfake_quantize_per_channel_affine\nReturns a new tensor with the data ininputfake quantized per channel usingscale,zero_point,quant_minandquant_max, across the channel specified byaxis.\ninput\nscale\nzero_point\nquant_min\nquant_max\naxis\nfake_quantize_per_tensor_affine\n\nfake_quantize_per_tensor_affine\nReturns a new tensor with the data ininputfake quantized usingscale,zero_point,quant_minandquant_max.\ninput\nscale\nzero_point\nquant_min\nquant_max\nfix\n\nfix\nAlias fortorch.trunc()\ntorch.trunc()\nfloat_power\n\nfloat_power\nRaisesinputto the power ofexponent, elementwise, in double precision.\ninput\nexponent\nfloor\n\nfloor\nReturns a new tensor with the floor of the elements ofinput, the largest integer less than or equal to each element.\ninput\nfloor_divide\n\nfloor_divide\n\nfmod\n\nfmod\nApplies C++'sstd::fmodentrywise.\nfrac\n\nfrac\nComputes the fractional portion of each element ininput.\ninput\nfrexp\n\nfrexp\nDecomposesinputinto mantissa and exponent tensors such thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent.\ninput\ngradient\n\ngradient\nEstimates the gradient of a functiong:Rn\u2192Rg : \\mathbb{R}^n \\rightarrow \\mathbb{R}g:Rn\u2192Rin one or more dimensions using thesecond-order accurate central differences methodand either first or second order estimates at the boundaries.\nimag\n\nimag\nReturns a new tensor containing imaginary values of theselftensor.\nself\nldexp\n\nldexp\nMultipliesinputby 2 **other.\ninput\nother\nlerp\n\nlerp\nDoes a linear interpolation of two tensorsstart(given byinput) andendbased on a scalar or tensorweightand returns the resultingouttensor.\nstart\ninput\nend\nweight\nout\nlgamma\n\nlgamma\nComputes the natural logarithm of the absolute value of the gamma function oninput.\ninput\nlog\n\nlog\nReturns a new tensor with the natural logarithm of the elements ofinput.\ninput\nlog10\n\nlog10\nReturns a new tensor with the logarithm to the base 10 of the elements ofinput.\ninput\nlog1p\n\nlog1p\nReturns a new tensor with the natural logarithm of (1 +input).\ninput\nlog2\n\nlog2\nReturns a new tensor with the logarithm to the base 2 of the elements ofinput.\ninput\nlogaddexp\n\nlogaddexp\nLogarithm of the sum of exponentiations of the inputs.\nlogaddexp2\n\nlogaddexp2\nLogarithm of the sum of exponentiations of the inputs in base-2.\nlogical_and\n\nlogical_and\nComputes the element-wise logical AND of the given input tensors.\nlogical_not\n\nlogical_not\nComputes the element-wise logical NOT of the given input tensor.\nlogical_or\n\nlogical_or\nComputes the element-wise logical OR of the given input tensors.\nlogical_xor\n\nlogical_xor\nComputes the element-wise logical XOR of the given input tensors.\nlogit\n\nlogit\nAlias fortorch.special.logit().\ntorch.special.logit()\nhypot\n\nhypot\nGiven the legs of a right triangle, return its hypotenuse.\ni0\n\ni0\nAlias fortorch.special.i0().\ntorch.special.i0()\nigamma\n\nigamma\nAlias fortorch.special.gammainc().\ntorch.special.gammainc()\nigammac\n\nigammac\nAlias fortorch.special.gammaincc().\ntorch.special.gammaincc()\nmul\n\nmul\nMultipliesinputbyother.\ninput\nother\nmultiply\n\nmultiply\nAlias fortorch.mul().\ntorch.mul()\nmvlgamma\n\nmvlgamma\nAlias fortorch.special.multigammaln().\ntorch.special.multigammaln()\nnan_to_num\n\nnan_to_num\nReplacesNaN, positive infinity, and negative infinity values ininputwith the values specified bynan,posinf, andneginf, respectively.\nNaN\ninput\nnan\nposinf\nneginf\nneg\n\nneg\nReturns a new tensor with the negative of the elements ofinput.\ninput\nnegative\n\nnegative\nAlias fortorch.neg()\ntorch.neg()\nnextafter\n\nnextafter\nReturn the next floating-point value afterinputtowardsother, elementwise.\ninput\nother\npolygamma\n\npolygamma\nAlias fortorch.special.polygamma().\ntorch.special.polygamma()\npositive\n\npositive\nReturnsinput.\ninput\npow\n\npow\nTakes the power of each element ininputwithexponentand returns a tensor with the result.\ninput\nexponent\nquantized_batch_norm\n\nquantized_batch_norm\nApplies batch normalization on a 4D (NCHW) quantized tensor.\nquantized_max_pool1d\n\nquantized_max_pool1d\nApplies a 1D max pooling over an input quantized tensor composed of several input planes.\nquantized_max_pool2d\n\nquantized_max_pool2d\nApplies a 2D max pooling over an input quantized tensor composed of several input planes.\nrad2deg\n\nrad2deg\nReturns a new tensor with each of the elements ofinputconverted from angles in radians to degrees.\ninput\nreal\n\nreal\nReturns a new tensor containing real values of theselftensor.\nself\nreciprocal\n\nreciprocal\nReturns a new tensor with the reciprocal of the elements ofinput\ninput\nremainder\n\nremainder\nComputesPython's modulus operationentrywise.\nround\n\nround\nRounds elements ofinputto the nearest integer.\ninput\nrsqrt\n\nrsqrt\nReturns a new tensor with the reciprocal of the square-root of each of the elements ofinput.\ninput\nsigmoid\n\nsigmoid\nAlias fortorch.special.expit().\ntorch.special.expit()\nsign\n\nsign\nReturns a new tensor with the signs of the elements ofinput.\ninput\nsgn\n\nsgn\nThis function is an extension of torch.sign() to complex tensors.\nsignbit\n\nsignbit\nTests if each element ofinputhas its sign bit set or not.\ninput\nsin\n\nsin\nReturns a new tensor with the sine of the elements ofinput.\ninput\nsinc\n\nsinc\nAlias fortorch.special.sinc().\ntorch.special.sinc()\nsinh\n\nsinh\nReturns a new tensor with the hyperbolic sine of the elements ofinput.\ninput\nsoftmax\n\nsoftmax\nAlias fortorch.nn.functional.softmax().\ntorch.nn.functional.softmax()\nsqrt\n\nsqrt\nReturns a new tensor with the square-root of the elements ofinput.\ninput\nsquare\n\nsquare\nReturns a new tensor with the square of the elements ofinput.\ninput\nsub\n\nsub\nSubtractsother, scaled byalpha, frominput.\nother\nalpha\ninput\nsubtract\n\nsubtract\nAlias fortorch.sub().\ntorch.sub()\ntan\n\ntan\nReturns a new tensor with the tangent of the elements ofinput.\ninput\ntanh\n\ntanh\nReturns a new tensor with the hyperbolic tangent of the elements ofinput.\ninput\ntrue_divide\n\ntrue_divide\nAlias fortorch.div()withrounding_mode=None.\ntorch.div()\nrounding_mode=None\ntrunc\n\ntrunc\nReturns a new tensor with the truncated integer values of the elements ofinput.\ninput\nxlogy\n\nxlogy\nAlias fortorch.special.xlogy().\ntorch.special.xlogy()\n\n## Reduction Ops#\n\nargmax\n\nargmax\nReturns the indices of the maximum value of all elements in theinputtensor.\ninput\nargmin\n\nargmin\nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension\namax\n\namax\nReturns the maximum value of each slice of theinputtensor in the given dimension(s)dim.\ninput\ndim\namin\n\namin\nReturns the minimum value of each slice of theinputtensor in the given dimension(s)dim.\ninput\ndim\naminmax\n\naminmax\nComputes the minimum and maximum values of theinputtensor.\ninput\nall\n\nall\nTests if all elements ininputevaluate toTrue.\ninput\nany\n\nany\nTests if any element ininputevaluates toTrue.\ninput\nmax\n\nmax\nReturns the maximum value of all elements in theinputtensor.\ninput\nmin\n\nmin\nReturns the minimum value of all elements in theinputtensor.\ninput\ndist\n\ndist\nReturns the p-norm of (input-other)\ninput\nother\nlogsumexp\n\nlogsumexp\nReturns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.\ninput\ndim\nmean\n\nmean\n\nnanmean\n\nnanmean\nComputes the mean of allnon-NaNelements along the specified dimensions.\nmedian\n\nmedian\nReturns the median of the values ininput.\ninput\nnanmedian\n\nnanmedian\nReturns the median of the values ininput, ignoringNaNvalues.\ninput\nNaN\nmode\n\nmode\nReturns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e. a value which appears most often in that row, andindicesis the index location of each mode value found.\n(values,indices)\nvalues\ninput\ndim\nindices\nnorm\n\nnorm\nReturns the matrix norm or vector norm of a given tensor.\nnansum\n\nnansum\nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.\nprod\n\nprod\nReturns the product of all elements in theinputtensor.\ninput\nquantile\n\nquantile\nComputes the q-th quantiles of each row of theinputtensor along the dimensiondim.\ninput\ndim\nnanquantile\n\nnanquantile\nThis is a variant oftorch.quantile()that \"ignores\"NaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.\ntorch.quantile()\nNaN\nq\nNaN\ninput\nstd\n\nstd\nCalculates the standard deviation over the dimensions specified bydim.\ndim\nstd_mean\n\nstd_mean\nCalculates the standard deviation and mean over the dimensions specified bydim.\ndim\nsum\n\nsum\nReturns the sum of all elements in theinputtensor.\ninput\nunique\n\nunique\nReturns the unique elements of the input tensor.\nunique_consecutive\n\nunique_consecutive\nEliminates all but the first element from every consecutive group of equivalent elements.\nvar\n\nvar\nCalculates the variance over the dimensions specified bydim.\ndim\nvar_mean\n\nvar_mean\nCalculates the variance and mean over the dimensions specified bydim.\ndim\ncount_nonzero\n\ncount_nonzero\nCounts the number of non-zero values in the tensorinputalong the givendim.\ninput\ndim\nhash_tensor\n\nhash_tensor\nReturns a hash of all elements in theinputtensor.\ninput\n\n## Comparison Ops#\n\nallclose\n\nallclose\nThis function checks ifinputandothersatisfy the condition:\ninput\nother\nargsort\n\nargsort\nReturns the indices that sort a tensor along a given dimension in ascending order by value.\neq\n\neq\nComputes element-wise equality\nequal\n\nequal\nTrueif two tensors have the same size and elements,Falseotherwise.\nTrue\nFalse\nge\n\nge\nComputesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.\ngreater_equal\n\ngreater_equal\nAlias fortorch.ge().\ntorch.ge()\ngt\n\ngt\nComputesinput>other\\text{input} > \\text{other}input>otherelement-wise.\ngreater\n\ngreater\nAlias fortorch.gt().\ntorch.gt()\nisclose\n\nisclose\nReturns a new tensor with boolean elements representing if each element ofinputis \"close\" to the corresponding element ofother.\ninput\nother\nisfinite\n\nisfinite\nReturns a new tensor with boolean elements representing if each element isfiniteor not.\nisin\n\nisin\nTests if each element ofelementsis intest_elements.\nelements\ntest_elements\nisinf\n\nisinf\nTests if each element ofinputis infinite (positive or negative infinity) or not.\ninput\nisposinf\n\nisposinf\nTests if each element ofinputis positive infinity or not.\ninput\nisneginf\n\nisneginf\nTests if each element ofinputis negative infinity or not.\ninput\nisnan\n\nisnan\nReturns a new tensor with boolean elements representing if each element ofinputis NaN or not.\ninput\nisreal\n\nisreal\nReturns a new tensor with boolean elements representing if each element ofinputis real-valued or not.\ninput\nkthvalue\n\nkthvalue\nReturns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.\n(values,indices)\nvalues\nk\ninput\ndim\nle\n\nle\nComputesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.\nless_equal\n\nless_equal\nAlias fortorch.le().\ntorch.le()\nlt\n\nlt\nComputesinput<other\\text{input} < \\text{other}input<otherelement-wise.\nless\n\nless\nAlias fortorch.lt().\ntorch.lt()\nmaximum\n\nmaximum\nComputes the element-wise maximum ofinputandother.\ninput\nother\nminimum\n\nminimum\nComputes the element-wise minimum ofinputandother.\ninput\nother\nfmax\n\nfmax\nComputes the element-wise maximum ofinputandother.\ninput\nother\nfmin\n\nfmin\nComputes the element-wise minimum ofinputandother.\ninput\nother\nne\n\nne\nComputesinput\u2260other\\text{input} \\neq \\text{other}input\ue020=otherelement-wise.\nnot_equal\n\nnot_equal\nAlias fortorch.ne().\ntorch.ne()\nsort\n\nsort\nSorts the elements of theinputtensor along a given dimension in ascending order by value.\ninput\ntopk\n\ntopk\nReturns theklargest elements of the giveninputtensor along a given dimension.\nk\ninput\nmsort\n\nmsort\nSorts the elements of theinputtensor along its first dimension in ascending order by value.\ninput\n\n## Spectral Ops#\n\nstft\n\nstft\nShort-time Fourier transform (STFT).\nistft\n\nistft\nInverse short time Fourier Transform.\nbartlett_window\n\nbartlett_window\nBartlett window function.\nblackman_window\n\nblackman_window\nBlackman window function.\nhamming_window\n\nhamming_window\nHamming window function.\nhann_window\n\nhann_window\nHann window function.\nkaiser_window\n\nkaiser_window\nComputes the Kaiser window with window lengthwindow_lengthand shape parameterbeta.\nwindow_length\nbeta\n\n## Other Operations#\n\natleast_1d\n\natleast_1d\nReturns a 1-dimensional view of each input tensor with zero dimensions.\natleast_2d\n\natleast_2d\nReturns a 2-dimensional view of each input tensor with zero dimensions.\natleast_3d\n\natleast_3d\nReturns a 3-dimensional view of each input tensor with zero dimensions.\nbincount\n\nbincount\nCount the frequency of each value in an array of non-negative ints.\nblock_diag\n\nblock_diag\nCreate a block diagonal matrix from provided tensors.\nbroadcast_tensors\n\nbroadcast_tensors\nBroadcasts the given tensors according toBroadcasting semantics.\nbroadcast_to\n\nbroadcast_to\nBroadcastsinputto the shapeshape.\ninput\nshape\nbroadcast_shapes\n\nbroadcast_shapes\nSimilar tobroadcast_tensors()but for shapes.\nbroadcast_tensors()\nbucketize\n\nbucketize\nReturns the indices of the buckets to which each value in theinputbelongs, where the boundaries of the buckets are set byboundaries.\ninput\nboundaries\ncartesian_prod\n\ncartesian_prod\nDo cartesian product of the given sequence of tensors.\ncdist\n\ncdist\nComputes batched the p-norm distance between each pair of the two collections of row vectors.\nclone\n\nclone\nReturns a copy ofinput.\ninput\ncombinations\n\ncombinations\nCompute combinations of lengthrrrof the given tensor.\ncorrcoef\n\ncorrcoef\nEstimates the Pearson product-moment correlation coefficient matrix of the variables given by theinputmatrix, where rows are the variables and columns are the observations.\ninput\ncov\n\ncov\nEstimates the covariance matrix of the variables given by theinputmatrix, where rows are the variables and columns are the observations.\ninput\ncross\n\ncross\nReturns the cross product of vectors in dimensiondimofinputandother.\ndim\ninput\nother\ncummax\n\ncummax\nReturns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.\n(values,indices)\nvalues\ninput\ndim\ncummin\n\ncummin\nReturns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.\n(values,indices)\nvalues\ninput\ndim\ncumprod\n\ncumprod\nReturns the cumulative product of elements ofinputin the dimensiondim.\ninput\ndim\ncumsum\n\ncumsum\nReturns the cumulative sum of elements ofinputin the dimensiondim.\ninput\ndim\ndiag\n\ndiag\nIfinputis a vector (1-D tensor), then returns a 2-D square tensor\nIfinputis a vector (1-D tensor), then returns a 2-D square tensor\ninput\ndiag_embed\n\ndiag_embed\nCreates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.\ndim1\ndim2\ninput\ndiagflat\n\ndiagflat\nIfinputis a vector (1-D tensor), then returns a 2-D square tensor\nIfinputis a vector (1-D tensor), then returns a 2-D square tensor\ninput\ndiagonal\n\ndiagonal\nReturns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.\ninput\ndim1\ndim2\ndiff\n\ndiff\nComputes the n-th forward difference along the given dimension.\neinsum\n\neinsum\nSums the product of the elements of the inputoperandsalong dimensions specified using a notation based on the Einstein summation convention.\noperands\nflatten\n\nflatten\nFlattensinputby reshaping it into a one-dimensional tensor.\ninput\nflip\n\nflip\nReverse the order of an n-D tensor along given axis in dims.\nfliplr\n\nfliplr\nFlip tensor in the left/right direction, returning a new tensor.\nflipud\n\nflipud\nFlip tensor in the up/down direction, returning a new tensor.\nkron\n\nkron\nComputes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.\ninput\nother\nrot90\n\nrot90\nRotate an n-D tensor by 90 degrees in the plane specified by dims axis.\ngcd\n\ngcd\nComputes the element-wise greatest common divisor (GCD) ofinputandother.\ninput\nother\nhistc\n\nhistc\nComputes the histogram of a tensor.\nhistogram\n\nhistogram\nComputes a histogram of the values in a tensor.\nhistogramdd\n\nhistogramdd\nComputes a multi-dimensional histogram of the values in a tensor.\nmeshgrid\n\nmeshgrid\nCreates grids of coordinates specified by the 1D inputs inattr:tensors.\nlcm\n\nlcm\nComputes the element-wise least common multiple (LCM) ofinputandother.\ninput\nother\nlogcumsumexp\n\nlogcumsumexp\nReturns the logarithm of the cumulative summation of the exponentiation of elements ofinputin the dimensiondim.\ninput\ndim\nravel\n\nravel\nReturn a contiguous flattened tensor.\nrenorm\n\nrenorm\nReturns a tensor where each sub-tensor ofinputalong dimensiondimis normalized such that thep-norm of the sub-tensor is lower than the valuemaxnorm\ninput\ndim\nmaxnorm\nrepeat_interleave\n\nrepeat_interleave\nRepeat elements of a tensor.\nroll\n\nroll\nRoll the tensorinputalong the given dimension(s).\ninput\nsearchsorted\n\nsearchsorted\nFind the indices from theinnermostdimension ofsorted_sequencesuch that, if the corresponding values invalueswere inserted before the indices, when sorted, the order of the correspondinginnermostdimension withinsorted_sequencewould be preserved.\nsorted_sequence\nvalues\nsorted_sequence\ntensordot\n\ntensordot\nReturns a contraction of a and b over multiple dimensions.\ntrace\n\ntrace\nReturns the sum of the elements of the diagonal of the input 2-D matrix.\ntril\n\ntril\nReturns the lower triangular part of the matrix (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0.\ninput\nout\ntril_indices\n\ntril_indices\nReturns the indices of the lower triangular part of arow-by-colmatrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.\nrow\ncol\ntriu\n\ntriu\nReturns the upper triangular part of a matrix (2-D tensor) or batch of matricesinput, the other elements of the result tensoroutare set to 0.\ninput\nout\ntriu_indices\n\ntriu_indices\nReturns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.\nrow\ncol\nunflatten\n\nunflatten\nExpands a dimension of the input tensor over multiple dimensions.\nvander\n\nvander\nGenerates a Vandermonde matrix.\nview_as_real\n\nview_as_real\nReturns a view ofinputas a real tensor.\ninput\nview_as_complex\n\nview_as_complex\nReturns a view ofinputas a complex tensor.\ninput\nresolve_conj\n\nresolve_conj\nReturns a new tensor with materialized conjugation ifinput's conjugate bit is set toTrue, else returnsinput.\ninput\ninput\nresolve_neg\n\nresolve_neg\nReturns a new tensor with materialized negation ifinput's negative bit is set toTrue, else returnsinput.\ninput\ninput\n\n## BLAS and LAPACK Operations#\n\naddbmm\n\naddbmm\nPerforms a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).\nbatch1\nbatch2\naddmm\n\naddmm\nPerforms a matrix multiplication of the matricesmat1andmat2.\nmat1\nmat2\naddmv\n\naddmv\nPerforms a matrix-vector product of the matrixmatand the vectorvec.\nmat\nvec\naddr\n\naddr\nPerforms the outer-product of vectorsvec1andvec2and adds it to the matrixinput.\nvec1\nvec2\ninput\nbaddbmm\n\nbaddbmm\nPerforms a batch matrix-matrix product of matrices inbatch1andbatch2.\nbatch1\nbatch2\nbmm\n\nbmm\nPerforms a batch matrix-matrix product of matrices stored ininputandmat2.\ninput\nmat2\nchain_matmul\n\nchain_matmul\nReturns the matrix product of theNNN2-D tensors.\ncholesky\n\ncholesky\nComputes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.\ncholesky_inverse\n\ncholesky_inverse\nComputes the inverse of a complex Hermitian or real symmetric positive-definite matrix given its Cholesky decomposition.\ncholesky_solve\n\ncholesky_solve\nComputes the solution of a system of linear equations with complex Hermitian or real symmetric positive-definite lhs given its Cholesky decomposition.\ndot\n\ndot\nComputes the dot product of two 1D tensors.\ngeqrf\n\ngeqrf\nThis is a low-level function for calling LAPACK's geqrf directly.\nger\n\nger\nAlias oftorch.outer().\ntorch.outer()\ninner\n\ninner\nComputes the dot product for 1D tensors.\ninverse\n\ninverse\nAlias fortorch.linalg.inv()\ntorch.linalg.inv()\ndet\n\ndet\nAlias fortorch.linalg.det()\ntorch.linalg.det()\nlogdet\n\nlogdet\nCalculates log determinant of a square matrix or batches of square matrices.\nslogdet\n\nslogdet\nAlias fortorch.linalg.slogdet()\ntorch.linalg.slogdet()\nlu\n\nlu\nComputes the LU factorization of a matrix or batches of matricesA.\nA\nlu_solve\n\nlu_solve\nReturns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromlu_factor().\nlu_factor()\nlu_unpack\n\nlu_unpack\nUnpacks the LU decomposition returned bylu_factor()into theP, L, Umatrices.\nlu_factor()\nmatmul\n\nmatmul\nMatrix product of two tensors.\nmatrix_power\n\nmatrix_power\nAlias fortorch.linalg.matrix_power()\ntorch.linalg.matrix_power()\nmatrix_exp\n\nmatrix_exp\nAlias fortorch.linalg.matrix_exp().\ntorch.linalg.matrix_exp()\nmm\n\nmm\nPerforms a matrix multiplication of the matricesinputandmat2.\ninput\nmat2\nmv\n\nmv\nPerforms a matrix-vector product of the matrixinputand the vectorvec.\ninput\nvec\norgqr\n\norgqr\nAlias fortorch.linalg.householder_product().\ntorch.linalg.householder_product()\normqr\n\normqr\nComputes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.\nouter\n\nouter\nOuter product ofinputandvec2.\ninput\nvec2\npinverse\n\npinverse\nAlias fortorch.linalg.pinv()\ntorch.linalg.pinv()\nqr\n\nqr\nComputes the QR decomposition of a matrix or a batch of matricesinput, and returns a namedtuple (Q, R) of tensors such thatinput=QR\\text{input} = Q Rinput=QRwithQQQbeing an orthogonal matrix or batch of orthogonal matrices andRRRbeing an upper triangular matrix or batch of upper triangular matrices.\ninput\nsvd\n\nsvd\nComputes the singular value decomposition of either a matrix or batch of matricesinput.\ninput\nsvd_lowrank\n\nsvd_lowrank\nReturn the singular value decomposition(U,S,V)of a matrix, batches of matrices, or a sparse matrixAAAsuch thatA\u2248Udiag\u2061(S)VHA \\approx U \\operatorname{diag}(S) V^{\\text{H}}A\u2248Udiag(S)VH.\n(U,S,V)\npca_lowrank\n\npca_lowrank\nPerforms linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.\nlobpcg\n\nlobpcg\nFind the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.\ntrapz\n\ntrapz\nAlias fortorch.trapezoid().\ntorch.trapezoid()\ntrapezoid\n\ntrapezoid\nComputes thetrapezoidal rulealongdim.\ndim\ncumulative_trapezoid\n\ncumulative_trapezoid\nCumulatively computes thetrapezoidal rulealongdim.\nCumulatively computes thetrapezoidal rulealongdim.\ndim\ntriangular_solve\n\ntriangular_solve\nSolves a system of equations with a square upper or lower triangular invertible matrixAAAand multiple right-hand sidesbbb.\nvdot\n\nvdot\nComputes the dot product of two 1D vectors along a dimension.\n\n## Foreach Operations#\n\nWarning\nThis API is in beta and subject to future changes.\nForward-mode AD is not supported.\n_foreach_abs\n\n_foreach_abs\nApplytorch.abs()to each Tensor of the input list.\ntorch.abs()\n_foreach_abs_\n\n_foreach_abs_\nApplytorch.abs()to each Tensor of the input list.\ntorch.abs()\n_foreach_acos\n\n_foreach_acos\nApplytorch.acos()to each Tensor of the input list.\ntorch.acos()\n_foreach_acos_\n\n_foreach_acos_\nApplytorch.acos()to each Tensor of the input list.\ntorch.acos()\n_foreach_asin\n\n_foreach_asin\nApplytorch.asin()to each Tensor of the input list.\ntorch.asin()\n_foreach_asin_\n\n_foreach_asin_\nApplytorch.asin()to each Tensor of the input list.\ntorch.asin()\n_foreach_atan\n\n_foreach_atan\nApplytorch.atan()to each Tensor of the input list.\ntorch.atan()\n_foreach_atan_\n\n_foreach_atan_\nApplytorch.atan()to each Tensor of the input list.\ntorch.atan()\n_foreach_ceil\n\n_foreach_ceil\nApplytorch.ceil()to each Tensor of the input list.\ntorch.ceil()\n_foreach_ceil_\n\n_foreach_ceil_\nApplytorch.ceil()to each Tensor of the input list.\ntorch.ceil()\n_foreach_cos\n\n_foreach_cos\nApplytorch.cos()to each Tensor of the input list.\ntorch.cos()\n_foreach_cos_\n\n_foreach_cos_\nApplytorch.cos()to each Tensor of the input list.\ntorch.cos()\n_foreach_cosh\n\n_foreach_cosh\nApplytorch.cosh()to each Tensor of the input list.\ntorch.cosh()\n_foreach_cosh_\n\n_foreach_cosh_\nApplytorch.cosh()to each Tensor of the input list.\ntorch.cosh()\n_foreach_erf\n\n_foreach_erf\nApplytorch.erf()to each Tensor of the input list.\ntorch.erf()\n_foreach_erf_\n\n_foreach_erf_\nApplytorch.erf()to each Tensor of the input list.\ntorch.erf()\n_foreach_erfc\n\n_foreach_erfc\nApplytorch.erfc()to each Tensor of the input list.\ntorch.erfc()\n_foreach_erfc_\n\n_foreach_erfc_\nApplytorch.erfc()to each Tensor of the input list.\ntorch.erfc()\n_foreach_exp\n\n_foreach_exp\nApplytorch.exp()to each Tensor of the input list.\ntorch.exp()\n_foreach_exp_\n\n_foreach_exp_\nApplytorch.exp()to each Tensor of the input list.\ntorch.exp()\n_foreach_expm1\n\n_foreach_expm1\nApplytorch.expm1()to each Tensor of the input list.\ntorch.expm1()\n_foreach_expm1_\n\n_foreach_expm1_\nApplytorch.expm1()to each Tensor of the input list.\ntorch.expm1()\n_foreach_floor\n\n_foreach_floor\nApplytorch.floor()to each Tensor of the input list.\ntorch.floor()\n_foreach_floor_\n\n_foreach_floor_\nApplytorch.floor()to each Tensor of the input list.\ntorch.floor()\n_foreach_log\n\n_foreach_log\nApplytorch.log()to each Tensor of the input list.\ntorch.log()\n_foreach_log_\n\n_foreach_log_\nApplytorch.log()to each Tensor of the input list.\ntorch.log()\n_foreach_log10\n\n_foreach_log10\nApplytorch.log10()to each Tensor of the input list.\ntorch.log10()\n_foreach_log10_\n\n_foreach_log10_\nApplytorch.log10()to each Tensor of the input list.\ntorch.log10()\n_foreach_log1p\n\n_foreach_log1p\nApplytorch.log1p()to each Tensor of the input list.\ntorch.log1p()\n_foreach_log1p_\n\n_foreach_log1p_\nApplytorch.log1p()to each Tensor of the input list.\ntorch.log1p()\n_foreach_log2\n\n_foreach_log2\nApplytorch.log2()to each Tensor of the input list.\ntorch.log2()\n_foreach_log2_\n\n_foreach_log2_\nApplytorch.log2()to each Tensor of the input list.\ntorch.log2()\n_foreach_neg\n\n_foreach_neg\nApplytorch.neg()to each Tensor of the input list.\ntorch.neg()\n_foreach_neg_\n\n_foreach_neg_\nApplytorch.neg()to each Tensor of the input list.\ntorch.neg()\n_foreach_tan\n\n_foreach_tan\nApplytorch.tan()to each Tensor of the input list.\ntorch.tan()\n_foreach_tan_\n\n_foreach_tan_\nApplytorch.tan()to each Tensor of the input list.\ntorch.tan()\n_foreach_sin\n\n_foreach_sin\nApplytorch.sin()to each Tensor of the input list.\ntorch.sin()\n_foreach_sin_\n\n_foreach_sin_\nApplytorch.sin()to each Tensor of the input list.\ntorch.sin()\n_foreach_sinh\n\n_foreach_sinh\nApplytorch.sinh()to each Tensor of the input list.\ntorch.sinh()\n_foreach_sinh_\n\n_foreach_sinh_\nApplytorch.sinh()to each Tensor of the input list.\ntorch.sinh()\n_foreach_round\n\n_foreach_round\nApplytorch.round()to each Tensor of the input list.\ntorch.round()\n_foreach_round_\n\n_foreach_round_\nApplytorch.round()to each Tensor of the input list.\ntorch.round()\n_foreach_sqrt\n\n_foreach_sqrt\nApplytorch.sqrt()to each Tensor of the input list.\ntorch.sqrt()\n_foreach_sqrt_\n\n_foreach_sqrt_\nApplytorch.sqrt()to each Tensor of the input list.\ntorch.sqrt()\n_foreach_lgamma\n\n_foreach_lgamma\nApplytorch.lgamma()to each Tensor of the input list.\ntorch.lgamma()\n_foreach_lgamma_\n\n_foreach_lgamma_\nApplytorch.lgamma()to each Tensor of the input list.\ntorch.lgamma()\n_foreach_frac\n\n_foreach_frac\nApplytorch.frac()to each Tensor of the input list.\ntorch.frac()\n_foreach_frac_\n\n_foreach_frac_\nApplytorch.frac()to each Tensor of the input list.\ntorch.frac()\n_foreach_reciprocal\n\n_foreach_reciprocal\nApplytorch.reciprocal()to each Tensor of the input list.\ntorch.reciprocal()\n_foreach_reciprocal_\n\n_foreach_reciprocal_\nApplytorch.reciprocal()to each Tensor of the input list.\ntorch.reciprocal()\n_foreach_sigmoid\n\n_foreach_sigmoid\nApplytorch.sigmoid()to each Tensor of the input list.\ntorch.sigmoid()\n_foreach_sigmoid_\n\n_foreach_sigmoid_\nApplytorch.sigmoid()to each Tensor of the input list.\ntorch.sigmoid()\n_foreach_trunc\n\n_foreach_trunc\nApplytorch.trunc()to each Tensor of the input list.\ntorch.trunc()\n_foreach_trunc_\n\n_foreach_trunc_\nApplytorch.trunc()to each Tensor of the input list.\ntorch.trunc()\n_foreach_zero_\n\n_foreach_zero_\nApplytorch.zero()to each Tensor of the input list.\ntorch.zero()\n\n## Utilities#\n\ncompiled_with_cxx11_abi\n\ncompiled_with_cxx11_abi\nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\nresult_type\n\nresult_type\nReturns thetorch.dtypethat would result from performing an arithmetic operation on the provided input tensors.\ntorch.dtype\ncan_cast\n\ncan_cast\nDetermines if a type conversion is allowed under PyTorch casting rules described in the type promotiondocumentation.\npromote_types\n\npromote_types\nReturns thetorch.dtypewith the smallest size and scalar kind that is not smaller nor of lower kind than eithertype1ortype2.\ntorch.dtype\nuse_deterministic_algorithms\n\nuse_deterministic_algorithms\nSets whether PyTorch operations must use \"deterministic\" algorithms.\nare_deterministic_algorithms_enabled\n\nare_deterministic_algorithms_enabled\nReturns True if the global deterministic flag is turned on.\nis_deterministic_algorithms_warn_only_enabled\n\nis_deterministic_algorithms_warn_only_enabled\nReturns True if the global deterministic flag is set to warn only.\nset_deterministic_debug_mode\n\nset_deterministic_debug_mode\nSets the debug mode for deterministic operations.\nget_deterministic_debug_mode\n\nget_deterministic_debug_mode\nReturns the current value of the debug mode for deterministic operations.\nset_float32_matmul_precision\n\nset_float32_matmul_precision\nSets the internal precision of float32 matrix multiplications.\nget_float32_matmul_precision\n\nget_float32_matmul_precision\nReturns the current value of float32 matrix multiplication precision.\nset_warn_always\n\nset_warn_always\nWhen this flag is False (default) then some PyTorch warnings may only appear once per process.\nget_device_module\n\nget_device_module\nReturns the module associated with a given device(e.g., torch.device('cuda'), \"mtia:0\", \"xpu\", ...).\nis_warn_always_enabled\n\nis_warn_always_enabled\nReturns True if the global warn_always flag is turned on.\nvmap\n\nvmap\nvmap is the vectorizing map;vmap(func)returns a new function that mapsfuncover some dimension of the inputs.\nvmap(func)\nfunc\n_assert\n\n_assert\nA wrapper around Python's assert which is symbolically traceable.\n\n## Symbolic Numbers#\n\nLike an int (including magic methods), but redirects all operations on the\nwrapped node. This is used in particular to symbolically record operations\nin the symbolic shape workflow.\nRepresent this int as an exact integer ratio\ntuple[\u2018SymInt\u2019,int]\nLike a float (including magic methods), but redirects all operations on the\nwrapped node. This is used in particular to symbolically record operations\nin the symbolic shape workflow.\nRepresent this float as an exact integer ratio\ntuple[int,int]\nReturns the complex conjugate of the float.\nSymFloat\nReturns the hexadecimal representation of the float.\nstr\nReturn True if the float is an integer.\nLike a bool (including magic methods), but redirects all operations on the\nwrapped node. This is used in particular to symbolically record operations\nin the symbolic shape workflow.\nUnlike regular bools, regular boolean operators will force extra guards instead\nof symbolically evaluate.  Use the bitwise operators instead to handle this.\nsym_float\n\nsym_float\nSymInt-aware utility for float casting.\nsym_fresh_size\n\nsym_fresh_size\n\nsym_int\n\nsym_int\nSymInt-aware utility for int casting.\nsym_max\n\nsym_max\nSymInt-aware utility for max which avoids branching on a < b.\nsym_min\n\nsym_min\nSymInt-aware utility for min().\nsym_not\n\nsym_not\nSymInt-aware utility for logical negation.\nsym_ite\n\nsym_ite\nSymInt-aware utility for ternary operator (tifbelsef.)\ntifbelsef\nsym_sum\n\nsym_sum\nN-ary add which is faster to compute for long lists than iterated binary addition.\n\n## Export Path#\n\nWarning\nThis feature is a prototype and may have compatibility breaking changes in the future.\nexport\ngenerated/exportdb/index\n\n## Control Flow#\n\nWarning\nThis feature is a prototype and may have compatibility breaking changes in the future.\ncond\n\ncond\nConditionally appliestrue_fnorfalse_fn.\n\n## Optimizations#\n\ncompile\n\ncompile\nOptimizes given model/function using TorchDynamo and specified backend.\ntorch.compile documentation\n\n## Operator Tags#\n\nMembers:\ncore\ncudagraph_unsafe\ndata_dependent_output\ndynamic_output_shape\nflexible_layout\ngenerated\ninplace_view\nmaybe_aliasing_or_mutating\nneeds_contiguous_strides\nneeds_exact_strides\nneeds_fixed_stride_order\nnondeterministic_bitwise\nnondeterministic_seeded\npointwise\npt2_compliant_tag\nview_copy",
    "url": "https://pytorch.org/docs/stable/torch.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ec5d1761d8e8b9c35cc4b9f5491ca71b",
    "source": "pytorch_docs",
    "title": "Custom Backends \u2014 PyTorch 2.9 documentation",
    "text": "\n## Custom Backends#\n\nCreated On: Jun 10, 2025 | Last Updated On: Jun 10, 2025\n\n## Overview#\n\ntorch.compileprovides a straightforward method to enable users\nto define custom backends.\ntorch.compile\nA backend function has the contract(gm:torch.fx.GraphModule,example_inputs:List[torch.Tensor])->Callable.\n(gm:torch.fx.GraphModule,example_inputs:List[torch.Tensor])->Callable\nBackend functions can be called by TorchDynamo, the graph tracing component oftorch.compile,\nafter tracing an FX graph and are\nexpected to return a compiled function that is equivalent to the traced FX graph.\nThe returned callable should have the same contract as theforwardfunction of the originaltorch.fx.GraphModulepassed into the backend:(*args:torch.Tensor)->List[torch.Tensor].\ntorch.compile\nforward\ntorch.fx.GraphModule\n(*args:torch.Tensor)->List[torch.Tensor]\nIn order for TorchDynamo to call your backend, pass your backend function as thebackendkwarg intorch.compile. For example,\nbackend\ntorch.compile\n\n```python\nimport torch\n\ndef my_custom_backend(gm, example_inputs):\n    return gm.forward\n\ndef f(...):\n    ...\n\nf_opt = torch.compile(f, backend=my_custom_backend)\n\n@torch.compile(backend=my_custom_backend)\ndef g(...):\n    ...\n\n```\n\nSee below for more examples.\n\n## Registering Custom Backends#\n\nYou can register your backend using theregister_backenddecorator, for example,\nregister_backend\n\n```python\nfrom torch._dynamo import register_backend\n\n@register_backend\ndef my_compiler(gm, example_inputs):\n    ...\n\n```\n\nBesides theregister_backenddecorator, if your backend is in another python package, you could also register your\nbackend through entry points of python package, which provides a way for a package to register a plugin for another one.\nregister_backend\nHint\nYou can learn more aboutentry_pointsin thepython packaging documentation.\nentry_points\nTo register your backend throughentry_points, you could add your backend function to thetorch_dynamo_backendsentry point group in thesetup.pyfile of your package like:\nentry_points\ntorch_dynamo_backends\nsetup.py\n\n```python\n...\nsetup(\n    ...\n    'torch_dynamo_backends': [\n        'my_compiler = your_module.submodule:my_compiler',\n    ]\n    ...\n)\n\n```\n\nPlease replace themy_compilerbefore=to the name of your backend\u2019s name and replace the part after=to\nthe module and function name of your backend function.\nThe entry point will be added to your python environment after the installation of the package.\nWhen you calltorch.compile(model,backend=\"my_compiler\"), PyTorch would first search the backend namedmy_compilerthat has been registered withregister_backend. If not found, it will continue to search in all backends registered\nviaentry_points.\nmy_compiler\n=\n=\ntorch.compile(model,backend=\"my_compiler\")\nmy_compiler\nregister_backend\nentry_points\nRegistration serves two purposes:\nYou can pass a string containing your backend function\u2019s name totorch.compileinstead of the function itself,\nfor example,torch.compile(model,backend=\"my_compiler\").\ntorch.compile\ntorch.compile(model,backend=\"my_compiler\")\nIt is required for use with theminifier. Any generated\ncode from the minifier must call your code that registers your backend function, typically through animportstatement.\nimport\n\n## Custom Backends after AOTAutograd#\n\nIt is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo.\nThis is useful for 2 main reasons:\nUsers can define backends that support model training, as AOTAutograd can generate the backward graph for compilation.\nAOTAutograd produces FX graphs consisting ofcore Aten ops. As a result,\ncustom backends only need to support the core Aten opset, which is a significantly smaller opset than the entire torch/Aten opset.\nWrap your backend withtorch._dynamo.backends.common.aot_autogradand usetorch.compilewith thebackendkwarg as before.\nBackend functions wrapped byaot_autogradshould have the same contract as before.\ntorch._dynamo.backends.common.aot_autograd\ntorch.compile\nbackend\naot_autograd\nBackend functions are passed toaot_autogradthrough thefw_compiler(forward compiler)\norbw_compiler(backward compiler) kwargs. Ifbw_compileris not specified, the backward compile function\ndefaults to the forward compile function.\naot_autograd\nfw_compiler\nbw_compiler\nbw_compiler\nOne caveat is that AOTAutograd requires compiled functions returned by backends to be \u201cboxed\u201d. This can be done by wrapping\nthe compiled function withfunctorch.compile.make_boxed_func.\nfunctorch.compile.make_boxed_func\nFor example,\n\n```python\nfrom torch._dynamo.backends.common import aot_autograd\nfrom functorch.compile import make_boxed_func\n\ndef my_compiler(gm, example_inputs):\n    return make_boxed_func(gm.forward)\n\nmy_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n\nmodel_opt = torch.compile(model, backend=my_backend)\n\n```\n\n\n## Examples#\n\n\n## Debugging Backend#\n\nIf you want to better understand what is going on during a\ncompilation, you can create a custom compiler, which is referred to as\nbackend in this section, that will print pretty print the fxGraphModuleextracted from Dynamo\u2019s bytecode analysis\nand return aforward()callable.\nGraphModule\nforward()\nFor example:\n\n```python\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef fn(x, y):\n    a = torch.cos(x)\n    b = torch.sin(y)\n    return a + b\nfn(torch.randn(10), torch.randn(10))\n\n```\n\nRunning the above example produces the following output:\n\n```python\nmy_compiler() called with FX graph:\nopcode         name    target                                                  args        kwargs\n-------------  ------  ------------------------------------------------------  ----------  --------\nplaceholder    x       x                                                       ()          {}\nplaceholder    y       y                                                       ()          {}\ncall_function  cos     <built-in method cos of type object at 0x7f1a894649a8>  (x,)        {}\ncall_function  sin     <built-in method sin of type object at 0x7f1a894649a8>  (y,)        {}\ncall_function  add     <built-in function add>                                 (cos, sin)  {}\noutput         output  output                                                  ((add,),)   {}\n\n```\n\nThis works fortorch.nn.Moduleas well as shown below:\ntorch.nn.Module\n\n```python\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\nclass MockModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n    def forward(self, x):\n        return self.relu(torch.cos(x))\nmod = MockModule()\noptimized_mod = torch.compile(mod, backend=my_compiler)\noptimized_mod(torch.randn(10))\n\n```\n\nLet\u2019s take a look at one more example with control flow:\n\n```python\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n\n```\n\nRunning this example produces the following output:\n\n```python\nmy_compiler() called with FX graph:\nopcode         name     target                                                  args              kwargs\n-------------  -------  ------------------------------------------------------  ----------------  --------\nplaceholder    a        a                                                       ()                {}\nplaceholder    b        b                                                       ()                {}\ncall_function  abs_1    <built-in method abs of type object at 0x7f8d259298a0>  (a,)              {}\ncall_function  add      <built-in function add>                                 (abs_1, 1)        {}\ncall_function  truediv  <built-in function truediv>                             (a, add)          {}\ncall_method    sum_1    sum                                                     (b,)              {}\ncall_function  lt       <built-in function lt>                                  (sum_1, 0)        {}\noutput         output   output                                                  ((truediv, lt),)  {}\n\nmy_compiler() called with FX graph:\nopcode         name    target                   args         kwargs\n-------------  ------  -----------------------  -----------  --------\nplaceholder    b       b                        ()           {}\nplaceholder    x       x                        ()           {}\ncall_function  mul     <built-in function mul>  (b, -1)      {}\ncall_function  mul_1   <built-in function mul>  (x, mul)     {}\noutput         output  output                   ((mul_1,),)  {}\n\nmy_compiler() called with FX graph:\nopcode         name    target                   args       kwargs\n-------------  ------  -----------------------  ---------  --------\nplaceholder    b       b                        ()         {}\nplaceholder    x       x                        ()         {}\ncall_function  mul     <built-in function mul>  (x, b)     {}\noutput         output  output                   ((mul,),)  {}\n\nThe order of the last two graphs is nondeterministic depending\non which one is encountered first by the just-in-time compiler.\n\n```\n\n\n## Speedy Backend#\n\nIntegrating a custom backend that offers superior performance is also\neasy and we\u2019ll integrate a real one\nwithoptimize_for_inference:\n\n```python\ndef optimize_for_inference_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    scripted = torch.jit.script(gm)\n    return torch.jit.optimize_for_inference(scripted)\n\n```\n\nAnd then you should be able to optimize any existing code with:\n\n```python\n@torch.compile(backend=optimize_for_inference_compiler)\ndef code_to_accelerate():\n    ...\n\n```\n\n\n## Composable Backends#\n\nTorchDynamo includes many backends, which can be listed withtorch._dynamo.list_backends(). You can combine these backends\ntogether with the following code:\ntorch._dynamo.list_backends()\n\n```python\nfrom torch._dynamo import lookup_backend\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    try:\n        trt_compiled = lookup_backend(\"tensorrt\")(gm, example_inputs)\n        if trt_compiled is not None:\n            return trt_compiled\n    except Exception:\n        pass\n    # first backend failed, try something else...\n    try:\n        inductor_compiled = lookup_backend(\"inductor\")(gm, example_inputs)\n        if inductor_compiled is not None:\n            return inductor_compiled\n    except Exception:\n        pass\n    return gm.forward\n\n```\n",
    "url": "https://pytorch.org/docs/stable/torch.compiler_custom_backends.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "208a8119d358d05019b004ad0b671391",
    "source": "pytorch_docs",
    "title": "torch.profiler \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.profiler#\n\nCreated On: Dec 18, 2020 | Last Updated On: Jun 13, 2025\n\n## Overview#\n\nPyTorch Profiler is a tool that allows the collection of performance metrics during training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace.\nNote\nAn earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated.\ntorch.autograd\n\n## API Reference#\n\nLow-level profiler wrap the autograd profile\nactivities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA,torch.profiler.ProfilerActivity.XPU.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA\nor (when available) ProfilerActivity.XPU.\ntorch.profiler.ProfilerActivity.CPU\ntorch.profiler.ProfilerActivity.CUDA\ntorch.profiler.ProfilerActivity.XPU\nrecord_shapes(bool) \u2013 save information about operator\u2019s input shapes.\nprofile_memory(bool) \u2013 track tensor memory allocation/deallocation (seeexport_memory_timelinefor more details).\nexport_memory_timeline\nwith_stack(bool) \u2013 record source information (file and line number) for the ops.\nwith_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).\nwith_modules(bool) \u2013 record module hierarchy (including function names)\ncorresponding to the callstack of the op. e.g. If module A\u2019s forward call\u2019s\nmodule B\u2019s forward which contains an aten::add op,\nthen aten::add\u2019s module hierarchy is A.B\nNote that this support exist, at the moment, only for TorchScript models\nand not eager mode models.\nexperimental_config(_ExperimentalConfig) \u2013 A set of experimental options\nused by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.\nexecution_trace_observer(ExecutionTraceObserver) \u2013 A PyTorch Execution Trace Observer object.PyTorch Execution Tracesoffer a graph based\nrepresentation of AI/ML workloads and enable replay benchmarks, simulators, and emulators.\nWhen this argument is included the observer start() and stop() will be called for the\nsame time window as PyTorch profiler.\nacc_events(bool) \u2013 Enable the accumulation of FunctionEvents across multiple profiling cycles\nNote\nThis API is experimental and subject to change in the future.\nEnabling shape and stack tracing results in additional overhead.\nWhen record_shapes=True is specified, profiler will temporarily hold references to the tensors;\nthat may further prevent certain optimizations that depend on the reference count and introduce\nextra tensor copies.\nAdds a user defined metadata with a string key and a string value\ninto the trace file\nAdds a user defined metadata with a string key and a valid json value\ninto the trace file\nReturns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished\nExports the collected trace in Chrome JSON format. If kineto is enabled, only\nlast cycle in schedule is exported.\nExport memory event information from the profiler collected\ntree for a given device, and export a timeline plot. There are 3\nexportable files usingexport_memory_timeline, each controlled by thepath\u2019s suffix.\nexport_memory_timeline\npath\nFor an HTML compatible plot, use the suffix.html, and a memory timeline\nplot will be embedded as a PNG file in the HTML file.\n.html\nFor plot points consisting of[times,[sizesbycategory]], wheretimesare timestamps andsizesare memory usage for each category.\nThe memory timeline plot will be saved a JSON (.json) or gzipped JSON\n(.json.gz) depending on the suffix.\n[times,[sizesbycategory]]\ntimes\nsizes\n.json\n.json.gz\nFor raw memory points, use the suffix.raw.json.gz. Each raw memory\nevent will consist of(timestamp,action,numbytes,category), whereactionis one of[PREEXISTING,CREATE,INCREMENT_VERSION,DESTROY],\nandcategoryis one of the enums fromtorch.profiler._memory_profiler.Category.\n.raw.json.gz\n(timestamp,action,numbytes,category)\naction\n[PREEXISTING,CREATE,INCREMENT_VERSION,DESTROY]\ncategory\ntorch.profiler._memory_profiler.Category\nOutput: Memory timeline written as gzipped JSON, JSON, or HTML.\nSave stack traces to a file\npath(str) \u2013 save stacks file to this location;\nmetric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d\nAverages events, grouping them by operator name and (optionally) input shapes, stack\nand overload name.\nNote\nTo use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager.\nPreset a user defined metadata when the profiler is not started\nand added into the trace file later.\nMetadata is in the format of a string key and a valid json value\nToggle collection of activities on/off at any point of collection. Currently supports toggling Torch Ops\n(CPU) and CUDA activity supported in Kineto\nactivities(iterable) \u2013 list of activity groups to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA\ntorch.profiler.ProfilerActivity.CPU\ntorch.profiler.ProfilerActivity.CUDA\nExamples:\n\n```python\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ]\n) as p:\n    code_to_profile_0()\n    // turn off collection of all CUDA activity\n    p.toggle_collection_dynamic(False, [torch.profiler.ProfilerActivity.CUDA])\n    code_to_profile_1()\n    // turn on collection of all CUDA activity\n    p.toggle_collection_dynamic(True, [torch.profiler.ProfilerActivity.CUDA])\n    code_to_profile_2()\nprint(p.key_averages().table(\n    sort_by=\"self_cuda_time_total\", row_limit=-1))\n\n```\n\nProfiler context manager.\nactivities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA,torch.profiler.ProfilerActivity.XPU.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA\nor (when available) ProfilerActivity.XPU.\ntorch.profiler.ProfilerActivity.CPU\ntorch.profiler.ProfilerActivity.CUDA\ntorch.profiler.ProfilerActivity.XPU\nschedule(Callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step.\nProfilerAction\non_trace_ready(Callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling.\nschedule\nProfilerAction.RECORD_AND_SAVE\nrecord_shapes(bool) \u2013 save information about operator\u2019s input shapes.\nprofile_memory(bool) \u2013 track tensor memory allocation/deallocation.\nwith_stack(bool) \u2013 record source information (file and line number) for the ops.\nwith_flops(bool) \u2013 use formula to estimate the FLOPs (floating point operations) of specific operators\n(matrix multiplication and 2D convolution).\nwith_modules(bool) \u2013 record module hierarchy (including function names)\ncorresponding to the callstack of the op. e.g. If module A\u2019s forward call\u2019s\nmodule B\u2019s forward which contains an aten::add op,\nthen aten::add\u2019s module hierarchy is A.B\nNote that this support exist, at the moment, only for TorchScript models\nand not eager mode models.\nexperimental_config(_ExperimentalConfig) \u2013 A set of experimental options\nused for Kineto library features. Note, backward compatibility is not guaranteed.\nexecution_trace_observer(ExecutionTraceObserver) \u2013 A PyTorch Execution Trace Observer object.PyTorch Execution Tracesoffer a graph based\nrepresentation of AI/ML workloads and enable replay benchmarks, simulators, and emulators.\nWhen this argument is included the observer start() and stop() will be called for the\nsame time window as PyTorch profiler. See the examples section below for a code sample.\nacc_events(bool) \u2013 Enable the accumulation of FunctionEvents across multiple profiling cycles\nuse_cuda(bool) \u2013Deprecated since version 1.8.1:useactivitiesinstead.\nDeprecated since version 1.8.1:useactivitiesinstead.\nactivities\nNote\nUseschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager.\nschedule()\nNote\nUsetensorboard_trace_handler()to generate result files for TensorBoard:\ntensorboard_trace_handler()\non_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)\non_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)\nAfter profiling, result files can be found in the specified directory. Use the command:\ntensorboard--logdirdir_name\ntensorboard--logdirdir_name\nto see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin\nNote\nEnabling shape and stack tracing results in additional overhead.\nWhen record_shapes=True is specified, profiler will temporarily hold references to the tensors;\nthat may further prevent certain optimizations that depend on the reference count and introduce\nextra tensor copies.\nExamples:\n\n```python\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ]\n) as p:\n    code_to_profile()\nprint(p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=-1))\n\n```\n\nUsing the profiler\u2019sschedule,on_trace_readyandstepfunctions:\nschedule\non_trace_ready\nstep\n\n```python\n# Non-default profiler schedule allows user to turn profiler on and off\n# on different iterations of the training loop;\n# trace_handler is called every time a new trace becomes available\ndef trace_handler(prof):\n    print(\n        prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=-1)\n    )\n    # prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n\n\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ],\n    # In this example with wait=1, warmup=1, active=2, repeat=1,\n    # profiler will skip the first step/iteration,\n    # start warming up on the second, record\n    # the third and the forth iterations,\n    # after which the trace will become available\n    # and on_trace_ready (when set) is called;\n    # the cycle repeats starting with the next step\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=2, repeat=1),\n    on_trace_ready=trace_handler,\n    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n    # used when outputting for tensorboard\n) as p:\n    for iter in range(N):\n        code_iteration_to_profile(iter)\n        # send a signal to the profiler that the next iteration has started\n        p.step()\n\n```\n\nThe following sample shows how to setup up an Execution Trace Observer (execution_trace_observer)\n\n```python\nwith torch.profiler.profile(\n    ...\n    execution_trace_observer=(\n        ExecutionTraceObserver().register_callback(\"./execution_trace.json\")\n    ),\n) as p:\n    for iter in range(N):\n        code_iteration_to_profile(iter)\n        p.step()\n\n```\n\nYou can also refer to test_execution_trace_with_kineto() in tests/profiler/test_profiler.py.\nNote: One can also pass any object satisfying the _ITraceObserver interface.\nReturns the current trace ID.\nSets a callback to be called when a new trace ID is generated.\nSignals the profiler that the next profiling step has started.\nProfiler actions that can be taken at the specified intervals\nMembers:\nCPU\nXPU\nMTIA\nCUDA\nHPU\nPrivateUse1\nReturns a callable that can be used as profilerscheduleargument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with therepeatparameter, the zero value means that\nthe cycles will continue until the profiling is finished.\nschedule\nskip_first\nwait\nwarmup\nactive\nwait\nrepeat\nTheskip_first_waitparameter controls whether the firstwaitstage should be skipped.\nThis can be useful if a user wants to wait longer thanskip_firstbetween cycles, but not\nfor the first profile. For example, ifskip_firstis 10 andwaitis 20, the first cycle will\nwait 10 + 20 = 30 steps before warmup ifskip_first_waitis zero, but will wait only 10\nsteps ifskip_first_waitis non-zero. All subsequent cycles will then wait 20 steps between the\nlast active and warmup.\nskip_first_wait\nwait\nskip_first\nskip_first\nwait\nskip_first_wait\nskip_first_wait\nCallable\nOutputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.\ndir_name\nworker_name\n\n## Intel Instrumentation and Tracing Technology APIs#\n\nCheck if ITT feature is available or not\nDescribe an instantaneous event that occurred at some point.\nmsg(str) \u2013 ASCII message to associate with the event.\nPushes a range onto a stack of nested range span.  Returns zero-based\ndepth of the range that is started.\nmsg(str) \u2013 ASCII message to associate with range\nPops a range off of a stack of nested range spans. Returns the\nzero-based depth of the range that is ended.",
    "url": "https://pytorch.org/docs/stable/profiler.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d6abdab3531263bdb4e0df996f5e2d82",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/func.batch_norm.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d5e822d127278cd63da8a2787eddf4c5",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/community/design.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8f053e4ad0835760736b875c55eb2546",
    "source": "pytorch_docs",
    "title": "Where to apply torch.compile? \u2014 PyTorch 2.9 documentation",
    "text": "\n## Where to apply torch.compile?#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nWe recommend applyingtorch.compileto the highest-level function that doesn\u2019t cause excessive problems.\nTypically, it is:\ntorch.compile\nyourtrainorevalstep with the optimizer but without the loop,\ntrain\neval\nyour top-levelnn.Module\nnn.Module\nor some sub-nn.Modules.\nnn.Module\ntorch.compilespecifically doesn\u2019t handle distributed wrapper modules like DDP or FSDP very well,\nso consider applyingtorch.compileto the inner module passed to the wrapper.\ntorch.compile\ntorch.compile\n\n```python\n# inference\nmodel = ...\nmodel.compile()\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = model(inp)\n\n```\n\n\n```python\n# training\nmodel = ...\nopt = torch.optim.Adam(model.parameters())\n\n@torch.compile\ndef train(mod, data):\n    opt.zero_grad(True)\n    pred = mod(data[0])\n    loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n    loss.backward()\n    opt.step()\n\nfor _ in range(N_ITERS):\n    inp = ...\n    train(model, inp)\n\n```\n\n\n```python\n# DistributedDataParallel\nmodel = ...\nmodel.compile()\nmodel_ddp = DistributedDataParallel(model, ...)\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = model_ddp(inp)\n\n```\n\n\n## compile(model)vsmodel.compile()#\n\ncompile(model)\nmodel.compile()\nDue to nuances to howtorch.compileinteracts withnn.Moduleinstances,\nwe advise using the.compile()method ofnn.Moduleinstances if you wish to compile them as\ntop-level functions. Nested module calls will be traced correctly -\nthere is no need to call.compile()in that case.\ntorch.compile\nnn.Module\n.compile()\nnn.Module\n.compile()\n\n```python\n# DO NOT DO THIS\nmodel = MyModel()\nmodel = torch.compile(model)\nmodel(inp)\n\n# DO THIS\nmodel = MyModel()\nmodel.compile()\nmodel(inp)\n\n# this is also acceptable\n@torch.compile\ndef fn(model, inp):\n    return model(inp)\nmodel = MyModel()\nfn(model, inp)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.where_to_apply_compile.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "faa657cdf6eafd39e64081179873b3f8",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/fx.experimental.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3e06a78021982f31fd0166bbcbbaebb6",
    "source": "pytorch_docs",
    "title": "Broadcasting semantics \u2014 PyTorch 2.9 documentation",
    "text": "\n## Broadcasting semantics#\n\nCreated On: Apr 27, 2017 | Last Updated On: Jan 31, 2021\nMany PyTorch operations support NumPy\u2019s broadcasting semantics.\nSeehttps://numpy.org/doc/stable/user/basics.broadcasting.htmlfor details.\nIn short, if a PyTorch operation supports broadcast, then its Tensor arguments can be\nautomatically expanded to be of equal sizes (without making copies of the data).\n\n## General semantics#\n\nTwo tensors are \u201cbroadcastable\u201d if the following rules hold:\nEach tensor has at least one dimension.\nWhen iterating over the dimension sizes, starting at the trailing dimension,\nthe dimension sizes must either be equal, one of them is 1, or one of them\ndoes not exist.\nFor Example:\n\n```python\n>>> x=torch.empty(5,7,3)\n>>> y=torch.empty(5,7,3)\n# same shapes are always broadcastable (i.e. the above rules always hold)\n\n>>> x=torch.empty((0,))\n>>> y=torch.empty(2,2)\n# x and y are not broadcastable, because x does not have at least 1 dimension\n\n# can line up trailing dimensions\n>>> x=torch.empty(5,3,4,1)\n>>> y=torch.empty(  3,1,1)\n# x and y are broadcastable.\n# 1st trailing dimension: both have size 1\n# 2nd trailing dimension: y has size 1\n# 3rd trailing dimension: x size == y size\n# 4th trailing dimension: y dimension doesn't exist\n\n# but:\n>>> x=torch.empty(5,2,4,1)\n>>> y=torch.empty(  3,1,1)\n# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n\n```\n\nIf two tensorsx,yare \u201cbroadcastable\u201d, the resulting tensor size\nis calculated as follows:\nx\ny\nIf the number of dimensions ofxandyare not equal, prepend 1\nto the dimensions of the tensor with fewer dimensions to make them equal length.\nx\ny\nThen, for each dimension size, the resulting dimension size is the max of the sizes ofxandyalong that dimension.\nx\ny\nFor Example:\n\n```python\n# can line up trailing dimensions to make reading easier\n>>> x=torch.empty(5,1,4,1)\n>>> y=torch.empty(  3,1,1)\n>>> (x+y).size()\ntorch.Size([5, 3, 4, 1])\n\n# but not necessary:\n>>> x=torch.empty(1)\n>>> y=torch.empty(3,1,7)\n>>> (x+y).size()\ntorch.Size([3, 1, 7])\n\n>>> x=torch.empty(5,2,4,1)\n>>> y=torch.empty(3,1,1)\n>>> (x+y).size()\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n```\n\n\n## In-place semantics#\n\nOne complication is that in-place operations do not allow the in-place tensor to change shape\nas a result of the broadcast.\nFor Example:\n\n```python\n>>> x=torch.empty(5,3,4,1)\n>>> y=torch.empty(3,1,1)\n>>> (x.add_(y)).size()\ntorch.Size([5, 3, 4, 1])\n\n# but:\n>>> x=torch.empty(1,3,1)\n>>> y=torch.empty(3,1,7)\n>>> (x.add_(y)).size()\nRuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.\n\n```\n\n\n## Backwards compatibility#\n\nPrior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes,\nas long as the number of elements in each tensor was equal.  The pointwise operation would then be carried\nout by viewing each tensor as 1-dimensional.  PyTorch now supports broadcasting and the \u201c1-dimensional\u201d\npointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are\nnot broadcastable, but have the same number of elements.\nNote that the introduction of broadcasting can cause backwards incompatible changes in the case where\ntwo tensors do not have the same shape, but are broadcastable and have the same number of elements.\nFor Example:\n\n```python\n>>> torch.add(torch.ones(4,1), torch.randn(4))\n\n```\n\nwould previously produce a Tensor with size: torch.Size([4,1]), but now produces a Tensor with size: torch.Size([4,4]).\nIn order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist,\nyou may settorch.utils.backcompat.broadcast_warning.enabledtoTrue, which will generate a python warning\nin such cases.\nFor Example:\n\n```python\n>>> torch.utils.backcompat.broadcast_warning.enabled=True\n>>> torch.add(torch.ones(4,1), torch.ones(4))\n__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.\nChanging behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.\n\n```\n",
    "url": "https://pytorch.org/docs/stable/notes/broadcasting.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "608be233147cb90709a309b35cd4fa99",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/torch.operator.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a2dfdd850962df4b4d170670c5192f00",
    "source": "pytorch_docs",
    "title": "Subprocess Handling \u2014 PyTorch 2.9 documentation",
    "text": "\n## Subprocess Handling#\n\nCreated On: Mar 08, 2024 | Last Updated On: Mar 08, 2024\n\n## Retrieve SubprocessHandler#\n\nSubprocessHandler\n\n## SubprocessHandler#\n\nConvenience wrapper around python\u2019ssubprocess.Popen. Keeps track of\nmeta-objects associated to the process (e.g. stdout and stderr redirect fds).\nsubprocess.Popen",
    "url": "https://pytorch.org/docs/stable/elastic/subprocess_handler.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "acd2e96217307da9c4e4400f0f11feec",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/accelerator/operators.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f79d4f578c88d2a9fe33e8c94439f923",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/jit_utils.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f818599bc1df3891c5fe38aedcf4e24d",
    "source": "pytorch_docs",
    "title": "Custom Operators \u2014 PyTorch 2.9 documentation",
    "text": "\n## Custom Operators#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nSummary:\nUse custom operators to havetorch.compiletreat a function as opaque.torch.compilewill never trace into the function and Inductor (the backend) will run the function as-is.\ntorch.compile\ntorch.compile\nYou may wish to use a custom operator in any of the following situations:\nYour code calls some C/C++/CUDA code. Dynamo is a Python bytecode interpreter and generally does not know how to handle calls to C/C++/CUDA functions that are bound to Python.\nDynamo and non-strict tracing have trouble tracing through a function and you want it to be ignored bytorch.compile.\ntorch.compile\nPlease seethe Python custom ops tutorialfor more details on how to wrap a Python function into atorch.compile-understood custom operator.\ntorch.compile\nFor more advanced use cases, you may wish to use our C++ Custom Operator API; please seeherefor more information.",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.custom_ops.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f7b80b3dec5d468ad803de2d5d7b0d99",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.fullgraph_true.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c145a46bab44315525642ad2890f9dd6",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/nested.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "63ee6308a36e87acf2e04b75290b5d17",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/jit.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f273df8453cdde5cd2cd485bd4d4df31",
    "source": "pytorch_docs",
    "title": "IRs \u2014 PyTorch 2.9 documentation",
    "text": "\n## IRs#\n\nCreated On: Dec 13, 2022 | Last Updated On: Jul 16, 2025\nPyTorch 2.0 offers two set of IRs for backends to interface with: Core Aten IR and Prims IR.\n\n## Core Aten IR#\n\nCore aten ops is the core subset of aten operators that can be used to compose other operators.\nCore aten IR is fully functional, and there is noinplaceor_outvariants in this opset.\nIn contrast to Prims IR, core aten ops reuses the existing aten ops in \u201cnative_functions.yaml\u201d,\nand it doesn\u2019t further decompose ops into explicit type promotion and broadcasting ops.\nThis opset is designed to serve as the functional IR to interface with backends.\ninplace\n_out\nWarning\nThis opset is still under active development, more ops will be added in the future.\nOperator\nSchema\naten._adaptive_avg_pool2d\naten._adaptive_avg_pool2d\n_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor\naten._adaptive_avg_pool2d_backward\naten._adaptive_avg_pool2d_backward\n_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor\naten._adaptive_avg_pool3d\naten._adaptive_avg_pool3d\n_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor\naten._cdist_forward\naten._cdist_forward\n_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor\naten._embedding_bag\naten._embedding_bag\n_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)\naten._fft_r2c\naten._fft_r2c\n_fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor\naten._local_scalar_dense\naten._local_scalar_dense\n_local_scalar_dense(Tensor self) -> Scalar\naten._log_softmax\naten._log_softmax\n_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor\naten._native_batch_norm_legit\naten._native_batch_norm_legit\n_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)\naten._native_batch_norm_legit.no_stats\naten._native_batch_norm_legit.no_stats\n_native_batch_norm_legit.no_stats(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)\naten._native_batch_norm_legit_no_training\naten._native_batch_norm_legit_no_training\n_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)\naten._pdist_forward\naten._pdist_forward\n_pdist_forward(Tensor self, float p=2) -> Tensor\naten._softmax\naten._softmax\n_softmax(Tensor self, int dim, bool half_to_float) -> Tensor\naten._to_copy\naten._to_copy\n_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor\naten.abs\naten.abs\nabs(Tensor self) -> Tensor\naten.acos\naten.acos\nacos(Tensor self) -> Tensor\naten.acosh\naten.acosh\nacosh(Tensor self) -> Tensor\naten.adaptive_avg_pool1d\naten.adaptive_avg_pool1d\nadaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor\naten.add.Scalar\naten.add.Scalar\nadd.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor\naten.add.Tensor\naten.add.Tensor\nadd.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\naten.addmm\naten.addmm\naddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor\naten.alias\naten.alias\nalias(Tensor(a) self) -> Tensor(a)\naten.amax\naten.amax\namax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor\naten.amin\naten.amin\namin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor\naten.any\naten.any\nany(Tensor self) -> Tensor\naten.any.dim\naten.any.dim\nany.dim(Tensor self, int dim, bool keepdim=False) -> Tensor\naten.any.dims\naten.any.dims\nany.dims(Tensor self, int[]? dim=None, bool keepdim=False) -> Tensor\naten.arange.start_step\naten.arange.start_step\narange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.argmax\naten.argmax\nargmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor\naten.argmin\naten.argmin\nargmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor\naten.as_strided\naten.as_strided\nas_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)\naten.asin\naten.asin\nasin(Tensor self) -> Tensor\naten.asinh\naten.asinh\nasinh(Tensor self) -> Tensor\naten.atan\naten.atan\natan(Tensor self) -> Tensor\naten.atan2\naten.atan2\natan2(Tensor self, Tensor other) -> Tensor\naten.atan2.out\naten.atan2.out\natan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)\naten.atanh\naten.atanh\natanh(Tensor self) -> Tensor\naten.avg_pool1d\naten.avg_pool1d\navg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor\naten.avg_pool2d\naten.avg_pool2d\navg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor\naten.avg_pool2d_backward\naten.avg_pool2d_backward\navg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor\naten.avg_pool3d\naten.avg_pool3d\navg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor\naten.bitwise_and.Scalar\naten.bitwise_and.Scalar\nbitwise_and.Scalar(Tensor self, Scalar other) -> Tensor\naten.bitwise_and.Tensor\naten.bitwise_and.Tensor\nbitwise_and.Tensor(Tensor self, Tensor other) -> Tensor\naten.bitwise_not\naten.bitwise_not\nbitwise_not(Tensor self) -> Tensor\naten.bitwise_or.Scalar\naten.bitwise_or.Scalar\nbitwise_or.Scalar(Tensor self, Scalar other) -> Tensor\naten.bitwise_or.Tensor\naten.bitwise_or.Tensor\nbitwise_or.Tensor(Tensor self, Tensor other) -> Tensor\naten.bitwise_xor.Scalar\naten.bitwise_xor.Scalar\nbitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor\naten.bitwise_xor.Tensor\naten.bitwise_xor.Tensor\nbitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor\naten.bmm\naten.bmm\nbmm(Tensor self, Tensor mat2) -> Tensor\naten.cat\naten.cat\ncat(Tensor[] tensors, int dim=0) -> Tensor\naten.ceil\naten.ceil\nceil(Tensor self) -> Tensor\naten.clamp\naten.clamp\nclamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor\naten.clamp.Tensor\naten.clamp.Tensor\nclamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor\naten.clone\naten.clone\nclone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor\naten.col2im\naten.col2im\ncol2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor\naten.constant_pad_nd\naten.constant_pad_nd\nconstant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor\naten.convolution\naten.convolution\nconvolution(Tensor input, Tensor weight, Tensor? bias, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups) -> Tensor\naten.convolution_backward\naten.convolution_backward\nconvolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, SymInt[] stride, SymInt[] padding, SymInt[] dilation, bool transposed, SymInt[] output_padding, SymInt groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)\naten.copy\naten.copy\ncopy(Tensor self, Tensor src, bool non_blocking=False) -> Tensor\naten.cos\naten.cos\ncos(Tensor self) -> Tensor\naten.cosh\naten.cosh\ncosh(Tensor self) -> Tensor\naten.cumsum\naten.cumsum\ncumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor\naten.diagonal\naten.diagonal\ndiagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -> Tensor(a)\naten.div.Scalar\naten.div.Scalar\ndiv.Scalar(Tensor self, Scalar other) -> Tensor\naten.div.Scalar_mode\naten.div.Scalar_mode\ndiv.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor\naten.div.Tensor\naten.div.Tensor\ndiv.Tensor(Tensor self, Tensor other) -> Tensor\naten.div.Tensor_mode\naten.div.Tensor_mode\ndiv.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor\naten.elu\naten.elu\nelu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor\naten.embedding\naten.embedding\nembedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor\naten.embedding_dense_backward\naten.embedding_dense_backward\nembedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor\naten.empty.memory_format\naten.empty.memory_format\nempty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor\naten.empty_strided\naten.empty_strided\nempty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.eq.Scalar\naten.eq.Scalar\neq.Scalar(Tensor self, Scalar other) -> Tensor\naten.eq.Tensor\naten.eq.Tensor\neq.Tensor(Tensor self, Tensor other) -> Tensor\naten.erf\naten.erf\nerf(Tensor self) -> Tensor\naten.exp\naten.exp\nexp(Tensor self) -> Tensor\naten.expand\naten.expand\nexpand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)\naten.expm1\naten.expm1\nexpm1(Tensor self) -> Tensor\naten.fill.Scalar\naten.fill.Scalar\nfill.Scalar(Tensor self, Scalar value) -> Tensor\naten.flip\naten.flip\nflip(Tensor self, int[] dims) -> Tensor\naten.floor\naten.floor\nfloor(Tensor self) -> Tensor\naten.fmod.Scalar\naten.fmod.Scalar\nfmod.Scalar(Tensor self, Scalar other) -> Tensor\naten.fmod.Tensor\naten.fmod.Tensor\nfmod.Tensor(Tensor self, Tensor other) -> Tensor\naten.full\naten.full\nfull(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.full_like\naten.full_like\nfull_like(Tensor self, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor\naten.gather\naten.gather\ngather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor\naten.ge.Scalar\naten.ge.Scalar\nge.Scalar(Tensor self, Scalar other) -> Tensor\naten.ge.Tensor\naten.ge.Tensor\nge.Tensor(Tensor self, Tensor other) -> Tensor\naten.gelu\naten.gelu\ngelu(Tensor self, *, str approximate=\u2019none\u2019) -> Tensor\naten.grid_sampler_2d\naten.grid_sampler_2d\ngrid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor\naten.gt.Scalar\naten.gt.Scalar\ngt.Scalar(Tensor self, Scalar other) -> Tensor\naten.gt.Tensor\naten.gt.Tensor\ngt.Tensor(Tensor self, Tensor other) -> Tensor\naten.hardtanh\naten.hardtanh\nhardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor\naten.index.Tensor\naten.index.Tensor\nindex.Tensor(Tensor self, Tensor?[] indices) -> Tensor\naten.index_put\naten.index_put\nindex_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor\naten.index_select\naten.index_select\nindex_select(Tensor self, int dim, Tensor index) -> Tensor\naten.isinf\naten.isinf\nisinf(Tensor self) -> Tensor\naten.isnan\naten.isnan\nisnan(Tensor self) -> Tensor\naten.le.Scalar\naten.le.Scalar\nle.Scalar(Tensor self, Scalar other) -> Tensor\naten.le.Tensor\naten.le.Tensor\nle.Tensor(Tensor self, Tensor other) -> Tensor\naten.leaky_relu\naten.leaky_relu\nleaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor\naten.log\naten.log\nlog(Tensor self) -> Tensor\naten.log10\naten.log10\nlog10(Tensor self) -> Tensor\naten.log1p\naten.log1p\nlog1p(Tensor self) -> Tensor\naten.log2\naten.log2\nlog2(Tensor self) -> Tensor\naten.logical_and\naten.logical_and\nlogical_and(Tensor self, Tensor other) -> Tensor\naten.logical_not\naten.logical_not\nlogical_not(Tensor self) -> Tensor\naten.logical_or\naten.logical_or\nlogical_or(Tensor self, Tensor other) -> Tensor\naten.logical_xor\naten.logical_xor\nlogical_xor(Tensor self, Tensor other) -> Tensor\naten.lt.Scalar\naten.lt.Scalar\nlt.Scalar(Tensor self, Scalar other) -> Tensor\naten.lt.Tensor\naten.lt.Tensor\nlt.Tensor(Tensor self, Tensor other) -> Tensor\naten.masked_scatter\naten.masked_scatter\nmasked_scatter(Tensor self, Tensor mask, Tensor source) -> Tensor\naten.max.dim\naten.max.dim\nmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)\naten.max_pool2d_with_indices\naten.max_pool2d_with_indices\nmax_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)\naten.max_pool2d_with_indices_backward\naten.max_pool2d_with_indices_backward\nmax_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor\naten.max_pool3d_with_indices\naten.max_pool3d_with_indices\nmax_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)\naten.maximum\naten.maximum\nmaximum(Tensor self, Tensor other) -> Tensor\naten.mean\naten.mean\nmean(Tensor self, *, ScalarType? dtype=None) -> Tensor\naten.mean.dim\naten.mean.dim\nmean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor\naten.min.dim\naten.min.dim\nmin.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)\naten.minimum\naten.minimum\nminimum(Tensor self, Tensor other) -> Tensor\naten.mm\naten.mm\nmm(Tensor self, Tensor mat2) -> Tensor\naten.mul.Scalar\naten.mul.Scalar\nmul.Scalar(Tensor self, Scalar other) -> Tensor\naten.mul.Tensor\naten.mul.Tensor\nmul.Tensor(Tensor self, Tensor other) -> Tensor\naten.native_dropout\naten.native_dropout\nnative_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)\naten.native_group_norm\naten.native_group_norm\nnative_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)\naten.native_group_norm_backward\naten.native_group_norm_backward\nnative_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)\naten.native_layer_norm\naten.native_layer_norm\nnative_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)\naten.native_layer_norm_backward\naten.native_layer_norm_backward\nnative_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)\naten.ne.Scalar\naten.ne.Scalar\nne.Scalar(Tensor self, Scalar other) -> Tensor\naten.ne.Tensor\naten.ne.Tensor\nne.Tensor(Tensor self, Tensor other) -> Tensor\naten.neg\naten.neg\nneg(Tensor self) -> Tensor\naten.nonzero\naten.nonzero\nnonzero(Tensor self) -> Tensor\naten.permute\naten.permute\npermute(Tensor(a) self, int[] dims) -> Tensor(a)\naten.pow.Scalar\naten.pow.Scalar\npow.Scalar(Scalar self, Tensor exponent) -> Tensor\naten.pow.Tensor_Scalar\naten.pow.Tensor_Scalar\npow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor\naten.pow.Tensor_Tensor\naten.pow.Tensor_Tensor\npow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor\naten.prod\naten.prod\nprod(Tensor self, *, ScalarType? dtype=None) -> Tensor\naten.prod.dim_int\naten.prod.dim_int\nprod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor\naten.rand\naten.rand\nrand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.randn\naten.randn\nrandn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.randperm\naten.randperm\nrandperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.reciprocal\naten.reciprocal\nreciprocal(Tensor self) -> Tensor\naten.reflection_pad1d\naten.reflection_pad1d\nreflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor\naten.reflection_pad2d\naten.reflection_pad2d\nreflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor\naten.reflection_pad3d\naten.reflection_pad3d\nreflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor\naten.relu\naten.relu\nrelu(Tensor self) -> Tensor\naten.remainder.Scalar\naten.remainder.Scalar\nremainder.Scalar(Tensor self, Scalar other) -> Tensor\naten.remainder.Tensor\naten.remainder.Tensor\nremainder.Tensor(Tensor self, Tensor other) -> Tensor\naten.repeat\naten.repeat\nrepeat(Tensor self, SymInt[] repeats) -> Tensor\naten.replication_pad2d\naten.replication_pad2d\nreplication_pad2d(Tensor self, SymInt[4] padding) -> Tensor\naten.replication_pad3d\naten.replication_pad3d\nreplication_pad3d(Tensor self, SymInt[6] padding) -> Tensor\naten.resize_\naten.resize_\nresize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -> Tensor(a!)\naten.round\naten.round\nround(Tensor self) -> Tensor\naten.rsqrt\naten.rsqrt\nrsqrt(Tensor self) -> Tensor\naten.scalar_tensor\naten.scalar_tensor\nscalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor\naten.scatter.src\naten.scatter.src\nscatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor\naten.scatter.value\naten.scatter.value\nscatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor\naten.scatter_add\naten.scatter_add\nscatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor\naten.scatter_reduce.two\naten.scatter_reduce.two\nscatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor\naten.select.int\naten.select.int\nselect.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)\naten.select_scatter\naten.select_scatter\nselect_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor\naten.sigmoid\naten.sigmoid\nsigmoid(Tensor self) -> Tensor\naten.sign\naten.sign\nsign(Tensor self) -> Tensor\naten.sin\naten.sin\nsin(Tensor self) -> Tensor\naten.sinh\naten.sinh\nsinh(Tensor self) -> Tensor\naten.slice.Tensor\naten.slice.Tensor\nslice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)\naten.slice_scatter\naten.slice_scatter\nslice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor\naten.sort\naten.sort\nsort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)\naten.split_with_sizes\naten.split_with_sizes\nsplit_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]\naten.sqrt\naten.sqrt\nsqrt(Tensor self) -> Tensor\naten.squeeze.dim\naten.squeeze.dim\nsqueeze.dim(Tensor(a) self, int dim) -> Tensor(a)\naten.squeeze.dims\naten.squeeze.dims\nsqueeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)\naten.sub.Scalar\naten.sub.Scalar\nsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor\naten.sub.Tensor\naten.sub.Tensor\nsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor\naten.sum.dim_IntList\naten.sum.dim_IntList\nsum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor\naten.sym_is_contiguous\naten.sym_is_contiguous\nsym_is_contiguous(Tensor self, MemoryFormat memory_format=contiguous_format) -> SymBool\naten.sym_numel\naten.sym_numel\nsym_numel(Tensor self) -> SymInt\naten.sym_size.int\naten.sym_size.int\nsym_size.int(Tensor self, int dim) -> SymInt\naten.sym_storage_offset\naten.sym_storage_offset\nsym_storage_offset(Tensor self) -> SymInt\naten.sym_stride.int\naten.sym_stride.int\nsym_stride.int(Tensor self, int dim) -> SymInt\naten.tan\naten.tan\ntan(Tensor self) -> Tensor\naten.tanh\naten.tanh\ntanh(Tensor self) -> Tensor\naten.topk\naten.topk\ntopk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)\naten.trunc\naten.trunc\ntrunc(Tensor self) -> Tensor\naten.unsqueeze\naten.unsqueeze\nunsqueeze(Tensor(a) self, int dim) -> Tensor(a)\naten.upsample_bilinear2d.vec\naten.upsample_bilinear2d.vec\nupsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor\naten.upsample_nearest2d.vec\naten.upsample_nearest2d.vec\nupsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor\naten.var.correction\naten.var.correction\nvar.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor\naten.var.dim\naten.var.dim\nvar.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor\naten.view\naten.view\nview(Tensor(a) self, SymInt[] size) -> Tensor(a)\naten.where.self\naten.where.self\nwhere.self(Tensor condition, Tensor self, Tensor other) -> Tensor\n\n## Prims IR#\n\nPrims IR is a set of primitive operators that can be used to compose other operators.\nPrims IR is a lower level opset than core aten IR, and it further decomposes ops into explicit\ntype promotion and broadcasting ops: prims.convert_element_type and prims.broadcast_in_dim.\nThis opset is designed to interface with compiler backends.\nWarning\nThis opset is still under active development, more ops will be added in the future.\nOperator\nSchema\nprims.abs\nprims.abs\n(Tensor self) -> Tensor\nprims.acos\nprims.acos\n(Tensor self) -> Tensor\nprims.acosh\nprims.acosh\n(Tensor self) -> Tensor\nprims.asin\nprims.asin\n(Tensor self) -> Tensor\nprims.asinh\nprims.asinh\n(Tensor self) -> Tensor\nprims.atan\nprims.atan\n(Tensor self) -> Tensor\nprims.atanh\nprims.atanh\n(Tensor self) -> Tensor\nprims.cos\nprims.cos\n(Tensor self) -> Tensor\nprims.cosh\nprims.cosh\n(Tensor self) -> Tensor\nprims.bessel_i0\nprims.bessel_i0\n(Tensor self) -> Tensor\nprims.bessel_i0e\nprims.bessel_i0e\n(Tensor self) -> Tensor\nprims.bessel_i1\nprims.bessel_i1\n(Tensor self) -> Tensor\nprims.bessel_i1e\nprims.bessel_i1e\n(Tensor self) -> Tensor\nprims.bessel_j0\nprims.bessel_j0\n(Tensor self) -> Tensor\nprims.bessel_j1\nprims.bessel_j1\n(Tensor self) -> Tensor\nprims.bitwise_not\nprims.bitwise_not\n(Tensor self) -> Tensor\nprims.cbrt\nprims.cbrt\n(Tensor self) -> Tensor\nprims.ceil\nprims.ceil\n(Tensor self) -> Tensor\nprims.conj_physical\nprims.conj_physical\n(Tensor self) -> Tensor\nprims.digamma\nprims.digamma\n(Tensor self) -> Tensor\nprims.erf\nprims.erf\n(Tensor self) -> Tensor\nprims.erf_inv\nprims.erf_inv\n(Tensor self) -> Tensor\nprims.erfc\nprims.erfc\n(Tensor self) -> Tensor\nprims.erfcx\nprims.erfcx\n(Tensor self) -> Tensor\nprims.exp\nprims.exp\n(Tensor self) -> Tensor\nprims.expm1\nprims.expm1\n(Tensor self) -> Tensor\nprims.exp2\nprims.exp2\n(Tensor self) -> Tensor\nprims.fill\nprims.fill\n(Tensor self, Scalar value) -> Tensor\nprims.floor\nprims.floor\n(Tensor self) -> Tensor\nprims.imag\nprims.imag\n(Tensor(a) self) -> Tensor(a)\nprims.isfinite\nprims.isfinite\n(Tensor self) -> Tensor\nprims.lgamma\nprims.lgamma\n(Tensor self) -> Tensor\nprims.log\nprims.log\n(Tensor self) -> Tensor\nprims.log1p\nprims.log1p\n(Tensor self) -> Tensor\nprims.log2\nprims.log2\n(Tensor self) -> Tensor\nprims.log10\nprims.log10\n(Tensor self) -> Tensor\nprims.ndtri\nprims.ndtri\n(Tensor self) -> Tensor\nprims.neg\nprims.neg\n(Tensor self) -> Tensor\nprims.real\nprims.real\n(Tensor(a) self) -> Tensor(a)\nprims.reciprocal\nprims.reciprocal\n(Tensor self) -> Tensor\nprims.round\nprims.round\n(Tensor self) -> Tensor\nprims.sign\nprims.sign\n(Tensor self) -> Tensor\nprims.signbit\nprims.signbit\n(Tensor self) -> Tensor\nprims.sin\nprims.sin\n(Tensor self) -> Tensor\nprims.sinh\nprims.sinh\n(Tensor self) -> Tensor\nprims.spherical_bessel_j0\nprims.spherical_bessel_j0\n(Tensor self) -> Tensor\nprims.sqrt\nprims.sqrt\n(Tensor self) -> Tensor\nprims.tan\nprims.tan\n(Tensor self) -> Tensor\nprims.tanh\nprims.tanh\n(Tensor self) -> Tensor\nprims.trunc\nprims.trunc\n(Tensor self) -> Tensor\nprims.add\nprims.add\n(Tensor self, Tensor other) -> Tensor\nprims.atan2\nprims.atan2\n(Tensor self, Tensor other) -> Tensor\nprims.bitwise_and\nprims.bitwise_and\n(Tensor self, Tensor other) -> Tensor\nprims.bitwise_or\nprims.bitwise_or\n(Tensor self, Tensor other) -> Tensor\nprims.bitwise_xor\nprims.bitwise_xor\n(Tensor self, Tensor other) -> Tensor\nprims.div\nprims.div\n(Tensor self, Tensor other) -> Tensor\nprims.eq\nprims.eq\n(Tensor self, Tensor other) -> Tensor\nprims.fmax\nprims.fmax\n(Tensor self, Tensor other) -> Tensor\nprims.fmin\nprims.fmin\n(Tensor self, Tensor other) -> Tensor\nprims.fmod\nprims.fmod\n(Tensor self, Tensor other) -> Tensor\nprims.frexp\nprims.frexp\n(Tensor self) -> (Tensor mantissa, Tensor exponent)\nprims.gcd\nprims.gcd\n(Tensor self, Tensor other) -> Tensor\nprims.ge\nprims.ge\n(Tensor self, Tensor other) -> Tensor\nprims.gt\nprims.gt\n(Tensor self, Tensor other) -> Tensor\nprims.hypot\nprims.hypot\n(Tensor self, Tensor other) -> Tensor\nprims.igamma\nprims.igamma\n(Tensor self, Tensor other) -> Tensor\nprims.igammac\nprims.igammac\n(Tensor self, Tensor other) -> Tensor\nprims.le\nprims.le\n(Tensor self, Tensor other) -> Tensor\nprims.lt\nprims.lt\n(Tensor self, Tensor other) -> Tensor\nprims.maximum\nprims.maximum\n(Tensor self, Tensor other) -> Tensor\nprims.minimum\nprims.minimum\n(Tensor self, Tensor other) -> Tensor\nprims.mul\nprims.mul\n(Tensor self, Tensor other) -> Tensor\nprims.ne\nprims.ne\n(Tensor self, Tensor other) -> Tensor\nprims.nextafter\nprims.nextafter\n(Tensor self, Tensor other) -> Tensor\nprims.pow\nprims.pow\n(Tensor self, Tensor other) -> Tensor\nprims.remainder\nprims.remainder\n(Tensor self, Tensor other) -> Tensor\nprims.rsqrt\nprims.rsqrt\n(Tensor self) -> Tensor\nprims.shift_left\nprims.shift_left\n(Tensor self, Tensor other) -> Tensor\nprims.shift_right_arithmetic\nprims.shift_right_arithmetic\n(Tensor self, Tensor other) -> Tensor\nprims.sub\nprims.sub\n(Tensor self, Tensor other) -> Tensor\nprims.zeta\nprims.zeta\n(Tensor self, Tensor other) -> Tensor\nprims.as_strided\nprims.as_strided\n(Tensor(a!) a, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor(a!)\nprims.broadcast_in_dim\nprims.broadcast_in_dim\n(Tensor(a) a, SymInt[] shape, int[] broadcast_dimensions) -> Tensor(a)\nprims.collapse_view\nprims.collapse_view\n(Tensor(a) a, int start, int end) -> Tensor(a)\nprims.conj\nprims.conj\n(Tensor(a) a) -> Tensor(a)\nprims.split_dim\nprims.split_dim\n(Tensor(a) a, int dim, SymInt outer_length) -> Tensor(a)\nprims.squeeze\nprims.squeeze\n(Tensor(a) a, int[] dimensions) -> Tensor(a)\nprims.transpose\nprims.transpose\n(Tensor(a) a, int[] permutation) -> Tensor(a)\nprims.view_of\nprims.view_of\n(Tensor(a) a) -> Tensor(a)\nprims.view_of_dtype\nprims.view_of_dtype\n(Tensor(a) a, ScalarType dtype) -> Tensor(a)\nprims.as_strided_scatter\nprims.as_strided_scatter\n(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor\nprims.collapse\nprims.collapse\n(Tensor a, int start, int end) -> Tensor\nprims.cat\nprims.cat\n(Tensor[] tensors, int dim) -> Tensor\nprims.reshape\nprims.reshape\n(Tensor a, SymInt[] shape) -> Tensor\nprims.rev\nprims.rev\n(Tensor a, int[] dims) -> Tensor\nprims.where\nprims.where\n(Tensor pred, Tensor a, Tensor b) -> Tensor\nprims.clone\nprims.clone\n(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor\nprims.convert_element_type\nprims.convert_element_type\n(Tensor a, ScalarType dtype) -> Tensor\nprims.device_put\nprims.device_put\n(Tensor a, Device device, bool non_blocking=False) -> Tensor\nprims.item\nprims.item\n(Tensor a) -> Scalar\nprims.maximum_value\nprims.maximum_value\n(ScalarType dtype) -> Scalar\nprims.minimum_value\nprims.minimum_value\n(ScalarType dtype) -> Scalar\nprims.copy_strided\nprims.copy_strided\n(Tensor a, SymInt[] stride) -> Tensor\nprims.copy_to\nprims.copy_to\n(Tensor(a!) a, Tensor b) -> Tensor(a!)\nprims.resize\nprims.resize\n(Tensor(a!) a, SymInt[] shape) -> Tensor(a!)\nprims.amax\nprims.amax\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.amin\nprims.amin\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.prod\nprims.prod\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.sum\nprims.sum\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.xor_sum\nprims.xor_sum\n(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor\nprims.var\nprims.var\n(Tensor inp, int[]? dims, float? correction=1, *, ScalarType? output_dtype=None) -> Tensor\nprims.empty_strided\nprims.empty_strided\n(SymInt[] shape, SymInt[] strides, *, ScalarType dtype, Device device, bool requires_grad) -> Tensor\nprims.empty_permuted\nprims.empty_permuted\n(SymInt[] shape, int[] physical_layout, *, ScalarType dtype, Device device, bool requires_grad) -> Tensor\nprims.scalar_tensor\nprims.scalar_tensor\n(Scalar s, *, ScalarType? dtype=None, Device? device=None) -> Tensor\nprims.iota\nprims.iota\n(SymInt length, *, SymInt start, SymInt step, ScalarType dtype, Device device, bool requires_grad) -> Tensor\nprims.svd\nprims.svd\n(Tensor A, *, bool full_matrices) -> (Tensor U, Tensor S, Tensor Vh)\nprims.normal\nprims.normal\n(SymInt[] shape, *, Scalar mean, Scalar std, ScalarType dtype, Device device, bool requires_grad, Generator? generator=None) -> Tensor\nprims.uniform\nprims.uniform\n(SymInt[] shape, *, Scalar low, Scalar high, ScalarType dtype, Device device, Generator? generator=None) -> Tensor\nprims.fft_r2c\nprims.fft_r2c\n(Tensor self, *, int[] dim, bool onesided) -> Tensor\nprims.fft_c2c\nprims.fft_c2c\n(Tensor self, *, int[] dim, bool forward) -> Tensor\nprims.fft_c2r\nprims.fft_c2r\n(Tensor self, *, int[] dim, SymInt last_dim_size) -> Tensor\nprims._make_token\nprims._make_token\n() -> Tensor\nprims._sink_tokens\nprims._sink_tokens\n(Tensor[] tokens) -> ()",
    "url": "https://pytorch.org/docs/stable/torch.compiler_ir.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8f43f1bc4b55d8503c9bbd394c2fc963",
    "source": "pytorch_docs",
    "title": "torch.load \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.load#\n\nLoads an object saved withtorch.save()from a file.\ntorch.save()\ntorch.load()uses Python\u2019s unpickling facilities but treats storages,\nwhich underlie tensors, specially. They are first deserialized on the\nCPU and are then moved to the device they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised. However, storages can be dynamically remapped to an alternative\nset of devices using themap_locationargument.\ntorch.load()\nmap_location\nIfmap_locationis a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed tomap_location. The builtin location tags are'cpu'for CPU tensors and'cuda:device_id'(e.g.'cuda:2') for CUDA tensors.map_locationshould return eitherNoneor a storage. Ifmap_locationreturns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise,torch.load()will\nfall back to the default behavior, as ifmap_locationwasn\u2019t specified.\nmap_location\nmap_location\n'cpu'\n'cuda:device_id'\n'cuda:2'\nmap_location\nNone\nmap_location\ntorch.load()\nmap_location\nIfmap_locationis atorch.deviceobject or a string containing\na device tag, it indicates the location where all tensors should be loaded.\nmap_location\ntorch.device\nOtherwise, ifmap_locationis a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values).\nmap_location\nUser extensions can register their own location tags and tagging and\ndeserialization methods usingtorch.serialization.register_package().\ntorch.serialization.register_package()\nSeeLayout Controlfor more advanced tools to manipulate a checkpoint.\nf(Union[str,PathLike[str],IO[bytes]]) \u2013 a file-like object (has to implementread(),readline(),tell(), andseek()),\nor a string or os.PathLike object containing a file name\nread()\nreadline()\ntell()\nseek()\nmap_location(Optional[Union[Callable[[Storage,str],Storage],device,str,dict[str,str]]]) \u2013 a function,torch.device, string or a dict specifying how to remap storage\nlocations\ntorch.device\npickle_module(Optional[Any]) \u2013 module used for unpickling metadata and objects (has to\nmatch thepickle_moduleused to serialize file)\npickle_module\nweights_only(Optional[bool]) \u2013 Indicates whether unpickler should be restricted to\nloading only tensors, primitive types, dictionaries\nand any types added viatorch.serialization.add_safe_globals().\nSeetorch.load with weights_only=Truefor more details.\ntorch.serialization.add_safe_globals()\nmmap(Optional[bool]) \u2013 Indicates whether the file should be mapped rather than loading all the storages into memory.\nTypically, tensor storages in the file will first be moved from disk to CPU memory, after which they\nare moved to the location that they were tagged with when saving, or specified bymap_location. This\nsecond step is a no-op if the final location is CPU. When themmapflag is set, instead of copying the\ntensor storages from disk to CPU memory in the first step,fis mapped, which means tensor storages\nwill be lazily loaded when their data is accessed.\nmap_location\nmmap\nf\npickle_load_args(Any) \u2013 (Python 3 only) optional keyword arguments passed over topickle_module.load()andpickle_module.Unpickler(), e.g.,errors=....\npickle_module.load()\npickle_module.Unpickler()\nerrors=...\nAny\nWarning\ntorch.load()unlessweights_onlyparameter is set toTrue,\nusespicklemodule implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource in an unsafe mode, or that could have been tampered with.Only load data you trust.\ntorch.load()\npickle\nNote\nWhen you calltorch.load()on a file which contains GPU tensors, those tensors\nwill be loaded to GPU by default. You can calltorch.load(..,map_location='cpu')and thenload_state_dict()to avoid GPU RAM surge when loading a model checkpoint.\ntorch.load()\ntorch.load(..,map_location='cpu')\nload_state_dict()\nNote\nBy default, we decode byte strings asutf-8.  This is to avoid a common error\ncaseUnicodeDecodeError:'ascii'codeccan'tdecodebyte0x...when loading files saved by Python 2 in Python 3.  If this default\nis incorrect, you may use an extraencodingkeyword argument to specify how\nthese objects should be loaded, e.g.,encoding='latin1'decodes them\nto strings usinglatin1encoding, andencoding='bytes'keeps them\nas byte arrays which can be decoded later withbyte_array.decode(...).\nutf-8\nUnicodeDecodeError:'ascii'codeccan'tdecodebyte0x...\nencoding\nencoding='latin1'\nlatin1\nencoding='bytes'\nbyte_array.decode(...)\nExample\n\n```python\n>>> torch.load(\"tensors.pt\", weights_only=True)\n# Load all tensors onto the CPU\n>>> torch.load(\n...     \"tensors.pt\",\n...     map_location=torch.device(\"cpu\"),\n...     weights_only=True,\n... )\n# Load all tensors onto the CPU, using a function\n>>> torch.load(\n...     \"tensors.pt\",\n...     map_location=lambda storage, loc: storage,\n...     weights_only=True,\n... )\n# Load all tensors onto GPU 1\n>>> torch.load(\n...     \"tensors.pt\",\n...     map_location=lambda storage, loc: storage.cuda(1),\n...     weights_only=True,\n... )  # type: ignore[attr-defined]\n# Map tensors from GPU 1 to GPU 0\n>>> torch.load(\n...     \"tensors.pt\",\n...     map_location={\"cuda:1\": \"cuda:0\"},\n...     weights_only=True,\n... )\n# Load tensor from io.BytesIO object\n# Loading from a buffer setting weights_only=False, warning this can be unsafe\n>>> with open(\"tensor.pt\", \"rb\") as f:\n...     buffer = io.BytesIO(f.read())\n>>> torch.load(buffer, weights_only=False)\n# Load a module with 'ascii' encoding for unpickling\n# Loading from a module setting weights_only=False, warning this can be unsafe\n>>> torch.load(\"module.pt\", encoding=\"ascii\", weights_only=False)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/torch.load.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "dd8f9e83c6f7c7f46390b6069cc8e6de",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/add_hparam.png",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "19cdbab9cf60d19b384221e7a2b4f3c0",
    "source": "pytorch_docs",
    "title": "torch.jit.optimize_for_inference \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.jit.optimize_for_inference#\n\nPerform a set of optimization passes to optimize a model for the purposes of inference.\nIf the model is not already frozen, optimize_for_inference\nwill invoketorch.jit.freezeautomatically.\nIn addition to generic optimizations that should speed up your model regardless\nof environment, prepare for inference will also bake in build specific settings\nsuch as the presence of CUDNN or MKLDNN, and may in the future make transformations\nwhich speed things up on one machine but slow things down on another. Accordingly,\nserialization is not implemented following invokingoptimize_for_inferenceand\nis not guaranteed.\nThis is still in prototype, and may have the potential to slow down your model.\nPrimary use cases that have been targeted so far have been vision models on cpu\nand gpu to a lesser extent.\nExample (optimizing a module with Conv->Batchnorm):\n\n```python\nimport torch\n\nin_channels, out_channels = 3, 32\nconv = torch.nn.Conv2d(\n    in_channels, out_channels, kernel_size=3, stride=2, bias=True\n)\nbn = torch.nn.BatchNorm2d(out_channels, eps=0.001)\nmod = torch.nn.Sequential(conv, bn)\nfrozen_mod = torch.jit.optimize_for_inference(torch.jit.script(mod.eval()))\nassert \"batch_norm\" not in str(frozen_mod.graph)\n# if built with MKLDNN, convolution will be run with MKLDNN weights\nassert \"MKLDNN\" in frozen_mod.graph\n\n```\n\nScriptModule",
    "url": "https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9f6daad91d3f09677a52b1e83bdaf9d3",
    "source": "pytorch_docs",
    "title": "Miscellaneous Environment Variables \u2014 PyTorch 2.9 documentation",
    "text": "\n## Miscellaneous Environment Variables#\n\nCreated On: Jun 17, 2025 | Last Updated On: Jun 17, 2025\nVariable\nDescription\nTORCH_FORCE_WEIGHTS_ONLY_LOAD\nTORCH_FORCE_WEIGHTS_ONLY_LOAD\nIf set to [1,y,yes,true], thetorch.loadwill useweights_only=True. This will happen even ifweights_only=Falsewas passed at the callsite. For more documentation on this, seetorch.load.\n1\ny\nyes\ntrue\ntorch.load\nweights_only=True\nweights_only=False\ntorch.load\nTORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\nTORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\nIf set to [1,y,yes,true], thetorch.loadwill useweights_only=Falseif theweights_onlyvariable was not passed at the callsite. For more documentation on this, seetorch.load.\n1\ny\nyes\ntrue\ntorch.load\nweights_only=False\nweights_only\ntorch.load\nTORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\nTORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\nUnder some conditions, autograd threads can hang on shutdown, therefore we do not wait for them to shutdown indefinitely but rely on a timeout that is by default set to10seconds. This environment variable can be used to set the timeout in seconds.\n10\nTORCH_DEVICE_BACKEND_AUTOLOAD\nTORCH_DEVICE_BACKEND_AUTOLOAD\nIf set to1, out-of-tree backend extensions will be automatically imported when runningimporttorch.\n1\nimporttorch",
    "url": "https://pytorch.org/docs/stable/miscellaneous_environment_variables.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ec93ec31c88a0f22c5bdfc10ac8d2db8",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/cuda.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "367c6c44a8332835a309c8aa5770e5a7",
    "source": "pytorch_docs",
    "title": "torch.nn.functional \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.nn.functional#\n\nCreated On: Jun 11, 2019 | Last Updated On: Mar 25, 2024\n\n## Convolution functions#\n\nconv1d\n\nconv1d\nApplies a 1D convolution over an input signal composed of several input planes.\nconv2d\n\nconv2d\nApplies a 2D convolution over an input image composed of several input planes.\nconv3d\n\nconv3d\nApplies a 3D convolution over an input image composed of several input planes.\nconv_transpose1d\n\nconv_transpose1d\nApplies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \"deconvolution\".\nconv_transpose2d\n\nconv_transpose2d\nApplies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\".\nconv_transpose3d\n\nconv_transpose3d\nApplies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\"\nunfold\n\nunfold\nExtract sliding local blocks from a batched input tensor.\nfold\n\nfold\nCombine an array of sliding local blocks into a large containing tensor.\n\n## Pooling functions#\n\navg_pool1d\n\navg_pool1d\nApplies a 1D average pooling over an input signal composed of several input planes.\navg_pool2d\n\navg_pool2d\nApplies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.\navg_pool3d\n\navg_pool3d\nApplies 3D average-pooling operation inkT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kWregions by step sizesT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sWsteps.\nmax_pool1d\n\nmax_pool1d\nApplies a 1D max pooling over an input signal composed of several input planes.\nmax_pool2d\n\nmax_pool2d\nApplies a 2D max pooling over an input signal composed of several input planes.\nmax_pool3d\n\nmax_pool3d\nApplies a 3D max pooling over an input signal composed of several input planes.\nmax_unpool1d\n\nmax_unpool1d\nCompute a partial inverse ofMaxPool1d.\nMaxPool1d\nmax_unpool2d\n\nmax_unpool2d\nCompute a partial inverse ofMaxPool2d.\nMaxPool2d\nmax_unpool3d\n\nmax_unpool3d\nCompute a partial inverse ofMaxPool3d.\nMaxPool3d\nlp_pool1d\n\nlp_pool1d\nApply a 1D power-average pooling over an input signal composed of several input planes.\nlp_pool2d\n\nlp_pool2d\nApply a 2D power-average pooling over an input signal composed of several input planes.\nlp_pool3d\n\nlp_pool3d\nApply a 3D power-average pooling over an input signal composed of several input planes.\nadaptive_max_pool1d\n\nadaptive_max_pool1d\nApplies a 1D adaptive max pooling over an input signal composed of several input planes.\nadaptive_max_pool2d\n\nadaptive_max_pool2d\nApplies a 2D adaptive max pooling over an input signal composed of several input planes.\nadaptive_max_pool3d\n\nadaptive_max_pool3d\nApplies a 3D adaptive max pooling over an input signal composed of several input planes.\nadaptive_avg_pool1d\n\nadaptive_avg_pool1d\nApplies a 1D adaptive average pooling over an input signal composed of several input planes.\nadaptive_avg_pool2d\n\nadaptive_avg_pool2d\nApply a 2D adaptive average pooling over an input signal composed of several input planes.\nadaptive_avg_pool3d\n\nadaptive_avg_pool3d\nApply a 3D adaptive average pooling over an input signal composed of several input planes.\nfractional_max_pool2d\n\nfractional_max_pool2d\nApplies 2D fractional max pooling over an input signal composed of several input planes.\nfractional_max_pool3d\n\nfractional_max_pool3d\nApplies 3D fractional max pooling over an input signal composed of several input planes.\n\n## Attention Mechanisms#\n\nThetorch.nn.attention.biasmodule contains attention_biases that are designed to be used with\nscaled_dot_product_attention.\ntorch.nn.attention.bias\nscaled_dot_product_attention\n\nscaled_dot_product_attention\nscaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,\n\n## Non-linear activation functions#\n\nthreshold\n\nthreshold\nApply a threshold to each element of the input Tensor.\nthreshold_\n\nthreshold_\nIn-place version ofthreshold().\nthreshold()\nrelu\n\nrelu\nApplies the rectified linear unit function element-wise.\nrelu_\n\nrelu_\nIn-place version ofrelu().\nrelu()\nhardtanh\n\nhardtanh\nApplies the HardTanh function element-wise.\nhardtanh_\n\nhardtanh_\nIn-place version ofhardtanh().\nhardtanh()\nhardswish\n\nhardswish\nApply hardswish function, element-wise.\nrelu6\n\nrelu6\nApplies the element-wise functionReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).\nelu\n\nelu\nApply the Exponential Linear Unit (ELU) function element-wise.\nelu_\n\nelu_\nIn-place version ofelu().\nelu()\nselu\n\nselu\nApplies element-wise,SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717andscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.\ncelu\n\ncelu\nApplies element-wise,CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).\nleaky_relu\n\nleaky_relu\nApplies element-wise,LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)\nleaky_relu_\n\nleaky_relu_\nIn-place version ofleaky_relu().\nleaky_relu()\nprelu\n\nprelu\nApplies element-wise the functionPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)where weight is a learnable parameter.\nrrelu\n\nrrelu\nRandomized leaky ReLU.\nrrelu_\n\nrrelu_\nIn-place version ofrrelu().\nrrelu()\nglu\n\nglu\nThe gated linear unit.\ngelu\n\ngelu\nWhen the approximate argument is 'none', it applies element-wise the functionGELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)\nlogsigmoid\n\nlogsigmoid\nApplies element-wiseLogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)\nhardshrink\n\nhardshrink\nApplies the hard shrinkage function element-wise\ntanhshrink\n\ntanhshrink\nApplies element-wise,Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)\nsoftsign\n\nsoftsign\nApplies element-wise, the functionSoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b\nsoftplus\n\nsoftplus\nApplies element-wise, the functionSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).\nsoftmin\n\nsoftmin\nApply a softmin function.\nsoftmax\n\nsoftmax\nApply a softmax function.\nsoftshrink\n\nsoftshrink\nApplies the soft shrinkage function elementwise\ngumbel_softmax\n\ngumbel_softmax\nSample from the Gumbel-Softmax distribution (Link 1Link 2) and optionally discretize.\nlog_softmax\n\nlog_softmax\nApply a softmax followed by a logarithm.\ntanh\n\ntanh\nApplies element-wise,Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b\nsigmoid\n\nsigmoid\nApplies the element-wise functionSigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b\nhardsigmoid\n\nhardsigmoid\nApply the Hardsigmoid function element-wise.\nsilu\n\nsilu\nApply the Sigmoid Linear Unit (SiLU) function, element-wise.\nmish\n\nmish\nApply the Mish function, element-wise.\nbatch_norm\n\nbatch_norm\nApply Batch Normalization for each channel across a batch of data.\ngroup_norm\n\ngroup_norm\nApply Group Normalization for last certain number of dimensions.\ninstance_norm\n\ninstance_norm\nApply Instance Normalization independently for each channel in every data sample within a batch.\nlayer_norm\n\nlayer_norm\nApply Layer Normalization for last certain number of dimensions.\nlocal_response_norm\n\nlocal_response_norm\nApply local response normalization over an input signal.\nrms_norm\n\nrms_norm\nApply Root Mean Square Layer Normalization.\nnormalize\n\nnormalize\nPerformLpL_pLp\u200bnormalization of inputs over specified dimension.\n\n## Linear functions#\n\nlinear\n\nlinear\nApplies a linear transformation to the incoming data:y=xAT+by = xA^T + by=xAT+b.\nbilinear\n\nbilinear\nApplies a bilinear transformation to the incoming data:y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b\n\n## Dropout functions#\n\ndropout\n\ndropout\nDuring training, randomly zeroes some elements of the input tensor with probabilityp.\np\nalpha_dropout\n\nalpha_dropout\nApply alpha dropout to the input.\nfeature_alpha_dropout\n\nfeature_alpha_dropout\nRandomly masks out entire channels (a channel is a feature map).\ndropout1d\n\ndropout1d\nRandomly zero out entire channels (a channel is a 1D feature map).\ndropout2d\n\ndropout2d\nRandomly zero out entire channels (a channel is a 2D feature map).\ndropout3d\n\ndropout3d\nRandomly zero out entire channels (a channel is a 3D feature map).\n\n## Sparse functions#\n\nembedding\n\nembedding\nGenerate a simple lookup table that looks up embeddings in a fixed dictionary and size.\nembedding_bag\n\nembedding_bag\nCompute sums, means or maxes ofbagsof embeddings.\none_hot\n\none_hot\nTakes LongTensor with index values of shape(*)and returns a tensor of shape(*,num_classes)that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.\n(*)\n(*,num_classes)\n\n## Distance functions#\n\npairwise_distance\n\npairwise_distance\nSeetorch.nn.PairwiseDistancefor details\ntorch.nn.PairwiseDistance\ncosine_similarity\n\ncosine_similarity\nReturns cosine similarity betweenx1andx2, computed along dim.\nx1\nx2\npdist\n\npdist\nComputes the p-norm distance between every pair of row vectors in the input.\n\n## Loss functions#\n\nbinary_cross_entropy\n\nbinary_cross_entropy\nCompute Binary Cross Entropy between the target and input probabilities.\nbinary_cross_entropy_with_logits\n\nbinary_cross_entropy_with_logits\nCompute Binary Cross Entropy between target and input logits.\npoisson_nll_loss\n\npoisson_nll_loss\nCompute the Poisson negative log likelihood loss.\ncosine_embedding_loss\n\ncosine_embedding_loss\nCompute the cosine embedding loss.\ncross_entropy\n\ncross_entropy\nCompute the cross entropy loss between input logits and target.\nctc_loss\n\nctc_loss\nCompute the Connectionist Temporal Classification loss.\ngaussian_nll_loss\n\ngaussian_nll_loss\nCompute the Gaussian negative log likelihood loss.\nhinge_embedding_loss\n\nhinge_embedding_loss\nCompute the hinge embedding loss.\nkl_div\n\nkl_div\nCompute the KL Divergence loss.\nl1_loss\n\nl1_loss\nCompute the L1 loss, with optional weighting.\nmse_loss\n\nmse_loss\nCompute the element-wise mean squared error, with optional weighting.\nmargin_ranking_loss\n\nmargin_ranking_loss\nCompute the margin ranking loss.\nmultilabel_margin_loss\n\nmultilabel_margin_loss\nCompute the multilabel margin loss.\nmultilabel_soft_margin_loss\n\nmultilabel_soft_margin_loss\nCompute the multilabel soft margin loss.\nmulti_margin_loss\n\nmulti_margin_loss\nCompute the multi margin loss, with optional weighting.\nnll_loss\n\nnll_loss\nCompute the negative log likelihood loss.\nhuber_loss\n\nhuber_loss\nCompute the Huber loss, with optional weighting.\nsmooth_l1_loss\n\nsmooth_l1_loss\nCompute the Smooth L1 loss.\nsoft_margin_loss\n\nsoft_margin_loss\nCompute the soft margin loss.\ntriplet_margin_loss\n\ntriplet_margin_loss\nCompute the triplet loss between given input tensors and a margin greater than 0.\ntriplet_margin_with_distance_loss\n\ntriplet_margin_with_distance_loss\nCompute the triplet margin loss for input tensors using a custom distance function.\n\n## Vision functions#\n\npixel_shuffle\n\npixel_shuffle\nRearranges elements in a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)to a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is theupscale_factor.\nupscale_factor\npixel_unshuffle\n\npixel_unshuffle\nReverses thePixelShuffleoperation by rearranging elements in a tensor of shape(\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)to a tensor of shape(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is thedownscale_factor.\nPixelShuffle\ndownscale_factor\npad\n\npad\nPads tensor.\ninterpolate\n\ninterpolate\nDown/up samples the input.\nupsample\n\nupsample\nUpsample input.\nupsample_nearest\n\nupsample_nearest\nUpsamples the input, using nearest neighbours' pixel values.\nupsample_bilinear\n\nupsample_bilinear\nUpsamples the input, using bilinear upsampling.\ngrid_sample\n\ngrid_sample\nCompute grid sample.\naffine_grid\n\naffine_grid\nGenerate 2D or 3D flow field (sampling grid), given a batch of affine matricestheta.\ntheta\n\n## DataParallel functions (multi-GPU, distributed)#\n\n\n## data_parallel#\n\ntorch.nn.parallel.data_parallel\ntorch.nn.parallel.data_parallel\nEvaluate module(input) in parallel across the GPUs given in device_ids.",
    "url": "https://pytorch.org/docs/stable/nn.functional.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b0d4ef233fdf33cab2a95c9525bdc80c",
    "source": "pytorch_docs",
    "title": "torch.sparse \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.sparse#\n\nCreated On: Apr 26, 2017 | Last Updated On: Jun 18, 2025\nWarning\nThe PyTorch API of sparse tensors is in beta and may change in the near future.\nWe highly welcome feature requests, bug reports and general suggestions as GitHub issues.\n\n## Why and when to use sparsity#\n\nBy default, PyTorch storestorch.Tensorelements contiguously in\nphysical memory. This leads to efficient implementations of various array\nprocessing algorithms that require fast access to elements.\ntorch.Tensor\nNow, some users might decide to represent data such as graph adjacency\nmatrices, pruned weights or points clouds by Tensors whoseelements are\nmostly zero valued. We recognize these are important applications and aim\nto provide performance optimizations for these use cases via sparse storage formats.\nVarious sparse storage formats such as COO, CSR/CSC, semi-structured, LIL, etc. have been\ndeveloped over the years. While they differ in exact layouts, they all\ncompress data through efficient representation of zero valued elements.\nWe call the uncompressed valuesspecifiedin contrast tounspecified,\ncompressed elements.\nBy compressing repeat zeros sparse storage formats aim to save memory\nand computational resources on various CPUs and GPUs. Especially for high\ndegrees of sparsity or highly structured sparsity this can have significant\nperformance implications. As such sparse storage formats can be seen as a\nperformance optimization.\nLike many other performance optimization sparse storage formats are not\nalways advantageous. When trying sparse formats for your use case\nyou might find your execution time to increase rather than decrease.\nPlease feel encouraged to open a GitHub issue if you analytically\nexpected to see a stark increase in performance but measured a\ndegradation instead. This helps us prioritize the implementation\nof efficient kernels and wider performance optimizations.\nWe make it easy to try different sparsity layouts, and convert between them,\nwithout being opinionated on what\u2019s best for your particular application.\n\n## Functionality overview#\n\nWe want it to be straightforward to construct a sparse Tensor from a\ngiven dense Tensor by providing conversion routines for each layout.\nIn the next example we convert a 2D Tensor with default dense (strided)\nlayout to a 2D Tensor backed by the COO memory layout. Only values and\nindices of non-zero elements are stored in this case.\n\n```python\n>>> a = torch.tensor([[0, 2.], [3, 0]])\n>>> a.to_sparse()\ntensor(indices=tensor([[0, 1],\n                       [1, 0]]),\n       values=tensor([2., 3.]),\n       size=(2, 2), nnz=2, layout=torch.sparse_coo)\n\n```\n\nPyTorch currently supportsCOO,CSR,CSC,BSR, andBSC.\nWe also have a prototype implementation to support :ref:semi-structured sparsity<sparse-semi-structured-docs>.\nPlease see the references for more details.\nNote that we provide slight generalizations of these formats.\nBatching: Devices such as GPUs require batching for optimal performance and\nthus we support batch dimensions.\nWe currently offer a very simple version of batching where each component of a sparse format\nitself is batched. This also requires the same number of specified elements per batch entry.\nIn this example we construct a 3D (batched) CSR Tensor from a 3D dense Tensor.\n\n```python\n>>> t = torch.tensor([[[1., 0], [2., 3.]], [[4., 0], [5., 6.]]])\n>>> t.dim()\n3\n>>> t.to_sparse_csr()\ntensor(crow_indices=tensor([[0, 1, 3],\n                            [0, 1, 3]]),\n       col_indices=tensor([[0, 0, 1],\n                           [0, 0, 1]]),\n       values=tensor([[1., 2., 3.],\n                      [4., 5., 6.]]), size=(2, 2, 2), nnz=3,\n       layout=torch.sparse_csr)\n\n```\n\nDense dimensions: On the other hand, some data such as Graph embeddings might be\nbetter viewed as sparse collections of vectors instead of scalars.\nIn this example we create a 3D Hybrid COO Tensor with 2 sparse and 1 dense dimension\nfrom a 3D strided Tensor. If an entire row in the 3D strided Tensor is zero, it is\nnot stored. If however any of the values in the row are non-zero, they are stored\nentirely. This reduces the number of indices since we need one index one per row instead\nof one per element. But it also increases the amount of storage for the values. Since\nonly rows that areentirelyzero can be emitted and the presence of any non-zero\nvalued elements cause the entire row to be stored.\n\n```python\n>>> t = torch.tensor([[[0., 0], [1., 2.]], [[0., 0], [3., 4.]]])\n>>> t.to_sparse(sparse_dim=2)\ntensor(indices=tensor([[0, 1],\n                       [1, 1]]),\n       values=tensor([[1., 2.],\n                      [3., 4.]]),\n       size=(2, 2, 2), nnz=2, layout=torch.sparse_coo)\n\n```\n\n\n## Operator overview#\n\nFundamentally, operations on Tensor with sparse storage formats behave the same as\noperations on Tensor with strided (or other) storage formats. The particularities of\nstorage, that is the physical layout of the data, influences the performance of\nan operation but should not influence the semantics.\nWe are actively increasing operator coverage for sparse tensors. Users should not\nexpect support same level of support as for dense Tensors yet.\nSee ouroperatordocumentation for a list.\n\n```python\n>>> b = torch.tensor([[0, 0, 1, 2, 3, 0], [4, 5, 0, 6, 0, 0]])\n>>> b_s = b.to_sparse_csr()\n>>> b_s.cos()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nRuntimeError: unsupported tensor layout: SparseCsr\n>>> b_s.sin()\ntensor(crow_indices=tensor([0, 3, 6]),\n       col_indices=tensor([2, 3, 4, 0, 1, 3]),\n       values=tensor([ 0.8415,  0.9093,  0.1411, -0.7568, -0.9589, -0.2794]),\n       size=(2, 6), nnz=6, layout=torch.sparse_csr)\n\n```\n\nAs shown in the example above, we don\u2019t support non-zero preserving unary\noperators such as cos. The output of a non-zero preserving unary operation\nwill not be able to take advantage of sparse storage formats to the same\nextent as the input and potentially result in a catastrophic increase in memory.\nWe instead rely on the user to explicitly convert to a dense Tensor first and\nthen run the operation.\n\n```python\n>>> b_s.to_dense().cos()\ntensor([[ 1.0000, -0.4161],\n        [-0.9900,  1.0000]])\n\n```\n\nWe are aware that some users want to ignore compressed zeros for operations such\nascosinstead of preserving the exact semantics of the operation. For this we\ncan point to torch.masked and its MaskedTensor, which is in turn also backed and\npowered by sparse storage formats and kernels.\nAlso note that, for now, the user doesn\u2019t have a choice of the output layout. For example,\nadding a sparse Tensor to a regular strided Tensor results in a strided Tensor. Some\nusers might prefer for this to stay a sparse layout, because they know the result will\nstill be sufficiently sparse.\n\n```python\n>>> a + b.to_sparse()\ntensor([[0., 3.],\n        [3., 0.]])\n\n```\n\nWe acknowledge that access to kernels that can efficiently produce different output\nlayouts can be very useful. A subsequent operation might significantly benefit from\nreceiving a particular layout. We are working on an API to control the result layout\nand recognize it is an important feature to plan a more optimal path of execution for\nany given model.\n\n## Sparse Semi-Structured Tensors#\n\nWarning\nSparse semi-structured tensors are currently a prototype feature and subject to change. Please feel free to open an issue to report a bug or if you have feedback to share.\nSemi-Structured sparsity is a sparse data layout that was first introduced in NVIDIA\u2019s Ampere architecture. It is also referred to asfine-grained structured sparsityor2:4 structured sparsity.\nThis sparse layout storesnelements out of every2nelements, withnbeing determined by the width of the Tensor\u2019s data type (dtype). The most frequently used dtype is float16, wheren=2, thus the term \u201c2:4 structured sparsity.\u201d\nSemi-structured sparsity is explained in greater detail inthis NVIDIA blog post.\nIn PyTorch, semi-structured sparsity is implemented via a Tensor subclass.\nBy subclassing, we can override__torch_dispatch__, allowing us to use faster sparse kernels when performing matrix multiplication.\nWe can also store the tensor in it\u2019s compressed form inside the subclass to reduce memory overhead.\n__torch_dispatch__\nIn this compressed form, the sparse tensor is stored by retaining only thespecifiedelements and some metadata, which encodes the mask.\nNote\nThe specified elements and metadata mask of a semi-structured sparse tensor are stored together in a single\nflat compressed tensor. They are appended to each other to form a contiguous chunk of memory.\ncompressed tensor = [ specified elements of original tensor |   metadata_mask ]\nFor an original tensor of size(r, c)we expect the firstm * k // 2elements to be the kept elements\nand the rest of the tensor is metadata.\nIn order to make it easier for the user to view the specified elements\nand mask, one can use.indices()and.values()to access the mask and specified elements respectively.\n.indices()\n.values()\n.values()returns the specified elements in a tensor of size(r, c//2)and with the same dtype as the dense matrix.\n.values()\n.indices()returns the metadata_mask in a tensor of size(r, c//2 )and with element typetorch.int16if dtype is torch.float16 or torch.bfloat16, and element typetorch.int32if dtype is torch.int8.\n.indices()\ntorch.int16\ntorch.int32\nFor 2:4 sparse tensors, the metadata overhead is minor - just 2 bits per specified element.\nNote\nIt\u2019s important to note thattorch.float32is only supported for 1:2 sparsity. Therefore, it does not follow the same formula as above.\ntorch.float32\nHere, we break down how to calculate the compression ratio ( size dense / size sparse) of a 2:4 sparse tensor.\nLet(r, c) = tensor.shapeande = bitwidth(tensor.dtype), soe = 16fortorch.float16andtorch.bfloat16ande = 8fortorch.int8.\ntorch.float16\ntorch.bfloat16\ntorch.int8\nUsing these calculations, we can determine the total memory footprint for both the original dense and the new sparse representation.\nThis gives us a simple formula for the compression ratio, which is dependent only on the bitwidth of the tensor datatype.\nBy using this formula, we find that the compression ratio is 56.25% fortorch.float16ortorch.bfloat16, and 62.5% fortorch.int8.\ntorch.float16\ntorch.bfloat16\ntorch.int8\n\n## Constructing Sparse Semi-Structured Tensors#\n\nYou can transform a dense tensor into a sparse semi-structured tensor by simply using thetorch.to_sparse_semi_structuredfunction.\ntorch.to_sparse_semi_structured\nPlease also note that we only support CUDA tensors since hardware compatibility for semi-structured sparsity is limited to NVIDIA GPUs.\nThe following datatypes are supported for semi-structured sparsity. Note that each datatype has its own shape constraints and compression factor.\nPyTorch dtype\nShape Constraints\nCompression Factor\nSparsity Pattern\ntorch.float16\ntorch.float16\nTensor must be 2D and (r, c) must both be a positive multiple of 64\n9/16\n2:4\ntorch.bfloat16\ntorch.bfloat16\nTensor must be 2D and (r, c) must both be a positive multiple of 64\n9/16\n2:4\ntorch.int8\ntorch.int8\nTensor must be 2D and (r, c) must both be a positive multiple of 128\n10/16\n2:4\nTo construct a semi-structured sparse tensor, start by creating a regular dense tensor that adheres to a 2:4 (or semi-structured) sparse format.\nTo do this we  tile a small 1x4 strip to create a 16x16 dense float16 tensor.\nAfterwards, we can callto_sparse_semi_structuredfunction to compress it for accelerated inference.\nto_sparse_semi_structured\n\n```python\n>>> from torch.sparse import to_sparse_semi_structured\n>>> A = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda()\ntensor([[0., 0., 1.,  ..., 0., 1., 1.],\n        [0., 0., 1.,  ..., 0., 1., 1.],\n        [0., 0., 1.,  ..., 0., 1., 1.],\n        ...,\n        [0., 0., 1.,  ..., 0., 1., 1.],\n        [0., 0., 1.,  ..., 0., 1., 1.],\n        [0., 0., 1.,  ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)\n>>> A_sparse = to_sparse_semi_structured(A)\nSparseSemiStructuredTensor(shape=torch.Size([128, 128]), transposed=False, values=tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16), metadata=tensor([[-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        ...,\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n        [-4370, -4370, -4370,  ..., -4370, -4370, -4370]], device='cuda:0',\ndtype=torch.int16))\n\n```\n\n\n## Sparse Semi-Structured Tensor Operations#\n\nCurrently, the following operations are supported for semi-structured sparse tensors:\ntorch.addmm(bias, dense, sparse.t())\ntorch.mm(dense, sparse)\ntorch.mm(sparse, dense)\naten.linear.default(dense, sparse, bias)\naten.t.default(sparse)\naten.t.detach(sparse)\nTo use these ops, simply pass the output ofto_sparse_semi_structured(tensor)instead of usingtensoronce your tensor has 0s in a semi-structured sparse format, like this:\nto_sparse_semi_structured(tensor)\ntensor\n\n```python\n>>> a = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).half().cuda()\n>>> b = torch.rand(64, 64).half().cuda()\n>>> c = torch.mm(a, b)\n>>> a_sparse = to_sparse_semi_structured(a)\n>>> torch.allclose(c, torch.mm(a_sparse, b))\nTrue\n\n```\n\n\n## Accelerating nn.Linear with semi-structured sparsity#\n\nYou can accelerate the linear layers in your model if the weights are already semi-structured sparse with just a few lines of code:\n\n```python\n>>> input = torch.rand(64, 64).half().cuda()\n>>> mask = torch.Tensor([0, 0, 1, 1]).tile((64, 16)).cuda().bool()\n>>> linear = nn.Linear(64, 64).half().cuda()\n>>> linear.weight = nn.Parameter(to_sparse_semi_structured(linear.weight.masked_fill(~mask, 0)))\n\n```\n\n\n## Sparse COO tensors#\n\nPyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular,\nthe indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64,\nindices\n(ndim,nse)\ntorch.int64\nthe corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type,\nvalues\n(nse,)\nwherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements.\nndim\nnse\nNote\nThe memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data).\n(ndim*8+<sizeofelementtypeinbytes>)*nse\nThe memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>.\nproduct(<tensorshape>)*<sizeofelementtypeinbytes>\nFor example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format.\n(2*8+4)*100000=2000000\n10000*10000*4=400000000\n\n## Construction#\n\nA sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor().\ntorch.sparse_coo_tensor()\nSuppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:\n\n```python\n>>> i = [[0, 1, 1], [2, 0, 2]]\n>>> v =  [3, 4, 5]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3, 4, 5]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n>>> s.to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n\n```\n\nNote that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:\ni\n\n```python\n>>> i = [[0, 2], [1, 0], [1, 2]]\n>>> v =  [3,      4,      5    ]\n>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\n>>> # Or another equivalent formulation to get s\n>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\n>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n\n```\n\nAn empty sparse COO tensor can be constructed by specifying its size\nonly:\n\n```python\n>>> torch.sparse_coo_tensor(size=(2, 3))\ntensor(indices=tensor([], size=(2, 0)),\n       values=tensor([], size=(0,)),\n       size=(2, 3), nnz=0, layout=torch.sparse_coo)\n\n```\n\n\n## Sparse hybrid COO tensors#\n\nPyTorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors.\nPyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave:\nvalues\nthe indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64,\nindices\n(sparse_dims,nse)\ntorch.int64\nthe corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type.\nvalues\n(nse,dense_dims)\nNote\nWe use (M + K)-dimensional tensor to denote a N-dimensional sparse\nhybrid tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.\nSuppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write\n\n```python\n>>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([[3, 4],\n                      [5, 6],\n                      [7, 8]]),\n       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)\n\n```\n\n\n```python\n>>> s.to_dense()\ntensor([[[0, 0],\n         [0, 0],\n         [3, 4]],\n        [[5, 6],\n         [0, 0],\n         [7, 8]]])\n\n```\n\nIn general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants:\ns\nM=s.sparse_dim()\nK=s.dense_dim()\nM+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions,\nM+K==len(s.shape)==s.ndim\ns.indices().shape==(M,nse)- sparse indices are stored\nexplicitly,\ns.indices().shape==(M,nse)\ns.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors,\ns.values().shape==(nse,)+s.shape[M:M+K]\ns.values().layout==torch.strided- values are stored as\nstrided tensors.\ns.values().layout==torch.strided\nNote\nDense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.\nNote\nTo be sure that a constructed sparse tensor has consistent indices,\nvalues, and size, the invariant checks can be enabled per tensor\ncreation viacheck_invariants=Truekeyword argument, or\nglobally usingtorch.sparse.check_sparse_tensor_invariantscontext manager instance. By default, the sparse tensor invariants\nchecks are disabled.\ncheck_invariants=True\ntorch.sparse.check_sparse_tensor_invariants\n\n## Uncoalesced sparse COO tensors#\n\nPyTorch sparse COO tensor format permits sparseuncoalescedtensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor:\n3\n4\n1\n\n```python\n>>> i = [[1, 1]]\n>>> v =  [3, 4]\n>>> s=torch.sparse_coo_tensor(i, v, (3,))\n>>> s\ntensor(indices=tensor([[1, 1]]),\n       values=tensor(  [3, 4]),\n       size=(3,), nnz=2, layout=torch.sparse_coo)\n\n```\n\nwhile the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:\n\n```python\n>>> s.coalesce()\ntensor(indices=tensor([[1]]),\n       values=tensor([7]),\n       size=(3,), nnz=1, layout=torch.sparse_coo)\n\n```\n\nIn general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties:\ntorch.Tensor.coalesce()\nthe indices of specified tensor elements are unique,\nthe indices are sorted in lexicographical order,\ntorch.Tensor.is_coalesced()returnsTrue.\ntorch.Tensor.is_coalesced()\nTrue\nNote\nFor the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a sparse coalesced or uncoalesced tensor.\nHowever, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.\nFor instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:\n\n```python\n>>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n>>> a + b\ntensor(indices=tensor([[0, 0, 1, 1]]),\n       values=tensor([7, 8, 5, 6]),\n       size=(2,), nnz=4, layout=torch.sparse_coo)\n\n```\n\nIf you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large.\ntorch.Tensor.add()\nOn the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products.\n\n## Working with sparse COO tensors#\n\nLet\u2019s consider the following example:\n\n```python\n>>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n\n```\n\nAs mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, one can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties:\ntorch.Tensor\ntorch.Tensor.is_sparse\ntorch.Tensor.layout\n\n```python\n>>> isinstance(s, torch.Tensor)\nTrue\n>>> s.is_sparse\nTrue\n>>> s.layout == torch.sparse_coo\nTrue\n\n```\n\nThe number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance:\ntorch.Tensor.sparse_dim()\ntorch.Tensor.dense_dim()\n\n```python\n>>> s.sparse_dim(), s.dense_dim()\n(2, 1)\n\n```\n\nIfsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values().\ns\ntorch.Tensor.indices()\ntorch.Tensor.values()\nNote\nCurrently, one can acquire the COO format data only when the tensor\ninstance is coalesced:\n\n```python\n>>> s.indices()\nRuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first\n\n```\n\nFor acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices():\ntorch.Tensor._values()\ntorch.Tensor._indices()\n\n```python\n>>> s._indices()\ntensor([[0, 1, 1],\n        [2, 0, 2]])\n\n```\n\nWarning\nCallingtorch.Tensor._values()will return adetachedtensor.\nTo track gradients,torch.Tensor.coalesce().values()must be\nused instead.\ntorch.Tensor._values()\ntorch.Tensor.coalesce().values()\nConstructing a new sparse COO tensor results a tensor that is not\ncoalesced:\n\n```python\n>>> s.is_coalesced()\nFalse\n\n```\n\nbut one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method:\ntorch.Tensor.coalesce()\n\n```python\n>>> s2 = s.coalesce()\n>>> s2.indices()\ntensor([[0, 1, 1],\n       [2, 0, 2]])\n\n```\n\nWhen working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on a sparse uncoalesced tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general.\nc*(a+b)==c*a+c*b\nsqrt(a+b)==sqrt(a)+sqrt(b)\nSlicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:\n\n```python\n>>> s[1]\ntensor(indices=tensor([[0, 2]]),\n       values=tensor([[5, 6],\n                      [7, 8]]),\n       size=(3, 2), nnz=2, layout=torch.sparse_coo)\n>>> s[1, 0, 1]\ntensor(6)\n>>> s[1, 0, 1:]\ntensor([6])\n\n```\n\nIn PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity.\ntorch.sparse.softmax()\n\n## Sparse Compressed Tensors#\n\nSparse Compressed Tensors represents a class of sparse tensors that\nhave a common feature of compressing the indices of a certain dimension\nusing an encoding that enables certain optimizations on linear algebra\nkernels of sparse compressed tensors. This encoding is based on theCompressed Sparse Row (CSR)format that PyTorch sparse compressed\ntensors extend with the support of sparse tensor batches, allowing\nmulti-dimensional tensor values, and storing sparse tensor values in\ndense blocks.\nNote\nWe use (B + M + K)-dimensional tensor to denote a N-dimensional\nsparse compressed hybrid tensor, where B, M, and K are the numbers\nof batch, sparse, and dense dimensions, respectively, such thatB+M+K==Nholds. The number of sparse dimensions for\nsparse compressed tensors is always two,M==2.\nB+M+K==N\nM==2\nNote\nWe say that an indices tensorcompressed_indicesuses CSR\nencoding if the following invariants are satisfied:\ncompressed_indices\ncompressed_indicesis a contiguous strided 32 or 64 bit\ninteger tensor\ncompressed_indices\ncompressed_indicesshape is(*batchsize,compressed_dim_size+1)wherecompressed_dim_sizeis the\nnumber of compressed dimensions (e.g. rows or columns)\ncompressed_indices\n(*batchsize,compressed_dim_size+1)\ncompressed_dim_size\ncompressed_indices[...,0]==0where...denotes batch\nindices\ncompressed_indices[...,0]==0\n...\ncompressed_indices[...,compressed_dim_size]==nsewherenseis the number of specified elements\ncompressed_indices[...,compressed_dim_size]==nse\nnse\n0<=compressed_indices[...,i]-compressed_indices[...,i-1]<=plain_dim_sizefori=1,...,compressed_dim_size,\nwhereplain_dim_sizeis the number of plain dimensions\n(orthogonal to compressed dimensions, e.g. columns or rows).\n0<=compressed_indices[...,i]-compressed_indices[...,i-1]<=plain_dim_size\ni=1,...,compressed_dim_size\nplain_dim_size\nTo be sure that a constructed sparse tensor has consistent indices,\nvalues, and size, the invariant checks can be enabled per tensor\ncreation viacheck_invariants=Truekeyword argument, or\nglobally usingtorch.sparse.check_sparse_tensor_invariantscontext manager instance. By default, the sparse tensor invariants\nchecks are disabled.\ncheck_invariants=True\ntorch.sparse.check_sparse_tensor_invariants\nNote\nThe generalization of sparse compressed layouts to N-dimensional\ntensors can lead to some confusion regarding the count of specified\nelements. When a sparse compressed tensor contains batch dimensions\nthe number of specified elements will correspond to the number of such\nelements per-batch. When a sparse compressed tensor has dense dimensions\nthe element considered is now the K-dimensional array. Also for block\nsparse compressed layouts the 2-D block is considered as the element\nbeing specified.  Take as an example a 3-dimensional block sparse\ntensor, with one batch dimension of lengthb, and a block\nshape ofp,q. If this tensor hasnspecified elements, then\nin fact we havenblocks specified per batch. This tensor would\nhavevalueswith shape(b,n,p,q). This interpretation of the\nnumber of specified elements comes from all sparse compressed layouts\nbeing derived from the compression of a 2-dimensional matrix. Batch\ndimensions are treated as stacking of sparse matrices, dense dimensions\nchange the meaning of the element from a simple scalar value to an\narray with its own dimensions.\nb\np,q\nn\nn\nvalues\n(b,n,p,q)\n\n## Sparse CSR Tensor#\n\nThe primary advantage of the CSR format over the COO format is better\nuse of storage and much faster computation operations such as sparse\nmatrix-vector multiplication using MKL and MAGMA backends.\nIn the simplest case, a (0 + 2 + 0)-dimensional sparse CSR tensor\nconsists of three 1-D tensors:crow_indices,col_indicesandvalues:\ncrow_indices\ncol_indices\nvalues\nThecrow_indicestensor consists of compressed row\nindices. This is a 1-D tensor of sizenrows+1(the number of\nrows plus 1). The last element ofcrow_indicesis the number\nof specified elements,nse. This tensor encodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the\nnumber before it denotes the number of elements in a given row.\ncrow_indices\nnrows+1\ncrow_indices\nnse\nvalues\ncol_indices\nThecol_indicestensor contains the column indices of each\nelement. This is a 1-D tensor of sizense.\ncol_indices\nnse\nThevaluestensor contains the values of the CSR tensor\nelements. This is a 1-D tensor of sizense.\nvalues\nnse\nNote\nThe index tensorscrow_indicesandcol_indicesshould have\nelement type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix operations,\nusetorch.int32. This is as a result of the default linking of\npytorch being with MKL LP64, which uses 32 bit integer indexing.\ncrow_indices\ncol_indices\ntorch.int64\ntorch.int32\ntorch.int32\nIn the general case, the (B + 2 + K)-dimensional sparse CSR tensor\nconsists of two (B + 1)-dimensional index tensorscrow_indicesandcol_indices, and of (1 + K)-dimensionalvaluestensor such\nthat\ncrow_indices\ncol_indices\nvalues\ncrow_indices.shape==(*batchsize,nrows+1)\ncrow_indices.shape==(*batchsize,nrows+1)\ncol_indices.shape==(*batchsize,nse)\ncol_indices.shape==(*batchsize,nse)\nvalues.shape==(nse,*densesize)\nvalues.shape==(nse,*densesize)\nwhile the shape of the sparse CSR tensor is(*batchsize,nrows,ncols,*densesize)wherelen(batchsize)==Bandlen(densesize)==K.\n(*batchsize,nrows,ncols,*densesize)\nlen(batchsize)==B\nlen(densesize)==K\nNote\nThe batches of sparse CSR tensors are dependent: the number of\nspecified elements in all batches must be the same. This somewhat\nartificial constraint allows efficient storage of the indices of\ndifferent CSR batches.\nNote\nThe number of sparse and dense dimensions can be acquired usingtorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim()methods. The batch dimensions can be computed from the tensor\nshape:batchsize=tensor.shape[:-tensor.sparse_dim()-tensor.dense_dim()].\ntorch.Tensor.sparse_dim()\ntorch.Tensor.dense_dim()\nbatchsize=tensor.shape[:-tensor.sparse_dim()-tensor.dense_dim()]\nNote\nThe memory consumption of a sparse CSR tensor is at least(nrows*8+(8+<sizeofelementtypeinbytes>*prod(densesize))*nse)*prod(batchsize)bytes (plus a constant\noverhead from storing other tensor data).\n(nrows*8+(8+<sizeofelementtypeinbytes>*prod(densesize))*nse)*prod(batchsize)\nWith the same example data ofthe note in sparse COO format\nintroduction, the memory consumption of a 10 000\nx 10 000 tensor with 100 000 non-zero 32-bit floating point numbers\nis at least(10000*8+(8+4*1)*100000)*1=1280000bytes when using CSR tensor layout. Notice the 1.6 and 310 fold\nsavings from using CSR storage format compared to using the COO and\nstrided formats, respectively.\n(10000*8+(8+4*1)*100000)*1=1280000\nSparse CSR tensors can be directly constructed by using thetorch.sparse_csr_tensor()function. The user must supply the row\nand column indices and values tensors separately where the row indices\nmust be specified using the CSR compression encoding.  Thesizeargument is optional and will be deduced from thecrow_indicesandcol_indicesif it is not present.\ntorch.sparse_csr_tensor()\nsize\ncrow_indices\ncol_indices\n\n```python\n>>> crow_indices = torch.tensor([0, 2, 4])\n>>> col_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csr = torch.sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\n>>> csr\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n       dtype=torch.float64)\n>>> csr.to_dense()\ntensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)\n\n```\n\nNote\nThe values of sparse dimensions in deducedsizeis computed\nfrom the size ofcrow_indicesand the maximal index value incol_indices. If the number of columns needs to be larger than\nin the deducedsizethen thesizeargument must be\nspecified explicitly.\nsize\ncrow_indices\ncol_indices\nsize\nsize\nThe simplest way of constructing a 2-D sparse CSR tensor from a\nstrided or sparse COO tensor is to usetorch.Tensor.to_sparse_csr()method. Any zeros in the (strided)\ntensor will be interpreted as missing values in the sparse tensor:\ntorch.Tensor.to_sparse_csr()\n\n```python\n>>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)\n>>> sp = a.to_sparse_csr()\n>>> sp\ntensor(crow_indices=tensor([0, 1, 3, 3]),\n      col_indices=tensor([2, 0, 1]),\n      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)\n\n```\n\nThe sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors.\ntensor.matmul()\n\n```python\n>>> vec = torch.randn(4, 1, dtype=torch.float64)\n>>> sp.matmul(vec)\ntensor([[0.9078],\n        [1.3180],\n        [0.0000]], dtype=torch.float64)\n\n```\n\n\n## Sparse CSC Tensor#\n\nThe sparse CSC (Compressed Sparse Column) tensor format implements the\nCSC format for storage of 2 dimensional tensors with an extension to\nsupporting batches of sparse CSC tensors and values being\nmulti-dimensional tensors.\nNote\nSparse CSC tensor is essentially a transpose of the sparse CSR\ntensor when the transposition is about swapping the sparse\ndimensions.\nSimilarly tosparse CSR tensors, a sparse CSC\ntensor consists of three tensors:ccol_indices,row_indicesandvalues:\nccol_indices\nrow_indices\nvalues\nTheccol_indicestensor consists of compressed column\nindices. This is a (B + 1)-D tensor of shape(*batchsize,ncols+1).\nThe last element is the number of specified\nelements,nse. This tensor encodes the index invaluesandrow_indicesdepending on where the given column starts. Each\nsuccessive number in the tensor subtracted by the number before it\ndenotes the number of elements in a given column.\nccol_indices\n(*batchsize,ncols+1)\nnse\nvalues\nrow_indices\nTherow_indicestensor contains the row indices of each\nelement. This is a (B + 1)-D tensor of shape(*batchsize,nse).\nrow_indices\n(*batchsize,nse)\nThevaluestensor contains the values of the CSC tensor\nelements. This is a (1 + K)-D tensor of shape(nse,*densesize).\nvalues\n(nse,*densesize)\nSparse CSC tensors can be directly constructed by using thetorch.sparse_csc_tensor()function. The user must supply the row\nand column indices and values tensors separately where the column indices\nmust be specified using the CSR compression encoding.  Thesizeargument is optional and will be deduced from therow_indicesandccol_indicestensors if it is not present.\ntorch.sparse_csc_tensor()\nsize\nrow_indices\nccol_indices\n\n```python\n>>> ccol_indices = torch.tensor([0, 2, 4])\n>>> row_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csc = torch.sparse_csc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)\n>>> csc\ntensor(ccol_indices=tensor([0, 2, 4]),\n       row_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n       dtype=torch.float64, layout=torch.sparse_csc)\n>>> csc.to_dense()\ntensor([[1., 3.],\n        [2., 4.]], dtype=torch.float64)\n\n```\n\nNote\nThe sparse CSC tensor constructor function has the compressed\ncolumn indices argument before the row indices argument.\nThe (0 + 2 + 0)-dimensional sparse CSC tensors can be constructed from\nany two-dimensional tensor usingtorch.Tensor.to_sparse_csc()method. Any zeros in the (strided) tensor will be interpreted as\nmissing values in the sparse tensor:\ntorch.Tensor.to_sparse_csc()\n\n```python\n>>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype=torch.float64)\n>>> sp = a.to_sparse_csc()\n>>> sp\ntensor(ccol_indices=tensor([0, 1, 2, 3, 3]),\n       row_indices=tensor([1, 1, 0]),\n       values=tensor([1., 2., 1.]), size=(3, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_csc)\n\n```\n\n\n## Sparse BSR Tensor#\n\nThe sparse BSR (Block compressed Sparse Row) tensor format implements the\nBSR format for storage of two-dimensional tensors with an extension to\nsupporting batches of sparse BSR tensors and values being blocks of\nmulti-dimensional tensors.\nA sparse BSR tensor consists of three tensors:crow_indices,col_indicesandvalues:\ncrow_indices\ncol_indices\nvalues\nThecrow_indicestensor consists of compressed row\nindices. This is a (B + 1)-D tensor of shape(*batchsize,nrowblocks+1).  The last element is the number of specified blocks,nse. This tensor encodes the index invaluesandcol_indicesdepending on where the given column block\nstarts. Each successive number in the tensor subtracted by the\nnumber before it denotes the number of blocks in a given row.\ncrow_indices\n(*batchsize,nrowblocks+1)\nnse\nvalues\ncol_indices\nThecol_indicestensor contains the column block indices of each\nelement. This is a (B + 1)-D tensor of shape(*batchsize,nse).\ncol_indices\n(*batchsize,nse)\nThevaluestensor contains the values of the sparse BSR tensor\nelements collected into two-dimensional blocks. This is a (1 + 2 +\nK)-D tensor of shape(nse,nrowblocks,ncolblocks,*densesize).\nvalues\n(nse,nrowblocks,ncolblocks,*densesize)\nSparse BSR tensors can be directly constructed by using thetorch.sparse_bsr_tensor()function. The user must supply the row\nand column block indices and values tensors separately where the row block indices\nmust be specified using the CSR compression encoding.\nThesizeargument is optional and will be deduced from thecrow_indicesandcol_indicestensors if it is not present.\ntorch.sparse_bsr_tensor()\nsize\ncrow_indices\ncol_indices\n\n```python\n>>> crow_indices = torch.tensor([0, 2, 4])\n>>> col_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],\n...                        [[3, 4, 5], [9, 10, 11]],\n...                        [[12, 13, 14], [18, 19, 20]],\n...                        [[15, 16, 17], [21, 22, 23]]])\n>>> bsr = torch.sparse_bsr_tensor(crow_indices, col_indices, values, dtype=torch.float64)\n>>> bsr\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([[[ 0.,  1.,  2.],\n                       [ 6.,  7.,  8.]],\n                      [[ 3.,  4.,  5.],\n                       [ 9., 10., 11.]],\n                      [[12., 13., 14.],\n                       [18., 19., 20.]],\n                      [[15., 16., 17.],\n                       [21., 22., 23.]]]),\n       size=(4, 6), nnz=4, dtype=torch.float64, layout=torch.sparse_bsr)\n>>> bsr.to_dense()\ntensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9., 10., 11.],\n        [12., 13., 14., 15., 16., 17.],\n        [18., 19., 20., 21., 22., 23.]], dtype=torch.float64)\n\n```\n\nThe (0 + 2 + 0)-dimensional sparse BSR tensors can be constructed from\nany two-dimensional tensor usingtorch.Tensor.to_sparse_bsr()method that also requires the specification of the values block size:\ntorch.Tensor.to_sparse_bsr()\n\n```python\n>>> dense = torch.tensor([[0, 1, 2, 3, 4, 5],\n...                       [6, 7, 8, 9, 10, 11],\n...                       [12, 13, 14, 15, 16, 17],\n...                       [18, 19, 20, 21, 22, 23]])\n>>> bsr = dense.to_sparse_bsr(blocksize=(2, 3))\n>>> bsr\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([[[ 0,  1,  2],\n                       [ 6,  7,  8]],\n                      [[ 3,  4,  5],\n                       [ 9, 10, 11]],\n                      [[12, 13, 14],\n                       [18, 19, 20]],\n                      [[15, 16, 17],\n                       [21, 22, 23]]]), size=(4, 6), nnz=4,\n       layout=torch.sparse_bsr)\n\n```\n\n\n## Sparse BSC Tensor#\n\nThe sparse BSC (Block compressed Sparse Column) tensor format implements the\nBSC format for storage of two-dimensional tensors with an extension to\nsupporting batches of sparse BSC tensors and values being blocks of\nmulti-dimensional tensors.\nA sparse BSC tensor consists of three tensors:ccol_indices,row_indicesandvalues:\nccol_indices\nrow_indices\nvalues\nTheccol_indicestensor consists of compressed column\nindices. This is a (B + 1)-D tensor of shape(*batchsize,ncolblocks+1).  The last element is the number of specified blocks,nse. This tensor encodes the index invaluesandrow_indicesdepending on where the given row block\nstarts. Each successive number in the tensor subtracted by the\nnumber before it denotes the number of blocks in a given column.\nccol_indices\n(*batchsize,ncolblocks+1)\nnse\nvalues\nrow_indices\nTherow_indicestensor contains the row block indices of each\nelement. This is a (B + 1)-D tensor of shape(*batchsize,nse).\nrow_indices\n(*batchsize,nse)\nThevaluestensor contains the values of the sparse BSC tensor\nelements collected into two-dimensional blocks. This is a (1 + 2 +\nK)-D tensor of shape(nse,nrowblocks,ncolblocks,*densesize).\nvalues\n(nse,nrowblocks,ncolblocks,*densesize)\nSparse BSC tensors can be directly constructed by using thetorch.sparse_bsc_tensor()function. The user must supply the row\nand column block indices and values tensors separately where the column block indices\nmust be specified using the CSR compression encoding.\nThesizeargument is optional and will be deduced from theccol_indicesandrow_indicestensors if it is not present.\ntorch.sparse_bsc_tensor()\nsize\nccol_indices\nrow_indices\n\n```python\n>>> ccol_indices = torch.tensor([0, 2, 4])\n>>> row_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([[[0, 1, 2], [6, 7, 8]],\n...                        [[3, 4, 5], [9, 10, 11]],\n...                        [[12, 13, 14], [18, 19, 20]],\n...                        [[15, 16, 17], [21, 22, 23]]])\n>>> bsc = torch.sparse_bsc_tensor(ccol_indices, row_indices, values, dtype=torch.float64)\n>>> bsc\ntensor(ccol_indices=tensor([0, 2, 4]),\n       row_indices=tensor([0, 1, 0, 1]),\n       values=tensor([[[ 0.,  1.,  2.],\n                       [ 6.,  7.,  8.]],\n                      [[ 3.,  4.,  5.],\n                       [ 9., 10., 11.]],\n                      [[12., 13., 14.],\n                       [18., 19., 20.]],\n                      [[15., 16., 17.],\n                       [21., 22., 23.]]]), size=(4, 6), nnz=4,\n       dtype=torch.float64, layout=torch.sparse_bsc)\n\n```\n\n\n## Tools for working with sparse compressed tensors#\n\nAll sparse compressed tensors \u2014 CSR, CSC, BSR, and BSC tensors \u2014\nare conceptually very similar in that their indices data is split\ninto two parts: so-called compressed indices that use the CSR\nencoding, and so-called plain indices that are orthogonal to the\ncompressed indices. This allows various tools on these tensors to\nshare the same implementations that are parameterized by tensor\nlayout.\nSparse CSR, CSC, BSR, and CSC tensors can be constructed by usingtorch.sparse_compressed_tensor()function that have the same\ninterface as the above discussed constructor functionstorch.sparse_csr_tensor(),torch.sparse_csc_tensor(),torch.sparse_bsr_tensor(), andtorch.sparse_bsc_tensor(),\nrespectively, but with an extra requiredlayoutargument. The\nfollowing example illustrates a method of constructing CSR and CSC\ntensors using the same input data by specifying the corresponding\nlayout parameter to thetorch.sparse_compressed_tensor()function:\ntorch.sparse_compressed_tensor()\ntorch.sparse_csr_tensor()\ntorch.sparse_csc_tensor()\ntorch.sparse_bsr_tensor()\ntorch.sparse_bsc_tensor()\nlayout\ntorch.sparse_compressed_tensor()\n\n```python\n>>> compressed_indices = torch.tensor([0, 2, 4])\n>>> plain_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csr = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csr)\n>>> csr\ntensor(crow_indices=tensor([0, 2, 4]),\n       col_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,\n       layout=torch.sparse_csr)\n>>> csc = torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, layout=torch.sparse_csc)\n>>> csc\ntensor(ccol_indices=tensor([0, 2, 4]),\n       row_indices=tensor([0, 1, 0, 1]),\n       values=tensor([1, 2, 3, 4]), size=(2, 2), nnz=4,\n       layout=torch.sparse_csc)\n>>> (csr.transpose(0, 1).to_dense() == csc.to_dense()).all()\ntensor(True)\n\n```\n\n\n## Supported operations#\n\n\n## Linear Algebra operations#\n\nThe following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication.\nT[layout]\nM[layout]\nV[layout]\nf\n*\n@\nPyTorch operation\nSparse grad?\nLayout signature\ntorch.mv()\ntorch.mv()\nno\nM[sparse_coo]@V[strided]->V[strided]\nM[sparse_coo]@V[strided]->V[strided]\ntorch.mv()\ntorch.mv()\nno\nM[sparse_csr]@V[strided]->V[strided]\nM[sparse_csr]@V[strided]->V[strided]\ntorch.matmul()\ntorch.matmul()\nno\nM[sparse_coo]@M[strided]->M[strided]\nM[sparse_coo]@M[strided]->M[strided]\ntorch.matmul()\ntorch.matmul()\nno\nM[sparse_csr]@M[strided]->M[strided]\nM[sparse_csr]@M[strided]->M[strided]\ntorch.matmul()\ntorch.matmul()\nno\nM[SparseSemiStructured]@M[strided]->M[strided]\nM[SparseSemiStructured]@M[strided]->M[strided]\ntorch.matmul()\ntorch.matmul()\nno\nM[strided]@M[SparseSemiStructured]->M[strided]\nM[strided]@M[SparseSemiStructured]->M[strided]\ntorch.mm()\ntorch.mm()\nno\nM[sparse_coo]@M[strided]->M[strided]\nM[sparse_coo]@M[strided]->M[strided]\ntorch.mm()\ntorch.mm()\nno\nM[SparseSemiStructured]@M[strided]->M[strided]\nM[SparseSemiStructured]@M[strided]->M[strided]\ntorch.mm()\ntorch.mm()\nno\nM[strided]@M[SparseSemiStructured]->M[strided]\nM[strided]@M[SparseSemiStructured]->M[strided]\ntorch.sparse.mm()\ntorch.sparse.mm()\nyes\nM[sparse_coo]@M[strided]->M[strided]\nM[sparse_coo]@M[strided]->M[strided]\ntorch.smm()\ntorch.smm()\nno\nM[sparse_coo]@M[strided]->M[sparse_coo]\nM[sparse_coo]@M[strided]->M[sparse_coo]\ntorch.hspmm()\ntorch.hspmm()\nno\nM[sparse_coo]@M[strided]->M[hybridsparse_coo]\nM[sparse_coo]@M[strided]->M[hybridsparse_coo]\ntorch.bmm()\ntorch.bmm()\nno\nT[sparse_coo]@T[strided]->T[strided]\nT[sparse_coo]@T[strided]->T[strided]\ntorch.addmm()\ntorch.addmm()\nno\nf*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]\nf*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]\ntorch.addmm()\ntorch.addmm()\nno\nf*M[strided]+f*(M[SparseSemiStructured]@M[strided])->M[strided]\nf*M[strided]+f*(M[SparseSemiStructured]@M[strided])->M[strided]\ntorch.addmm()\ntorch.addmm()\nno\nf*M[strided]+f*(M[strided]@M[SparseSemiStructured])->M[strided]\nf*M[strided]+f*(M[strided]@M[SparseSemiStructured])->M[strided]\ntorch.sparse.addmm()\ntorch.sparse.addmm()\nyes\nf*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]\nf*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided]\ntorch.sparse.spsolve()\ntorch.sparse.spsolve()\nno\nSOLVE(M[sparse_csr],V[strided])->V[strided]\nSOLVE(M[sparse_csr],V[strided])->V[strided]\ntorch.sspaddmm()\ntorch.sspaddmm()\nno\nf*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo]\nf*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo]\ntorch.lobpcg()\ntorch.lobpcg()\nno\nGENEIG(M[sparse_coo])->M[strided],M[strided]\nGENEIG(M[sparse_coo])->M[strided],M[strided]\ntorch.pca_lowrank()\ntorch.pca_lowrank()\nyes\nPCA(M[sparse_coo])->M[strided],M[strided],M[strided]\nPCA(M[sparse_coo])->M[strided],M[strided],M[strided]\ntorch.svd_lowrank()\ntorch.svd_lowrank()\nyes\nSVD(M[sparse_coo])->M[strided],M[strided],M[strided]\nSVD(M[sparse_coo])->M[strided],M[strided],M[strided]\nwhere \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments.\ntorch.smm()\nNote\nCurrently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t().\nM[strided]@M[sparse_coo]\nD@S==(S.t()@D.t()).t()\n\n## Tensor methods and sparse#\n\nThe following Tensor methods are related to sparse tensors:\nTensor.is_sparse\nTensor.is_sparse\nIsTrueif the Tensor uses sparse COO storage layout,Falseotherwise.\nTrue\nFalse\nTensor.is_sparse_csr\nTensor.is_sparse_csr\nIsTrueif the Tensor uses sparse CSR storage layout,Falseotherwise.\nTrue\nFalse\nTensor.dense_dim\nTensor.dense_dim\nReturn the number of dense dimensions in asparse tensorself.\nself\nTensor.sparse_dim\nTensor.sparse_dim\nReturn the number of sparse dimensions in asparse tensorself.\nself\nTensor.sparse_mask\nTensor.sparse_mask\nReturns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask.\nself\nmask\nTensor.to_sparse\nTensor.to_sparse\nReturns a sparse copy of the tensor.\nTensor.to_sparse_coo\nTensor.to_sparse_coo\nConvert a tensor tocoordinate format.\nTensor.to_sparse_csr\nTensor.to_sparse_csr\nConvert a tensor to compressed row storage format (CSR).\nTensor.to_sparse_csc\nTensor.to_sparse_csc\nConvert a tensor to compressed column storage (CSC) format.\nTensor.to_sparse_bsr\nTensor.to_sparse_bsr\nConvert a tensor to a block sparse row (BSR) storage format of given blocksize.\nTensor.to_sparse_bsc\nTensor.to_sparse_bsc\nConvert a tensor to a block sparse column (BSC) storage format of given blocksize.\nTensor.to_dense\nTensor.to_dense\nCreates a strided copy ofselfifselfis not a strided tensor, otherwise returnsself.\nself\nself\nself\nTensor.values\nTensor.values\nReturn the values tensor of asparse COO tensor.\nThe following Tensor methods are specific to sparse COO tensors:\nTensor.coalesce\nTensor.coalesce\nReturns a coalesced copy ofselfifselfis anuncoalesced tensor.\nself\nself\nTensor.sparse_resize_\nTensor.sparse_resize_\nResizesselfsparse tensorto the desired size and the number of sparse and dense dimensions.\nself\nTensor.sparse_resize_and_clear_\nTensor.sparse_resize_and_clear_\nRemoves all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions.\nself\nself\nTensor.is_coalesced\nTensor.is_coalesced\nReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise.\nTrue\nself\nFalse\nTensor.indices\nTensor.indices\nReturn the indices tensor of asparse COO tensor.\nThe following methods are specific tosparse CSR tensorsandsparse BSR tensors:\nTensor.crow_indices\nTensor.crow_indices\nReturns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.\nself\nself\nsparse_csr\nTensor.col_indices\nTensor.col_indices\nReturns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr.\nself\nself\nsparse_csr\nThe following methods are specific tosparse CSC tensorsandsparse BSC tensors:\nTensor.row_indices\nTensor.row_indices\n\nTensor.ccol_indices\nTensor.ccol_indices\n\nThe following Tensor methods support sparse COO tensors:\nadd()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_()\nadd()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()\n\n## Torch functions specific to sparse Tensors#\n\nsparse_coo_tensor\n\nsparse_coo_tensor\nConstructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.\nindices\nsparse_csr_tensor\n\nsparse_csr_tensor\nConstructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices.\ncrow_indices\ncol_indices\nsparse_csc_tensor\n\nsparse_csc_tensor\nConstructs asparse tensor in CSC (Compressed Sparse Column)with specified values at the givenccol_indicesandrow_indices.\nccol_indices\nrow_indices\nsparse_bsr_tensor\n\nsparse_bsr_tensor\nConstructs asparse tensor in BSR (Block Compressed Sparse Row))with specified 2-dimensional blocks at the givencrow_indicesandcol_indices.\ncrow_indices\ncol_indices\nsparse_bsc_tensor\n\nsparse_bsc_tensor\nConstructs asparse tensor in BSC (Block Compressed Sparse Column))with specified 2-dimensional blocks at the givenccol_indicesandrow_indices.\nccol_indices\nrow_indices\nsparse_compressed_tensor\n\nsparse_compressed_tensor\nConstructs asparse tensor in Compressed Sparse format - CSR, CSC, BSR, or BSC -with specified values at the givencompressed_indicesandplain_indices.\ncompressed_indices\nplain_indices\nsparse.sum\nsparse.sum\nReturn the sum of each row of the given sparse tensor.\nsparse.addmm\nsparse.addmm\nThis function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse COO matrixmat1.\ntorch.addmm()\nmat1\nsparse.sampled_addmm\nsparse.sampled_addmm\nPerforms a matrix multiplication of the dense matricesmat1andmat2at the locations specified by the sparsity pattern ofinput.\nmat1\nmat2\ninput\nsparse.mm\nsparse.mm\nPerforms a matrix multiplication of the sparse matrixmat1\nmat1\nsspaddmm\n\nsspaddmm\nMatrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.\nmat1\nmat2\ninput\nhspmm\n\nhspmm\nPerforms a matrix multiplication of asparse COO matrixmat1and a strided matrixmat2.\nmat1\nmat2\nsmm\n\nsmm\nPerforms a matrix multiplication of the sparse matrixinputwith the dense matrixmat.\ninput\nmat\nsparse.softmax\nsparse.softmax\nApplies a softmax function.\nsparse.spsolve\nsparse.spsolve\nComputes the solution of a square system of linear equations with a unique solution.\nsparse.log_softmax\nsparse.log_softmax\nApplies a softmax function followed by logarithm.\nsparse.spdiags\nsparse.spdiags\nCreates a sparse 2D tensor by placing the values from rows ofdiagonalsalong specified diagonals of the output\ndiagonals\n\n## Other functions#\n\nThe followingtorchfunctions support sparse tensors:\ntorch\ncat()dstack()empty()empty_like()hstack()index_select()is_complex()is_floating_point()is_nonzero()is_same_size()is_signed()is_tensor()lobpcg()mm()native_norm()pca_lowrank()select()stack()svd_lowrank()unsqueeze()vstack()zeros()zeros_like()\ncat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like()\nTo manage checking sparse tensor invariants, see:\nsparse.check_sparse_tensor_invariants\nsparse.check_sparse_tensor_invariants\nA tool to control checking sparse tensor invariants.\nTo use sparse tensors withgradcheck()function,\nsee:\ngradcheck()\nsparse.as_sparse_gradcheck\nsparse.as_sparse_gradcheck\nDecorate function, to extend gradcheck for sparse tensors.\n\n## Zero-preserving unary functions#\n\nWe aim to support all \u2018zero-preserving unary functions\u2019: functions of one argument that map zero to zero.\nIf you find that we are missing a zero-preserving unary function\nthat you need, please feel encouraged to open an issue for a feature request.\nAs always please kindly try the search function first before opening an issue.\nThe following operators currently support sparse COO/CSR/CSC/BSR/CSR tensor inputs.\nabs()asin()asinh()atan()atanh()ceil()conj_physical()floor()log1p()neg()round()sin()sinh()sign()sgn()signbit()tan()tanh()trunc()expm1()sqrt()angle()isinf()isposinf()isneginf()isnan()erf()erfinv()\nabs()\nasin()\nasinh()\natan()\natanh()\nceil()\nconj_physical()\nfloor()\nlog1p()\nneg()\nround()\nsin()\nsinh()\nsign()\nsgn()\nsignbit()\ntan()\ntanh()\ntrunc()\nexpm1()\nsqrt()\nangle()\nisinf()\nisposinf()\nisneginf()\nisnan()\nerf()\nerfinv()",
    "url": "https://pytorch.org/docs/stable/sparse.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1eeff3ace9437f2725238fd7c9ca44bc",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/community/contribution_guide.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6207ddff238a670c061cc8221b56f857",
    "source": "pytorch_docs",
    "title": "Quantization API Reference \u2014 PyTorch 2.9 documentation",
    "text": "\n## Quantization API Reference#\n\nCreated On: Jul 25, 2020 | Last Updated On: Jun 18, 2025\n\n## torch.ao.quantization#\n\nThis module contains Eager mode quantization APIs.\n\n## Top level APIs#\n\nquantize\n\nquantize\nQuantize the input float model with post training static quantization.\nquantize_dynamic\n\nquantize_dynamic\nConverts a float model to dynamic (i.e.\nquantize_qat\n\nquantize_qat\nDo quantization aware training and output a quantized model\nprepare\n\nprepare\nPrepares a copy of the model for quantization calibration or quantization-aware training.\nprepare_qat\n\nprepare_qat\nPrepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.\nconvert\n\nconvert\nConverts submodules in input module to a different module according tomappingby callingfrom_floatmethod on the target module class.\n\n## Preparing model for quantization#\n\nfuse_modules.fuse_modules\nfuse_modules.fuse_modules\nFuse a list of modules into a single module.\nQuantStub\n\nQuantStub\nQuantize stub module, before calibration, this is same as an observer, it will be swapped asnnq.Quantizeinconvert.\nDeQuantStub\n\nDeQuantStub\nDequantize stub module, before calibration, this is same as identity, this will be swapped asnnq.DeQuantizeinconvert.\nQuantWrapper\n\nQuantWrapper\nA wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.\nadd_quant_dequant\n\nadd_quant_dequant\nWrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.\n\n## Utility functions#\n\nswap_module\n\nswap_module\nSwaps the module if it has a quantized counterpart and it has anobserverattached.\npropagate_qconfig_\n\npropagate_qconfig_\nPropagate qconfig through the module hierarchy and assignqconfigattribute on each leaf module\ndefault_eval_fn\n\ndefault_eval_fn\nDefine the default evaluation function.\n\n## torch.ao.quantization.quantize_fx#\n\nThis module contains FX graph mode quantization APIs (prototype).\nprepare_fx\n\nprepare_fx\nPrepare a model for post training quantization\nprepare_qat_fx\n\nprepare_qat_fx\nPrepare a model for quantization aware training\nconvert_fx\n\nconvert_fx\nConvert a calibrated or trained model to a quantized model\nfuse_fx\n\nfuse_fx\nFuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode.\n\n## torch.ao.quantization.qconfig_mapping#\n\nThis module contains QConfigMapping for configuring FX graph mode quantization.\nQConfigMapping\n\nQConfigMapping\nMapping from model ops totorch.ao.quantization.QConfigs.\ntorch.ao.quantization.QConfig\nget_default_qconfig_mapping\n\nget_default_qconfig_mapping\nReturn the default QConfigMapping for post training quantization.\nget_default_qat_qconfig_mapping\n\nget_default_qat_qconfig_mapping\nReturn the default QConfigMapping for quantization aware training.\n\n## torch.ao.quantization.backend_config#\n\nThis module contains BackendConfig, a config object that defines how quantization is supported\nin a backend. Currently only used by FX Graph Mode Quantization, but we may extend Eager Mode\nQuantization to work with this as well.\nBackendConfig\n\nBackendConfig\nConfig that defines the set of patterns that can be quantized on a given backend, and how reference quantized models can be produced from these patterns.\nBackendPatternConfig\n\nBackendPatternConfig\nConfig object that specifies quantization behavior for a given operator pattern.\nDTypeConfig\n\nDTypeConfig\nConfig object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases.\nDTypeWithConstraints\n\nDTypeWithConstraints\nConfig for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used inDTypeConfig.\nDTypeConfig\nObservationType\n\nObservationType\nAn enum that represents different ways of how an operator/operator pattern should be observed\n\n## torch.ao.quantization.fx.custom_config#\n\nThis module contains a few CustomConfig classes that\u2019s used in both eager mode and FX graph mode quantization\nFuseCustomConfig\n\nFuseCustomConfig\nCustom configuration forfuse_fx().\nfuse_fx()\nPrepareCustomConfig\n\nPrepareCustomConfig\nCustom configuration forprepare_fx()andprepare_qat_fx().\nprepare_fx()\nprepare_qat_fx()\nConvertCustomConfig\n\nConvertCustomConfig\nCustom configuration forconvert_fx().\nconvert_fx()\nStandaloneModuleConfigEntry\n\nStandaloneModuleConfigEntry\n\n\n## torch.ao.quantization.quantizer#\n\n\n## torch.ao.quantization.pt2e (quantization in pytorch 2.0 export implementation)#\n\n\n## torch.ao.quantization.pt2e.export_utils#\n\nmodel_is_exported\n\nmodel_is_exported\nReturn True if thetorch.nn.Modulewas exported, False otherwise (e.g.\n\n## torch.ao.quantization.pt2e.lowering#\n\nlower_pt2e_quantized_to_x86\n\nlower_pt2e_quantized_to_x86\nLower a PT2E-qantized model to x86 backend.\n\n## PT2 Export (pt2e) Numeric Debugger#\n\ngenerate_numeric_debug_handle\n\ngenerate_numeric_debug_handle\nAttach numeric_debug_handle_id for all nodes in the graph module of the given ExportedProgram, like conv2d, squeeze, conv1d, etc, except for placeholder.\nCUSTOM_KEY\n\nCUSTOM_KEY\nstr(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str\nNUMERIC_DEBUG_HANDLE_KEY\n\nNUMERIC_DEBUG_HANDLE_KEY\nstr(object='') -> str str(bytes_or_buffer[, encoding[, errors]]) -> str\nprepare_for_propagation_comparison\n\nprepare_for_propagation_comparison\nAdd output loggers to node that has numeric_debug_handle\nextract_results_from_loggers\n\nextract_results_from_loggers\nFor a given model, extract the tensors stats and related information for each debug handle.\ncompare_results\n\ncompare_results\nGiven two dict mapping fromdebug_handle_id(int) to list of tensors return a map fromdebug_handle_idtoNodeAccuracySummarythat contains comparison information like SQNR, MSE etc.\n\n## torch (quantization related functions)#\n\nThis describes the quantization related functions of thetorchnamespace.\ntorch\nquantize_per_tensor\n\nquantize_per_tensor\nConverts a float tensor to a quantized tensor with given scale and zero point.\nquantize_per_channel\n\nquantize_per_channel\nConverts a float tensor to a per-channel quantized tensor with given scales and zero points.\ndequantize\n\ndequantize\nReturns an fp32 Tensor by dequantizing a quantized Tensor\n\n## torch.Tensor (quantization related methods)#\n\nQuantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor.\nview\n\nview\nReturns a new tensor with the same data as theselftensor but of a differentshape.\nself\nshape\nas_strided\n\nas_strided\nSeetorch.as_strided()\ntorch.as_strided()\nexpand\n\nexpand\nReturns a new view of theselftensor with singleton dimensions expanded to a larger size.\nself\nflatten\n\nflatten\nSeetorch.flatten()\ntorch.flatten()\nselect\n\nselect\nSeetorch.select()\ntorch.select()\nne\n\nne\nSeetorch.ne().\ntorch.ne()\neq\n\neq\nSeetorch.eq()\ntorch.eq()\nge\n\nge\nSeetorch.ge().\ntorch.ge()\nle\n\nle\nSeetorch.le().\ntorch.le()\ngt\n\ngt\nSeetorch.gt().\ntorch.gt()\nlt\n\nlt\nSeetorch.lt().\ntorch.lt()\ncopy_\n\ncopy_\nCopies the elements fromsrcintoselftensor and returnsself.\nsrc\nself\nself\nclone\n\nclone\nSeetorch.clone()\ntorch.clone()\ndequantize\n\ndequantize\nGiven a quantized Tensor, dequantize it and return the dequantized float Tensor.\nequal\n\nequal\nSeetorch.equal()\ntorch.equal()\nint_repr\n\nint_repr\nGiven a quantized Tensor,self.int_repr()returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.\nself.int_repr()\nmax\n\nmax\nSeetorch.max()\ntorch.max()\nmean\n\nmean\nSeetorch.mean()\ntorch.mean()\nmin\n\nmin\nSeetorch.min()\ntorch.min()\nq_scale\n\nq_scale\nGiven a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().\nq_zero_point\n\nq_zero_point\nGiven a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().\nq_per_channel_scales\n\nq_per_channel_scales\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.\nq_per_channel_zero_points\n\nq_per_channel_zero_points\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.\nq_per_channel_axis\n\nq_per_channel_axis\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.\nresize_\n\nresize_\nResizesselftensor to the specified size.\nself\nsort\n\nsort\nSeetorch.sort()\ntorch.sort()\ntopk\n\ntopk\nSeetorch.topk()\ntorch.topk()\n\n## torch.ao.quantization.observer#\n\nThis module contains observers which are used to collect statistics about\nthe values observed during calibration (PTQ) or training (QAT).\nObserverBase\n\nObserverBase\nBase observer Module.\nMinMaxObserver\n\nMinMaxObserver\nObserver module for computing the quantization parameters based on the running min and max values.\nMovingAverageMinMaxObserver\n\nMovingAverageMinMaxObserver\nObserver module for computing the quantization parameters based on the moving average of the min and max values.\nPerChannelMinMaxObserver\n\nPerChannelMinMaxObserver\nObserver module for computing the quantization parameters based on the running per channel min and max values.\nMovingAveragePerChannelMinMaxObserver\n\nMovingAveragePerChannelMinMaxObserver\nObserver module for computing the quantization parameters based on the running per channel min and max values.\nHistogramObserver\n\nHistogramObserver\nThe module records the running histogram of tensor values along with min/max values.\nPlaceholderObserver\n\nPlaceholderObserver\nObserver that doesn't do anything and just passes its configuration to the quantized module's.from_float().\n.from_float()\nRecordingObserver\n\nRecordingObserver\nThe module is mainly for debug and records the tensor values during runtime.\nNoopObserver\n\nNoopObserver\nObserver that doesn't do anything and just passes its configuration to the quantized module's.from_float().\n.from_float()\nget_observer_state_dict\n\nget_observer_state_dict\nReturns the state dict corresponding to the observer stats.\nload_observer_state_dict\n\nload_observer_state_dict\nGiven input model and a state_dict containing model observer stats, load the stats back into the model.\ndefault_observer\n\ndefault_observer\nDefault observer for static quantization, usually used for debugging.\ndefault_placeholder_observer\n\ndefault_placeholder_observer\nDefault placeholder observer, usually used for quantization to torch.float16.\ndefault_debug_observer\n\ndefault_debug_observer\nDefault debug-only observer.\ndefault_weight_observer\n\ndefault_weight_observer\nDefault weight observer.\ndefault_histogram_observer\n\ndefault_histogram_observer\nDefault histogram observer, usually used for PTQ.\ndefault_per_channel_weight_observer\n\ndefault_per_channel_weight_observer\nDefault per-channel weight observer, usually used on backends where per-channel weight quantization is supported, such asfbgemm.\ndefault_dynamic_quant_observer\n\ndefault_dynamic_quant_observer\nDefault observer for dynamic quantization.\ndefault_float_qparams_observer\n\ndefault_float_qparams_observer\nDefault observer for a floating point zero-point.\nAffineQuantizedObserverBase\n\nAffineQuantizedObserverBase\nObserver module for affine quantization (pytorch/ao)\nGranularity\n\nGranularity\nBase class for representing the granularity of quantization.\nMappingType\n\nMappingType\nHow floating point number is mapped to integer number\nPerAxis\n\nPerAxis\nRepresents per-axis granularity in quantization.\nPerBlock\n\nPerBlock\nRepresents per-block granularity in quantization.\nPerGroup\n\nPerGroup\nRepresents per-channel group granularity in quantization.\nPerRow\n\nPerRow\nRepresents row-wise granularity in quantization.\nPerTensor\n\nPerTensor\nRepresents per-tensor granularity in quantization.\nPerToken\n\nPerToken\nRepresents per-token granularity in quantization.\nTorchAODType\n\nTorchAODType\nPlaceholder for dtypes that do not exist in PyTorch core yet.\nZeroPointDomain\n\nZeroPointDomain\nEnum that indicate whether zero_point is in integer domain or floating point domain\nget_block_size\n\nget_block_size\nGet the block size based on the input shape and granularity type.\n\n## torch.ao.quantization.fake_quantize#\n\nThis module implements modules which are used to perform fake quantization\nduring QAT.\nFakeQuantizeBase\n\nFakeQuantizeBase\nBase fake quantize module.\nFakeQuantize\n\nFakeQuantize\nSimulate the quantize and dequantize operations in training time.\nFixedQParamsFakeQuantize\n\nFixedQParamsFakeQuantize\nSimulate quantize and dequantize in training time.\nFusedMovingAvgObsFakeQuantize\n\nFusedMovingAvgObsFakeQuantize\nDefine a fused module to observe the tensor.\ndefault_fake_quant\n\ndefault_fake_quant\nDefault fake_quant for activations.\ndefault_weight_fake_quant\n\ndefault_weight_fake_quant\nDefault fake_quant for weights.\ndefault_per_channel_weight_fake_quant\n\ndefault_per_channel_weight_fake_quant\nDefault fake_quant for per-channel weights.\ndefault_histogram_fake_quant\n\ndefault_histogram_fake_quant\nFake_quant for activations using a histogram..\ndefault_fused_act_fake_quant\n\ndefault_fused_act_fake_quant\nFused version ofdefault_fake_quant, with improved performance.\ndefault_fused_wt_fake_quant\n\ndefault_fused_wt_fake_quant\nFused version ofdefault_weight_fake_quant, with improved performance.\ndefault_fused_per_channel_wt_fake_quant\n\ndefault_fused_per_channel_wt_fake_quant\nFused version ofdefault_per_channel_weight_fake_quant, with improved performance.\ndisable_fake_quant\n\ndisable_fake_quant\nDisable fake quantization for the module.\nenable_fake_quant\n\nenable_fake_quant\nEnable fake quantization for the module.\ndisable_observer\n\ndisable_observer\nDisable observation for this module.\nenable_observer\n\nenable_observer\nEnable observation for this module.\n\n## torch.ao.quantization.qconfig#\n\nThis module definesQConfigobjects which are used\nto configure quantization settings for individual ops.\nQConfig\nQConfig\n\nQConfig\nDescribes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.\ndefault_qconfig\n\ndefault_qconfig\nDefault qconfig configuration.\ndefault_debug_qconfig\n\ndefault_debug_qconfig\nDefault qconfig configuration for debugging.\ndefault_per_channel_qconfig\n\ndefault_per_channel_qconfig\nDefault qconfig configuration for per channel weight quantization.\ndefault_dynamic_qconfig\n\ndefault_dynamic_qconfig\nDefault dynamic qconfig.\nfloat16_dynamic_qconfig\n\nfloat16_dynamic_qconfig\nDynamic qconfig with weights quantized totorch.float16.\nfloat16_static_qconfig\n\nfloat16_static_qconfig\nDynamic qconfig with both activations and weights quantized totorch.float16.\nper_channel_dynamic_qconfig\n\nper_channel_dynamic_qconfig\nDynamic qconfig with weights quantized per channel.\nfloat_qparams_weight_only_qconfig\n\nfloat_qparams_weight_only_qconfig\nDynamic qconfig with weights quantized with a floating point zero_point.\ndefault_qat_qconfig\n\ndefault_qat_qconfig\nDefault qconfig for QAT.\ndefault_weight_only_qconfig\n\ndefault_weight_only_qconfig\nDefault qconfig for quantizing weights only.\ndefault_activation_only_qconfig\n\ndefault_activation_only_qconfig\nDefault qconfig for quantizing activations only.\ndefault_qat_qconfig_v2\n\ndefault_qat_qconfig_v2\nFused version ofdefault_qat_config, has performance benefits.\n\n## torch.ao.nn.intrinsic#\n\nThis module implements the combined (fused) modules conv + relu which can\nthen be quantized.\nConvReLU1d\n\nConvReLU1d\nThis is a sequential container which calls the Conv1d and ReLU modules.\nConvReLU2d\n\nConvReLU2d\nThis is a sequential container which calls the Conv2d and ReLU modules.\nConvReLU3d\n\nConvReLU3d\nThis is a sequential container which calls the Conv3d and ReLU modules.\nLinearReLU\n\nLinearReLU\nThis is a sequential container which calls the Linear and ReLU modules.\nConvBn1d\n\nConvBn1d\nThis is a sequential container which calls the Conv 1d and Batch Norm 1d modules.\nConvBn2d\n\nConvBn2d\nThis is a sequential container which calls the Conv 2d and Batch Norm 2d modules.\nConvBn3d\n\nConvBn3d\nThis is a sequential container which calls the Conv 3d and Batch Norm 3d modules.\nConvBnReLU1d\n\nConvBnReLU1d\nThis is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules.\nConvBnReLU2d\n\nConvBnReLU2d\nThis is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules.\nConvBnReLU3d\n\nConvBnReLU3d\nThis is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules.\nBNReLU2d\n\nBNReLU2d\nThis is a sequential container which calls the BatchNorm 2d and ReLU modules.\nBNReLU3d\n\nBNReLU3d\nThis is a sequential container which calls the BatchNorm 3d and ReLU modules.\n\n## torch.ao.nn.intrinsic.qat#\n\nThis module implements the versions of those fused operations needed for\nquantization aware training.\nLinearReLU\n\nLinearReLU\nA LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBn1d\n\nConvBn1d\nA ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBnReLU1d\n\nConvBnReLU1d\nA ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBn2d\n\nConvBn2d\nA ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBnReLU2d\n\nConvBnReLU2d\nA ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvReLU2d\n\nConvReLU2d\nA ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.\nConvBn3d\n\nConvBn3d\nA ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvBnReLU3d\n\nConvBnReLU3d\nA ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.\nConvReLU3d\n\nConvReLU3d\nA ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.\nupdate_bn_stats\n\nupdate_bn_stats\n\nfreeze_bn_stats\n\nfreeze_bn_stats\n\n\n## torch.ao.nn.intrinsic.quantized#\n\nThis module implements the quantized implementations of fused operations\nlike conv + relu. No BatchNorm variants as it\u2019s usually folded into convolution\nfor inference.\nBNReLU2d\n\nBNReLU2d\nA BNReLU2d module is a fused module of BatchNorm2d and ReLU\nBNReLU3d\n\nBNReLU3d\nA BNReLU3d module is a fused module of BatchNorm3d and ReLU\nConvReLU1d\n\nConvReLU1d\nA ConvReLU1d module is a fused module of Conv1d and ReLU\nConvReLU2d\n\nConvReLU2d\nA ConvReLU2d module is a fused module of Conv2d and ReLU\nConvReLU3d\n\nConvReLU3d\nA ConvReLU3d module is a fused module of Conv3d and ReLU\nLinearReLU\n\nLinearReLU\nA LinearReLU module fused from Linear and ReLU modules\n\n## torch.ao.nn.intrinsic.quantized.dynamic#\n\nThis module implements the quantized dynamic implementations of fused operations\nlike linear + relu.\nLinearReLU\n\nLinearReLU\nA LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization.\n\n## torch.ao.nn.qat#\n\nThis module implements versions of the key nn modulesConv2d()andLinear()which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization.\nConv2d\n\nConv2d\nA Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.\nConv3d\n\nConv3d\nA Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training.\nLinear\n\nLinear\nA linear module attached with FakeQuantize modules for weight, used for quantization aware training.\n\n## torch.ao.nn.qat.dynamic#\n\nThis module implements versions of the key nn modules such asLinear()which run in FP32 but with rounding applied to simulate the effect of INT8\nquantization and will be dynamically quantized during inference.\nLinear\n\nLinear\nA linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training.\n\n## torch.ao.nn.quantized#\n\nThis module implements the quantized versions of the nn layers such as~torch.nn.Conv2dandtorch.nn.ReLU.\n~torch.nn.Conv2d\ntorch.nn.ReLU\nReLU6\n\nReLU6\nApplies the element-wise function:\nHardswish\n\nHardswish\nThis is the quantized version ofHardswish.\nHardswish\nELU\n\nELU\nThis is the quantized equivalent ofELU.\nELU\nLeakyReLU\n\nLeakyReLU\nThis is the quantized equivalent ofLeakyReLU.\nLeakyReLU\nSigmoid\n\nSigmoid\nThis is the quantized equivalent ofSigmoid.\nSigmoid\nBatchNorm2d\n\nBatchNorm2d\nThis is the quantized version ofBatchNorm2d.\nBatchNorm2d\nBatchNorm3d\n\nBatchNorm3d\nThis is the quantized version ofBatchNorm3d.\nBatchNorm3d\nConv1d\n\nConv1d\nApplies a 1D convolution over a quantized input signal composed of several quantized input planes.\nConv2d\n\nConv2d\nApplies a 2D convolution over a quantized input signal composed of several quantized input planes.\nConv3d\n\nConv3d\nApplies a 3D convolution over a quantized input signal composed of several quantized input planes.\nConvTranspose1d\n\nConvTranspose1d\nApplies a 1D transposed convolution operator over an input image composed of several input planes.\nConvTranspose2d\n\nConvTranspose2d\nApplies a 2D transposed convolution operator over an input image composed of several input planes.\nConvTranspose3d\n\nConvTranspose3d\nApplies a 3D transposed convolution operator over an input image composed of several input planes.\nEmbedding\n\nEmbedding\nA quantized Embedding module with quantized packed weights as inputs.\nEmbeddingBag\n\nEmbeddingBag\nA quantized EmbeddingBag module with quantized packed weights as inputs.\nFloatFunctional\n\nFloatFunctional\nState collector class for float operations.\nFXFloatFunctional\n\nFXFloatFunctional\nmodule to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly\nQFunctional\n\nQFunctional\nWrapper class for quantized operations.\nLinear\n\nLinear\nA quantized linear module with quantized tensor as inputs and outputs.\nLayerNorm\n\nLayerNorm\nThis is the quantized version ofLayerNorm.\nLayerNorm\nGroupNorm\n\nGroupNorm\nThis is the quantized version ofGroupNorm.\nGroupNorm\nInstanceNorm1d\n\nInstanceNorm1d\nThis is the quantized version ofInstanceNorm1d.\nInstanceNorm1d\nInstanceNorm2d\n\nInstanceNorm2d\nThis is the quantized version ofInstanceNorm2d.\nInstanceNorm2d\nInstanceNorm3d\n\nInstanceNorm3d\nThis is the quantized version ofInstanceNorm3d.\nInstanceNorm3d\n\n## torch.ao.nn.quantized.functional#\n\nFunctional interface (quantized).\nThis module implements the quantized versions of the functional layers such as~torch.nn.functional.conv2dandtorch.nn.functional.relu. Note:torch.nn.functional.relu~torch.nn.functional.relutorch.nn.functional.relusupports quantized inputs.\navg_pool2d\n\navg_pool2d\nApplies 2D average-pooling operation inkH\u00d7kWkH \\times kWkH\u00d7kWregions by step sizesH\u00d7sWsH \\times sWsH\u00d7sWsteps.\navg_pool3d\n\navg_pool3d\nApplies 3D average-pooling operation inkDtimeskH\u00d7kWkD \\ times kH \\times kWkDtimeskH\u00d7kWregions by step sizesD\u00d7sH\u00d7sWsD \\times sH \\times sWsD\u00d7sH\u00d7sWsteps.\nadaptive_avg_pool2d\n\nadaptive_avg_pool2d\nApplies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.\nadaptive_avg_pool3d\n\nadaptive_avg_pool3d\nApplies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes.\nconv1d\n\nconv1d\nApplies a 1D convolution over a quantized 1D input composed of several input planes.\nconv2d\n\nconv2d\nApplies a 2D convolution over a quantized 2D input composed of several input planes.\nconv3d\n\nconv3d\nApplies a 3D convolution over a quantized 3D input composed of several input planes.\ninterpolate\n\ninterpolate\nDown/up samples the input to either the givensizeor the givenscale_factor\nsize\nscale_factor\nlinear\n\nlinear\nApplies a linear transformation to the incoming quantized data:y=xAT+by = xA^T + by=xAT+b.\nmax_pool1d\n\nmax_pool1d\nApplies a 1D max pooling over a quantized input signal composed of several quantized input planes.\nmax_pool2d\n\nmax_pool2d\nApplies a 2D max pooling over a quantized input signal composed of several quantized input planes.\ncelu\n\ncelu\nApplies the quantized CELU function element-wise.\nleaky_relu\n\nleaky_relu\nQuantized version of the.\nhardtanh\n\nhardtanh\nThis is the quantized version ofhardtanh().\nhardtanh()\nhardswish\n\nhardswish\nThis is the quantized version ofhardswish().\nhardswish()\nthreshold\n\nthreshold\nApplies the quantized version of the threshold function element-wise:\nelu\n\nelu\nThis is the quantized version ofelu().\nelu()\nhardsigmoid\n\nhardsigmoid\nThis is the quantized version ofhardsigmoid().\nhardsigmoid()\nclamp\n\nclamp\nfloat(input, min_, max_) -> Tensor\nupsample\n\nupsample\nUpsamples the input to either the givensizeor the givenscale_factor\nsize\nscale_factor\nupsample_bilinear\n\nupsample_bilinear\nUpsamples the input, using bilinear upsampling.\nupsample_nearest\n\nupsample_nearest\nUpsamples the input, using nearest neighbours' pixel values.\n\n## torch.ao.nn.quantizable#\n\nThis module implements the quantizable versions of some of the nn layers.\nThese modules can be used in conjunction with the custom module mechanism,\nby providing thecustom_module_configargument to both prepare and convert.\ncustom_module_config\nLSTM\n\nLSTM\nA quantizable long short-term memory (LSTM).\nMultiheadAttention\n\nMultiheadAttention\n\n\n## torch.ao.nn.quantized.dynamic#\n\nDynamically quantizedLinear,LSTM,LSTMCell,GRUCell, andRNNCell.\nLinear\nLSTM\nLSTMCell\nGRUCell\nRNNCell\nLinear\n\nLinear\nA dynamic quantized linear module with floating point tensor as inputs and outputs.\nLSTM\n\nLSTM\nA dynamic quantized LSTM module with floating point tensor as inputs and outputs.\nGRU\n\nGRU\nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\nRNNCell\n\nRNNCell\nAn Elman RNN cell with tanh or ReLU non-linearity.\nLSTMCell\n\nLSTMCell\nA long short-term memory (LSTM) cell.\nGRUCell\n\nGRUCell\nA gated recurrent unit (GRU) cell\n\n## Quantized dtypes and quantization schemes#\n\nNote that operator implementations currently only\nsupport per channel quantization for weights of theconvandlinearoperators. Furthermore, the input data is\nmapped linearly to the quantized data and vice versa\nas follows:\nwhereclamp(.)\\text{clamp}(.)clamp(.)is the same asclamp()while the\nscalesssand zero pointzzzare then computed\nas described inMinMaxObserver, specifically:\nclamp()\nMinMaxObserver\nwhere :math:[x_\\text{min},x_\\text{max}]denotes the range of the input data while\n:math:Q_\\text{min}and :math:Q_\\text{max}are respectively the minimum and maximum values of the quantized dtype.\n[x_\\text{min},x_\\text{max}]\nQ_\\text{min}\nQ_\\text{max}\nNote that the choice of :math:sand :math:zimplies that zero is represented with no quantization error whenever zero is within\nthe range of the input data or symmetric quantization is being used.\ns\nz\nAdditional data types and quantization schemes can be implemented through\nthecustomoperatormechanism<https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>_.\ncustomoperatormechanism<https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>\ntorch.qscheme\u2014 Type to describe the quantization scheme of a tensor.\nSupported types:\ntorch.qscheme\ntorch.per_tensor_affine\u2014 per tensor, asymmetric\ntorch.per_tensor_affine\ntorch.per_channel_affine\u2014 per channel, asymmetric\ntorch.per_channel_affine\ntorch.per_tensor_symmetric\u2014 per tensor, symmetric\ntorch.per_tensor_symmetric\ntorch.per_channel_symmetric\u2014 per channel, symmetric\ntorch.per_channel_symmetric\ntorch.dtype\u2014 Type to describe the data. Supported types:\ntorch.dtype\ntorch.quint8\u2014 8-bit unsigned integer\ntorch.quint8\ntorch.qint8\u2014 8-bit signed integer\ntorch.qint8\ntorch.qint32\u2014 32-bit signed integer\ntorch.qint32\nQAT Modules.\nThis package is in the process of being deprecated.\nPlease, usetorch.ao.nn.qat.modulesinstead.\nQAT Dynamic Modules.\nThis package is in the process of being deprecated.\nPlease, usetorch.ao.nn.qat.dynamicinstead.\nThis file is in the process of migration totorch/ao/quantization, and\nis kept here for compatibility while the migration process is ongoing.\nIf you are adding a new entry/functionality, please, add it to the\nappropriate files undertorch/ao/quantization/fx/, while adding an import statement\nhere.\nQAT Dynamic Modules.\nThis package is in the process of being deprecated.\nPlease, usetorch.ao.nn.qat.dynamicinstead.\nQuantized Modules.\nThetorch.nn.quantizednamespace is in the process of being deprecated.\nPlease, usetorch.ao.nn.quantizedinstead.\nQuantized Dynamic Modules.\nThis file is in the process of migration totorch/ao/nn/quantized/dynamic,\nand is kept here for compatibility while the migration process is ongoing.\nIf you are adding a new entry/functionality, please, add it to the\nappropriate file under thetorch/ao/nn/quantized/dynamic,\nwhile adding an import statement here.",
    "url": "https://pytorch.org/docs/stable/quantization-support.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "739fda84923d8ec509956d6a6f2e3580",
    "source": "pytorch_docs",
    "title": "PyTorch Symmetric Memory \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch Symmetric Memory#\n\nCreated On: Nov 10, 2025 | Last Updated On: Nov 10, 2025\nNote\ntorch.distributed._symmetric_memoryis currently in alpha state and under\ndevelopment. API changes may be possible.\ntorch.distributed._symmetric_memory\n\n## Why Symmetric Memory?#\n\nWith rapidly evolving parallelization techniques, existing frameworks and\nlibraries often struggle to keep up, and developers increasingly rely on custom\nimplementations directly scheduling communications and computations. In recent\nyears we\u2019ve witnessed a shift from primarily relying on one-dimensional\ndata-parallelism techniques to multi-dimensional parallelism ones. The latter\nhave different latency requirements for different types of communications and\nthus require fine-grained overlapping of compute and communications.\nTo minimize compute interference, they also require the use of copy engines and\nnetwork interface cards (NICs) to drive communication. Network transport\nprotocols such as remote direct memory access (RDMA) enhance the performance by\nenabling direct, high-speed, and low-latency communication between processors\nand memory. This increase in variety indicates the need for finer-grained\ncommunication primitives than are offered today by high-level collective APIs,\nones that would enable developers to implement specific algorithms tailored for\ntheir use cases, such as low-latency collectives, fine-grained\ncompute-communications overlap, or custom fusions.\nFurthermore, today\u2019s advanced AI systems connect GPUs with high-bandwidth links\n(such as NVLinks, InfiniBand or RoCE), making GPU global memory directly\naccessible to peers. Such connections present a great opportunity for\nprogrammers to program the system as a single, gigantic GPU with vast accessible\nmemory, instead of programming singular \u201cGPU islands.\u201d\nIn this document, we will show how you can use PyTorch Symmetric Memory to\nprogram modern GPU systems as a \u201csingle GPU\u201d and achieve fine-grained remote\naccess.\n\n## What PyTorch Symmetric Memory unlocks?#\n\nPyTorch Symmetric Memory unlocks three new capabilities:\nCustomized communication patterns: Increased flexibility in kernel writing\nallows developers to write custom kernels that implement their custom\ncomputations and communications, directly tailored to the need of the\napplication. It will also be straightforward to add support for new data types\nalong with the special compute that those data types might require, even if it\u2019s\nnot present yet in the standard libraries.\nIn-kernel compute-comm fusion: Device-initiated communication capability\nallows developers to write kernels with both computation and communication\ninstructions, allowing for the fusion of computation and data movement in the\nsmallest possible granularity.\nLow-latency remote access: Network transport protocols like RDMA enhance the\nperformance of symmetric memory in networked environments by enabling direct,\nhigh-speed, and low-latency communication between processors and memory. RDMA\neliminates the overhead associated with the traditional network stack and CPU\ninvolvement. It also offloads data transfer from the compute to the NICs,\nfreeing up compute resources for computational tasks.\nNext, we will show you how PyTorch Symmetric Memory (SymmMem) enables new\napplications with the above capabilities.\n\n## A \u201cHello World\u201d example#\n\nThe PyTorch SymmMem programming model involves two key elements:\ncreating symmetric tensors\ncreating SymmMem kernels\nTo create symmetric tensors, one can use thetorch.distributed._symmetric_memorypackage:\ntorch.distributed._symmetric_memory\n\n```python\nimport torch.distributed._symmetric_memory as symm_mem\n\nt = symm_mem.empty(128, device=torch.device(\"cuda\", rank))\nhdl = symm_mem.rendezvous(t, group)\n\n```\n\nThesymm_mem.emptyfunction creates a tensor that is backed by a symmetric\nmemory allocation. Therendezvousfunction establishes a rendezvous with peers\nin the group, and returns a handle to the symmetric memory allocation. The\nhandle provides method to access information related to the symmetric memory\nallocation, such as pointers to symmetric buffer on peer ranks, multicast\npointer (if supported), and signal pads.\nsymm_mem.empty\nrendezvous\nTheemptyandrendezvousfunctions must be called in the same order on all\nranks in the group.\nempty\nrendezvous\nThen, collectives can be called on these tensors. For example, to perform a\none-shot all-reduce:\n\n```python\n# Most SymmMem ops are under the torch.ops.symm_mem namespace\ntorch.ops.symm_mem.one_shot_all_reduce(t, \"sum\", group)\n\n```\n\nPlease note thattorch.ops.symm_memis an \u201cop namespace\u201d instead of a python\nmodule. Therefore, you can\u2019t import it byimporttorch.ops.symm_mem, neither\ncan you import an op byfromtorch.ops.symm_memimportone_shot_all_reduce.\nYou can call the op directly as in the example above.\ntorch.ops.symm_mem\nimporttorch.ops.symm_mem\nfromtorch.ops.symm_memimportone_shot_all_reduce\n\n## Write your own kernel#\n\nTo write your own kernel doing communications with symmetric memory, you\u2019ll need\naccess to the addresses of mapped peer buffers and access to signal pads that\nare required for synchronization. In the kernel you\u2019ll also need to perform\ncorrect synchronizations to make sure that peers are ready for communication,\nand signal to them that this GPU is ready.\nPyTorch Symmetric Memory provides CUDA Graph-compatible synchronization\nprimitives that operate on the signal pad accompanying each symmetric memory\nallocation. Kernels using symmetric memory can be written both in CUDA and in\nTriton. Here\u2019s an example allocating symmetric tensor and exchanging handles:\n\n```python\nimport torch.distributed._symmetric_memory as symm_mem\n\ndist.init_process_group()\nrank = dist.get_rank()\n\n# Allocate a tensor\nt = symm_mem.empty(4096, device=f\"cuda:{rank}\")\n# Establish symmetric memory and obtain the handle\nhdl = symm_mem.rendezvous(t, dist.group.WORLD)\n\n```\n\nAccess to buffer pointers, multimem pointer, and signal pads is provided via:\n\n```python\nhdl.buffer_ptrs\nhdl.multicast_ptr\nhdl.signal_pad_ptrs\n\n```\n\nData pointed to bybuffer_ptrscan be accessed just like regular local data,\nand any necessary compute can also be performed in the usual ways. As with local\ndata, you can and should use vectorized accesses to improve efficiency.\nbuffer_ptrs\nSymmetric memory is especially convenient for writing kernels in Triton. While\npreviously Triton removed the barriers to writing efficient CUDA code, now\ncommunications can be added easily to Triton kernels. The kernel below\ndemonstrates a low-latency, all-reduce kernel written in Triton.\n\n```python\n@triton.jit\ndef one_shot_all_reduce_kernel(\n    buf_tuple,\n    signal_pad_ptrs,\n    output_ptr,\n    numel: tl.constexpr,\n    rank: tl.constexpr,\n    world_size: tl.constexpr,\n    BLOCK_SIZE: tl.constexpr,\n):\n    ptx_utils.symm_mem_sync(\n        signal_pad_ptrs, None, rank, world_size, hasSubsequenceMemAccess=True\n    )\n\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n\n    while block_start < numel:\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < numel\n        acc = tl.zeros((BLOCK_SIZE,), dtype=tl.bfloat16)\n\n        for i in tl.static_range(world_size):\n            buffer_rank = buf_tuple[i]\n            x = tl.load(buffer_rank + offsets, mask=mask)\n            acc += x\n\n        tl.store(output_ptr + offsets, acc, mask=mask)\n        block_start += tl.num_programs(axis=0) * BLOCK_SIZE\n\n    ptx_utils.symm_mem_sync(\n        signal_pad_ptrs, None, rank, world_size, hasPreviousMemAccess=True\n    )\n\n```\n\nSynchronizations at the beginning and the end of the kernel above guarantee that\nall the processes see consistent data. The bulk of the kernel is recognizable\nTriton code, and Triton will optimize it behind the scene, making sure memory\naccesses are performed in an efficient way with vectorization and unrolling. As\nwith all Triton kernels, it is easily modifiable to add extra computations or\nchange the communication algorithm. Visit\nhttps://github.com/meta-pytorch/kraken/blob/main/kraken to see additional\nutilities and examples of using symmetric memory to implement common patterns in\nTriton.\n\n## Scale out#\n\nLarge language models distribute experts onto more than 8 GPUs, hence requiring\nmulti-node access capability. NICs capable of RDMA come to help. In addition,\nsoftware libraries such as NVSHMEM or rocSHMEM abstract away the programming\ndifference between intra-node access and inter-node access with primitives that\nare slightly higher level than pointer access, such as put and get.\nPyTorch provides NVSHMEM plugins to augment Triton kernels\u2019 cross-node\ncapabilities. As shown in the code snippet below, one can initiate a cross-node\nput command within the kernel.\n\n```python\nimport torch.distributed._symmetric_memory._nvshmem_triton as nvshmem\nfrom torch.distributed._symmetric_memory._nvshmem_triton import requires_nvshmem\n\n@requires_nvshmem\n@triton.jit\ndef my_put_kernel(\n    dest,\n    src,\n    nelems,\n    pe,\n):\n    nvshmem.put(dest, src, nelems, pe)\n\n```\n\nTherequires_nvshmemdecorator is used to indicate that the kernel requires\nthe NVSHMEM device library as an external dependency. When Triton compiles the\nkernel, the decorator will search your system paths for the NVSHMEM device\nlibrary. If it is available, Triton will include the necessary device assembly\nto use the NVSHMEM functions.\nrequires_nvshmem\n\n## API Reference#\n\nSimilar totorch.empty(). The returned tensor can be used bytorch._distributed._symmetric_memory.rendezvous()to establish a\nsymmetric memory tensor among participating processes.\ntorch.empty()\ntorch._distributed._symmetric_memory.rendezvous()\nsize(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple.\ndtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_dtype()).\ntorch.dtype\nNone\ntorch.set_default_dtype()\ndevice(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_device()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.\ntorch.device\nNone\ntorch.set_default_device()\ndevice\nEstablish a symmetric memory tensor among participating processes. This is\na collective operation.\ntensor(torch.Tensor) \u2013 the local tensor used to establish the symmetric memory tensor.\nIt must be allocated viatorch._distributed._symmetric_memory.empty(). The shape,\ndtype, and device type must be identical across all participating processes.\ntorch.Tensor\ntorch._distributed._symmetric_memory.empty()\ngroup(Union[str,torch.distributed.ProcessGroup]) \u2013 The group identifying the\nparticipating processes. This can be either a group name or a process group object.\ntorch.distributed.ProcessGroup\n_SymmetricMemory\nCheck if NVSHMEM is available in current build and on current system.\nbool\nSet the backend for symmetric memory allocation. This is a global setting\nand affects all subsequent calls totorch._distributed._symmetric_memory.empty().  Note that the backend\ncannot be changed once a symmetric memory tensor has been allocated.\ntorch._distributed._symmetric_memory.empty()\nbackend(str) \u2013 the backend for symmetric memory allocation. Currently,\nonly\u201cNVSHMEM\u201d,\u201cCUDA\u201d,\u201cNCCL\u201dare supported.\nGet the backend for symmetric memory allocation for a given device. If not\nfound, return None.\ndevice(torch.deviceor str) \u2013 the device for which to get the backend.\nstr| None\n\n## Op Reference#\n\nNote\nThe following ops are hosted in thetorch.ops.symm_memnamespace. You can call\nthem directly viatorch.ops.symm_mem.<op_name>.\ntorch.ops.symm_mem\ntorch.ops.symm_mem.<op_name>\nPerforms a multimem all-reduce operation on the input tensor. This operation\nrequires hardware support for multimem operations. On NVIDIA GPUs, NVLink\nSHARP is required.\ninput(Tensor) \u2013 Input tensor to perform all-reduce on. Must be symmetric.\nreduce_op(str) \u2013 Reduction operation to perform. Currently only \u201csum\u201d is supported.\ngroup_name(str) \u2013 Name of the group to perform all-reduce on.\nPerforms a multimem all-gather operation on the input tensor. This operation requires hardware support for multimem operations. On NVIDIA GPUs, NVLink SHARP is required.\ninput(Tensor) \u2013 Input tensor to perform all-gather on.\ngroup_name(str) \u2013 Name of the group to perform all-gather on.\nout(Tensor) \u2013 Output tensor to store the result of the all-gather operation. Must be symmetric.\nPerforms a one-shot all-reduce operation on the input tensor.\ninput(Tensor) \u2013 Input tensor to perform all-reduce on. Must be symmetric.\nreduce_op(str) \u2013 Reduction operation to perform. Currently only \u201csum\u201d is supported.\ngroup_name(str) \u2013 Name of the group to perform all-reduce on.\nPerforms a one-shot all-reduce operation based on the input tensor and writes the result to the output tensor.\ninput(Tensor) \u2013 Input tensor to perform all-reduce on. Must be symmetric.\nreduce_op(str) \u2013 Reduction operation to perform. Currently only \u201csum\u201d is supported.\ngroup_name(str) \u2013 Name of the group to perform all-reduce on.\nout(Tensor) \u2013 Output tensor to store the result of the all-reduce operation. Can be a regular tensor.\nPerforms a two-shot all-reduce operation on the input tensor.\ninput(Tensor) \u2013 Input tensor to perform all-reduce on. Must be symmetric.\nreduce_op(str) \u2013 Reduction operation to perform. Currently only \u201csum\u201d is supported.\ngroup_name(str) \u2013 Name of the group to perform all-reduce on.\nPerforms an all-to-all-v operation using NVSHMEM, with split information provided on device.\ninput(Tensor) \u2013 Input tensor to perform all-to-all on. Must be symmetric.\nout(Tensor) \u2013 Output tensor to store the result of the all-to-all operation. Must be symmetric.\nin_splits(Tensor) \u2013 Tensor containing splits of data to send to each peer. Must be symmetric. Must be of size (group_size,). The splits are in the unit of elements in the 1st dimension.\nout_splits_offsets(Tensor) \u2013 Tensor containing the splits and offsets of data received from each peer. Must be symmetric. Must be of size (2, group_size). The rows are (in order): output splits and output offsets.\ngroup_name(str) \u2013 Name of the group to perform all-to-all on.\nPerform a 2D all-to-all-v operation using NVSHMEM, with split information provided on device. In Mixture of Experts models, this operation can be used to dispatch tokens.\ninput(Tensor) \u2013 Input tensor to perform all-to-all on. Must be symmetric.\nout(Tensor) \u2013 Output tensor to store the result of the all-to-all operation. Must be symmetric.\nin_splits(Tensor) \u2013 Tensor containing the splits of data to send to each expert. Must be symmetric. Must be of size (group_size * ne,), where ne is the number of experts per rank. The splits are in the unit of elements in the 1st dimension.\nout_splits_offsets(Tensor) \u2013 Tensor containing the splits and offsets of data received from each peer. Must be symmetric. Must be of size (2, group_size * ne). The rows are (in order): output splits and output offsets.\ngroup_name(str) \u2013 Name of the group to perform all-to-all on.\nmajor_align(int) \u2013 Optional alignment for the major dimension of the output chunk for each expert. If not provided, the alignment is assumed to be 1. Any alignment adjustment will be reflected in the output offsets.\nA 2D AllToAllv shuffle is illustrated below:\n(world_size = 2, ne = 2, total number of experts = 4):\n\n```python\nSource: |       Rank 0      |       Rank 1      |\n        | c0 | c1 | c2 | c3 | d0 | d1 | d2 | d3 |\n\nDest  : |       Rank 0      |       Rank 1      |\n        | c0 | d0 | c1 | d1 | c2 | d2 | c3 | d3 |\n\n```\n\nwhere eachc_i/d_iare slices of theinputtensor, targeting experti, with length indicated by input splits.  That is, the 2D AllToAllv\nshuffle achieves a transpose from rank-major order at input to expert-major\norder at output.\nIfmajor_alignis not 1, the output offsets of c1, c2, c3 will be\nup-aligned to this value. For example, if c0 has length 5 and d0 has\nlength 7 (making a total of 12), and if themajor_alignis set to 16,\nthe output offset of c1 will be 16. Similar for c2 and c3. This value has\nno effect on the offset of the minor dimension, i.e.  d0, d1, d2 and d3.\nNote: since cutlass does not support empty bins, we set the aligned length\ntomajor_alignif it is 0. Seepytorch/pytorch#152668.\nPerform a 2D AllToAllv shuffle operation, with input split and offset\ninformation provided on device. The input offsets are not required to be\nexact prefix sum of the input splits, i.e. paddings are allowed between the\nsplit chunks. The paddings, however, will not be transferred to peer\nranks.\nIn Mixture of Experts models, this operation can be used to combine tokens\nprocessed by experts on parallel ranks. This operation can be viewed as an\n\u201creverse\u201d operation to theall_to_all_vdev_2doperation (which shuffles\ntokens to experts).\ninput(Tensor) \u2013 Input tensor to perform all-to-all on. Must be symmetric.\nout(Tensor) \u2013 Output tensor to store the result of the all-to-all operation. Must be symmetric.\nin_splits_offsets(Tensor) \u2013 Tensor containing the splits and offsets of data to send to each expert. Must be symmetric. Must be of size (2, group_size * ne), whereneis the number of experts. The rows are (in order): input splits and input offsets. The splits are in the unit of elements in the 1st dimension.\nout_splits_offsets(Tensor) \u2013 Tensor containing the splits and offsets of data received from each peer. Must be symmetric. Must be of size (2, group_size * ne). The rows are (in order): output splits and output offsets.\ngroup_name(str) \u2013 Name of the group to perform all-to-all on.",
    "url": "https://pytorch.org/docs/stable/symmetric_memory.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5587d2228be311e664e7d17ac266a309",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/mps_environment_variables.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e4fd37001efd1121a008df6384d2fc6a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/add_images.png",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "dde136a3f7bc89b1ec3348f05285d988",
    "source": "pytorch_docs",
    "title": "torch.compiler \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.compiler#\n\nCreated On: Jul 28, 2023 | Last Updated On: Jul 28, 2025\ntorch.compileris a namespace through which some of the internal compiler\nmethods are surfaced for user consumption. The main function and the feature in\nthis namespace istorch.compile.\ntorch.compiler\ntorch.compile\ntorch.compileis a PyTorch function introduced in PyTorch 2.x that aims to\nsolve the problem of accurate graph capturing in PyTorch and ultimately enable\nsoftware engineers to run their PyTorch programs faster.torch.compileis\nwritten in Python and it marks the transition of PyTorch from C++ to Python.\ntorch.compile\ntorch.compile\ntorch.compileleverages the following underlying technologies:\ntorch.compile\nTorchDynamo (torch._dynamo)is an internal API that uses a CPython\nfeature called the Frame Evaluation API to safely capture PyTorch graphs.\nMethods that are available externally for PyTorch users are surfaced\nthrough thetorch.compilernamespace.\ntorch.compiler\nTorchInductoris the defaulttorch.compiledeep learning compiler\nthat generates fast code for multiple accelerators and backends. You\nneed to use a backend compiler to make speedups throughtorch.compilepossible. For NVIDIA, AMD and Intel GPUs, it leverages OpenAI Triton as the key\nbuilding block.\ntorch.compile\ntorch.compile\nAOT Autogradcaptures not only the user-level code, but also backpropagation,\nwhich results in capturing the backwards pass \u201cahead-of-time\u201d. This enables\nacceleration of both forwards and backwards pass using TorchInductor.\nTo better understand howtorch.compiletracing behavior on your code, or to\nlearn more about the internals oftorch.compile, please refer to thetorch.compileprogramming model.\ntorch.compile\ntorch.compile\ntorch.compile\nNote\nIn some cases, the termstorch.compile, TorchDynamo,torch.compilermight be used interchangeably in this documentation.\ntorch.compile\ntorch.compiler\nAs mentioned above, to run your workflows faster,torch.compilethrough\nTorchDynamo requires a backend that converts the captured graphs into a fast\nmachine code. Different backends can result in various optimization gains.\nThe default backend is called TorchInductor, also known asinductor,\nTorchDynamo has a list of supported backends developed by our partners,\nwhich can be seen by runningtorch.compiler.list_backends()each of which\nwith its optional dependencies.\ntorch.compile\ntorch.compiler.list_backends()\nSome of the most commonly used backends include:\nTraining & inference backends\nBackend\nDescription\ntorch.compile(m,backend=\"inductor\")\ntorch.compile(m,backend=\"inductor\")\nUses the TorchInductor backend.Read more\ntorch.compile(m,backend=\"cudagraphs\")\ntorch.compile(m,backend=\"cudagraphs\")\nCUDA graphs with AOT Autograd.Read more\ntorch.compile(m,backend=\"ipex\")\ntorch.compile(m,backend=\"ipex\")\nUses IPEX on CPU.Read more\nInference-only backends\nBackend\nDescription\ntorch.compile(m,backend=\"tensorrt\")\ntorch.compile(m,backend=\"tensorrt\")\nUses Torch-TensorRT for inference optimizations. Requiresimporttorch_tensorrtin the calling script to register backend.Read more\nimporttorch_tensorrt\ntorch.compile(m,backend=\"ipex\")\ntorch.compile(m,backend=\"ipex\")\nUses IPEX for inference on CPU.Read more\ntorch.compile(m,backend=\"tvm\")\ntorch.compile(m,backend=\"tvm\")\nUses Apache TVM for inference optimizations.Read more\ntorch.compile(m,backend=\"openvino\")\ntorch.compile(m,backend=\"openvino\")\nUses OpenVINO for inference optimizations.Read more\n\n## Read More#\n\nGetting Started for PyTorch Users\ntorch.compile\n`torch.compile` Programming Model\nfullgraph=True\ntorch._dynamo.nonstrict_trace\nfullgraph=False\ncompile(model)\nmodel.compile()\nerror_on_graph_break\nerror_on_graph_break(False)\nerror_on_graph_break(True)\nerror_on_graph_break\nfullgraph\nfullgraph=True/False\nerror_on_graph_break\nDeep Dive for PyTorch Developers\nHowTo for PyTorch Backend Vendors",
    "url": "https://pytorch.org/docs/stable/torch.compiler.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e0c50fc5108f0a0560c9e4e4747f8363",
    "source": "pytorch_docs",
    "title": "Pipeline Parallelism \u2014 PyTorch 2.9 documentation",
    "text": "\n## Pipeline Parallelism#\n\nCreated On: Jun 16, 2025 | Last Updated On: Aug 13, 2025\nNote\ntorch.distributed.pipeliningis currently in alpha state and under\ndevelopment. API changes may be possible. It was migrated from thePiPPyproject.\ntorch.distributed.pipelining\n\n## Why Pipeline Parallel?#\n\nPipeline Parallelism is one of theprimitiveparallelism for deep learning.\nIt allows theexecutionof a model to be partitioned such that multiplemicro-batchescan execute different parts of the model code concurrently.\nPipeline parallelism can be an effective technique for:\nlarge-scale training\nbandwidth-limited clusters\nlarge model inference\nThe above scenarios share a commonality that the computation per device cannot\nhide the communication of conventional parallelism, for example, the weight\nall-gather of FSDP.\n\n## What istorch.distributed.pipelining?#\n\ntorch.distributed.pipelining\nWhile promising for scaling, pipelining is often difficult to implement because\nit needs topartition the executionof a model in addition to model weights.\nThe partitioning of execution often requires intrusive code changes to your\nmodel. Another aspect of complexity comes fromscheduling micro-batches in a\ndistributed environment, withdata flow dependencyconsidered.\nThepipeliningpackage provides a toolkit that does said thingsautomaticallywhich allows easy implementation of pipeline parallelism\nongeneralmodels.\npipelining\nIt consists of two parts: asplitting frontendand adistributed runtime.\nThe splitting frontend takes your model code as-is, splits it up into \u201cmodel\npartitions\u201d, and captures the data-flow relationship. The distributed runtime\nexecutes the pipeline stages on different devices in parallel, handling things\nlike micro-batch splitting, scheduling, communication, and gradient propagation,\netc.\nOverall, thepipeliningpackage provides the following features:\npipelining\nSplitting of model code based on simple specification.\nRich support for pipeline schedules, including GPipe, 1F1B,\nInterleaved 1F1B and Looped BFS, and providing the infrastructure for writing\ncustomized schedules.\nFirst-class support for cross-host pipeline parallelism, as this is where PP\nis typically used (over slower interconnects).\nComposability with other PyTorch parallel techniques such as data parallel\n(DDP, FSDP) or tensor parallel. TheTorchTitanproject demonstrates a \u201c3D parallel\u201d\napplication on the Llama model.\n\n## Step 1: buildPipelineStage#\n\nPipelineStage\nBefore we can use aPipelineSchedule, we need to createPipelineStageobjects that wrap the part of the model running in that stage. ThePipelineStageis responsible for allocating communication buffers and\ncreating send/recv ops to communicate with its peers. It manages intermediate\nbuffers e.g. for the outputs of forward that have not been consumed yet, and it\nprovides a utility for running the backwards for the stage model.\nPipelineSchedule\nPipelineStage\nPipelineStage\nAPipelineStageneeds to know the input and output shapes for the stage\nmodel, so that it can correctly allocate communication buffers. The shapes must\nbe static, e.g. at runtime the shapes can not change from step to step. A classPipeliningShapeErrorwill be raised if runtime shapes do not match the\nexpected shapes. When composing with other paralleisms or applying mixed\nprecision, these techniques must be taken into account so thePipelineStageknows the correct shape (and dtype) for the output of the stage module at\nruntime.\nPipelineStage\nPipeliningShapeError\nPipelineStage\nUsers may construct aPipelineStageinstance directly, by passing in annn.Modulerepresenting the portion of the model that should run on the\nstage. This may require changes to the original model code. See the example\ninOption 1: splitting a model manually.\nPipelineStage\nnn.Module\nAlternatively, the splitting frontend can use graph partitioning to split your\nmodel into a series ofnn.Moduleautomatically. This technique requires the\nmodel is traceable withtorch.Export. Composability of the resultingnn.Modulewith other parallelism techniques is experimental, and may require\nsome workarounds. Usage of this frontend may be more appealing if the user\ncannot easily change the model code. SeeOption 2: splitting a model automaticallyfor more\ninformation.\nnn.Module\ntorch.Export\nnn.Module\n\n## Step 2: usePipelineSchedulefor execution#\n\nPipelineSchedule\nWe can now attach thePipelineStageto a pipeline schedule, and run the\nschedule with input data. Here is a GPipe example:\nPipelineStage\n\n```python\nfrom torch.distributed.pipelining import ScheduleGPipe\n\n# Create a schedule\nschedule = ScheduleGPipe(stage, n_microbatches)\n\n# Input data (whole batch)\nx = torch.randn(batch_size, in_dim, device=device)\n\n# Run the pipeline with input `x`\n# `x` will be divided into microbatches automatically\nif rank == 0:\n    schedule.step(x)\nelse:\n    output = schedule.step()\n\n```\n\nNote that the above code needs to be launched for each worker, thus we use a\nlauncher service to launch multiple processes:\n\n```python\ntorchrun --nproc_per_node=2 example.py\n\n```\n\n\n## Options for Splitting a Model#\n\n\n## Option 1: splitting a model manually#\n\nTo directly construct aPipelineStage, the user is responsible for providing\na singlenn.Moduleinstance that owns the relevantnn.Parametersandnn.Buffers, and defines aforward()method that executes the operations\nrelevant for that stage. For example, a condensed version of the Transformer\nclass defined in Torchtitan shows a pattern of building an easily partitionable\nmodel.\nPipelineStage\nnn.Module\nnn.Parameters\nnn.Buffers\nforward()\n\n```python\nclass Transformer(nn.Module):\n    def __init__(self, model_args: ModelArgs):\n        super().__init__()\n\n        self.tok_embeddings = nn.Embedding(...)\n\n        # Using a ModuleDict lets us delete layers without affecting names,\n        # ensuring checkpoints will correctly save and load.\n        self.layers = torch.nn.ModuleDict()\n        for layer_id in range(model_args.n_layers):\n            self.layers[str(layer_id)] = TransformerBlock(...)\n\n        self.output = nn.Linear(...)\n\n    def forward(self, tokens: torch.Tensor):\n        # Handling layers being 'None' at runtime enables easy pipeline splitting\n        h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\n\n        for layer in self.layers.values():\n            h = layer(h, self.freqs_cis)\n\n        h = self.norm(h) if self.norm else h\n        output = self.output(h).float() if self.output else h\n        return output\n\n```\n\nA model defined in this manner can be easily configured per stage by first\ninitializing the whole model (using meta-device to avoid OOM errors), deleting\nundesired layers for that stage, and then creating a PipelineStage that wraps\nthe model. For example:\n\n```python\nwith torch.device(\"meta\"):\n    assert num_stages == 2, \"This is a simple 2-stage example\"\n\n    # we construct the entire model, then delete the parts we do not need for this stage\n    # in practice, this can be done using a helper function that automatically divides up layers across stages.\n    model = Transformer()\n\n    if stage_index == 0:\n        # prepare the first stage model\n        del model.layers[\"1\"]\n        model.norm = None\n        model.output = None\n\n    elif stage_index == 1:\n        # prepare the second stage model\n        model.tok_embeddings = None\n        del model.layers[\"0\"]\n\n    from torch.distributed.pipelining import PipelineStage\n    stage = PipelineStage(\n        model,\n        stage_index,\n        num_stages,\n        device,\n    )\n\n```\n\nWhen composing with other Data or Model parallelism techniques,output_argsmay also be required, if the output shape/dtype of the model chunk will be\naffected.\noutput_args\n\n## Option 2: splitting a model automatically#\n\nIf you have a full model and do not want to spend time on modifying it into a\nsequence of \u201cmodel partitions\u201d, thepipelineAPI is here to help.\nHere is a brief example:\npipeline\n\n```python\nclass Model(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.emb = torch.nn.Embedding(10, 3)\n        self.layers = torch.nn.ModuleList(\n            Layer() for _ in range(2)\n        )\n        self.lm = LMHead()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.emb(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.lm(x)\n        return x\n\n```\n\nIf we print the model, we can see multiple hierarchies, which makes it hard to split by hand:\n\n```python\nModel(\n  (emb): Embedding(10, 3)\n  (layers): ModuleList(\n    (0-1): 2 x Layer(\n      (lin): Linear(in_features=3, out_features=3, bias=True)\n    )\n  )\n  (lm): LMHead(\n    (proj): Linear(in_features=3, out_features=3, bias=True)\n  )\n)\n\n```\n\nLet us see how thepipelineAPI works:\npipeline\n\n```python\nfrom torch.distributed.pipelining import pipeline, SplitPoint\n\n# An example micro-batch input\nx = torch.LongTensor([1, 2, 4, 5])\n\npipe = pipeline(\n    module=mod,\n    mb_args=(x,),\n    split_spec={\n        \"layers.1\": SplitPoint.BEGINNING,\n    }\n)\n\n```\n\nThepipelineAPI splits your model given asplit_spec, whereSplitPoint.BEGINNINGstands for adding a split pointbeforeexecution of certain submodule in theforwardfunction, and\nsimilarly,SplitPoint.ENDfor split pointaftersuch.\npipeline\nsplit_spec\nSplitPoint.BEGINNING\nforward\nSplitPoint.END\nIf weprint(pipe), we can see:\nprint(pipe)\n\n```python\nGraphModule(\n  (submod_0): GraphModule(\n    (emb): InterpreterModule()\n    (layers): Module(\n      (0): InterpreterModule(\n        (lin): InterpreterModule()\n      )\n    )\n  )\n  (submod_1): GraphModule(\n    (layers): Module(\n      (1): InterpreterModule(\n        (lin): InterpreterModule()\n      )\n    )\n    (lm): InterpreterModule(\n      (proj): InterpreterModule()\n    )\n  )\n)\n\ndef forward(self, x):\n    submod_0 = self.submod_0(x);  x = None\n    submod_1 = self.submod_1(submod_0);  submod_0 = None\n    return (submod_1,)\n\n```\n\nThe \u201cmodel partitions\u201d are represented by submodules (submod_0,submod_1), each of which is reconstructed with original model operations, weights\nand hierarchies. In addition, a \u201croot-level\u201dforwardfunction is\nreconstructed to capture the data flow between those partitions. Such data flow\nwill be replayed by the pipeline runtime later, in a distributed fashion.\nsubmod_0\nsubmod_1\nforward\nThePipeobject provides a method for retrieving the \u201cmodel partitions\u201d:\nPipe\n\n```python\nstage_mod : nn.Module = pipe.get_stage_module(stage_idx)\n\n```\n\nThe returnedstage_modis ann.Module, with which you can create an\noptimizer, save or load checkpoints, or apply other parallelisms.\nstage_mod\nnn.Module\nPipealso allows you to create a distributed stage runtime on a device given\naProcessGroup:\nPipe\nProcessGroup\n\n```python\nstage = pipe.build_stage(stage_idx, device, group)\n\n```\n\nAlternatively, if you would like to build the stage runtime later after some\nmodification to thestage_mod, you can use a functional version of thebuild_stageAPI. For example:\nstage_mod\nbuild_stage\n\n```python\nfrom torch.distributed.pipelining import build_stage\nfrom torch.nn.parallel import DistributedDataParallel\n\ndp_mod = DistributedDataParallel(stage_mod)\ninfo = pipe.info()\nstage = build_stage(dp_mod, stage_idx, info, device, group)\n\n```\n\nNote\nThepipelinefrontend uses a tracer (torch.export) to capture your\nmodel into a single graph. If your model is not full-graph\u2019able, you can use\nour manual frontend below.\npipeline\ntorch.export\n\n## Hugging Face Examples#\n\nIn thePiPPyrepo where this package was\noriginal created, we kept examples based on unmodified Hugging Face models.\nSee theexamples/huggingfacedirectory.\nExamples include:\nGPT2\nLlama\n\n## Technical Deep Dive#\n\n\n## How does thepipelineAPI split a model?#\n\npipeline\nFirst, thepipelineAPI turns our model into a directed acyclic graph (DAG)\nby tracing the model. It traces the model usingtorch.export\u2013 a PyTorch 2\nfull-graph capturing tool.\npipeline\ntorch.export\nThen, it groups together theoperations and parametersneeded by a stage\ninto a reconstructed submodule:submod_0,submod_1, \u2026\nsubmod_0\nsubmod_1\nDifferent from conventional submodule access methods likeModule.children(),\nthepipelineAPI does not only cut the module structure of your model, but\nalso theforwardfunction of your model.\nModule.children()\npipeline\nThis is necessary because model structure likeModule.children()merely\ncaptures information duringModule.__init__(), and does not capture any\ninformation aboutModule.forward(). Said differently,Module.children()lacks information about the following aspects key to pipelininig:\nModule.children()\nModule.__init__()\nModule.forward()\nModule.children()\nExecution order of child modules inforward\nforward\nActivation flows between child modules\nWhether there are any functional operators between child modules (for example,reluoraddoperations will not be captured byModule.children()).\nrelu\nadd\nModule.children()\nThepipelineAPI, on the contrary, makes sure that theforwardbehavior\nis truly preserved. It also captures the activation flow between the partitions,\nhelping the distributed runtime to make correct send/receive calls without human\nintervention.\npipeline\nforward\nAnother flexibility of thepipelineAPI is that split points can be at\narbitrary levels within your model hierarchy. In the split partitions, the original model\nhierarchy related to that partition will be reconstructed at no cost to you.\nAt a result, fully-qualified names (FQNs) pointing to a submodule or parameter\nwould be still valid, and services that relies on FQNs (such as FSDP, TP or\ncheckpointing) can still run with your partitioned modules with almost zero code\nchange.\npipeline\n\n## Implementing Your Own Schedule#\n\nYou can implement your own pipeline schedule by extending one of the following two class:\nPipelineScheduleSingle\nPipelineScheduleSingle\nPipelineScheduleMulti\nPipelineScheduleMulti\nPipelineScheduleSingleis for schedules that assignsonly onestage per rank.PipelineScheduleMultiis for schedules that assigns multiple stages per rank.\nPipelineScheduleSingle\nPipelineScheduleMulti\nFor example,ScheduleGPipeandSchedule1F1Bare subclasses ofPipelineScheduleSingle.\nWhereas,ScheduleInterleaved1F1B,ScheduleLoopedBFS,ScheduleInterleavedZeroBubble, andScheduleZBVZeroBubbleare subclasses ofPipelineScheduleMulti.\nScheduleGPipe\nSchedule1F1B\nPipelineScheduleSingle\nScheduleInterleaved1F1B\nScheduleLoopedBFS\nScheduleInterleavedZeroBubble\nScheduleZBVZeroBubble\nPipelineScheduleMulti\n\n## Logging#\n\nYou can turn on additional logging using theTORCH_LOGSenvironment variable fromtorch._logging:\nTORCH_LOGS\nTORCH_LOGS=+ppwill displaylogging.DEBUGmessages and all levels above it.\nTORCH_LOGS=+pp\nlogging.DEBUG\nTORCH_LOGS=ppwill displaylogging.INFOmessages and above.\nTORCH_LOGS=pp\nlogging.INFO\nTORCH_LOGS=-ppwill displaylogging.WARNINGmessages and above.\nTORCH_LOGS=-pp\nlogging.WARNING\n\n## API Reference#\n\n\n## Model Split APIs#\n\nThe following set of APIs transform your model into a pipeline representation.\nEnum representing the points at which a split can occur in the execution of a submodule.\n:ivar BEGINNING: Represents adding a split pointbeforethe execution of a certain submodule in theforwardfunction.\n:ivar END: Represents adding a split pointafterthe execution of a certain submodule in theforwardfunction.\nSplit a module based on a specification.\nSeePipefor more details.\nmodule(Module) \u2013 The module to be split.\nmb_args(tuple[Any,...]) \u2013 Example positional inputs, in micro-batch form.\nmb_kwargs(Optional[dict[str,Any]]) \u2013 Example keyword inputs, in micro-batch form. (default:None)\nsplit_spec(Optional[dict[str,torch.distributed.pipelining._IR.SplitPoint]]) \u2013 A dictionary using submodule names as split marker. (default:None)\nsplit_policy(Optional[Callable[[GraphModule],GraphModule]]) \u2013 The policy to use for splitting the module. (default:None)\nA pipeline representation of classPipe.\npipe_split is a special operator that is used to mark the boundary between\nstages in a module. It is used to split the module into stages. It is a\nno-op if your annotated module is run eagerly.\nExample\n\n```python\n>>> def forward(self, x):\n>>>     x = torch.mm(x, self.mm_param)\n>>>     x = torch.relu(x)\n>>>     pipe_split()\n>>>     x = self.lin(x)\n>>>     return x\n\n```\n\nThe above example will be split into two stages.\n\n## Microbatch Utilities#\n\nClass used to specify chunking of inputs\nGiven a sequence of args and kwargs, split them into a number of chunks\naccording to  their respective chunking specs.\nargs(tuple[Any,...]) \u2013 Tuple of args\nkwargs(Optional[dict[str,Any]]) \u2013 Dict of kwargs\nchunks(int) \u2013 Number of chunks to split the args and kwargs into\nargs_chunk_spec(Optional[tuple[torch.distributed.pipelining.microbatch.TensorChunkSpec,...]]) \u2013 chunking specs for args, in same shape as args\nkwargs_chunk_spec(Optional[dict[str,torch.distributed.pipelining.microbatch.TensorChunkSpec]]) \u2013 chunking specs for kwargs, in same shape as kwargs\nList of sharded args\nkwargs_split: List of sharded kwargs\nargs_split\nGiven a list of chunks, merge them into a single value according to\nthe chunk spec.\nchunks(list[Any]) \u2013 list of chunks\nchunk_spec\u2013 Chunking spec for the chunks\nMerged value\nvalue\n\n## Pipeline Stages#\n\nA class representing a pipeline stage in a pipeline parallelism setup.\nPipelineStage assumes sequential partitioning of the model, i.e. the model is split into chunks where outputs from\none chunk feed into inputs of the next chunk, with no skip connections.\nPipelineStage performs runtime shape/dtype inference automatically by propagating the outputs from stage0 to\nstage1 and so forth, in linear order.  To bypass shape inference, pass theinput_argsandoutput_argsto each\nPipelineStage instance.\nsubmodule(nn.Module) \u2013 The PyTorch module wrapped by this stage.\nstage_index(int) \u2013 The ID of this stage.\nnum_stages(int) \u2013 The total number of stages.\ndevice(torch.device) \u2013 The device where this stage is located.\ninput_args(Union[torch.Tensor,Tuple[torch.tensor]],optional) \u2013 The input arguments for the submodule.\noutput_args(Union[torch.Tensor,Tuple[torch.tensor]],optional) \u2013 The output arguments for the submodule.\ngroup(dist.ProcessGroup,optional) \u2013 The process group for distributed training. If None, default group.\ndw_builder(Optional[Callable[[],Callable[...,None]]) \u2013 If provided, dw_builder will build a new dw_runner function\nthat will the W action (input weights) for F, I, W (Fwd, Input, Weight) zero bubble schedules.\nCreate a pipeline stage given a stage_module to be wrapped by this stage\nand pipeline information.\nstage_module(torch.nn.Module) \u2013 the module to be wrapped by this stage\nstage_index(int) \u2013 the index of this stage in the pipeline\npipe_info(PipeInfo) \u2013 information about the pipeline, can be retrieved bypipe.info()\ndevice(torch.device) \u2013 the device to be used by this stage\ngroup(Optional[dist.ProcessGroup]) \u2013 the process group to be used by this stage\na pipeline stage that can run withPipelineSchedules.\n_PipelineStage\n\n## Pipeline Schedules#\n\nThe GPipe schedule.\nWill go through all the microbatches in a fill-drain manner.\nThe 1F1B schedule.\nWill perform one forward and one backward on the microbatches in steady state.\nThe Interleaved 1F1B schedule.\nSeehttps://arxiv.org/pdf/2104.04473for details.\nWill perform one forward and one backward on the microbatches in steady\nstate and supports multiple stages per rank. When microbatches are ready for\nmultiple local stages, Interleaved 1F1B prioritizes the earlier microbatch\n(also called \u201cdepth first\u201d).\nThis schedule is mostly similar to the original paper.\nIt differs by being relaxing the requirement of num_microbatch % pp_size == 0.\nUsing the flex_pp schedule, we will have num_rounds = max(1, n_microbatches // pp_group_size) and\nit works as long as n_microbatches % num_rounds is 0. As a few examples, support\npp_group_size = 4, n_microbatches = 10. We will have num_rounds = 2 and n_microbatches % 2 is 0.\npp_group_size = 4, n_microbatches = 3. We will have num_rounds = 1 and n_microbatches % 1 is 0.\nBreadth-First Pipeline Parallelism.\nSeehttps://arxiv.org/abs/2211.05953for details.\nSimilar to Interleaved 1F1B, Looped BFS supports multiple stages per rank.\nWhat is different is that when microbatches are ready for multiple local\nstages, Loops BFS will prioritizes the earlier stage, running all available\nmicrobatches at once.\nThe Interleaved Zero Bubble schedule.\nSeehttps://arxiv.org/pdf/2401.10241for details.\nWill perform one forward and one backward on inputs for the microbatches in steady\nstate and supports multiple stages per rank. Uses the backward for weights to fill in\nthe pipeline bubble.\nIn particular this is implementing the ZB1P schedule in the paper.\nThe Zero Bubble schedule (ZBV variant).\nSeehttps://arxiv.org/pdf/2401.10241Section 6 for details.\nThis schedules requires exactly two stages per rank.\nThis schedule will perform one forward and one backward on inputs for the microbatches in steady\nstate and supports multiple stages per rank. Uses backward with respect to weights to fill in\nthe pipeline bubble.\nThis ZB-V schedule would have the \u201czero bubble\u201d property only if time forward == time backward input == time backward weights.\nIn practice, this is not likely true for real models so alternatively\na greedy scheduler could be implemented for unequal/unbalanced time.\nThe DualPipeV schedule. A more efficient schedule variant based on the\nDualPipe schedule introduced by DeepSeek inhttps://arxiv.org/pdf/2412.19437\nBased on the open sourced code fromdeepseek-ai/DualPipe\nBase class for single-stage schedules.\nImplements thestepmethod.\nDerived classes should implement_step_microbatches.\nGradients are scaled by num_microbatches depending on thescale_gradsargument, defaulting to True.  This setting\nshould match the configuration of your loss_fn, which may either average losses (scale_grads=True)\nor sum losses (scale_grads=False).\nRun one iteration of the pipeline schedule withwhole-batchinput.\nWill chunk the input into microbatches automatically, and go through the\nmicrobatches according to the schedule implementation.\nargs: positional arguments to the model (as in non-pipeline case).\nkwargs: keyword arguments to the model (as in non-pipeline case).\ntarget: target for the loss function.\nlosses: a list to store the losses for each microbatch.\nBase class for multi-stage schedules.\nImplements thestepmethod.\nGradients are scaled by num_microbatches depending on thescale_gradsargument, defaulting to True.  This setting\nshould match the configuration of your loss_fn, which may either average losses (scale_grads=True)\nor sum losses (scale_grads=False).\nRun one iteration of the pipeline schedule withwhole-batchinput.\nWill chunk the input into microbatches automatically, and go through the\nmicrobatches according to the schedule implementation.\nargs: positional arguments to the model (as in non-pipeline case).\nkwargs: keyword arguments to the model (as in non-pipeline case).\ntarget: target for the loss function.\nlosses: a list to store the losses for each microbatch.",
    "url": "https://pytorch.org/docs/stable/distributed.pipelining.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "63089fc6a9d4a8103e84a4c4092c80f8",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/mtia.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d69fd0ab69d7a41ed024b6dc156d31ed",
    "source": "pytorch_docs",
    "title": "Rendezvous \u2014 PyTorch 2.9 documentation",
    "text": "\n## Rendezvous#\n\nCreated On: May 04, 2021 | Last Updated On: May 22, 2024\nIn the context of Torch Distributed Elastic we use the termrendezvousto\nrefer to a particular functionality that combines adistributed\nsynchronizationprimitive withpeer discovery.\nIt is used by Torch Distributed Elastic to gather participants of a training\njob (i.e. nodes) such that they all agree on the same list of participants and\neveryone\u2019s roles, as well as make a consistent collective decision on when\ntraining can begin/resume.\nTorch Distributed Elastic rendezvous provides the following critical\nfunctionalities:\nBarrier:\nNodes performing rendezvous will all block until the rendezvous is considered\ncomplete - this happens when at leastmintotal number of nodes have joined\nthe rendezvous barrier (for the same job). This also implies the barrier is not\nnecessarily of fixed size.\nmin\nThere\u2019s an additional small waiting time after reachingminnumber of\nnodes - this is used to ensure the rendezvous is not completed \u201ctoo quickly\u201d\n(which could potentially exclude additional nodes attempting to join at\napproximately the same time).\nmin\nIfmaxnumber of nodes is gathered at the barrier, the rendezvous is\ncompleted immediately.\nmax\nThere\u2019s also an overall timeout which causes the rendezvous to fail ifminnumber of nodes is never reached - this is meant to be a simple fail-safe to\nhelp release partially allocated job resources, in case there\u2019s a problem with\nthe resource manager, and is meant to be interpreted as non-retryable.\nmin\nExclusivity:\nA simple distributed barrier would not be sufficient, as we also need to ensure\nthat only one group of nodes exists at any given time (for a given job). In\nother words, new nodes (i.e. joining late) should not be able to form a parallel\nindependent group of workers for the same job.\nTorch Distributed Elastic rendezvous ensures that if a group of nodes has\nalready completed a rendezvous (and hence might already be training), then\nadditional \u201clate\u201d nodes attempting to rendezvous will only announce themselves\nas waiting, and will have to wait until the (previously completed) existing\nrendezvous is destroyed first.\nConsistency:\nWhen a rendezvous is completed, all its members will agree on the job membership\nand everyone\u2019s role in it. This role is represented using an integer, called\nrank, that is between between 0 and world size.\nNote that ranks arenot stable, in the sense that the same node can be\nassigned a different rank in the next (re-)rendezvous.\nFault-tolerance:\nTorch Distributed Elastic rendezvous is designed to tolerate node failures\nduring the rendezvous process. Should a process crash (or lose network\nconnectivity, etc), between joining the rendezvous and it being completed, then\na re-rendezvous with remaining healthy nodes will happen automatically.\nA node can also failafterit has completed (orhas been observedby other\nnodes to have completed) the rendezvous - this scenario will be handled by the\nTorch Distributed Elastictrain_loopinstead (where it will also trigger a\nre-rendezvous).\ntrain_loop\nShared key-value store:\nWhen the rendezvous is completed, a shared key-value store is created and\nreturned. This store implements atorch.distributed.StoreAPI (seedistributed communication docs).\ntorch.distributed.Store\nThis store is only shared by the members of the completed rendezvous. It\nis intended to be used by Torch Distributed Elastic to exchange information\nnecessary to initialize job control and data-planes.\nWaiting workers and rendezvous closing:\nTorch Distributed Elastic rendezvous handler object provides additional\nfunctionalities, which are technically not part of the rendezvous process:\nQuerying how many workers arrived late at the barrier, who can participate innextrendezvous.\nSetting the rendezvousclosedto signal all nodes not to participate in\nnext rendezvous.\nDynamicRendezvousHandler:\nTorch Distributed Elastic comes with theDynamicRendezvousHandlerclass that implements the rendezvous mechanism described above. It is a backend-\nagnostic type that expects a particularRendezvousBackendinstance\nto be specified during construction.\nDynamicRendezvousHandler\nRendezvousBackend\nTorch distributed users can either implement their own backend type or use one\nof the following implementations that come with PyTorch:\nC10dRendezvousBackend: Uses a C10d store (by defaultTCPStore) as the rendezvous backend. The main advantage of using a C10d\nstore is that it requires no 3rd-party dependency (such as etcd) to establish\na rendezvous.\nC10dRendezvousBackend\nTCPStore\nEtcdRendezvousBackend: Supersedes the legacyEtcdRendezvousHandlerclass. Passing anEtcdRendezvousBackendinstance toDynamicRendezvousHandleris functionally equivalent to\ninstantiating anEtcdRendezvousHandler.\nEtcdRendezvousBackend\nEtcdRendezvousHandler\nEtcdRendezvousBackend\nDynamicRendezvousHandler\nEtcdRendezvousHandler\n\n```python\nstore = TCPStore(\"localhost\")\n\nbackend = C10dRendezvousBackend(store, \"my_run_id\")\n\nrdzv_handler = DynamicRendezvousHandler.from_backend(\n    run_id=\"my_run_id\", store=store, backend=backend, min_nodes=2, max_nodes=4\n)\n\n```\n\nBelow is a state diagram describing how rendezvous works.\n\n## Registry#\n\nHold the parameters to construct aRendezvousHandler.\nRendezvousHandler\nbackend(str) \u2013 The name of the backend to use to handle the rendezvous.\nendpoint(str) \u2013 The endpoint of the rendezvous, usually in form <hostname>[:<port>].\nrun_id(str) \u2013 The id of the rendezvous.\nmin_nodes(int) \u2013 The minimum number of nodes to admit to the rendezvous.\nmax_nodes(int) \u2013 The maximum number of nodes to admit to the rendezvous.\nlocal_addr(Optional[str]) \u2013 The address of the local node.\n**kwargs\u2013 Additional parameters for the specified backend.\nReturn the value forkeyifkeyexists, elsedefault.\nkey\nkey\ndefault\nAny\nReturn the value forkeyas abool.\nkey\nbool\nOptional[bool]\nReturn the value forkeyas anint.\nkey\nint\nOptional[int]\nRepresent a registry ofRendezvousHandlerbackends.\nRendezvousHandler\n\n## Handler#\n\nMain rendezvous interface.\nNote\nDistributed Torch users normallydo notneed to implement their ownRendezvousHandler. An implementation based on C10d Store is already\nprovided, and is recommended for most users.\nRendezvousHandler\nReturn the name of the rendezvous backend.\nstr\nReturn the run id of the rendezvous.\nThe run id is a user-defined id that uniquely identifies an instance of\na distributed application. It typically maps to a job id and is used to\nallow nodes to join the correct distributed application.\nstr\nCheck whether the rendezvous has been closed.\nA closed rendezvous means all future attempts to re-rendezvous within\nsame job will fail.\nis_closed()andset_closed()have semantics of eventual\npropagation and should not be used for synchronization. The intention is\nthat if at least one node decides the job is finished, it will close the\nrendezvous, and other nodes will soon observe this and stop running as\nwell.\nis_closed()\nset_closed()\nbool\nMain entry-point into the rendezvous barrier.\nBlocks until the rendezvous is complete and the current process is\nincluded in the formed worker group, or a timeout occurs, or the\nrendezvous was marked closed.\nInstance ofRendezvousInfo.\nRendezvousInfo\nRendezvousClosedError\u2013 The rendezvous is closed.\nRendezvousConnectionError\u2013 The connection to the rendezvous backend has failed.\nRendezvousStateError\u2013 The rendezvous state is corrupt.\nRendezvousTimeoutError\u2013 The rendezvous did not complete on time.\nRendezvousInfo\nReturn the number of nodes who arrived late at the rendezvous\nbarrier, hence were not included in the current worker group.\nCallers should periodically call this method to check whether new\nnodes are waiting to join the job and if so admit them by callingnext_rendezvous()(re-rendezvous).\nnext_rendezvous()\nint\nMark the rendezvous as closed.\nClose all resources that were open for the rendezvous.\nExample:\n\n```python\nrdzv_handler = ...\ntry:\n    store, rank, world_size = rdzv_handler.next_rendezvous()\nfinally:\n    rdzv_handler.shutdown()\n\n```\n\nbool\nIndicates that store reference returned bynext_rendezvous()can be shared with user\napplications and will be available during application lifecycle.\nnext_rendezvous()\nRendezvous handler impl will share store details as instance ofRendezvousStoreInfo.\nApplications as a convention useMASTER_ADDR/MASTER_PORTenv variables to lookup the store.\nRendezvousStoreInfo\n\n## Dataclasses#\n\nHolds the information about the rendezvous.\nStore address and port that can be used to bootstrap trainer distributed comms\nFactory method, finds unused new port on rank0 host and addr/port info with all ranks.\nIf master_addr/master_port is knowns (useful when sharing existing tcp store server) use the constructor.\nrank(int) \u2013 rank of the current node\nstore(Store) \u2013 store to use for rendezvous\nlocal_addr(Optional[str]) \u2013 address of the current node, if not provided will be resolved from hostname\nserver_port(Optional[int]) \u2013 port of the TCPStore server, when the TCPStore is shared.\nRendezvousStoreInfo\n\n## Exceptions#\n\nRepresents the base type for rendezvous errors.\nRaised when a rendezvous is closed.\nRaised when a rendezvous did not complete on time.\nRaised when the connection to a rendezvous backend has failed.\nRaised when the state of a rendezvous is corrupt.\nRaised when node wasn\u2019t not included in rendezvous and gracefully exits.\nException is a mechanism to exit the stack, however does not mean a failure.\n\n## Implementations#\n\n\n## Dynamic Rendezvous#\n\nCreate a newDynamicRendezvousHandlerfrom the specified parameters.\nDynamicRendezvousHandler\nstore(Store) \u2013 The C10d store to return as part of the rendezvous.\nbackend(RendezvousBackend) \u2013 The backend to use to hold the rendezvous state.\nDynamicRendezvousHandler\nParameter\nDescription\njoin_timeout\nThe total time, in seconds, within which the\nrendezvous is expected to complete. Defaults to 600\nseconds.\nlast_call_timeout\nAn additional wait amount, in seconds, before\ncompleting the rendezvous once the minimum number of\nnodes has been reached. Defaults to 30 seconds.\nclose_timeout\nThe time, in seconds, within which the rendezvous is\nexpected to close after a call toRendezvousHandler.set_closed()orRendezvousHandler.shutdown(). Defaults to\n30 seconds.\nRendezvousHandler.set_closed()\nRendezvousHandler.shutdown()\nheartbeat\nThe time, in seconds, within which a keep-alive\nheartbeat is expected to complete\nRepresent a handler that sets up a rendezvous among a set of nodes.\nCreate a newDynamicRendezvousHandler.\nDynamicRendezvousHandler\nrun_id(str) \u2013 The run id of the rendezvous.\nstore(Store) \u2013 The C10d store to return as part of the rendezvous.\nbackend(RendezvousBackend) \u2013 The backend to use to hold the rendezvous state.\nmin_nodes(int) \u2013 The minimum number of nodes to admit to the rendezvous.\nmax_nodes(int) \u2013 The maximum number of nodes to admit to the rendezvous.\nlocal_addr(Optional[str]) \u2013 The local node address.\ntimeout(Optional[RendezvousTimeout]) \u2013 The timeout configuration of the rendezvous.\nkeep_alive_interval(int) \u2013 The amount of time a node waits before sending a heartbeat to keep\nit alive in the rendezvous.\nkeep_alive_max_attempt(int) \u2013 The maximum number of failed heartbeat attempts after which a node\nis considered dead.\nRepresent a backend that holds the rendezvous state.\nGet the rendezvous state.\nA tuple of the encoded rendezvous state and its fencing token orNoneif no state is found in the backend.\nNone\nRendezvousConnectionError\u2013 The connection to the backend has failed.\nRendezvousStateError\u2013 The rendezvous state is corrupt.\nOptional[tuple[bytes,Any]]\nGet the name of the backend.\nSet the rendezvous state.\nThe new rendezvous state is set conditionally:\nIf the specifiedtokenmatches the fencing token stored in the\nbackend, the state will be updated. The new state will be returned\nto the caller along with its fencing token.\ntoken\nIf the specifiedtokendoes not match the fencing token stored\nin the backend, the state won\u2019t be updated; instead the existing\nstate along with its fencing token will be returned to the caller.\ntoken\nIf the specifiedtokenisNone, the new state will be set\nonly if there is no existing state in the backend. Either the new\nstate or the existing state along with its fencing token will be\nreturned to the caller.\ntoken\nNone\nstate(bytes) \u2013 The encoded rendezvous state.\ntoken(Optional[Any]) \u2013 An optional fencing token that was retrieved by a previous call\ntoget_state()orset_state().\nget_state()\nset_state()\nA tuple of the serialized rendezvous state, its fencing token, and\na boolean value indicating whether our set attempt succeeded.\nRendezvousConnectionError\u2013 The connection to the backend has failed.\nRendezvousStateError\u2013 The rendezvous state is corrupt.\nOptional[tuple[bytes,Any,bool]]\nHold the timeout configuration of a rendezvous.\njoin(Optional[timedelta]) \u2013 The time within which the rendezvous is expected to complete.\nlast_call(Optional[timedelta]) \u2013 An additional wait amount before completing the rendezvous once the\nrendezvous has the minimum number of required participants.\nclose(Optional[timedelta]) \u2013 The time within which the rendezvous is expected to close after a\ncall toRendezvousHandler.set_closed()orRendezvousHandler.shutdown().\nRendezvousHandler.set_closed()\nRendezvousHandler.shutdown()\nheartbeat(Optional[timedelta]) \u2013 The time within which a keep-alive heartbeat is expected to\ncomplete.\nGet the close timeout.\nGet the keep-alive heartbeat timeout.\nGet the join timeout.\nGet the last call timeout.\nCreate a newC10dRendezvousBackendfrom the specified parameters.\nC10dRendezvousBackend\nParameter\nDescription\nstore_type\nThe type of the C10d store. The currently supported types\nare \u201ctcp\u201d and \u201cfile\u201d which correspond totorch.distributed.TCPStoreandtorch.distributed.FileStore, respectively.\nDefaults to \u201ctcp\u201d.\ntorch.distributed.TCPStore\ntorch.distributed.FileStore\nread_timeout\nThe read timeout, in seconds, for store operations.\nDefaults to 60 seconds.\nNote this only applies totorch.distributed.TCPStore. It is not relevant\ntotorch.distributed.FileStorewhich does not\ntake in timeout as a parameter.\ntorch.distributed.TCPStore\ntorch.distributed.FileStore\nis_host\nA boolean value indicating whether this backend instance\nwill host the C10d store. If not specified it will be\ninferred heuristically by matching the hostname or the IP\naddress of this machine against the specified rendezvous\nendpoint. Defaults toNone.\nNone\nNote that this configuration option only applies totorch.distributed.TCPStore. In normal\ncircumstances you can safely skip it; the only time when\nit is needed is if its value cannot be correctly\ndetermined (e.g. the rendezvous endpoint has a CNAME as\nthe hostname or does not match the FQDN of the machine).\ntorch.distributed.TCPStore\ntuple[torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend,torch.distributed.distributed_c10d.Store]\nRepresents a C10d-backed rendezvous backend.\nstore(Store) \u2013 Thetorch.distributed.Storeinstance to use to\ncommunicate with the C10d store.\ntorch.distributed.Store\nrun_id(str) \u2013 The run id of the rendezvous.\nSee base class.\nOptional[tuple[bytes,Any]]\nSee base class.\nSee base class.\nOptional[tuple[bytes,Any,bool]]\nCreate a newEtcdRendezvousBackendfrom the specified parameters.\nEtcdRendezvousBackend\nParameter\nDescription\nread_timeout\nThe read timeout, in seconds, for etcd operations.\nDefaults to 60 seconds.\nprotocol\nThe protocol to use to communicate with etcd. Valid\nvalues are \u201chttp\u201d and \u201chttps\u201d. Defaults to \u201chttp\u201d.\nssl_cert\nThe path to the SSL client certificate to use along with\nHTTPS. Defaults toNone.\nNone\nssl_cert_key\nThe path to the private key of the SSL client certificate\nto use along with HTTPS. Defaults toNone.\nNone\nca_cert\nThe path to the rool SSL authority certificate. Defaults\ntoNone.\nNone\ntuple[torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend,torch.distributed.distributed_c10d.Store]\nRepresents an etcd-based rendezvous backend.\nclient(Client) \u2013 Theetcd.Clientinstance to use to communicate with etcd.\netcd.Client\nrun_id(str) \u2013 The run id of the rendezvous.\nkey_prefix(Optional[str]) \u2013 The path under which to store the rendezvous state in etcd.\nttl(Optional[int]) \u2013 The TTL of the rendezvous state. If not specified, defaults to two hours.\nSee base class.\nOptional[tuple[bytes,Any]]\nSee base class.\nSee base class.\nOptional[tuple[bytes,Any,bool]]\n\n## Etcd Rendezvous (Legacy)#\n\nWarning\nTheDynamicRendezvousHandlerclass supersedes theEtcdRendezvousHandlerclass, and is recommended for most users.EtcdRendezvousHandleris in\nmaintenance mode and will be deprecated in the future.\nDynamicRendezvousHandler\nEtcdRendezvousHandler\nEtcdRendezvousHandler\nImplements atorch.distributed.elastic.rendezvous.RendezvousHandlerinterface\nbacked bytorch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous.EtcdRendezvousHandleruses a URL to configure the type of rendezvous to\nuse and to pass implementation specific configurations to the rendezvous\nmodule. The basic etcd rendezvous configuration URL looks like the following\ntorch.distributed.elastic.rendezvous.RendezvousHandler\ntorch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous\nEtcdRendezvousHandler\n\n```python\netcd://<etcd_address>:<port>/<job_id>?min_workers=<min_workers>&max_workers=<max_workers>  # noqa: W605\n\n-- example --\n\netcd://localhost:2379/1234?min_workers=1&max_workers=3\n\n```\n\nThe URL above is interpreted as follows:\nUse the rendezvous handler that is registered with theetcdscheme\netcd\nTheetcdendpoint to use islocalhost:2379\netcd\nlocalhost:2379\njob_id==1234is used as the prefix in etcd (this allows one to\nshare a common etcd server for multiple jobs so long as thejob_idsare guaranteed to be unique). Note that the job id can be\nany string (e.g. does not need to be a number) as long as it is\nunique.\njob_id==1234\njob_ids\nmin_workers=1andmax_workers=3specifies a range for\nmembership size - Torch Distributed Elastic starts running the job as\nlong as the cluster size is greater than or equal tomin_workersand admits up tomax_workersinto the cluster.\nmin_workers=1\nmax_workers=3\nmin_workers\nmax_workers\nBelow are a full list of the parameters that can be passed to etcd\nrendezvous:\nParameter\nDescription\nmin_workers\nminimum number of\nworkers for the\nrendezvous to be valid\nmax_workers\nmaximum number of\nworkers to admit\ntimeout\ntotal timeout within\nwhich next_rendezvous is\nexpected to succeed\n(default 600s)\nlast_call_timeout\nadditional wait amount\n(\u201clast call\u201d) after min\nnumber of workers has\nbeen reached (defaults\nto 30s)\netcd_prefix\npath prefix (from etcd\nroot), inside which all\netcd nodes will be\ncreated (defaults to/torchelastic/p2p)\n/torchelastic/p2p\n\n## Etcd Store#\n\nTheEtcdStoreis the C10dStoreinstance type returned bynext_rendezvous()when etcd is used as the rendezvous backend.\nEtcdStore\nStore\nnext_rendezvous()\nImplement a c10 Store interface by piggybacking on the rendezvous etcd instance.\nThis is the store object returned byEtcdRendezvous.\nEtcdRendezvous\nAtomically increment a value by an integer amount.\nThe integer is represented as a string using base 10. If key is not present,\na default value of0will be assumed.\n0\nthe new (incremented) value\nint\nCheck if all of the keys are immediately present (without waiting).\nbool\nGet a value by key, possibly doing a blocking wait.\nIf key is not immediately present, will do a blocking wait\nfor at mosttimeoutduration or until the key is published.\ntimeout\nvalue(bytes)\n(bytes)\nLookupError - If key still not published after timeout\u2013\nbytes\nWrite a key/value pair intoEtcdStore.\nEtcdStore\nBoth key and value may be either Pythonstrorbytes.\nstr\nbytes\nWait until all of the keys are published, or until timeout.\nLookupError - if timeout occurs\u2013\n\n## Etcd Server#\n\nTheEtcdServeris a convenience class that makes it easy for you to\nstart and stop an etcd server on a subprocess. This is useful for testing\nor single-node (multi-worker) deployments where manually setting up an\netcd server on the side is cumbersome.\nEtcdServer\nWarning\nFor production and multi-node deployments please consider\nproperly deploying a highly available etcd server as this is\nthe single point of failure for your distributed jobs.\nNote\ntested on etcd server v3.4.3.\nStarts and stops a local standalone etcd server on a random free\nport. Useful for single node, multi-worker launches or testing,\nwhere a sidecar etcd server is more convenient than having to\nseparately setup an etcd server.\nThis class registers a termination handler to shutdown the etcd\nsubprocess on exit. This termination handler is NOT a substitute for\ncalling thestop()method.\nstop()\nThe following fallback mechanism is used to find the etcd binary:\nUses env var TORCHELASTIC_ETCD_BINARY_PATH\nUses<thisfileroot>/bin/etcdif one exists\n<thisfileroot>/bin/etcd\nUsesetcdfromPATH\netcd\nPATH\nUsage\n\n```python\nserver = EtcdServer(\"/usr/bin/etcd\", 2379, \"/tmp/default.etcd\")\nserver.start()\nclient = server.get_client()\n# use client\nserver.stop()\n\n```\n\netcd_binary_path\u2013 path of etcd server binary (see above for fallback path)",
    "url": "https://pytorch.org/docs/stable/elastic/rendezvous.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "82f17dd78705464a6805c1d6413dc9a2",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/func.whirlwind_tour.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7012751d597b4bb1cb096d5a0806ff04",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/torch.cuda.CUDAGraph.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d3cffd76a12e1931f68bf3f5c075524e",
    "source": "pytorch_docs",
    "title": "torch.nn.init \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.nn.init#\n\nCreated On: Jun 11, 2019 | Last Updated On: Jul 07, 2022\nWarning\nAll the functions in this module are intended to be used to initialize neural network\nparameters, so they all run intorch.no_grad()mode and will not be taken into\naccount by autograd.\ntorch.no_grad()\nReturn the recommended gain value for the given nonlinearity function.\nThe values are as follows:\nnonlinearity\ngain\nLinear / Identity\n111\nConv{1,2,3}D\n111\nSigmoid\n111\nTanh\n53\\frac{5}{3}35\u200b\nReLU\n2\\sqrt{2}2\u200b\nLeaky Relu\n21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b\nSELU\n34\\frac{3}{4}43\u200b\nWarning\nIn order to implementSelf-Normalizing Neural Networks,\nyou should usenonlinearity='linear'instead ofnonlinearity='selu'.\nThis gives the initial weights a variance of1/N,\nwhich is necessary to induce a stable fixed point in the forward pass.\nIn contrast, the default gain forSELUsacrifices the normalization\neffect for more stable gradient flow in rectangular layers.\nnonlinearity='linear'\nnonlinearity='selu'\n1/N\nSELU\nnonlinearity(Literal['linear','conv1d','conv2d','conv3d','conv_transpose1d','conv_transpose2d','conv_transpose3d','sigmoid','tanh','relu','leaky_relu','selu']) \u2013 the non-linear function (nn.functionalname)\nparam(Optional[Union[int,float]]) \u2013 optional parameter for the non-linear function\nfloat\nExamples\n\n```python\n>>> gain = nn.init.calculate_gain(\n...     \"leaky_relu\", 0.2\n... )  # leaky_relu with negative_slope=0.2\n\n```\n\nFill the input Tensor with values drawn from the uniform distribution.\nU(a,b)\\mathcal{U}(a, b)U(a,b).\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\na(float) \u2013 the lower bound of the uniform distribution\nb(float) \u2013 the upper bound of the uniform distribution\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.uniform_(w)\n\n```\n\nFill the input Tensor with values drawn from the normal distribution.\nN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2).\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nmean(float) \u2013 the mean of the normal distribution\nstd(float) \u2013 the standard deviation of the normal distribution\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.normal_(w)\n\n```\n\nFill the input Tensor with the valueval\\text{val}val.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nval(float) \u2013 the value to fill the tensor with\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.constant_(w, 0.3)\n\n```\n\nFill the input Tensor with the scalar value1.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.ones_(w)\n\n```\n\nFill the input Tensor with the scalar value0.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)\n\n```\n\nFill the 2-dimensional inputTensorwith the identity matrix.\nPreserves the identity of the inputs inLinearlayers, where as\nmany inputs are preserved as possible.\ntensor(Tensor) \u2013 a 2-dimensionaltorch.Tensor\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.eye_(w)\n\n```\n\nFill the {3, 4, 5}-dimensional inputTensorwith the Dirac delta function.\nPreserves the identity of the inputs inConvolutionallayers, where as many input channels are preserved as possible. In case\nof groups>1, each group of channels preserves identity\ntensor(Tensor) \u2013 a {3, 4, 5}-dimensionaltorch.Tensor\ngroups(int,optional) \u2013 number of groups in the conv layer (default: 1)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 16, 5, 5)\n>>> nn.init.dirac_(w)\n>>> w = torch.empty(3, 24, 5, 5)\n>>> nn.init.dirac_(w, 3)\n\n```\n\nFill the inputTensorwith values using a Xavier uniform distribution.\nThe method is described inUnderstanding the difficulty of training\ndeep feedforward neural networks- Glorot, X. & Bengio, Y. (2010).\nThe resulting tensor will have values sampled fromU(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)where\nAlso known as Glorot initialization.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\ngain(float) \u2013 an optional scaling factor\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain(\"relu\"))\n\n```\n\nFill the inputTensorwith values using a Xavier normal distribution.\nThe method is described inUnderstanding the difficulty of training deep feedforward\nneural networks- Glorot, X. & Bengio, Y. (2010). The resulting tensor\nwill have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where\nAlso known as Glorot initialization.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\ngain(float) \u2013 an optional scaling factor\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.xavier_normal_(w)\n\n```\n\nFill the inputTensorwith values using a Kaiming uniform distribution.\nThe method is described inDelving deep into rectifiers: Surpassing\nhuman-level performance on ImageNet classification- He, K. et al. (2015).\nThe resulting tensor will have values sampled fromU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)where\nAlso known as He initialization.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\na(float) \u2013 the negative slope of the rectifier used after this layer (only\nused with'leaky_relu')\n'leaky_relu'\nmode(Literal['fan_in','fan_out']) \u2013 either'fan_in'(default) or'fan_out'. Choosing'fan_in'preserves the magnitude of the variance of the weights in the\nforward pass. Choosing'fan_out'preserves the magnitudes in the\nbackwards pass.\n'fan_in'\n'fan_out'\n'fan_in'\n'fan_out'\nnonlinearity(Literal['linear','conv1d','conv2d','conv3d','conv_transpose1d','conv_transpose2d','conv_transpose3d','sigmoid','tanh','relu','leaky_relu','selu']) \u2013 the non-linear function (nn.functionalname),\nrecommended to use only with'relu'or'leaky_relu'(default).\n'relu'\n'leaky_relu'\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode=\"fan_in\", nonlinearity=\"relu\")\n\n```\n\nNote\nBe aware thatfan_inandfan_outare calculated assuming\nthat the weight matrix is used in a transposed manner,\n(i.e.,x@w.TinLinearlayers, wherew.shape=[fan_out,fan_in]).\nThis is important for correct initialization.\nIf you plan to usex@w, wherew.shape=[fan_in,fan_out],\npass in a transposed weight matrix, i.e.nn.init.kaiming_uniform_(w.T,...).\nfan_in\nfan_out\nx@w.T\nLinear\nw.shape=[fan_out,fan_in]\nx@w\nw.shape=[fan_in,fan_out]\nnn.init.kaiming_uniform_(w.T,...)\nFill the inputTensorwith values using a Kaiming normal distribution.\nThe method is described inDelving deep into rectifiers: Surpassing\nhuman-level performance on ImageNet classification- He, K. et al. (2015).\nThe resulting tensor will have values sampled fromN(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)where\nAlso known as He initialization.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\na(float) \u2013 the negative slope of the rectifier used after this layer (only\nused with'leaky_relu')\n'leaky_relu'\nmode(Literal['fan_in','fan_out']) \u2013 either'fan_in'(default) or'fan_out'. Choosing'fan_in'preserves the magnitude of the variance of the weights in the\nforward pass. Choosing'fan_out'preserves the magnitudes in the\nbackwards pass.\n'fan_in'\n'fan_out'\n'fan_in'\n'fan_out'\nnonlinearity(Literal['linear','conv1d','conv2d','conv3d','conv_transpose1d','conv_transpose2d','conv_transpose3d','sigmoid','tanh','relu','leaky_relu','selu']) \u2013 the non-linear function (nn.functionalname),\nrecommended to use only with'relu'or'leaky_relu'(default).\n'relu'\n'leaky_relu'\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode=\"fan_out\", nonlinearity=\"relu\")\n\n```\n\nNote\nBe aware thatfan_inandfan_outare calculated assuming\nthat the weight matrix is used in a transposed manner,\n(i.e.,x@w.TinLinearlayers, wherew.shape=[fan_out,fan_in]).\nThis is important for correct initialization.\nIf you plan to usex@w, wherew.shape=[fan_in,fan_out],\npass in a transposed weight matrix, i.e.nn.init.kaiming_normal_(w.T,...).\nfan_in\nfan_out\nx@w.T\nLinear\nw.shape=[fan_out,fan_in]\nx@w\nw.shape=[fan_in,fan_out]\nnn.init.kaiming_normal_(w.T,...)\nFill the input Tensor with values drawn from a truncated normal distribution.\nThe values are effectively drawn from the\nnormal distributionN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2)with values outside[a,b][a, b][a,b]redrawn until they are within\nthe bounds. The method used for generating the random values works\nbest whena\u2264mean\u2264ba \\leq \\text{mean} \\leq ba\u2264mean\u2264b.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nmean(float) \u2013 the mean of the normal distribution\nstd(float) \u2013 the standard deviation of the normal distribution\na(float) \u2013 the minimum cutoff value\nb(float) \u2013 the maximum cutoff value\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.trunc_normal_(w)\n\n```\n\nFill the inputTensorwith a (semi) orthogonal matrix.\nDescribed inExact solutions to the nonlinear dynamics of learning in deep\nlinear neural networks- Saxe, A. et al. (2013). The input tensor must have\nat least 2 dimensions, and for tensors with more than 2 dimensions the\ntrailing dimensions are flattened.\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor, wheren\u22652n \\geq 2n\u22652\ngain(float) \u2013 optional scaling factor\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.orthogonal_(w)\n\n```\n\nFill the 2D inputTensoras a sparse matrix.\nThe non-zero elements will be drawn from the normal distributionN(0,0.01)\\mathcal{N}(0, 0.01)N(0,0.01), as described inDeep learning via\nHessian-free optimization- Martens, J. (2010).\ntensor(Tensor) \u2013 an n-dimensionaltorch.Tensor\nsparsity(float) \u2013 The fraction of elements in each column to be set to zero\nstd(float) \u2013 the standard deviation of the normal distribution used to generate\nthe non-zero values\ngenerator(Optional[Generator]) \u2013 the torch Generator to sample from (default: None)\nTensor\nExamples\n\n```python\n>>> w = torch.empty(3, 5)\n>>> nn.init.sparse_(w, sparsity=0.1)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/nn.init.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b3a1e5d7e1701ebb0448a4bef15ae28a",
    "source": "pytorch_docs",
    "title": "Multiprocessing best practices \u2014 PyTorch 2.9 documentation",
    "text": "\n## Multiprocessing best practices#\n\nCreated On: Jan 16, 2017 | Last Updated On: Jun 18, 2025\ntorch.multiprocessingis a drop in replacement for Python\u2019smultiprocessingmodule. It supports the exact same operations,\nbut extends it, so that all tensors sent through amultiprocessing.Queue, will have their data moved into shared\nmemory and will only send a handle to another process.\ntorch.multiprocessing\nmultiprocessing\nmultiprocessing.Queue\nNote\nWhen aTensoris sent to another process, theTensordata is shared. Iftorch.Tensor.gradis\nnotNone, it is also shared. After aTensorwithout\natorch.Tensor.gradfield is sent to the other process, it\ncreates a standard process-specific.gradTensorthat\nis not automatically shared across all processes, unlike how theTensor\u2019s data has been shared.\nTensor\nTensor\ntorch.Tensor.grad\nNone\nTensor\ntorch.Tensor.grad\n.grad\nTensor\nTensor\nThis allows to implement various training methods, like Hogwild, A3C, or any\nothers that require asynchronous operation.\n\n## Poison fork in multiprocessing#\n\nWhen using multiprocessing withaccelerators, a known issue called \u201cpoison fork\u201d may occur.\nThis happens when the accelerator\u2019s runtime is not fork safe and is initialized before a process forks, leading to\nruntime errors in child processes.\nAvoid initializing the accelerator in the main process before forking child processes.\nUse an alternative process start methods, such asspawnorforkserver, which ensures a clean initialization of each process.\nspawn\nforkserver\n\n## CUDA in multiprocessing#\n\nThe CUDA runtime has the limitation described inPoison fork in multiprocessingwhen using theforkstart method;\neither thespawnorforkserverstart method are required to use CUDA in subprocesses.\nfork\nspawn\nforkserver\nNote\nThe start method can be set via either creating a context withmultiprocessing.get_context(...)or directly usingmultiprocessing.set_start_method(...).\nmultiprocessing.get_context(...)\nmultiprocessing.set_start_method(...)\nUnlike CPU tensors, the sending process is required to keep the original tensor\nas long as the receiving process retains a copy of the tensor. It is implemented\nunder the hood but requires users to follow the best practices for the program\nto run correctly. For example, the sending process must stay alive as long as\nthe consumer process has references to the tensor, and the refcounting can not\nsave you if the consumer process exits abnormally via a fatal signal. Seethis section.\nSee also:Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel\n\n## Best practices and tips#\n\n\n## Avoiding and fighting deadlocks#\n\nThere are a lot of things that can go wrong when a new process is spawned, with\nthe most common cause of deadlocks being background threads. If there\u2019s any\nthread that holds a lock or imports a module, andforkis called, it\u2019s very\nlikely that the subprocess will be in a corrupted state and will deadlock or\nfail in a different way. Note that even if you don\u2019t, Python built in\nlibraries do - no need to look further thanmultiprocessing.multiprocessing.Queueis actually a very complex class, that\nspawns multiple threads used to serialize, send and receive objects, and they\ncan cause aforementioned problems too. If you find yourself in such situation\ntry using aSimpleQueue, that doesn\u2019t\nuse any additional threads.\nfork\nmultiprocessing\nmultiprocessing.Queue\nSimpleQueue\nWe\u2019re trying our best to make it easy for you and ensure these deadlocks don\u2019t\nhappen but some things are out of our control. If you have any issues you can\u2019t\ncope with for a while, try reaching out on forums, and we\u2019ll see if it\u2019s an\nissue we can fix.\n\n## Reuse buffers passed through a Queue#\n\nRemember that each time you put aTensorinto amultiprocessing.Queue, it has to be moved into shared memory.\nIf it\u2019s already shared, it is a no-op, otherwise it will incur an additional\nmemory copy that can slow down the whole process. Even if you have a pool of\nprocesses sending data to a single one, make it send the buffers back - this\nis nearly free and will let you avoid a copy when sending next batch.\nTensor\nmultiprocessing.Queue\n\n## Asynchronous multiprocess training (e.g. Hogwild)#\n\nUsingtorch.multiprocessing, it is possible to train a model\nasynchronously, with parameters either shared all the time, or being\nperiodically synchronized. In the first case, we recommend sending over the whole\nmodel object, while in the latter, we advise to only send thestate_dict().\ntorch.multiprocessing\nstate_dict()\nWe recommend usingmultiprocessing.Queuefor passing all kinds\nof PyTorch objects between processes. It is possible to e.g. inherit the tensors\nand storages already in shared memory, when using theforkstart method,\nhowever it is very bug prone and should be used with care, and only by advanced\nusers. Queues, even though they\u2019re sometimes a less elegant solution, will work\nproperly in all cases.\nmultiprocessing.Queue\nfork\nWarning\nYou should be careful about having global statements, that are not guarded\nwith anif__name__=='__main__'. If a different start method thanforkis used, they will be executed in all subprocesses.\nif__name__=='__main__'\nfork\nA concrete Hogwild implementation can be found in theexamples repository,\nbut to showcase the overall structure of the code, there\u2019s also a minimal\nexample below as well:\n\n```python\nimport torch.multiprocessing as mp\nfrom model import MyModel\n\ndef train(model):\n    # Construct data_loader, optimizer, etc.\n    for data, labels in data_loader:\n        optimizer.zero_grad()\n        loss_fn(model(data), labels).backward()\n        optimizer.step()  # This will update the shared parameters\n\nif __name__ == '__main__':\n    num_processes = 4\n    model = MyModel()\n    # NOTE: this is required for the ``fork`` method to work\n    model.share_memory()\n    processes = []\n    for rank in range(num_processes):\n        p = mp.Process(target=train, args=(model,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n\n```\n\n\n## CPU in multiprocessing#\n\nInappropriate multiprocessing can lead to CPU oversubscription, causing\ndifferent processes to compete for CPU resources, resulting in low\nefficiency.\nThis tutorial will explain what CPU oversubscription is and how to\navoid it.\n\n## CPU oversubscription#\n\nCPU oversubscription is a technical term that refers to a situation\nwhere the total number of vCPUs allocated to a system exceeds the total\nnumber of vCPUs available on the hardware.\nThis leads to severe contention for CPU resources. In such cases, there\nis frequent switching between processes, which increases processes\nswitching overhead and decreases overall system efficiency.\nSee CPU oversubscription with the code examples in the Hogwild\nimplementation found in theexample\nrepository.\nWhen running the training example with the following command on CPU\nusing 4 processes:\n\n```python\npython main.py --num-processes 4\n\n```\n\nAssuming there are N vCPUs available on the machine, executing the above\ncommand will generate 4 subprocesses. Each subprocess will allocate N\nvCPUs for itself, resulting in a requirement of 4*N vCPUs. However, the\nmachine only has N vCPUs available. Consequently, the different\nprocesses will compete for resources, leading to frequent process\nswitching.\nThe following observations indicate the presence of CPU over\nsubscription:\nHigh CPU Utilization: By using thehtopcommand, you can observe\nthat the CPU utilization is consistently high, often reaching or\nexceeding its maximum capacity. This indicates that the demand for\nCPU resources exceeds the available physical cores, causing\ncontention and competition among processes for CPU time.\nhtop\nFrequent Context Switching with Low System Efficiency: In an\noversubscribed CPU scenario, processes compete for CPU time, and the\noperating system needs to rapidly switch between different processes\nto allocate resources fairly. This frequent context switching adds\noverhead and reduces the overall system efficiency.\n\n## Avoid CPU oversubscription#\n\nA good way to avoid CPU oversubscription is proper resource allocation.\nEnsure that the number of processes or threads running concurrently does\nnot exceed the available CPU resources.\nIn this case, a solution would be to specify the appropriate number of\nthreads in the subprocesses. This can be achieved by setting the number\nof threads for each process using thetorch.set_num_threads(int)function in subprocess.\ntorch.set_num_threads(int)\nAssuming there are N vCPUs on the machine and M processes will be\ngenerated, the maximumnum_threadsvalue used by each process would\nbefloor(N/M). To avoid CPU oversubscription in the mnist_hogwild\nexample, the following changes are needed for the filetrain.pyinexample\nrepository.\nnum_threads\nfloor(N/M)\ntrain.py\n\n```python\ndef train(rank, args, model, device, dataset, dataloader_kwargs):\n    torch.manual_seed(args.seed + rank)\n\n    #### define the num threads used in current sub-processes\n    torch.set_num_threads(floor(N/M))\n\n    train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)\n\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    for epoch in range(1, args.epochs + 1):\n        train_epoch(epoch, args, model, device, train_loader, optimizer)\n\n```\n\nSetnum_threadfor each process usingtorch.set_num_threads(floor(N/M)). where you replace N with the\nnumber of vCPUs available and M with the chosen number of processes. The\nappropriatenum_threadvalue will vary depending on the specific\ntask at hand. However, as a general guideline, the maximum value for thenum_threadshould befloor(N/M)to avoid CPU oversubscription.\nIn themnist_hogwildtraining example, after avoiding CPU over\nsubscription, you can achieve a 30x performance boost.\nnum_thread\ntorch.set_num_threads(floor(N/M))\nnum_thread\nnum_thread\nfloor(N/M)",
    "url": "https://pytorch.org/docs/stable/notes/multiprocessing.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7b2153243a7cc17e47f911abd46a65ac",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/windows.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4a5c243d6e028997394904fb34d9f2c4",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/cond.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0e6e25f373506b91383642f984e29d9a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/model_zoo.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "fce58a420b6c83cfc291b01d59ef577e",
    "source": "pytorch_docs",
    "title": "Distributed communication package - torch.distributed \u2014 PyTorch 2.9 documentation",
    "text": "\n## Distributed communication package - torch.distributed#\n\nCreated On: Jul 12, 2017 | Last Updated On: Sep 04, 2025\nNote\nPlease refer toPyTorch Distributed Overviewfor a brief introduction to all features related to distributed training.\n\n## Backends#\n\ntorch.distributedsupports four built-in backends, each with\ndifferent capabilities. The table below shows which functions are available\nfor use with a CPU or GPU for each backend. For NCCL, GPU refers to CUDA GPU\nwhile for XCCL to XPU GPU.\ntorch.distributed\nMPI supports CUDA only if the implementation used to build PyTorch supports it.\nBackend\ngloo\ngloo\nmpi\nmpi\nnccl\nnccl\nxccl\nxccl\nDevice\nCPU\nGPU\nCPU\nGPU\nCPU\nGPU\nCPU\nGPU\nsend\n\u2713\n\u2718\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\nrecv\n\u2713\n\u2718\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\nbroadcast\n\u2713\n\u2713\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\nall_reduce\n\u2713\n\u2713\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\nreduce\n\u2713\n\u2713\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\nall_gather\n\u2713\n\u2713\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\ngather\n\u2713\n\u2713\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\nscatter\n\u2713\n\u2713\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\nreduce_scatter\n\u2713\n\u2713\n\u2718\n\u2718\n\u2718\n\u2713\n\u2718\n\u2713\nall_to_all\n\u2713\n\u2713\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\nbarrier\n\u2713\n\u2718\n\u2713\n?\n\u2718\n\u2713\n\u2718\n\u2713\n\n## Backends that come with PyTorch#\n\nPyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype).\nBy default for Linux, the Gloo and NCCL backends are built and included in PyTorch\ndistributed (NCCL only when building with CUDA). MPI is an optional backend that can only be\nincluded if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI\ninstalled.)\nNote\nAs of PyTorch v1.8, Windows supports all collective communications backend but NCCL,\nIf theinit_methodargument ofinit_process_group()points to a file it must adhere\nto the following schema:\ninit_method\ninit_process_group()\nLocal file system,init_method=\"file:///d:/tmp/some_file\"\ninit_method=\"file:///d:/tmp/some_file\"\nShared file system,init_method=\"file://////{machine_name}/{share_folder_name}/some_file\"\ninit_method=\"file://////{machine_name}/{share_folder_name}/some_file\"\nSame as on Linux platform, you can enable TcpStore by setting environment variables,\nMASTER_ADDR and MASTER_PORT.\n\n## Which backend to use?#\n\nIn the past, we were often asked: \u201cwhich backend should I use?\u201d.\nRule of thumb\nUse the NCCL backend for distributed training with CUDAGPU.\nUse the XCCL backend for distributed training with XPUGPU.\nUse the Gloo backend for distributed training withCPU.\nGPU hosts with InfiniBand interconnect\nUse NCCL, since it\u2019s the only backend that currently supports\nInfiniBand and GPUDirect.\nGPU hosts with Ethernet interconnect\nUse NCCL, since it currently provides the best distributed GPU\ntraining performance, especially for multiprocess single-node or\nmulti-node distributed training. If you encounter any problem with\nNCCL, use Gloo as the fallback option. (Note that Gloo currently\nruns slower than NCCL for GPUs.)\nCPU hosts with InfiniBand interconnect\nIf your InfiniBand has enabled IP over IB, use Gloo, otherwise,\nuse MPI instead. We are planning on adding InfiniBand support for\nGloo in the upcoming releases.\nCPU hosts with Ethernet interconnect\nUse Gloo, unless you have specific reasons to use MPI.\n\n## Common environment variables#\n\nBy default, both the NCCL and Gloo backends will try to find the right network interface to use.\nIf the automatically detected interface is not correct, you can override it using the following\nenvironment variables (applicable to the respective backend):\nNCCL_SOCKET_IFNAME, for exampleexportNCCL_SOCKET_IFNAME=eth0\nexportNCCL_SOCKET_IFNAME=eth0\nGLOO_SOCKET_IFNAME, for exampleexportGLOO_SOCKET_IFNAME=eth0\nexportGLOO_SOCKET_IFNAME=eth0\nIf you\u2019re using the Gloo backend, you can specify multiple interfaces by separating\nthem by a comma, like this:exportGLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3.\nThe backend will dispatch operations in a round-robin fashion across these interfaces.\nIt is imperative that all processes specify the same number of interfaces in this variable.\nexportGLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3\nDebugging- in case of NCCL failure, you can setNCCL_DEBUG=INFOto print an explicit\nwarning message as well as basic NCCL initialization information.\nNCCL_DEBUG=INFO\nYou may also useNCCL_DEBUG_SUBSYSto get more details about a specific\naspect of NCCL. For example,NCCL_DEBUG_SUBSYS=COLLwould print logs of\ncollective calls, which may be helpful when debugging hangs, especially those\ncaused by collective type or message size mismatch. In case of topology\ndetection failure, it would be helpful to setNCCL_DEBUG_SUBSYS=GRAPHto inspect the detailed detection result and save as reference if further help\nfrom NCCL team is needed.\nNCCL_DEBUG_SUBSYS\nNCCL_DEBUG_SUBSYS=COLL\nNCCL_DEBUG_SUBSYS=GRAPH\nPerformance tuning- NCCL performs automatic tuning based on its topology detection to save users\u2019\ntuning effort. On some socket-based systems, users may still try tuningNCCL_SOCKET_NTHREADSandNCCL_NSOCKS_PERTHREADto increase socket\nnetwork bandwidth. These two environment variables have been pre-tuned by NCCL\nfor some cloud providers, such as AWS or GCP.\nNCCL_SOCKET_NTHREADS\nNCCL_NSOCKS_PERTHREAD\nFor a full list of NCCL environment variables, please refer toNVIDIA NCCL\u2019s official documentation\nYou can tune NCCL communicators even further usingtorch.distributed.ProcessGroupNCCL.NCCLConfigandtorch.distributed.ProcessGroupNCCL.Options. Learn more about them usinghelp(e.g.help(torch.distributed.ProcessGroupNCCL.NCCLConfig)) in the interpreter.\ntorch.distributed.ProcessGroupNCCL.NCCLConfig\ntorch.distributed.ProcessGroupNCCL.Options\nhelp\nhelp(torch.distributed.ProcessGroupNCCL.NCCLConfig)\n\n## Basics#\n\nThetorch.distributedpackage provides PyTorch support and communication primitives\nfor multiprocess parallelism across several computation nodes running on one or more\nmachines. The classtorch.nn.parallel.DistributedDataParallel()builds on this\nfunctionality to provide synchronous distributed training as a wrapper around any\nPyTorch model. This differs from the kinds of parallelism provided byMultiprocessing package - torch.multiprocessingandtorch.nn.DataParallel()in that it supports\nmultiple network-connected machines and in that the user must explicitly launch a separate\ncopy of the main training script for each process.\ntorch.distributed\ntorch.nn.parallel.DistributedDataParallel()\ntorch.nn.DataParallel()\nIn the single-machine synchronous case,torch.distributedor thetorch.nn.parallel.DistributedDataParallel()wrapper may still have advantages over other\napproaches to data-parallelism, includingtorch.nn.DataParallel():\ntorch.distributed\ntorch.nn.parallel.DistributedDataParallel()\ntorch.nn.DataParallel()\nEach process maintains its own optimizer and performs a complete optimization step with each\niteration. While this may appear redundant, since the gradients have already been gathered\ntogether and averaged across processes and are thus the same for every process, this means\nthat no parameter broadcast step is needed, reducing time spent transferring tensors between\nnodes.\nEach process contains an independent Python interpreter, eliminating the extra interpreter\noverhead and \u201cGIL-thrashing\u201d that comes from driving several execution threads, model\nreplicas, or GPUs from a single Python process. This is especially important for models that\nmake heavy use of the Python runtime, including models with recurrent layers or many small\ncomponents.\n\n## Initialization#\n\nThe package needs to be initialized using thetorch.distributed.init_process_group()ortorch.distributed.device_mesh.init_device_mesh()function before calling any other methods.\nBoth block until all processes have joined.\ntorch.distributed.init_process_group()\ntorch.distributed.device_mesh.init_device_mesh()\nWarning\nInitialization is not thread-safe. Process group creation should be performed from a single thread, to prevent\ninconsistent \u2018UUID\u2019 assignment across ranks, and to prevent races during initialization that can lead to hangs.\nReturnTrueif the distributed package is available.\nTrue\nOtherwise,torch.distributeddoes not expose any other APIs. Currently,torch.distributedis available on Linux, MacOS and Windows. SetUSE_DISTRIBUTED=1to enable it when building PyTorch from source.\nCurrently, the default value isUSE_DISTRIBUTED=1for Linux and Windows,USE_DISTRIBUTED=0for MacOS.\ntorch.distributed\ntorch.distributed\nUSE_DISTRIBUTED=1\nUSE_DISTRIBUTED=1\nUSE_DISTRIBUTED=0\nbool\nInitialize the default distributed process group.\nThis will also initialize the distributed package.\nSpecifystore,rank, andworld_sizeexplicitly.\nstore\nrank\nworld_size\nSpecifyinit_method(a URL string) which indicates where/how\nto discover peers. Optionally specifyrankandworld_size,\nor encode all required parameters in the URL and omit them.\ninit_method\nrank\nworld_size\nIf neither is specified,init_methodis assumed to be \u201cenv://\u201d.\ninit_method\nbackend(strorBackend,optional) \u2013 The backend to use. Depending on\nbuild-time configurations, valid values includempi,gloo,nccl,ucc,xcclor one that is registered by a third-party\nplugin.\nSince 2.6, ifbackendis not provided, c10d will use a backend\nregistered for the device type indicated by thedevice_idkwarg\n(if provided). The known default registrations today are:ncclforcuda,glooforcpu,xcclforxpu.\nIf neitherbackendnordevice_idis provided, c10d will\ndetect the accelerator on the run-time machine and use a backend\nregistered for that detected accelerator (orcpu).\nThis field can be given as a lowercase string (e.g.,\"gloo\"),\nwhich can also be accessed viaBackendattributes (e.g.,Backend.GLOO).\nIf using multiple processes per machine withncclbackend, each\nprocess must have exclusive access to every GPU it uses, as sharing\nGPUs between processes can result in deadlock or NCCL invalid usage.uccbackend is experimental.\nDefault backend for the device can be queried withget_default_backend_for_device().\nmpi\ngloo\nnccl\nucc\nxccl\nbackend\nnccl\ncuda\ngloo\ncpu\nxccl\nxpu\nbackend\ndevice_id\ncpu\n\"gloo\"\nBackend\nBackend.GLOO\nnccl\nucc\nget_default_backend_for_device()\ninit_method(str,optional) \u2013 URL specifying how to initialize the\nprocess group. Default is \u201cenv://\u201d if noinit_methodorstoreis specified.\nMutually exclusive withstore.\ninit_method\nstore\nstore\nworld_size(int,optional) \u2013 Number of processes participating in\nthe job. Required ifstoreis specified.\nstore\nrank(int,optional) \u2013 Rank of the current process (it should be a\nnumber between 0 andworld_size-1).\nRequired ifstoreis specified.\nworld_size\nstore\nstore(Store,optional) \u2013 Key/value store accessible to all workers, used\nto exchange connection/address information.\nMutually exclusive withinit_method.\ninit_method\ntimeout(timedelta,optional) \u2013 Timeout for operations executed against\nthe process group. Default value is 10 minutes for NCCL and 30 minutes for other backends.\nThis is the duration after which collectives will be aborted asynchronously and the process will crash.\nThis is done since CUDA execution is async and it is no longer safe to continue executing user code since\nfailed async NCCL operations might result in subsequent CUDA operations running on corrupted data.\nWhen TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.\ngroup_name(str,optional,deprecated) \u2013 Group name. This argument is ignored\npg_options(ProcessGroupOptions,optional) \u2013 process group options\nspecifying what additional options need to be passed in during\nthe construction of specific process groups. As of now, the only\noptions we support isProcessGroupNCCL.Optionsfor thencclbackend,is_high_priority_streamcan be specified so that\nthe nccl backend can pick up high priority cuda streams when\nthere\u2019re compute kernels waiting. For other available options to config nccl,\nSeehttps://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-t\nProcessGroupNCCL.Options\nnccl\nis_high_priority_stream\ndevice_id(torch.device|int,optional) \u2013 a single, specific device\nthis process will work on, allowing for backend-specific\noptimizations.  Currently this has two effects, only under\nNCCL: the communicator is immediately formed (callingncclCommInit*immediately rather than the normal lazy\ncall) and sub-groups will usencclCommSplitwhen\npossible to avoid unnecessary overhead of group creation. If you\nwant to know NCCL initialization error early, you can also use this\nfield. If anintis provided, the API assumes that the accelerator\ntype at compile time will be used.\nncclCommInit*\nncclCommSplit\nNote\nTo enablebackend==Backend.MPI, PyTorch needs to be built from source\non a system that supports MPI.\nbackend==Backend.MPI\nNote\nSupport for multiple backends is experimental. Currently when no backend is\nspecified, bothglooandncclbackends will be created. Thegloobackend\nwill be used for collectives with CPU tensors and thencclbackend will be used\nfor collectives with CUDA tensors. A custom backend can be specified by passing in\na string with format \u201c<device_type>:<backend_name>,<device_type>:<backend_name>\u201d, e.g.\n\u201ccpu:gloo,cuda:custom_backend\u201d.\ngloo\nnccl\ngloo\nnccl\nInitializes aDeviceMeshbased ondevice_type,mesh_shape, andmesh_dim_namesparameters.\nThis creates a DeviceMesh with an n-dimensional array layout, wherenis the length ofmesh_shape.\nIfmesh_dim_namesis provided, each dimension is labeled asmesh_dim_names[i].\nNote\ninit_device_meshfollows SPMD programming model, meaning the same PyTorch Python program\nruns on all processes/ranks in the cluster. Ensuremesh_shape(the dimensions of the nD array\ndescribing device layout) is identical across all ranks. Inconsistentmesh_shapemay lead to hanging.\nNote\nIf no process group is found, init_device_mesh will initialize distributed process group/groups\nrequired for distributed communications behind the scene.\ndevice_type(str) \u2013 The device type of the mesh. Currently supports: \u201ccpu\u201d, \u201ccuda/cuda-like\u201d, \u201cxpu\u201d.\nPassing in a device type with a GPU index, such as \u201ccuda:0\u201d, is not allowed.\nmesh_shape(Tuple[int]) \u2013 A tuple defining the dimensions of the multi-dimensional array\ndescribing the layout of devices.\nmesh_dim_names(Tuple[str],optional) \u2013 A tuple of mesh dimension names to assign to each dimension\nof the multi-dimensional array describing the layout of devices. Its length must match the length\nofmesh_shape. Each string inmesh_dim_namesmust be unique.\nbackend_override(Dict[int|str,tuple[str,Options]|str|Options],optional) \u2013 Overrides for some or all of\nthe ProcessGroups that will be created for each mesh dimension. Each key can be either the index of a\ndimension or its name (if mesh_dim_names is provided). Each value can be a tuple containing the name\nof the backend and its options, or just one of these two components (in which case the other will be\nset to its default value).\nADeviceMeshobject representing the device layout.\nDeviceMesh\nDeviceMesh\nExample:\n\n```python\n>>> from torch.distributed.device_mesh import init_device_mesh\n>>>\n>>> mesh_1d = init_device_mesh(\"cuda\", mesh_shape=(8,))\n>>> mesh_2d = init_device_mesh(\"cuda\", mesh_shape=(2, 8), mesh_dim_names=(\"dp\", \"tp\"))\n\n```\n\nCheck if the default process group has been initialized.\nbool\nCheck if the MPI backend is available.\nbool\nCheck if the NCCL backend is available.\nbool\nCheck if the Gloo backend is available.\nbool\nCheck if the XCCL backend is available.\nbool\nCheck whether this process was launched withtorch.distributed.elastic(aka torchelastic).\ntorch.distributed.elastic\nThe existence ofTORCHELASTIC_RUN_IDenvironment\nvariable is used as a proxy to determine whether the current process\nwas launched with torchelastic. This is a reasonable proxy sinceTORCHELASTIC_RUN_IDmaps to the rendezvous id which is always a\nnon-null value indicating the job id for peer discovery purposes..\nTORCHELASTIC_RUN_ID\nTORCHELASTIC_RUN_ID\nbool\nReturn the default backend for the given device.\ndevice(Union[str,torch.device]) \u2013 The device to get the default backend for.\nThe default backend for the given device as a lower case string.\nstr\nCurrently three initialization methods are supported:\n\n## TCP initialization#\n\nThere are two ways to initialize using TCP, both requiring a network address\nreachable from all processes and a desiredworld_size. The first way\nrequires specifying an address that belongs to the rank 0 process. This\ninitialization method requires that all processes have manually specified ranks.\nworld_size\nNote that multicast address is not supported anymore in the latest distributed\npackage.group_nameis deprecated as well.\ngroup_name\n\n```python\nimport torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n\n```\n\n\n## Shared file-system initialization#\n\nAnother initialization method makes use of a file system that is shared and\nvisible from all machines in a group, along with a desiredworld_size. The URL should start\nwithfile://and contain a path to a non-existent file (in an existing\ndirectory) on a shared file system. File-system initialization will automatically\ncreate that file if it doesn\u2019t exist, but will not delete the file. Therefore, it\nis your responsibility to make sure that the file is cleaned up before the nextinit_process_group()call on the same file path/name.\nworld_size\nfile://\ninit_process_group()\nNote that automatic rank assignment is not supported anymore in the latest\ndistributed package andgroup_nameis deprecated as well.\ngroup_name\nWarning\nThis method assumes that the file system supports locking usingfcntl- most\nlocal systems and NFS support it.\nfcntl\nWarning\nThis method will always create the file and try its best to clean up and remove\nthe file at the end of the program. In other words, each initialization with\nthe file init method will need a brand new empty file in order for the initialization\nto succeed. If the same file used by the previous initialization (which happens not\nto get cleaned up) is used again, this is unexpected behavior and can often cause\ndeadlocks and failures. Therefore, even though this method will try its best to clean up\nthe file, if the auto-delete happens to be unsuccessful, it is your responsibility\nto ensure that the file is removed at the end of the training to prevent the same\nfile to be reused again during the next time. This is especially important\nif you plan to callinit_process_group()multiple times on the same file name.\nIn other words, if the file is not removed/cleaned up and you callinit_process_group()again on that file, failures are expected.\nThe rule of thumb here is that, make sure that the file is non-existent or\nempty every timeinit_process_group()is called.\ninit_process_group()\ninit_process_group()\ninit_process_group()\n\n```python\nimport torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n\n```\n\n\n## Environment variable initialization#\n\nThis method will read the configuration from environment variables, allowing\none to fully customize how the information is obtained. The variables to be set\nare:\nMASTER_PORT- required; has to be a free port on machine with rank 0\nMASTER_PORT\nMASTER_ADDR- required (except for rank 0); address of rank 0 node\nMASTER_ADDR\nWORLD_SIZE- required; can be set either here, or in a call to init function\nWORLD_SIZE\nRANK- required; can be set either here, or in a call to init function\nRANK\nThe machine with rank 0 will be used to set up all connections.\nThis is the default method, meaning thatinit_methoddoes not have to be specified (or\ncan beenv://).\ninit_method\nenv://\n\n## Improving initialization time#\n\nTORCH_GLOO_LAZY_INIT- establishes connections on demand rather than\nusing a full mesh which can greatly improve initialization time for non all2all\noperations.\nTORCH_GLOO_LAZY_INIT\n\n## Post-Initialization#\n\nOncetorch.distributed.init_process_group()was run, the following functions can be used. To\ncheck whether the process group has already been initialized usetorch.distributed.is_initialized().\ntorch.distributed.init_process_group()\ntorch.distributed.is_initialized()\nAn enum-like class for backends.\nAvailable backends: GLOO, NCCL, UCC, MPI, XCCL, and other registered backends.\nThe values of this class are lowercase strings, e.g.,\"gloo\". They can\nbe accessed as attributes, e.g.,Backend.NCCL.\n\"gloo\"\nBackend.NCCL\nThis class can be directly called to parse the string, e.g.,Backend(backend_str)will check ifbackend_stris valid, and\nreturn the parsed lowercase string if so. It also accepts uppercase strings,\ne.g.,Backend(\"GLOO\")returns\"gloo\".\nBackend(backend_str)\nbackend_str\nBackend(\"GLOO\")\n\"gloo\"\nNote\nThe entryBackend.UNDEFINEDis present but only used as\ninitial value of some fields. Users should neither use it directly\nnor assume its existence.\nBackend.UNDEFINED\nRegister a new backend with the given name and instantiating function.\nThis class method is used by 3rd partyProcessGroupextension to\nregister new backends.\nProcessGroup\nname(str) \u2013 Backend name of theProcessGroupextension. It\nshould match the one ininit_process_group().\nProcessGroup\ninit_process_group()\nfunc(function) \u2013 Function handler that instantiates the backend.\nThe function should be implemented in the backend\nextension and takes four arguments, includingstore,rank,world_size, andtimeout.\nstore\nrank\nworld_size\ntimeout\nextended_api(bool,optional) \u2013 Whether the backend supports extended argument structure.\nDefault:False. If set toTrue, the backend\nwill get an instance ofc10d::DistributedBackendOptions, and\na process group options object as defined by the backend implementation.\nFalse\nTrue\nc10d::DistributedBackendOptions\ndevice(strorlistofstr,optional) \u2013 device type this backend\nsupports, e.g. \u201ccpu\u201d, \u201ccuda\u201d, etc. IfNone,\nassuming both \u201ccpu\u201d and \u201ccuda\u201d\nNote\nThis support of 3rd party backend is experimental and subject to change.\nReturn the backend of the given process group.\ngroup(ProcessGroup,optional) \u2013 The process group to work on. The\ndefault is the general main process group. If another specific group\nis specified, the calling process must be part ofgroup.\ngroup\nThe backend of the given process group as a lower case string.\nBackend\nReturn the rank of the current process in the providedgroup, default otherwise.\ngroup\nRank is a unique identifier assigned to each process within a distributed\nprocess group. They are always consecutive integers ranging from 0 toworld_size.\nworld_size\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nThe rank of the process group\n-1, if not part of the group\nint\nReturn the number of processes in the current process group.\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nThe world size of the process group\n-1, if not part of the group\nint\n\n## Shutdown#\n\nIt is important to clean up resources on exit by callingdestroy_process_group().\ndestroy_process_group()\nThe simplest pattern to follow is to destroy every process group and backend by callingdestroy_process_group()with the default value of None for thegroupargument, at a\npoint in the training script where communications are no longer needed, usually near the\nend of main(). The call should be made once per trainer-process, not at the outer\nprocess-launcher level.\ndestroy_process_group()\ngroup\nifdestroy_process_group()is not called by all ranks in a pg within the timeout duration,\nespecially when there are multiple process-groups in the application e.g. for N-D parallelism,\nhangs on exit are possible. This is because the destructor for ProcessGroupNCCL calls ncclCommAbort,\nwhich must be called collectively, but the order of calling ProcessGroupNCCL\u2019s destructor if called\nby python\u2019s GC is not deterministic. Callingdestroy_process_group()helps by ensuring\nncclCommAbort is called in a consistent order across ranks, and avoids calling ncclCommAbort\nduring ProcessGroupNCCL\u2019s destructor.\ndestroy_process_group()\ndestroy_process_group()\n\n## Reinitialization#\n\ndestroy_process_groupcan also be used to destroy individual process groups. One use\ncase could be fault tolerant training, where a process group may be destroyed and then\na new one initialized during runtime. In this case, it\u2019s critical to synchronize the trainer\nprocesses using some means other than torch.distributed primitives _after_ calling destroy and\nbefore subsequently initializing. This behavior is currently unsupported/untested, due to\nthe difficulty of achieving this synchronization, and is considered a known issue. Please file\na github issue or RFC if this is a use case that\u2019s blocking you.\ndestroy_process_group\n\n## Groups#\n\nBy default collectives operate on the default group (also called the world) and\nrequire all processes to enter the distributed function call. However, some workloads can benefit\nfrom more fine-grained communication. This is where distributed groups come\ninto play.new_group()function can be\nused to create new groups, with arbitrary subsets of all processes. It returns\nan opaque group handle that can be given as agroupargument to all collectives\n(collectives are distributed functions to exchange information in certain well-known programming patterns).\nnew_group()\ngroup\nCreate a new distributed group.\nThis function requires that all processes in the main group (i.e. all\nprocesses that are part of the distributed job) enter this function, even\nif they are not going to be members of the group. Additionally, groups\nshould be created in the same order in all processes.\nWarning\nSafe concurrent usage:\nWhen using multiple process groups with theNCCLbackend, the user\nmust ensure a globally consistent execution order of collectives across\nranks.\nNCCL\nIf multiple threads within a process issue collectives, explicit\nsynchronization is necessary to ensure consistent ordering.\nWhen using async variants of torch.distributed communication APIs,\na work object is returned and the communication kernel is\nenqueued on a separate CUDA stream, allowing overlap of communication\nand computation. Once one or more async ops have been issued on one process\ngroup, they must be synchronized with other cuda streams by callingwork.wait()before using another process group.\nSeeUsing multiple NCCL communicators concurrently\n<https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using-multiple-nccl-communicators-concurrently>for more details.\nranks(list[int]) \u2013 List of ranks of group members. IfNone, will be\nset to all ranks. Default isNone.\nNone\nNone\ntimeout(timedelta,optional) \u2013 seeinit_process_groupfor details and default value.\nbackend(strorBackend,optional) \u2013 The backend to use. Depending on\nbuild-time configurations, valid values areglooandnccl.\nBy default uses the same backend as the global group. This field\nshould be given as a lowercase string (e.g.,\"gloo\"), which can\nalso be accessed viaBackendattributes (e.g.,Backend.GLOO). IfNoneis passed in, the backend\ncorresponding to the default process group will be used. Default isNone.\ngloo\nnccl\n\"gloo\"\nBackend\nBackend.GLOO\nNone\nNone\npg_options(ProcessGroupOptions,optional) \u2013 process group options\nspecifying what additional options need to be passed in during\nthe construction of specific process groups. i.e. for thencclbackend,is_high_priority_streamcan be specified so that\nprocess group can pick up high priority cuda streams. For other available options to config nccl,\nSeehttps://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclconfig-tuse_local_synchronization(bool, optional): perform a group-local barrier at the end of the process group creation.\nThis is different in that non-member ranks don\u2019t need to call into API and don\u2019t\njoin the barrier.\nnccl\nis_high_priority_stream\ngroup_desc(str,optional) \u2013 a string to describe the process group.\ndevice_id(torch.device,optional) \u2013 a single, specific device\nto \u201cbind\u201d this process to,  Thenew_groupcall will try to initialize\na communication backend immediately for the device if this field is given.\nA handle of distributed group that can be given to collective calls or\nGroupMember.NON_GROUP_MEMBER if the rank is not part ofranks.\nranks\nN.B. use_local_synchronization doesn\u2019t work with MPI.\nN.B. While use_local_synchronization=True can be significantly faster with larger\nclusters and small process groups, care must be taken since it changes cluster behavior\nas non-member ranks don\u2019t join the group barrier().\nN.B. use_local_synchronization=True can lead to deadlocks when each rank creates\nmultiple overlapping process groups. To avoid that, make sure all ranks follow the\nsame global creation order.\nTranslate a global rank into a group rank.\nglobal_rankmust be part ofgroupotherwise this raises RuntimeError.\nglobal_rank\ngroup\ngroup(ProcessGroup) \u2013 ProcessGroup to find the relative rank.\nglobal_rank(int) \u2013 Global rank to query.\nGroup rank ofglobal_rankrelative togroup\nglobal_rank\ngroup\nint\nN.B. calling this function on the default process group returns identity\nTranslate a group rank into a global rank.\ngroup_rankmust be part ofgroupotherwise this raises RuntimeError.\ngroup_rank\ngroup(ProcessGroup) \u2013 ProcessGroup to find the global rank from.\ngroup_rank(int) \u2013 Group rank to query.\nGlobal rank ofgroup_rankrelative togroup\ngroup_rank\ngroup\nint\nN.B. calling this function on the default process group returns identity\nGet all ranks associated withgroup.\ngroup\ngroup(Optional[ProcessGroup]) \u2013 ProcessGroup to get all ranks from.\nIf None, the default process group will be used.\nList of global ranks ordered by group rank.\nlist[int]\n\n## DeviceMesh#\n\nDeviceMesh is a higher level abstraction that manages process groups (or NCCL communicators).\nIt allows user to easily create inter node and intra node process groups without worrying about\nhow to set up the ranks correctly for different sub process groups, and it helps manage those\ndistributed process group easily.init_device_mesh()function can be\nused to create new DeviceMesh, with a mesh shape describing the device topology.\ninit_device_mesh()\nDeviceMesh represents a mesh of devices, where layout of devices could be\nrepresented as a n-d dimension array, and each value of the n-d dimensional\narray is the global id of the default process group ranks.\nDeviceMesh could be used to setup the N dimensional device connections across the cluster,\nand manage the ProcessGroups for N dimensional parallelisms. Communications could happen on\neach dimension of the DeviceMesh separately. DeviceMesh respects the device that user selects\nalready (i.e. if user calltorch.cuda.set_devicebefore the DeviceMesh initialization),\nand will select/set the device for the current process if user does not set the device\nbeforehand. Note that manual device selection should happen BEFORE the DeviceMesh initialization.\nDeviceMesh can also be used as a context manager when using together with DTensor APIs.\nNote\nDeviceMesh follows SPMD programming model, which means the same PyTorch Python program\nis running on all processes/ranks in the cluster. Therefore, users need to make sure themesharray (which describes the layout of devices) should be identical across all ranks.\nInconsistentmeshwill lead to silent hang.\ndevice_type(str) \u2013 The device type of the mesh. Currently supports: \u201ccpu\u201d, \u201ccuda/cuda-like\u201d.\nmesh(ndarray) \u2013 A multi-dimensional array or an integer tensor describing the layout\nof devices, where the IDs are global IDs of the default process group.\nADeviceMeshobject representing the device layout.\nDeviceMesh\nDeviceMesh\nThe following program runs on each process/rank in an SPMD manner. In this example, we have 2\nhosts with 4 GPUs each.\nA reduction over the first dimension of mesh will reduce across\ncolumns (0, 4), .. and (3, 7), a reduction over the second dimension\nof mesh reduces across rows (0, 1, 2, 3) and (4, 5, 6, 7).\nExample:\n\n```python\n>>> from torch.distributed.device_mesh import DeviceMesh\n>>>\n>>> # Initialize device mesh as (2, 4) to represent the topology\n>>> # of cross-host(dim 0), and within-host (dim 1).\n>>> mesh = DeviceMesh(device_type=\"cuda\", mesh=[[0, 1, 2, 3],[4, 5, 6, 7]])\n\n```\n\nConstructs aDeviceMeshwithdevice_typefrom an\nexistingProcessGroupor a list of existingProcessGroup.\nDeviceMesh\ndevice_type\nProcessGroup\nProcessGroup\nThe constructed device mesh has number of dimensions equal to the\nnumber of groups passed. For example, if a single process group is passed in,\nthe resulted DeviceMesh is a 1D mesh. If a list of 2 process groups is passed in,\nthe resulted DeviceMesh is a 2D mesh.\nIf more than one group is passed, then themeshandmesh_dim_namesarguments\nare required. The order of the process groups passed in determines the topology of\nthe mesh. For example, the first process group will be the 0th dimension of the DeviceMesh.\nThemeshtensor passed in must have the same number of dimensions as the number of process\ngroups passed in, and the order of the dimensions in themeshtensor must match the order\nin the process groups passed in.\nmesh\nmesh_dim_names\ngroup(ProcessGrouporlist[ProcessGroup]) \u2013 the existing ProcessGroup\nor a list of existing ProcessGroups.\ndevice_type(str) \u2013 The device type of the mesh. Currently supports: \u201ccpu\u201d,\n\u201ccuda/cuda-like\u201d. Passing in a device type with a GPU index, such as \u201ccuda:0\u201d,\nis not allowed.\nmesh(torch.TensororArrayLike,optional) \u2013 A multi-dimensional array or an\ninteger tensor describing the layout of devices, where the IDs are global IDs\nof the default process group. Default is None.\nmesh_dim_names(tuple[str],optional) \u2013 A tuple of mesh dimension names to assign\nto each dimension of the multi-dimensional array describing the layout of devices.\nIts length must match the length ofmesh_shape. Each string inmesh_dim_namesmust be unique. Default is None.\nADeviceMeshobject representing the device layout.\nDeviceMesh\nDeviceMesh\nReturns a list of ProcessGroups for all mesh dimensions.\nA list ofProcessGroupobject.\nProcessGroup\nlist[torch.distributed.distributed_c10d.ProcessGroup]\nReturn the relative indices of this rank relative to all\ndimensions of the mesh. If this rank is not part of the mesh, return None.\nOptional[list[int]]\nReturns the single ProcessGroup specified by mesh_dim, or, if mesh_dim is not specified and the\nDeviceMesh is 1-dimensional, returns the only ProcessGroup in the mesh.\nmesh_dim(str/python:int,optional) \u2013 it can be the name of the mesh dimension or the index\nNone.(of the mesh dimension. Default is) \u2013\nAProcessGroupobject.\nProcessGroup\nProcessGroup\nReturns the local rank of the given mesh_dim of the DeviceMesh.\nmesh_dim(str/python:int,optional) \u2013 it can be the name of the mesh dimension or the index\nNone.(of the mesh dimension. Default is) \u2013\nAn integer denotes the local rank.\nint\nThe following program runs on each process/rank in an SPMD manner. In this example, we have 2\nhosts with 4 GPUs each.\nCalling mesh_2d.get_local_rank(mesh_dim=0) on rank 0, 1, 2, 3 would return 0.\nCalling mesh_2d.get_local_rank(mesh_dim=0) on rank 4, 5, 6, 7 would return 1.\nCalling mesh_2d.get_local_rank(mesh_dim=1) on rank 0, 4 would return 0.\nCalling mesh_2d.get_local_rank(mesh_dim=1) on rank 1, 5 would return 1.\nCalling mesh_2d.get_local_rank(mesh_dim=1) on rank 2, 6 would return 2.\nCalling mesh_2d.get_local_rank(mesh_dim=1) on rank 3, 7 would return 3.\nExample:\n\n```python\n>>> from torch.distributed.device_mesh import DeviceMesh\n>>>\n>>> # Initialize device mesh as (2, 4) to represent the topology\n>>> # of cross-host(dim 0), and within-host (dim 1).\n>>> mesh = DeviceMesh(device_type=\"cuda\", mesh=[[0, 1, 2, 3],[4, 5, 6, 7]])\n\n```\n\nReturns the current global rank.\nint\n\n## Point-to-point communication#\n\nSend a tensor synchronously.\nWarning\ntagis not supported with the NCCL backend.\ntag\ntensor(Tensor) \u2013 Tensor to send.\ndst(int) \u2013 Destination rank on global process group (regardless ofgroupargument).\nDestination rank should not be the same as the rank of the current process.\ngroup\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\ntag(int,optional) \u2013 Tag to match send with remote recv\ngroup_dst(int,optional) \u2013 Destination rank ongroup.  Invalid to specify bothdstandgroup_dst.\ngroup\ndst\ngroup_dst\nReceives a tensor synchronously.\nWarning\ntagis not supported with the NCCL backend.\ntag\ntensor(Tensor) \u2013 Tensor to fill with received data.\nsrc(int,optional) \u2013 Source rank on global process group (regardless ofgroupargument).\nWill receive from any process if unspecified.\ngroup\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\ntag(int,optional) \u2013 Tag to match recv with remote send\ngroup_src(int,optional) \u2013 Destination rank ongroup.  Invalid to specify bothsrcandgroup_src.\ngroup\nsrc\ngroup_src\nSender rank\n-1, if not part of the group\nint\nisend()andirecv()return distributed request objects when used. In general, the type of this object is unspecified\nas they should never be created manually, but they are guaranteed to support two methods:\nisend()\nirecv()\nis_completed()- returns True if the operation has finished\nis_completed()\nwait()- will block the process until the operation is finished.is_completed()is guaranteed to return True once it returns.\nwait()\nis_completed()\nSend a tensor asynchronously.\nWarning\nModifyingtensorbefore the request completes causes undefined\nbehavior.\ntensor\nWarning\ntagis not supported with the NCCL backend.\ntag\nUnlike send, which is blocking, isend allows src == dst rank, i.e. send to self.\ntensor(Tensor) \u2013 Tensor to send.\ndst(int) \u2013 Destination rank on global process group (regardless ofgroupargument)\ngroup\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\ntag(int,optional) \u2013 Tag to match send with remote recv\ngroup_dst(int,optional) \u2013 Destination rank ongroup.  Invalid to specify bothdstandgroup_dst\ngroup\ndst\ngroup_dst\nA distributed request object.\nNone, if not part of the group\nOptional[Work]\nReceives a tensor asynchronously.\nWarning\ntagis not supported with the NCCL backend.\ntag\nUnlike recv, which is blocking, irecv allows src == dst rank, i.e. recv from self.\ntensor(Tensor) \u2013 Tensor to fill with received data.\nsrc(int,optional) \u2013 Source rank on global process group (regardless ofgroupargument).\nWill receive from any process if unspecified.\ngroup\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\ntag(int,optional) \u2013 Tag to match recv with remote send\ngroup_src(int,optional) \u2013 Destination rank ongroup.  Invalid to specify bothsrcandgroup_src.\ngroup\nsrc\ngroup_src\nA distributed request object.\nNone, if not part of the group\nOptional[Work]\nSends picklable objects inobject_listsynchronously.\nobject_list\nSimilar tosend(), but Python objects can be passed in.\nNote that all objects inobject_listmust be picklable in order to be\nsent.\nsend()\nobject_list\nobject_list(List[Any]) \u2013 List of input objects to sent.\nEach object must be picklable. Receiver must provide lists of equal sizes.\ndst(int) \u2013 Destination rank to sendobject_listto.\nDestination rank is based on global process group (regardless ofgroupargument)\nobject_list\ngroup\ngroup(Optional[ProcessGroup]) \u2013 (ProcessGroup, optional): The process group to work on. If None,\nthe default process group will be used. Default isNone.\nNone\ndevice(torch.device, optional) \u2013 If not None, the objects are\nserialized and converted to tensors which are moved to thedevicebefore sending. Default isNone.\ntorch.device\ndevice\nNone\ngroup_dst(int,optional) \u2013 Destination rank ongroup.\nMust specify one ofdstandgroup_dstbut not both\ngroup\ndst\ngroup_dst\nuse_batch(bool,optional) \u2013 If True, use batch p2p operations instead of\nregular send operations. This avoids initializing 2-rank communicators and\nuses existing entire group communicators. See batch_isend_irecv for usage and\nassumptions. Default isFalse.\nFalse\nNone.\nNone\nNote\nFor NCCL-based process groups, internal tensor representations\nof objects must be moved to the GPU device before communication takes\nplace. In this case, the device used is given bytorch.cuda.current_device()and it is the user\u2019s responsibility to\nensure that this is set so that each rank has an individual GPU, viatorch.cuda.set_device().\ntorch.cuda.current_device()\ntorch.cuda.set_device()\nWarning\nObject collectives have a number of serious performance and scalability\nlimitations.  SeeObject collectivesfor details.\nWarning\nsend_object_list()usespicklemodule implicitly, which\nis known to be insecure. It is possible to construct malicious pickle\ndata which will execute arbitrary code during unpickling. Only call this\nfunction with data you trust.\nsend_object_list()\npickle\nWarning\nCallingsend_object_list()with GPU tensors is not well supported\nand inefficient as it incurs GPU -> CPU transfer since tensors would be\npickled. Please consider usingsend()instead.\nsend_object_list()\nsend()\n\n```python\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes backend is not NCCL\n>>> device = torch.device(\"cpu\")\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 2.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>>     dist.send_object_list(objects, dst=1, device=device)\n>>> else:\n>>>     objects = [None, None, None]\n>>>     dist.recv_object_list(objects, src=0, device=device)\n>>> objects\n['foo', 12, {1: 2}]\n\n```\n\nReceives picklable objects inobject_listsynchronously.\nobject_list\nSimilar torecv(), but can receive Python objects.\nrecv()\nobject_list(List[Any]) \u2013 List of objects to receive into.\nMust provide a list of sizes equal to the size of the list being sent.\nsrc(int,optional) \u2013 Source rank from which to recvobject_list.\nSource rank is based on global process group (regardless ofgroupargument)\nWill receive from any rank if set to None. Default isNone.\nobject_list\ngroup\nNone\ngroup(Optional[ProcessGroup]) \u2013 (ProcessGroup, optional): The process group to work on. If None,\nthe default process group will be used. Default isNone.\nNone\ndevice(torch.device, optional) \u2013 If not None, receives on this device.\nDefault isNone.\ntorch.device\nNone\ngroup_src(int,optional) \u2013 Destination rank ongroup.  Invalid to specify bothsrcandgroup_src.\ngroup\nsrc\ngroup_src\nuse_batch(bool,optional) \u2013 If True, use batch p2p operations instead of\nregular send operations. This avoids initializing 2-rank communicators and\nuses existing entire group communicators. See batch_isend_irecv for usage and\nassumptions. Default isFalse.\nFalse\nSender rank. -1 if rank is not part of the group. If rank is part of the group,object_listwill contain the sent objects fromsrcrank.\nobject_list\nsrc\nNote\nFor NCCL-based process groups, internal tensor representations\nof objects must be moved to the GPU device before communication takes\nplace. In this case, the device used is given bytorch.cuda.current_device()and it is the user\u2019s responsibility to\nensure that this is set so that each rank has an individual GPU, viatorch.cuda.set_device().\ntorch.cuda.current_device()\ntorch.cuda.set_device()\nWarning\nObject collectives have a number of serious performance and scalability\nlimitations.  SeeObject collectivesfor details.\nWarning\nrecv_object_list()usespicklemodule implicitly, which\nis known to be insecure. It is possible to construct malicious pickle\ndata which will execute arbitrary code during unpickling. Only call this\nfunction with data you trust.\nrecv_object_list()\npickle\nWarning\nCallingrecv_object_list()with GPU tensors is not well supported\nand inefficient as it incurs GPU -> CPU transfer since tensors would be\npickled. Please consider usingrecv()instead.\nrecv_object_list()\nrecv()\n\n```python\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes backend is not NCCL\n>>> device = torch.device(\"cpu\")\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 2.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>>     dist.send_object_list(objects, dst=1, device=device)\n>>> else:\n>>>     objects = [None, None, None]\n>>>     dist.recv_object_list(objects, src=0, device=device)\n>>> objects\n['foo', 12, {1: 2}]\n\n```\n\nSend or Receive a batch of tensors asynchronously and return a list of requests.\nProcess each of the operations inp2p_op_listand return the corresponding\nrequests. NCCL, Gloo, and UCC backend are currently supported.\np2p_op_list\np2p_op_list(list[torch.distributed.distributed_c10d.P2POp]) \u2013 A list of point-to-point operations(type of each operator istorch.distributed.P2POp). The order of the isend/irecv in the list\nmatters and it needs to match with corresponding isend/irecv on the\nremote end.\ntorch.distributed.P2POp\nA list of distributed request objects returned by calling the corresponding\nop in the op_list.\nlist[torch.distributed.distributed_c10d.Work]\nExamples\n\n```python\n>>> send_tensor = torch.arange(2, dtype=torch.float32) + 2 * rank\n>>> recv_tensor = torch.randn(2, dtype=torch.float32)\n>>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1) % world_size)\n>>> recv_op = dist.P2POp(\n...     dist.irecv, recv_tensor, (rank - 1 + world_size) % world_size\n... )\n>>> reqs = batch_isend_irecv([send_op, recv_op])\n>>> for req in reqs:\n>>>     req.wait()\n>>> recv_tensor\ntensor([2, 3])     # Rank 0\ntensor([0, 1])     # Rank 1\n\n```\n\nNote\nNote that when this API is used with the NCCL PG backend, users must set\nthe current GPU device withtorch.cuda.set_device, otherwise it will\nlead to unexpected hang issues.\nIn addition, if this API is the first collective call in thegrouppassed todist.P2POp, all ranks of thegroupmust participate in\nthis API call; otherwise, the behavior is undefined. If this API call is\nnot the first collective call in thegroup, batched P2P operations\ninvolving only a subset of ranks of thegroupare allowed.\ngroup\ndist.P2POp\ngroup\ngroup\ngroup\nA class to build point-to-point operations forbatch_isend_irecv.\nbatch_isend_irecv\nThis class builds the type of P2P operation, communication buffer, peer rank,\nProcess Group, and tag. Instances of this class will be passed tobatch_isend_irecvfor point-to-point communications.\nbatch_isend_irecv\nop(Callable) \u2013 A function to send data to or receive data from a peer process.\nThe type ofopis eithertorch.distributed.isendortorch.distributed.irecv.\nop\ntorch.distributed.isend\ntorch.distributed.irecv\ntensor(Tensor) \u2013 Tensor to send or receive.\npeer(int,optional) \u2013 Destination or source rank.\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\ntag(int,optional) \u2013 Tag to match send with recv.\ngroup_peer(int,optional) \u2013 Destination or source rank.\n\n## Synchronous and asynchronous collective operations#\n\nEvery collective operation function supports the following two kinds of operations,\ndepending on the setting of theasync_opflag passed into the collective:\nasync_op\nSynchronous operation- the default mode, whenasync_opis set toFalse.\nWhen the function returns, it is guaranteed that\nthe collective operation is performed. In the case of CUDA operations, it is not guaranteed\nthat the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any\nfurther function calls utilizing the output of the collective call will behave as expected. For CUDA collectives,\nfunction calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of\nsynchronization under the scenario of running under different streams. For details on CUDA semantics such as stream\nsynchronization, seeCUDA Semantics.\nSee the below script to see examples of differences in these semantics for CPU and CUDA operations.\nasync_op\nFalse\nAsynchronous operation- whenasync_opis set to True. The collective operation function\nreturns a distributed request object. In general, you don\u2019t need to create it manually and it\nis guaranteed to support two methods:\nasync_op\nis_completed()- in the case of CPU collectives, returnsTrueif completed. In the case of CUDA operations,\nreturnsTrueif the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the\ndefault stream without further synchronization.\nis_completed()\nTrue\nTrue\nwait()- in the case of CPU collectives, will block the process until the operation is completed. In the case\nof CUDA collectives, will block the currently active CUDA stream until the operation is completed (but will not block the CPU).\nwait()\nget_future()- returnstorch._C.Futureobject. Supported for NCCL, also supported for most operations on GLOO\nand MPI, except for peer to peer operations.\nNote: as we continue adopting Futures and merging APIs,get_future()call might become redundant.\nget_future()\ntorch._C.Future\nget_future()\nExample\nThe following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams:\n\n```python\n# Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n\n```\n\n\n## Collective functions#\n\nBroadcasts the tensor to the whole group.\ntensormust have the same number of elements in all processes\nparticipating in the collective.\ntensor\ntensor(Tensor) \u2013 Data to be sent ifsrcis the rank of current\nprocess, and tensor to be used to save received data otherwise.\nsrc\nsrc(int) \u2013 Source rank on global process group (regardless ofgroupargument).\ngroup\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op\ngroup_src(int) \u2013 Source rank ongroup.  Must specify one ofgroup_srcandsrcbut not both.\ngroup\ngroup_src\nsrc\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\nBroadcasts picklable objects inobject_listto the whole group.\nobject_list\nSimilar tobroadcast(), but Python objects can be passed in.\nNote that all objects inobject_listmust be picklable in order to be\nbroadcasted.\nbroadcast()\nobject_list\nobject_list(List[Any]) \u2013 List of input objects to broadcast.\nEach object must be picklable. Only objects on thesrcrank will\nbe broadcast, but each rank must provide lists of equal sizes.\nsrc\nsrc(int) \u2013 Source rank from which to broadcastobject_list.\nSource rank is based on global process group (regardless ofgroupargument)\nobject_list\ngroup\ngroup(Optional[ProcessGroup]) \u2013 (ProcessGroup, optional): The process group to work on. If None,\nthe default process group will be used. Default isNone.\nNone\ndevice(torch.device, optional) \u2013 If not None, the objects are\nserialized and converted to tensors which are moved to thedevicebefore broadcasting. Default isNone.\ntorch.device\ndevice\nNone\ngroup_src(int) \u2013 Source rank ongroup.  Must not specify one ofgroup_srcandsrcbut not both.\ngroup\ngroup_src\nsrc\nNone. If rank is part of the group,object_listwill contain the\nbroadcasted objects fromsrcrank.\nNone\nobject_list\nsrc\nNote\nFor NCCL-based process groups, internal tensor representations\nof objects must be moved to the GPU device before communication takes\nplace. In this case, the device used is given bytorch.cuda.current_device()and it is the user\u2019s responsibility to\nensure that this is set so that each rank has an individual GPU, viatorch.cuda.set_device().\ntorch.cuda.current_device()\ntorch.cuda.set_device()\nNote\nNote that this API differs slightly from thebroadcast()collective since it does not provide anasync_ophandle and thus\nwill be a blocking call.\nbroadcast()\nasync_op\nWarning\nObject collectives have a number of serious performance and scalability\nlimitations.  SeeObject collectivesfor details.\nWarning\nbroadcast_object_list()usespicklemodule implicitly, which\nis known to be insecure. It is possible to construct malicious pickle\ndata which will execute arbitrary code during unpickling. Only call this\nfunction with data you trust.\nbroadcast_object_list()\npickle\nWarning\nCallingbroadcast_object_list()with GPU tensors is not well supported\nand inefficient as it incurs GPU -> CPU transfer since tensors would be\npickled. Please consider usingbroadcast()instead.\nbroadcast_object_list()\nbroadcast()\n\n```python\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> # Assumes backend is not NCCL\n>>> device = torch.device(\"cpu\")\n>>> dist.broadcast_object_list(objects, src=0, device=device)\n>>> objects\n['foo', 12, {1: 2}]\n\n```\n\nReduces the tensor data across all machines in a way that all get the final result.\nAfter the calltensoris going to be bitwise identical in all processes.\ntensor\nComplex tensors are supported.\ntensor(Tensor) \u2013 Input and output of the collective. The function\noperates in-place.\nop(optional) \u2013 One of the values fromtorch.distributed.ReduceOpenum.  Specifies an operation used for element-wise reductions.\ntorch.distributed.ReduceOp\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\nExamples\n\n```python\n>>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> device = torch.device(f\"cuda:{rank}\")\n>>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2], device='cuda:0') # Rank 0\ntensor([3, 4], device='cuda:1') # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6], device='cuda:0') # Rank 0\ntensor([4, 6], device='cuda:1') # Rank 1\n\n```\n\n\n```python\n>>> # All tensors below are of torch.cfloat type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.tensor(\n...     [1 + 1j, 2 + 2j], dtype=torch.cfloat, device=device\n... ) + 2 * rank * (1 + 1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0\ntensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4.+4.j, 6.+6.j], device='cuda:0') # Rank 0\ntensor([4.+4.j, 6.+6.j], device='cuda:1') # Rank 1\n\n```\n\nReduces the tensor data across all machines.\nOnly the process with rankdstis going to receive the final result.\ndst\ntensor(Tensor) \u2013 Input and output of the collective. The function\noperates in-place.\ndst(int) \u2013 Destination rank on global process group (regardless ofgroupargument)\ngroup\nop(optional) \u2013 One of the values fromtorch.distributed.ReduceOpenum.  Specifies an operation used for element-wise reductions.\ntorch.distributed.ReduceOp\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op\ngroup_dst(int) \u2013 Destination rank ongroup.  Must specify one ofgroup_dstanddstbut not both.\ngroup\ngroup_dst\ndst\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\nGathers tensors from the whole group in a list.\nComplex and uneven sized tensors are supported.\ntensor_list(list[Tensor]) \u2013 Output list. It should contain\ncorrectly-sized tensors to be used for output of the collective.\nUneven sized tensors are supported.\ntensor(Tensor) \u2013 Tensor to be broadcast from current process.\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\nExamples\n\n```python\n>>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> device = torch.device(f\"cuda:{rank}\")\n>>> tensor_list = [\n...     torch.zeros(2, dtype=torch.int64, device=device) for _ in range(2)\n... ]\n>>> tensor_list\n[tensor([0, 0], device='cuda:0'), tensor([0, 0], device='cuda:0')] # Rank 0\n[tensor([0, 0], device='cuda:1'), tensor([0, 0], device='cuda:1')] # Rank 1\n>>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2], device='cuda:0') # Rank 0\ntensor([3, 4], device='cuda:1') # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2], device='cuda:0'), tensor([3, 4], device='cuda:0')] # Rank 0\n[tensor([1, 2], device='cuda:1'), tensor([3, 4], device='cuda:1')] # Rank 1\n\n```\n\n\n```python\n>>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [\n...     torch.zeros(2, dtype=torch.cfloat, device=device) for _ in range(2)\n... ]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j], device='cuda:0'), tensor([0.+0.j, 0.+0.j], device='cuda:0')] # Rank 0\n[tensor([0.+0.j, 0.+0.j], device='cuda:1'), tensor([0.+0.j, 0.+0.j], device='cuda:1')] # Rank 1\n>>> tensor = torch.tensor(\n...     [1 + 1j, 2 + 2j], dtype=torch.cfloat, device=device\n... ) + 2 * rank * (1 + 1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0\ntensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j], device='cuda:0'), tensor([3.+3.j, 4.+4.j], device='cuda:0')] # Rank 0\n[tensor([1.+1.j, 2.+2.j], device='cuda:1'), tensor([3.+3.j, 4.+4.j], device='cuda:1')] # Rank 1\n\n```\n\nGather tensors from all ranks and put them in a single output tensor.\nThis function requires all tensors to be the same size on each process.\noutput_tensor(Tensor) \u2013 Output tensor to accommodate tensor elements\nfrom all ranks. It must be correctly sized to have one of the\nfollowing forms:\n(i) a concatenation of all the input tensors along the primary\ndimension; for definition of \u201cconcatenation\u201d, seetorch.cat();\n(ii) a stack of all the input tensors along the primary dimension;\nfor definition of \u201cstack\u201d, seetorch.stack().\nExamples below may better explain the supported output forms.\ntorch.cat()\ntorch.stack()\ninput_tensor(Tensor) \u2013 Tensor to be gathered from current rank.\nDifferent from theall_gatherAPI, the input tensors in this\nAPI must have the same size across all ranks.\nall_gather\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\nExamples\n\n```python\n>>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n>>> # We have two ranks.\n>>> device = torch.device(f\"cuda:{rank}\")\n>>> tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n>>> tensor_in\ntensor([1, 2], device='cuda:0') # Rank 0\ntensor([3, 4], device='cuda:1') # Rank 1\n>>> # Output in concatenation form\n>>> tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)\n>>> dist.all_gather_into_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([1, 2, 3, 4], device='cuda:0') # Rank 0\ntensor([1, 2, 3, 4], device='cuda:1') # Rank 1\n>>> # Output in stack form\n>>> tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)\n>>> dist.all_gather_into_tensor(tensor_out2, tensor_in)\n>>> tensor_out2\ntensor([[1, 2],\n        [3, 4]], device='cuda:0') # Rank 0\ntensor([[1, 2],\n        [3, 4]], device='cuda:1') # Rank 1\n\n```\n\nGathers picklable objects from the whole group into a list.\nSimilar toall_gather(), but Python objects can be passed in.\nNote that the object must be picklable in order to be gathered.\nall_gather()\nobject_list(list[Any]) \u2013 Output list. It should be correctly sized as the\nsize of the group for this collective and will contain the output.\nobj(Any) \u2013 Pickable Python object to be broadcast from current process.\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used. Default isNone.\nNone\nNone. If the calling rank is part of this group, the output of the\ncollective will be populated into the inputobject_list. If the\ncalling rank is not part of the group, the passed inobject_listwill\nbe unmodified.\nobject_list\nobject_list\nNote\nNote that this API differs slightly from theall_gather()collective since it does not provide anasync_ophandle and thus\nwill be a blocking call.\nall_gather()\nasync_op\nNote\nFor NCCL-based processed groups, internal tensor representations\nof objects must be moved to the GPU device before communication takes\nplace. In this case, the device used is given bytorch.cuda.current_device()and it is the user\u2019s responsibility to\nensure that this is set so that each rank has an individual GPU, viatorch.cuda.set_device().\ntorch.cuda.current_device()\ntorch.cuda.set_device()\nWarning\nObject collectives have a number of serious performance and scalability\nlimitations.  SeeObject collectivesfor details.\nWarning\nall_gather_object()usespicklemodule implicitly, which is\nknown to be insecure. It is possible to construct malicious pickle data\nwhich will execute arbitrary code during unpickling. Only call this\nfunction with data you trust.\nall_gather_object()\npickle\nWarning\nCallingall_gather_object()with GPU tensors is not well supported\nand inefficient as it incurs GPU -> CPU transfer since tensors would be\npickled. Please consider usingall_gather()instead.\nall_gather_object()\nall_gather()\n\n```python\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n\n```\n\nGathers a list of tensors in a single process.\nThis function requires all tensors to be the same size on each process.\ntensor(Tensor) \u2013 Input tensor.\ngather_list(list[Tensor],optional) \u2013 List of appropriately,\nsame-sized tensors to use for gathered data\n(default is None, must be specified on the destination rank)\ndst(int,optional) \u2013 Destination rank on global process group (regardless ofgroupargument).\n(If bothdstandgroup_dstare None, default is global rank 0)\ngroup\ndst\ngroup_dst\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op\ngroup_dst(int,optional) \u2013 Destination rank ongroup.  Invalid to specify bothdstandgroup_dst\ngroup\ndst\ngroup_dst\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\nNote\nNote that all Tensors in gather_list must have the same size.\n\n```python\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_size = 2\n>>> device = torch.device(f'cuda:{rank}')\n>>> tensor = torch.ones(tensor_size, device=device) + rank\n>>> if dist.get_rank() == 0:\n>>>     gather_list = [torch.zeros_like(tensor, device=device) for i in range(2)]\n>>> else:\n>>>     gather_list = None\n>>> dist.gather(tensor, gather_list, dst=0)\n>>> # Rank 0 gets gathered data.\n>>> gather_list\n[tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0')] # Rank 0\nNone                                                                   # Rank 1\n\n```\n\nGathers picklable objects from the whole group in a single process.\nSimilar togather(), but Python objects can be passed in. Note that the\nobject must be picklable in order to be gathered.\ngather()\nobj(Any) \u2013 Input object. Must be picklable.\nobject_gather_list(list[Any]) \u2013 Output list. On thedstrank, it\nshould be correctly sized as the size of the group for this\ncollective and will contain the output. Must beNoneon non-dst\nranks. (default isNone)\ndst\nNone\nNone\ndst(int,optional) \u2013 Destination rank on global process group (regardless ofgroupargument).\n(If bothdstandgroup_dstare None, default is global rank 0)\ngroup\ndst\ngroup_dst\ngroup(Optional[ProcessGroup]) \u2013 (ProcessGroup, optional): The process group to work on. If None,\nthe default process group will be used. Default isNone.\nNone\ngroup_dst(int,optional) \u2013 Destination rank ongroup.  Invalid to specify bothdstandgroup_dst\ngroup\ndst\ngroup_dst\nNone. On thedstrank,object_gather_listwill contain the\noutput of the collective.\ndst\nobject_gather_list\nNote\nNote that this API differs slightly from the gather collective\nsince it does not provide an async_op handle and thus will be a blocking\ncall.\nNote\nFor NCCL-based processed groups, internal tensor representations\nof objects must be moved to the GPU device before communication takes\nplace. In this case, the device used is given bytorch.cuda.current_device()and it is the user\u2019s responsibility to\nensure that this is set so that each rank has an individual GPU, viatorch.cuda.set_device().\ntorch.cuda.current_device()\ntorch.cuda.set_device()\nWarning\nObject collectives have a number of serious performance and scalability\nlimitations.  SeeObject collectivesfor details.\nWarning\ngather_object()usespicklemodule implicitly, which is\nknown to be insecure. It is possible to construct malicious pickle data\nwhich will execute arbitrary code during unpickling. Only call this\nfunction with data you trust.\ngather_object()\npickle\nWarning\nCallinggather_object()with GPU tensors is not well supported\nand inefficient as it incurs GPU -> CPU transfer since tensors would be\npickled. Please consider usinggather()instead.\ngather_object()\ngather()\n\n```python\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n...     gather_objects[dist.get_rank()],\n...     output if dist.get_rank() == 0 else None,\n...     dst=0\n... )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]\n\n```\n\nScatters a list of tensors to all processes in a group.\nEach process will receive exactly one tensor and store its data in thetensorargument.\ntensor\nComplex tensors are supported.\ntensor(Tensor) \u2013 Output tensor.\nscatter_list(list[Tensor]) \u2013 List of tensors to scatter (default is\nNone, must be specified on the source rank)\nsrc(int) \u2013 Source rank on global process group (regardless ofgroupargument).\n(If bothsrcandgroup_srcare None, default is global rank 0)\ngroup\nsrc\ngroup_src\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op\ngroup_src(int,optional) \u2013 Source rank ongroup.  Invalid to specify bothsrcandgroup_src\ngroup\nsrc\ngroup_src\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\nNote\nNote that all Tensors in scatter_list must have the same size.\n\n```python\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> tensor_size = 2\n>>> device = torch.device(f'cuda:{rank}')\n>>> output_tensor = torch.zeros(tensor_size, device=device)\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 2.\n>>>     # Only tensors, all of which must be the same size.\n>>>     t_ones = torch.ones(tensor_size, device=device)\n>>>     t_fives = torch.ones(tensor_size, device=device) * 5\n>>>     scatter_list = [t_ones, t_fives]\n>>> else:\n>>>     scatter_list = None\n>>> dist.scatter(output_tensor, scatter_list, src=0)\n>>> # Rank i gets scatter_list[i].\n>>> output_tensor\ntensor([1., 1.], device='cuda:0') # Rank 0\ntensor([5., 5.], device='cuda:1') # Rank 1\n\n```\n\nScatters picklable objects inscatter_object_input_listto the whole group.\nscatter_object_input_list\nSimilar toscatter(), but Python objects can be passed in. On\neach rank, the scattered object will be stored as the first element ofscatter_object_output_list. Note that all objects inscatter_object_input_listmust be picklable in order to be scattered.\nscatter()\nscatter_object_output_list\nscatter_object_input_list\nscatter_object_output_list(List[Any]) \u2013 Non-empty list whose first\nelement will store the object scattered to this rank.\nscatter_object_input_list(List[Any],optional) \u2013 List of input objects to scatter.\nEach object must be picklable. Only objects on thesrcrank will\nbe scattered, and the argument can beNonefor non-src ranks.\nsrc\nNone\nsrc(int) \u2013 Source rank from which to scatterscatter_object_input_list.\nSource rank is based on global process group (regardless ofgroupargument).\n(If bothsrcandgroup_srcare None, default is global rank 0)\nscatter_object_input_list\ngroup\nsrc\ngroup_src\ngroup(Optional[ProcessGroup]) \u2013 (ProcessGroup, optional): The process group to work on. If None,\nthe default process group will be used. Default isNone.\nNone\ngroup_src(int,optional) \u2013 Source rank ongroup.  Invalid to specify bothsrcandgroup_src\ngroup\nsrc\ngroup_src\nNone. If rank is part of the group,scatter_object_output_listwill have its first element set to the scattered object for this rank.\nNone\nscatter_object_output_list\nNote\nNote that this API differs slightly from the scatter collective\nsince it does not provide anasync_ophandle and thus will be a\nblocking call.\nasync_op\nWarning\nObject collectives have a number of serious performance and scalability\nlimitations.  SeeObject collectivesfor details.\nWarning\nscatter_object_list()usespicklemodule implicitly, which\nis known to be insecure. It is possible to construct malicious pickle\ndata which will execute arbitrary code during unpickling. Only call this\nfunction with data you trust.\nscatter_object_list()\npickle\nWarning\nCallingscatter_object_list()with GPU tensors is not well supported\nand inefficient as it incurs GPU -> CPU transfer since tensors would be\npickled. Please consider usingscatter()instead.\nscatter_object_list()\nscatter()\n\n```python\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]\n\n```\n\nReduces, then scatters a list of tensors to all processes in a group.\noutput(Tensor) \u2013 Output tensor.\ninput_list(list[Tensor]) \u2013 List of tensors to reduce and scatter.\nop(optional) \u2013 One of the values fromtorch.distributed.ReduceOpenum.  Specifies an operation used for element-wise reductions.\ntorch.distributed.ReduceOp\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op.\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group.\nReduces, then scatters a tensor to all ranks in a group.\noutput(Tensor) \u2013 Output tensor. It should have the same size across all\nranks.\ninput(Tensor) \u2013 Input tensor to be reduced and scattered. Its size\nshould be output tensor size times the world size. The input tensor\ncan have one of the following shapes:\n(i) a concatenation of the output tensors along the primary\ndimension, or\n(ii) a stack of the output tensors along the primary dimension.\nFor definition of \u201cconcatenation\u201d, seetorch.cat().\nFor definition of \u201cstack\u201d, seetorch.stack().\ntorch.cat()\ntorch.stack()\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op.\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group.\nExamples\n\n```python\n>>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n>>> # We have two ranks.\n>>> device = torch.device(f\"cuda:{rank}\")\n>>> tensor_out = torch.zeros(2, dtype=torch.int64, device=device)\n>>> # Input in concatenation form\n>>> tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device)\n>>> tensor_in\ntensor([0, 1, 2, 3], device='cuda:0') # Rank 0\ntensor([0, 1, 2, 3], device='cuda:1') # Rank 1\n>>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([0, 2], device='cuda:0') # Rank 0\ntensor([4, 6], device='cuda:1') # Rank 1\n>>> # Input in stack form\n>>> tensor_in = torch.reshape(tensor_in, (world_size, 2))\n>>> tensor_in\ntensor([[0, 1],\n        [2, 3]], device='cuda:0') # Rank 0\ntensor([[0, 1],\n        [2, 3]], device='cuda:1') # Rank 1\n>>> dist.reduce_scatter_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([0, 2], device='cuda:0') # Rank 0\ntensor([4, 6], device='cuda:1') # Rank 1\n\n```\n\nSplit input tensor and then scatter the split list to all processes in a group.\nLater the received tensors are concatenated from all the processes in the group\nand returned as a single output tensor.\nComplex tensors are supported.\noutput(Tensor) \u2013 Gathered concatenated output tensor.\ninput(Tensor) \u2013 Input tensor to scatter.\noutput_split_sizes\u2013 (list[Int], optional): Output split sizes for dim 0\nif specified None or empty, dim 0 ofoutputtensor must divide\nequally byworld_size.\noutput\nworld_size\ninput_split_sizes\u2013 (list[Int], optional): Input split sizes for dim 0\nif specified None or empty, dim 0 ofinputtensor must divide\nequally byworld_size.\ninput\nworld_size\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op.\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group.\nWarning\nall_to_all_singleis experimental and subject to change.\nExamples\n\n```python\n>>> input = torch.arange(4) + rank * 4\n>>> input\ntensor([0, 1, 2, 3])     # Rank 0\ntensor([4, 5, 6, 7])     # Rank 1\ntensor([8, 9, 10, 11])   # Rank 2\ntensor([12, 13, 14, 15]) # Rank 3\n>>> output = torch.empty([4], dtype=torch.int64)\n>>> dist.all_to_all_single(output, input)\n>>> output\ntensor([0, 4, 8, 12])    # Rank 0\ntensor([1, 5, 9, 13])    # Rank 1\ntensor([2, 6, 10, 14])   # Rank 2\ntensor([3, 7, 11, 15])   # Rank 3\n\n```\n\n\n```python\n>>> # Essentially, it is similar to following operation:\n>>> scatter_list = list(input.chunk(world_size))\n>>> gather_list = list(output.chunk(world_size))\n>>> for i in range(world_size):\n>>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n\n```\n\n\n```python\n>>> # Another example with uneven split\n>>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> output = ...\n>>> dist.all_to_all_single(output, input, output_splits, input_splits)\n>>> output\ntensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\ntensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\ntensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\ntensor([ 5, 17, 18, 24, 36])                                     # Rank 3\n\n```\n\n\n```python\n>>> # Another example with tensors of torch.cfloat type.\n>>> input = torch.tensor(\n...     [1 + 1j, 2 + 2j, 3 + 3j, 4 + 4j], dtype=torch.cfloat\n... ) + 4 * rank * (1 + 1j)\n>>> input\ntensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\ntensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\ntensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\ntensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n>>> output = torch.empty([4], dtype=torch.int64)\n>>> dist.all_to_all_single(output, input)\n>>> output\ntensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\ntensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\ntensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\ntensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3\n\n```\n\nScatters list of input tensors to all processes in a group and return gathered list of tensors in output list.\nComplex tensors are supported.\noutput_tensor_list(list[Tensor]) \u2013 List of tensors to be gathered one\nper rank.\ninput_tensor_list(list[Tensor]) \u2013 List of tensors to scatter one per rank.\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op.\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group.\nWarning\nall_to_allis experimental and subject to change.\nExamples\n\n```python\n>>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n\n```\n\n\n```python\n>>> # Essentially, it is similar to following operation:\n>>> scatter_list = input\n>>> gather_list = output\n>>> for i in range(world_size):\n>>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i)\n\n```\n\n\n```python\n>>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> input = list(input.split(input_splits))\n>>> input\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n>>> output = ...\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n\n```\n\n\n```python\n>>> # Another example with tensors of torch.cfloat type.\n>>> input = torch.tensor(\n...     [1 + 1j, 2 + 2j, 3 + 3j, 4 + 4j], dtype=torch.cfloat\n... ) + 4 * rank * (1 + 1j)\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0\n[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1\n[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2\n[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0\n[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1\n[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2\n[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3\n\n```\n\nSynchronize all processes.\nThis collective blocks processes until the whole group enters this function,\nif async_op is False, or if async work handle is called on wait().\ngroup(ProcessGroup,optional) \u2013 The process group to work on. If None,\nthe default process group will be used.\nasync_op(bool,optional) \u2013 Whether this op should be an async op\ndevice_ids([int],optional) \u2013 List of device/GPU ids. Only one id is expected.\nAsync work handle, if async_op is set to True.\nNone, if not async_op or if not part of the group\nNote\nProcessGroupNCCLnow blocks the cpu thread till the completion of the barrier collective.\nNote\nProcessGroupNCCLimplements barrier as an all_reduce of a 1-element tensor. A device must be chosen\nfor allocating this tensor.  The device choice is made by checking in this order (1) the first device passed todevice_idsarg of barrier if not None, (2) the device passed to init_process_group if not None, (3) the device\nthat was first used with this process group, if another collective with tensor inputs has been performed, (4)\nthe device index indicated by the global rank mod local device count.\nSynchronize processes similar totorch.distributed.barrier, but consider a configurable timeout.\ntorch.distributed.barrier\nIt is able to report ranks that did not pass this barrier within the provided timeout.\nSpecifically, for non-zero ranks, will block until a send/recv is processed from rank 0.\nRank 0 will block until all send /recv from other ranks are processed, and will report\nfailures for ranks that failed to respond in time. Note that if one rank does not reach the\nmonitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.\nThis collective will block all processes/ranks in the group, until the\nwhole group exits the function successfully, making it useful for debugging\nand synchronizing. However, it can have a performance impact and should only\nbe used for debugging or scenarios that require full synchronization points\non the host-side. For debugging purposes, this barrier can be inserted\nbefore the application\u2019s collective calls to check if any ranks are\ndesynchronized.\nNote\nNote that this collective is only supported with the GLOO backend.\ngroup(ProcessGroup,optional) \u2013 The process group to work on. IfNone, the default process group will be used.\nNone\ntimeout(datetime.timedelta,optional) \u2013 Timeout for monitored_barrier.\nIfNone, the default process group timeout will be used.\nNone\nwait_all_ranks(bool,optional) \u2013 Whether to collect all failed ranks or\nnot. By default, this isFalseandmonitored_barrieron rank 0\nwill throw on the first failed rank it encounters in order to fail\nfast. By settingwait_all_ranks=Truemonitored_barrierwill\ncollect all failed ranks and throw an error containing information\nabout all failed ranks.\nFalse\nmonitored_barrier\nwait_all_ranks=True\nmonitored_barrier\nNone.\nNone\n\n```python\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() != 1:\n>>>     dist.monitored_barrier() # Raises exception indicating that\n>>> # rank 1 did not call into monitored_barrier.\n>>> # Example with wait_all_ranks=True\n>>> if dist.get_rank() == 0:\n>>>     dist.monitored_barrier(wait_all_ranks=True) # Raises exception\n>>> # indicating that ranks 1, 2, ... world_size - 1 did not call into\n>>> # monitored_barrier.\n\n```\n\nAWorkobject represents the handle to a pending asynchronous operation in\nPyTorch\u2019s distributed package. It is returned by non-blocking collective operations,\nsuch asdist.all_reduce(tensor, async_op=True).\nBlocks the currently active GPU stream on the operation to\ncomplete. For GPU based collectives this is equivalent to\nsynchronize. For CPU initiated collectives such as with Gloo this\nwill block the CUDA stream until the operation is complete.\nThis returns immediately in all cases.\nTo check whether an operation was successful you should check the\nWork object result asynchronously.\nAtorch.futures.Futureobject which is associated with the completion of\ntheWork. As an example, a future object can be retrieved\nbyfut=process_group.allreduce(tensors).get_future().\ntorch.futures.Future\nWork\nfut=process_group.allreduce(tensors).get_future()\nBelow is an example of a simple allreduce DDP communication hook that usesget_futureAPI to retrieve a Future associated with the completion ofallreduce.\nget_future\nallreduce\n\n```python\n>>> def allreduce(process_group: dist.ProcessGroup, bucket: dist.GradBucket): -> torch.futures.Future\n>>>     group_to_use = process_group if process_group is not None else torch.distributed.group.WORLD\n>>>     tensor = bucket.buffer().div_(group_to_use.size())\n>>>     return torch.distributed.all_reduce(tensor, group=group_to_use, async_op=True).get_future()\n>>> ddp_model.register_comm_hook(state=None, hook=allreduce)\n\n```\n\nWarning\nget_futureAPI supports NCCL, and partially GLOO and MPI backends\n(no support for peer-to-peer operations like send/recv) and will return atorch.futures.Future.\nget_future\ntorch.futures.Future\nIn the example above,allreducework will be done on GPU using NCCL backend,fut.wait()will return after synchronizing the appropriate NCCL streams\nwith PyTorch\u2019s current device streams to ensure we can have asynchronous CUDA\nexecution and it does not wait for the entire operation to complete on GPU. Note thatCUDAFuturedoes not supportTORCH_NCCL_BLOCKING_WAITflag or NCCL\u2019sbarrier().\nIn addition, if a callback function was added byfut.then(), it will wait untilWorkNCCL\u2019s NCCL streams synchronize withProcessGroupNCCL\u2019s dedicated callback\nstream and invoke the callback inline after running the callback on the callback stream.fut.then()will return anotherCUDAFuturethat holds the return value of the\ncallback and aCUDAEventthat recorded the callback stream.\nallreduce\nfut.wait()\nCUDAFuture\nTORCH_NCCL_BLOCKING_WAIT\nbarrier()\nfut.then()\nWorkNCCL\nProcessGroupNCCL\nfut.then()\nCUDAFuture\nCUDAEvent\nFor CPU work,fut.done()returns true when work has been completed and value()\ntensors are ready.\nfut.done()\nFor GPU work,fut.done()returns true only whether the operation has been enqueued.\nfut.done()\nFor mixed CPU-GPU work (e.g. sending GPU tensors with GLOO),fut.done()returns\ntrue when tensors have arrived on respective nodes, but not yet necessarily synched on\nrespective GPUs (similarly to GPU work).\nfut.done()\nAtorch.futures.Futureobject of int type which maps to the enum type of WorkResult\nAs an example, a future object can be retrieved\nbyfut=process_group.allreduce(tensor).get_future_result().\ntorch.futures.Future\nfut=process_group.allreduce(tensor).get_future_result()\nusers can usefut.wait()to blocking wait for the completion of the work and\nget the WorkResult byfut.value().\nAlso, users can usefut.then(call_back_func)to register a callback function to be called\nwhen the work is completed, without blocking the current thread.\nfut.wait()\nfut.value()\nfut.then(call_back_func)\nWarning\nget_future_resultAPI supports NCCL\nget_future_result\ntrue/false.\nwork.wait(timeout)\n# some handling\nWarning\nIn normal cases, users do not need to set the timeout.\ncalling wait() is the same as calling synchronize():\nLetting the current stream block on the completion of the NCCL work.\nHowever, if timeout is set, it will block the CPU thread until the NCCL work is completed\nor timed out. If timeout, exception will be thrown.\nAn enum-like class for available reduction operations:SUM,PRODUCT,MIN,MAX,BAND,BOR,BXOR, andPREMUL_SUM.\nSUM\nPRODUCT\nMIN\nMAX\nBAND\nBOR\nBXOR\nPREMUL_SUM\nBAND,BOR, andBXORreductions are not available when\nusing theNCCLbackend.\nBAND\nBOR\nBXOR\nNCCL\nAVGdivides values by the world size before summing across ranks.AVGis only available with theNCCLbackend,\nand only for NCCL versions 2.10 or later.\nAVG\nAVG\nNCCL\nPREMUL_SUMmultiplies inputs by a given scalar locally before reduction.PREMUL_SUMis only available with theNCCLbackend,\nand only available for NCCL versions 2.11 or later. Users are supposed to\nusetorch.distributed._make_nccl_premul_sum.\nPREMUL_SUM\nPREMUL_SUM\nNCCL\ntorch.distributed._make_nccl_premul_sum\nAdditionally,MAX,MINandPRODUCTare not supported for complex tensors.\nMAX\nMIN\nPRODUCT\nThe values of this class can be accessed as attributes, e.g.,ReduceOp.SUM.\nThey are used in specifying strategies for reduction collectives, e.g.,reduce().\nReduceOp.SUM\nreduce()\nThis class does not support__members__property.\n__members__\nDeprecated enum-like class for reduction operations:SUM,PRODUCT,MIN, andMAX.\nSUM\nPRODUCT\nMIN\nMAX\nReduceOpis recommended to use instead.\nReduceOp\n\n## Distributed Key-Value Store#\n\nThe distributed package comes with a distributed key-value store, which can be\nused to share information between processes in the group as well as to\ninitialize the distributed package intorch.distributed.init_process_group()(by explicitly creating the store\nas an alternative to specifyinginit_method.) There are 3 choices for\nKey-Value Stores:TCPStore,FileStore, andHashStore.\ntorch.distributed.init_process_group()\ninit_method\nTCPStore\nFileStore\nHashStore\nBase class for all store implementations, such as the 3 provided by PyTorch\ndistributed: (TCPStore,FileStore,\nandHashStore).\nTCPStore\nFileStore\nHashStore\nThe first call to add for a givenkeycreates a counter associated\nwithkeyin the store, initialized toamount. Subsequent calls to add\nwith the samekeyincrement the counter by the specifiedamount.\nCallingadd()with a key that has already\nbeen set in the store byset()will result\nin an exception.\nkey\nkey\namount\nkey\namount\nadd()\nset()\nkey(str) \u2013 The key in the store whose counter will be incremented.\namount(int) \u2013 The quantity by which the counter will be incremented.\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")\n\n```\n\nAppend the key-value pair into the store based on the suppliedkeyandvalue. Ifkeydoes not exists in the store, it will be created.\nkey\nvalue\nkey\nkey(str) \u2013 The key to be appended to the store.\nvalue(str) \u2013 The value associated withkeyto be added to the store.\nkey\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.append(\"first_key\", \"po\")\n>>> store.append(\"first_key\", \"tato\")\n>>> # Should return \"potato\"\n>>> store.get(\"first_key\")\n\n```\n\nThe call to check whether a given list ofkeyshave value stored in\nthe store. This call immediately returns in normal cases but still suffers\nfrom some edge deadlock cases, e.g, calling check after TCPStore has been destroyed.\nCallingcheck()with a list of keys that\none wants to check whether stored in the store or not.\nkeys\ncheck()\nkeys(list[str]) \u2013 The keys to query whether stored in the store.\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> # Should return 7\n>>> store.check([\"first_key\"])\n\n```\n\nClones the store and returns a new object that points to the same underlying\nstore. The returned store can be used concurrently with the original object.\nThis is intended to provide a safe way to use a store from multiple threads by\ncloning one store per thread.\nInserts the key-value pair into the store based on the suppliedkeyand\nperforms comparison betweenexpected_valueanddesired_valuebefore inserting.desired_valuewill only be set ifexpected_valuefor thekeyalready exists in the store or ifexpected_valueis an empty string.\nkey\nexpected_value\ndesired_value\ndesired_value\nexpected_value\nkey\nexpected_value\nkey(str) \u2013 The key to be checked in the store.\nexpected_value(str) \u2013 The value associated withkeyto be checked before insertion.\nkey\ndesired_value(str) \u2013 The value associated withkeyto be added to the store.\nkey\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"key\", \"first_value\")\n>>> store.compare_set(\"key\", \"first_value\", \"second_value\")\n>>> # Should return \"second_value\"\n>>> store.get(\"key\")\n\n```\n\nDeletes the key-value pair associated withkeyfrom the store. Returnstrueif the key was successfully deleted, andfalseif it was not.\nkey\nWarning\nThedelete_keyAPI is only supported by theTCPStoreandHashStore. Using this API\nwith theFileStorewill result in an exception.\ndelete_key\nTCPStore\nHashStore\nFileStore\nkey(str) \u2013 The key to be deleted from the store\nTrueifkeywas deleted, otherwiseFalse.\nkey\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")\n\n```\n\nRetrieves the value associated with the givenkeyin the store. Ifkeyis not\npresent in the store, the function will wait fortimeout, which is defined\nwhen initializing the store, before throwing an exception.\nkey\nkey\ntimeout\nkey(str) \u2013 The function will return the value associated with this key.\nValue associated withkeyifkeyis in the store.\nkey\nkey\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n\n```\n\nReturns true if the store supports extended operations.\nRetrieve all values inkeys. If any key inkeysis not\npresent in the store, the function will wait fortimeout\nkeys\nkeys\ntimeout\nkeys(List[str]) \u2013 The keys to be retrieved from the store.\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"po\")\n>>> store.set(\"second_key\", \"tato\")\n>>> # Should return [b\"po\", b\"tato\"]\n>>> store.multi_get([\"first_key\", \"second_key\"])\n\n```\n\nInserts a list key-value pair into the store based on the suppliedkeysandvalues\nkeys\nvalues\nkeys(List[str]) \u2013 The keys to insert.\nvalues(List[str]) \u2013 The values to insert.\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.multi_set([\"first_key\", \"second_key\"], [\"po\", \"tato\"])\n>>> # Should return b\"po\"\n>>> store.get(\"first_key\")\n\n```\n\nReturns the number of keys set in the store. Note that this number will typically\nbe one greater than the number of keys added byset()andadd()since one key is used to coordinate all\nthe workers using the store.\nset()\nadd()\nWarning\nWhen used with theTCPStore,num_keysreturns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.\nTCPStore\nnum_keys\nThe number of keys present in the store.\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()\n\n```\n\nReturns the length of the specified queue.\nIf the queue doesn\u2019t exist it returns 0.\nSee queue_push for more details.\nkey(str) \u2013 The key of the queue to get the length.\nPops a value from the specified queue or waits until timeout if the queue is empty.\nSee queue_push for more details.\nIf block is False, a dist.QueueEmptyError will be raised if the queue is empty.\nkey(str) \u2013 The key of the queue to pop from.\nblock(bool) \u2013 Whether to block waiting for the key or immediately return.\nPushes a value into the specified queue.\nUsing the same key for queues and set/get operations may result in unexpected\nbehavior.\nwait/check operations are supported for queues.\nwait with queues will only wake one waiting worker rather than all.\nkey(str) \u2013 The key of the queue to push to.\nvalue(str) \u2013 The value to push into the queue.\nInserts the key-value pair into the store based on the suppliedkeyandvalue. Ifkeyalready exists in the store, it will overwrite the old\nvalue with the new suppliedvalue.\nkey\nvalue\nkey\nvalue\nkey(str) \u2013 The key to be added to the store.\nvalue(str) \u2013 The value associated withkeyto be added to the store.\nkey\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n\n```\n\nSets the store\u2019s default timeout. This timeout is used during initialization and inwait()andget().\nwait()\nget()\ntimeout(timedelta) \u2013 timeout to be set in the store.\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])\n\n```\n\nGets the timeout of the store.\nOverloaded function.\nwait(self: torch._C._distributed_c10d.Store, arg0: collections.abc.Sequence[str]) -> None\nWaits for each key inkeysto be added to the store. If not all keys are\nset before thetimeout(set during store initialization), thenwaitwill throw an exception.\nkeys\ntimeout\nwait\nkeys(list) \u2013 List of keys on which to wait until they are set in the store.\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])\n\n```\n\nwait(self: torch._C._distributed_c10d.Store, arg0: collections.abc.Sequence[str], arg1: datetime.timedelta) -> None\nWaits for each key inkeysto be added to the store, and throws an exception\nif the keys have not been set by the suppliedtimeout.\nkeys\ntimeout\nkeys(list) \u2013 List of keys on which to wait until they are set in the store.\ntimeout(timedelta) \u2013 Time to wait for the keys to be added before throwing an exception.\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))\n\n```\n\nA TCP-based distributed key-value store implementation. The server store holds\nthe data, while the client stores can connect to the server store over TCP and\nperform actions such asset()to insert a key-value\npair,get()to retrieve a key-value pair, etc. There\nshould always be one server store initialized because the client store(s) will wait for\nthe server to establish a connection.\nset()\nget()\nhost_name(str) \u2013 The hostname or IP Address the server store should run on.\nport(int) \u2013 The port on which the server store should listen for incoming requests.\nworld_size(int,optional) \u2013 The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).\nis_master(bool,optional) \u2013 True when initializing the server store and False for client stores. Default is False.\ntimeout(timedelta,optional) \u2013 Timeout used by the store during initialization and for methods such asget()andwait(). Default is timedelta(seconds=300)\nget()\nwait()\nwait_for_workers(bool,optional) \u2013 Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.\nmulti_tenant(bool,optional) \u2013 If True, allTCPStoreinstances in the current process with the same host/port will use the same underlyingTCPServer. Default is False.\nTCPStore\nTCPServer\nmaster_listen_fd(int,optional) \u2013 If specified, the underlyingTCPServerwill listen on this file descriptor, which must be a socket already bound toport. To bind an ephemeral port we recommend setting the port to 0 and reading.port. Default is None (meaning the server creates a new socket and attempts to bind it toport).\nTCPServer\nport\n.port\nport\nuse_libuv(bool,optional) \u2013 If True, use libuv forTCPServerbackend. Default is True.\nTCPServer\n\n```python\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")\n\n```\n\nCreates a new TCPStore.\nGets the hostname on which the store listens for requests.\nReturns True if it\u2019s using the libuv backend.\nGets the port number on which the store listens for requests.\nA thread-safe store implementation based on an underlying hashmap. This store can be used\nwithin the same process (for example, by other threads), but cannot be used across processes.\n\n```python\n>>> import torch.distributed as dist\n>>> store = dist.HashStore()\n>>> # store can be used from other threads\n>>> # Use any of the store methods after initialization\n>>> store.set(\"first_key\", \"first_value\")\n\n```\n\nCreates a new HashStore.\nA store implementation that uses a file to store the underlying key-value pairs.\nfile_name(str) \u2013 path of the file in which to store the key-value pairs\nworld_size(int,optional) \u2013 The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).\n\n```python\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")\n\n```\n\nCreates a new FileStore.\nGets the path of the file used by FileStore to store key-value pairs.\nA wrapper around any of the 3 key-value stores (TCPStore,FileStore, andHashStore)\nthat adds a prefix to each key inserted to the store.\nTCPStore\nFileStore\nHashStore\nprefix(str) \u2013 The prefix string that is prepended to each key before being inserted into the store.\nstore(torch.distributed.store) \u2013 A store object that forms the underlying key-value store.\nCreates a new PrefixStore.\nGets the underlying store object that PrefixStore wraps around.\n\n## Profiling Collective Communication#\n\nNote that you can usetorch.profiler(recommended, only available after 1.8.1) ortorch.autograd.profilerto profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,nccl,mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:\ntorch.profiler\ntorch.autograd.profiler\ngloo\nnccl\nmpi\n\n```python\nimport torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor)\n\n```\n\nPlease refer to theprofiler documentationfor a full overview of profiler features.\n\n## Multi-GPU collective functions#\n\nWarning\nThe multi-GPU functions (which stand for multiple GPUs per CPU thread) are\ndeprecated. As of today, PyTorch Distributed\u2019s preferred programming model\nis one device per thread, as exemplified by the APIs in this document. If\nyou are a backend developer and want to support multiple devices per thread,\nplease contact PyTorch Distributed\u2019s maintainers.\n\n## Object collectives#\n\nWarning\nObject collectives have a number of serious limitations. Read further to determine\nif they are safe to use for your use case.\nObject collectives are a set of collective-like operations that work on arbitrary\nPython objects, as long as they can be pickled. There are various collective patterns\nimplemented (e.g. broadcast, all_gather, \u2026) but they each roughly follow this pattern:\nconvert the input object into a pickle (raw bytes), then shove it into a byte tensor\ncommunicate the size of this byte tensor to peers (first collective operation)\nallocate appropriately sized tensor to perform the real collective\ncommunicate the object data (second collective operation)\nconvert raw data back into Python (unpickle)\nObject collectives sometimes have surprising performance or memory characteristics that lead to\nlong runtimes or OOMs, and thus they should be used with caution. Here are some common issues.\nAsymmetric pickle/unpickle time- Pickling objects can be slow, depending on the number, type and size of the objects.\nWhen the collective has a fan-in (e.g. gather_object), the receiving rank(s) must unpickle N times more objects than\nthe sending rank(s) had to pickle, which can cause other ranks to time out on their next collective.\nInefficient tensor communication- Tensors should be sent via regular collective APIs, not object collective APIs.\nIt is possible to send Tensors via object collective APIs, but they will be serialized and deserialized (including a\nCPU-sync and device-to-host copy in the case of non-CPU tensors), and in almost every case other than debugging or\ntroubleshooting code, it would be worth the trouble to refactor the code to use non-object collectives instead.\nUnexpected tensor devices- If you still want to send tensors via object collectives, there is another aspect\nspecific to cuda (and possibly other accelerators) tensors. If you pickle a tensor that is currently oncuda:3, and\nthen unpickle it, you will get another tensor oncuda:3regardless of which process you are on, or which CUDA device\nis the \u2018default\u2019 device for that process. With regular tensor collective APIs, \u2018output tensors\u2019 will always be on the\nsame, local device, which is generally what you\u2019d expect.\ncuda:3\ncuda:3\nUnpickling a tensor will implicitly activate a CUDA context if it is the first\ntime a GPU is used by the process, which can waste significant amounts of GPU memory. This issue can be avoided by\nmoving tensors to CPU before passing them as inputs to an object collective.\n\n## Third-party backends#\n\nBesides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports\nthird-party backends through a run-time register mechanism.\nFor references on how to develop a third-party backend through C++ Extension,\nplease refer toTutorials - Custom C++ and CUDA Extensionsandtest/cpp_extensions/cpp_c10d_extension.cpp. The capability of third-party\nbackends are decided by their own implementations.\ntest/cpp_extensions/cpp_c10d_extension.cpp\nThe new backend derives fromc10d::ProcessGroupand registers the backend\nname and the instantiating interface throughtorch.distributed.Backend.register_backend()when imported.\nc10d::ProcessGroup\ntorch.distributed.Backend.register_backend()\nWhen manually importing this backend and invokingtorch.distributed.init_process_group()with the corresponding backend name, thetorch.distributedpackage runs on\nthe new backend.\ntorch.distributed.init_process_group()\ntorch.distributed\nWarning\nThe support of third-party backend is experimental and subject to change.\n\n## Launch utility#\n\nThetorch.distributedpackage also provides a launch utility intorch.distributed.launch. This helper utility can be used to launch\nmultiple processes per node for distributed training.\ntorch.distributed\ntorch.distributed.launch\nModuletorch.distributed.launch.\ntorch.distributed.launch\ntorch.distributed.launchis a module that spawns up multiple distributed\ntraining processes on each of the training nodes.\ntorch.distributed.launch\nWarning\nThis module is going to be deprecated in favor oftorchrun.\nThe utility can be used for single-node distributed training, in which one or\nmore processes per node will be spawned. The utility can be used for either\nCPU training or GPU training. If the utility is used for GPU training,\neach distributed process will be operating on a single GPU. This can achieve\nwell-improved single-node training performance. It can also be used in\nmulti-node distributed training, by spawning up multiple processes on each node\nfor well-improved multi-node distributed training performance as well.\nThis will especially be beneficial for systems with multiple Infiniband\ninterfaces that have direct-GPU support, since all of them can be utilized for\naggregated communication bandwidth.\nIn both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(--nproc-per-node). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (nproc_per_node),\nand each process will be operating on a single GPU fromGPU 0 to\nGPU (nproc_per_node - 1).\n--nproc-per-node\nnproc_per_node\nHow to use this module:\nSingle-Node multi-process distributed training\n\n```python\npython -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script)\n\n```\n\nMulti-Node multi-process distributed training: (e.g. two nodes)\nNode 1:(IP: 192.168.1.1, and has a free port: 1234)\n\n```python\npython -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node-rank=0 --master-addr=\"192.168.1.1\"\n           --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n\n```\n\nNode 2:\n\n```python\npython -m torch.distributed.launch --nproc-per-node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node-rank=1 --master-addr=\"192.168.1.1\"\n           --master-port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n\n```\n\nTo look up what optional arguments this module offers:\n\n```python\npython -m torch.distributed.launch --help\n\n```\n\nImportant Notices:\n1. This utility and multi-process distributed (single-node or\nmulti-node) GPU training currently only achieves the best performance using\nthe NCCL distributed backend. Thus NCCL backend is the recommended backend to\nuse for GPU training.\n2. In your training program, you must parse the command-line argument:--local-rank=LOCAL_PROCESS_RANK, which will be provided by this module.\nIf your training program uses GPUs, you should ensure that your code only\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:\n--local-rank=LOCAL_PROCESS_RANK\nParsing the local_rank argument\n\n```python\n>>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local-rank\", \"--local_rank\", type=int)\n>>> args = parser.parse_args()\n\n```\n\nSet your device to local rank using either\n\n```python\n>>> torch.cuda.set_device(args.local_rank)  # before your code runs\n\n```\n\nor\n\n```python\n>>> with torch.cuda.device(args.local_rank):\n>>>    # your code to run\n>>>    ...\n\n```\n\nChanged in version 2.0.0:The launcher will passes the--local-rank=<rank>argument to your script.\nFrom PyTorch 2.0.0 onwards, the dashed--local-rankis preferred over the\npreviously used underscored--local_rank.\n--local-rank=<rank>\n--local-rank\n--local_rank\nFor backward compatibility, it may be necessary for users to handle both\ncases in their argument parsing code. This means including both\"--local-rank\"and\"--local_rank\"in the argument parser. If only\"--local_rank\"is\nprovided, the launcher will trigger an error: \u201cerror: unrecognized arguments:\n\u2013local-rank=<rank>\u201d. For training code that only supports PyTorch 2.0.0+,\nincluding\"--local-rank\"should be sufficient.\n\"--local-rank\"\n\"--local_rank\"\n\"--local_rank\"\n\"--local-rank\"\n3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. It is strongly recommended\nthatinit_method=env://. Other init methods (e.g.tcp://) may work,\nbutenv://is the one that is officially supported by this module.\ninit_method=env://\ntcp://\nenv://\n\n```python\n>>> torch.distributed.init_process_group(backend='YOUR BACKEND',\n>>>                                      init_method='env://')\n\n```\n\n4. In your training program, you can either use regular distributed functions\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\nhere is how to configure it.\ntorch.nn.parallel.DistributedDataParallel()\ntorch.nn.parallel.DistributedDataParallel()\n\n```python\n>>> model = torch.nn.parallel.DistributedDataParallel(model,\n>>>                                                   device_ids=[args.local_rank],\n>>>                                                   output_device=args.local_rank)\n\n```\n\nPlease ensure thatdevice_idsargument is set to be the only GPU device id\nthat your code will be operating on. This is generally the local rank of the\nprocess. In other words, thedevice_idsneeds to be[args.local_rank],\nandoutput_deviceneeds to beargs.local_rankin order to use this\nutility\ndevice_ids\ndevice_ids\n[args.local_rank]\noutput_device\nargs.local_rank\n5. Another way to passlocal_rankto the subprocesses via environment variableLOCAL_RANK. This behavior is enabled when you launch the script with--use-env=True. You must adjust the subprocess example above to replaceargs.local_rankwithos.environ['LOCAL_RANK']; the launcher\nwill not pass--local-rankwhen you specify this flag.\nlocal_rank\nLOCAL_RANK\n--use-env=True\nargs.local_rank\nos.environ['LOCAL_RANK']\n--local-rank\nWarning\nlocal_rankis NOT globally unique: it is only unique per process\non a machine.  Thus, don\u2019t use it to decide if you should, e.g.,\nwrite to a networked filesystem.  Seepytorch/pytorch#12042for an example of\nhow things can go wrong if you don\u2019t do this correctly.\nlocal_rank\n\n## Spawn utility#\n\nTheMultiprocessing package - torch.multiprocessingpackage also provides aspawnfunction intorch.multiprocessing.spawn(). This helper function\ncan be used to spawn multiple processes. It works by passing in the\nfunction that you want to run and spawns N processes to run it. This\ncan be used for multiprocess distributed training as well.\nspawn\ntorch.multiprocessing.spawn()\nFor references on how to use it, please refer toPyTorch example - ImageNet\nimplementation\nNote that this function requires Python 3.4 or higher.\n\n## Debuggingtorch.distributedapplications#\n\ntorch.distributed\nDebugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks.torch.distributedprovides\na suite of tools to help debug training applications in a self-serve fashion:\ntorch.distributed\n\n## Python Breakpoint#\n\nIt is extremely convenient to use python\u2019s debugger in a distributed environment, but because it does not work out of the box many people do not use it at all.\nPyTorch offers a customized wrapper around pdb that streamlines the process.\ntorch.distributed.breakpointmakes this process easy. Internally, it customizespdb\u2019s breakpoint behavior in two ways but otherwise behaves as normalpdb.\ntorch.distributed.breakpoint\npdb\npdb\nAttaches the debugger only on one rank (specified by the user).\nEnsures all other ranks stop, by using atorch.distributed.barrier()that will release once the debugged rank issues acontinue\ntorch.distributed.barrier()\ncontinue\nReroutes stdin from the child process such that it connects to your terminal.\nTo use it, simply issuetorch.distributed.breakpoint(rank)on all ranks, using the same value forrankin each case.\ntorch.distributed.breakpoint(rank)\nrank\n\n## Monitored Barrier#\n\nAs of v1.10,torch.distributed.monitored_barrier()exists as an alternative totorch.distributed.barrier()which fails with helpful information about which rank may be faulty\nwhen crashing, i.e. not all ranks calling intotorch.distributed.monitored_barrier()within the provided timeout.torch.distributed.monitored_barrier()implements a host-side\nbarrier usingsend/recvcommunication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge\nthe barrier in time. As an example, consider the following function where rank 1 fails to call intotorch.distributed.monitored_barrier()(in practice this could be due\nto an application bug or hang in a previous collective):\ntorch.distributed.monitored_barrier()\ntorch.distributed.barrier()\ntorch.distributed.monitored_barrier()\ntorch.distributed.monitored_barrier()\nsend\nrecv\ntorch.distributed.monitored_barrier()\n\n```python\nimport os\nfrom datetime import timedelta\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    # monitored barrier requires gloo process group to perform host-side sync.\n    group_gloo = dist.new_group(backend=\"gloo\")\n    if rank not in [1]:\n        dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    mp.spawn(worker, nprocs=2, args=())\n\n```\n\nThe following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further:\n\n```python\nRuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms\n Original exception:\n[gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594\n\n```\n\n\n## TORCH_DISTRIBUTED_DEBUG#\n\nTORCH_DISTRIBUTED_DEBUG\nWithTORCH_CPP_LOG_LEVEL=INFO, the environment variableTORCH_DISTRIBUTED_DEBUGcan be used to trigger additional useful logging and collective synchronization checks to ensure all ranks\nare synchronized appropriately.TORCH_DISTRIBUTED_DEBUGcan be set to eitherOFF(default),INFO, orDETAILdepending on the debugging level\nrequired. Please note that the most verbose option,DETAILmay impact the application performance and thus should only be used when debugging issues.\nTORCH_CPP_LOG_LEVEL=INFO\nTORCH_DISTRIBUTED_DEBUG\nTORCH_DISTRIBUTED_DEBUG\nOFF\nINFO\nDETAIL\nDETAIL\nSettingTORCH_DISTRIBUTED_DEBUG=INFOwill result in additional debug logging when models trained withtorch.nn.parallel.DistributedDataParallel()are initialized, andTORCH_DISTRIBUTED_DEBUG=DETAILwill additionally log runtime performance statistics a select number of iterations. These runtime statistics\ninclude data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:\nTORCH_DISTRIBUTED_DEBUG=INFO\ntorch.nn.parallel.DistributedDataParallel()\nTORCH_DISTRIBUTED_DEBUG=DETAIL\n\n```python\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\nclass TwoLinLayerNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Linear(10, 10, bias=False)\n        self.b = torch.nn.Linear(10, 1, bias=False)\n\n    def forward(self, x):\n        a = self.a(x)\n        b = self.b(x)\n        return (a, b)\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    print(\"init model\")\n    model = TwoLinLayerNet().cuda()\n    print(\"init ddp\")\n    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n    inp = torch.randn(10, 10).cuda()\n    print(\"train\")\n\n    for _ in range(20):\n        output = ddp_model(inp)\n        loss = output[0] + output[1]\n        loss.sum().backward()\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\n        \"TORCH_DISTRIBUTED_DEBUG\"\n    ] = \"DETAIL\"  # set to DETAIL for runtime logging.\n    mp.spawn(worker, nprocs=2, args=())\n\n```\n\nThe following logs are rendered at initialization time:\n\n```python\nI0607 16:10:35.739390 515217 logger.cpp:173] [Rank 0]: DDP Initialized with:\nbroadcast_buffers: 1\nbucket_cap_bytes: 26214400\nfind_unused_parameters: 0\ngradient_as_bucket_view: 0\nis_multi_device_module: 0\niteration: 0\nnum_parameter_tensors: 2\noutput_device: 0\nrank: 0\ntotal_parameter_size_bytes: 440\nworld_size: 2\nbackend_name: nccl\nbucket_sizes: 440\ncuda_visible_devices: N/A\ndevice_ids: 0\ndtypes: float\nmaster_addr: localhost\nmaster_port: 29501\nmodule_name: TwoLinLayerNet\nnccl_async_error_handling: N/A\nnccl_blocking_wait: N/A\nnccl_debug: WARN\nnccl_ib_timeout: N/A\nnccl_nthreads: N/A\nnccl_socket_ifname: N/A\ntorch_distributed_debug: INFO\n\n```\n\nThe following logs are rendered during runtime (whenTORCH_DISTRIBUTED_DEBUG=DETAILis set):\nTORCH_DISTRIBUTED_DEBUG=DETAIL\n\n```python\nI0607 16:18:58.085681 544067 logger.cpp:344] [Rank 1 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 40838608\n Avg backward compute time: 5983335\nAvg backward comm. time: 4326421\n Avg backward comm/comp overlap time: 4207652\nI0607 16:18:58.085693 544066 logger.cpp:344] [Rank 0 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 42850427\n Avg backward compute time: 3885553\nAvg backward comm. time: 2357981\n Avg backward comm/comp overlap time: 2234674\n\n```\n\nIn addition,TORCH_DISTRIBUTED_DEBUG=INFOenhances crash logging intorch.nn.parallel.DistributedDataParallel()due to unused parameters in the model. Currently,find_unused_parameters=Truemust be passed intotorch.nn.parallel.DistributedDataParallel()initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required\nto be used in loss computation astorch.nn.parallel.DistributedDataParallel()does not support unused parameters in the backwards pass. These constraints are challenging especially for larger\nmodels, thus when crashing with an error,torch.nn.parallel.DistributedDataParallel()will log the fully qualified name of all parameters that went unused. For example, in the above application,\nif we modifylossto be instead computed asloss=output[1], thenTwoLinLayerNet.adoes not receive a gradient in the backwards pass, and\nthus results inDDPfailing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:\nTORCH_DISTRIBUTED_DEBUG=INFO\ntorch.nn.parallel.DistributedDataParallel()\nfind_unused_parameters=True\ntorch.nn.parallel.DistributedDataParallel()\ntorch.nn.parallel.DistributedDataParallel()\ntorch.nn.parallel.DistributedDataParallel()\nloss\nloss=output[1]\nTwoLinLayerNet.a\nDDP\n\n```python\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing\n the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\nmaking sure all `forward` function outputs participate in calculating loss.\nIf you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va\nlue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\nParameters which did not receive grad for rank 0: a.weight\nParameter indices which did not receive grad for rank 0: 0\n\n```\n\nSettingTORCH_DISTRIBUTED_DEBUG=DETAILwill trigger additional consistency and synchronization checks on every collective call issued by the user\neither directly or indirectly (such as DDPallreduce). This is done by creating a wrapper process group that wraps all process groups returned bytorch.distributed.init_process_group()andtorch.distributed.new_group()APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process\ngroup, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include atorch.distributed.monitored_barrier(),\nwhich ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by\nensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the\napplication crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes intotorch.distributed.all_reduce():\nTORCH_DISTRIBUTED_DEBUG=DETAIL\nallreduce\ntorch.distributed.init_process_group()\ntorch.distributed.new_group()\ntorch.distributed.monitored_barrier()\ntorch.distributed.all_reduce()\n\n```python\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    tensor = torch.randn(10 if rank == 0 else 20).cuda()\n    dist.all_reduce(tensor)\n    torch.cuda.synchronize(device=rank)\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n    mp.spawn(worker, nprocs=2, args=())\n\n```\n\nWith theNCCLbackend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enablesTORCH_DISTRIBUTED_DEBUG=DETAILand reruns the application, the following error message reveals the root cause:\nNCCL\nTORCH_DISTRIBUTED_DEBUG=DETAIL\n\n```python\nwork = default_pg.allreduce([tensor], opts)\nRuntimeError: Error when verifying shape tensors for collective ALLREDUCE on rank 0. This likely indicates that input shapes into the collective are mismatched across ranks. Got shapes:  10\n[ torch.LongTensor{1} ]\n\n```\n\nNote\nFor fine-grained control of the debug level during runtime the functionstorch.distributed.set_debug_level(),torch.distributed.set_debug_level_from_env(), andtorch.distributed.get_debug_level()can also be used.\ntorch.distributed.set_debug_level()\ntorch.distributed.set_debug_level_from_env()\ntorch.distributed.get_debug_level()\nIn addition,TORCH_DISTRIBUTED_DEBUG=DETAILcan be used in conjunction withTORCH_SHOW_CPP_STACKTRACES=1to log the entire callstack when a collective desynchronization is detected. These\ncollective desynchronization checks will work for all applications that usec10dcollective calls backed by process groups created with thetorch.distributed.init_process_group()andtorch.distributed.new_group()APIs.\nTORCH_DISTRIBUTED_DEBUG=DETAIL\nTORCH_SHOW_CPP_STACKTRACES=1\nc10d\ntorch.distributed.init_process_group()\ntorch.distributed.new_group()\n\n## Logging#\n\nIn addition to explicit debugging support viatorch.distributed.monitored_barrier()andTORCH_DISTRIBUTED_DEBUG, the underlying C++ library oftorch.distributedalso outputs log\nmessages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The\nfollowing matrix shows how the log level can be adjusted via the combination ofTORCH_CPP_LOG_LEVELandTORCH_DISTRIBUTED_DEBUGenvironment variables.\ntorch.distributed.monitored_barrier()\nTORCH_DISTRIBUTED_DEBUG\ntorch.distributed\nTORCH_CPP_LOG_LEVEL\nTORCH_DISTRIBUTED_DEBUG\nTORCH_CPP_LOG_LEVEL\nTORCH_CPP_LOG_LEVEL\nTORCH_DISTRIBUTED_DEBUG\nTORCH_DISTRIBUTED_DEBUG\nEffective Log Level\nERROR\nERROR\nignored\nError\nWARNING\nWARNING\nignored\nWarning\nINFO\nINFO\nignored\nInfo\nINFO\nINFO\nINFO\nINFO\nDebug\nINFO\nINFO\nDETAIL\nDETAIL\nTrace (a.k.a. All)\nDistributed components raise custom Exception types derived fromRuntimeError:\nRuntimeError\ntorch.distributed.DistError: This is the base type of all distributed exceptions.\ntorch.distributed.DistError\ntorch.distributed.DistBackendError: This exception is thrown when a backend-specific error occurs. For example, if\ntheNCCLbackend is used and the user attempts to use a GPU that is not available to theNCCLlibrary.\ntorch.distributed.DistBackendError\nNCCL\nNCCL\ntorch.distributed.DistNetworkError: This exception is thrown when networking\nlibraries encounter errors (ex: Connection reset by peer)\ntorch.distributed.DistNetworkError\ntorch.distributed.DistStoreError: This exception is thrown when the Store encounters\nan error (ex: TCPStore timeout)\ntorch.distributed.DistStoreError\nException raised when an error occurs in the distributed library\nException raised when a backend error occurs in distributed\nException raised when a network error occurs in distributed\nException raised when an error occurs in the distributed store\nIf you are running single node training, it may be convenient to interactively breakpoint your script. We offer a way to conveniently breakpoint a single rank:\nSet a breakpoint, but only on a single rank.  All other ranks will wait for you to be\ndone with the breakpoint before continuing.\nrank(int) \u2013 Which rank to break on.  Default:0\n0\nskip(int) \u2013 Skip the firstskipcalls to this breakpoint. Default:0.\nskip\n0",
    "url": "https://pytorch.org/docs/stable/distributed.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "687788dbcdab6a3f17ca46e8de62c173",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/run.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e9571ad7c92b5b06563513e3ed29c7da",
    "source": "pytorch_docs",
    "title": "torch.are_deterministic_algorithms_enabled \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.are_deterministic_algorithms_enabled#\n\nReturns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details.\ntorch.use_deterministic_algorithms()\nbool",
    "url": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6bb3168fceb5e446c9ecd263d0170505",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_backward.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3dc55ae923a53721af8a01f2b159510f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/autograd.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "fb33583cb0e08ad6caa24f823c6e05a1",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/onnx_export.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0fabed75411f0f8d73798cf429948dc7",
    "source": "pytorch_docs",
    "title": "Dynamic Shapes \u2014 PyTorch 2.9 documentation",
    "text": "\n## Dynamic Shapes#\n\nCreated On: May 19, 2023 | Last Updated On: Jun 10, 2025\nCode:symbolic_shapes.py\nSee also:The dynamic shapes manual\n\n## Motivation#\n\nDeep learning compilers commonly only work for static shapes, that is to say, they produced compiled programs which only work for a single specific configuration of input shapes, and must recompile if any input shape changes. This assumption works great for the majority of commonly run deep learning models today, but there are a few situations where it is insufficient:\nSome dimensions, such as batch size or sequence length, may vary. For example, an inference service performing adaptive batching will execute inference requests with varying batch sizes depending on how many requests it received within its batching window. We may also want to consider padding out variable size sequences only to the maximum sequence length within a batch, which may vary from batch-to-batch.\nSome models exhibit data-dependent output shapes, that is to say, the size of their outputs and intermediates may depend on the actual input data which may vary across runs. For example, detection models may first generate a variable number of potential bounding boxes before running a more expensive image recognition model to identify if the subject is in a bounding box. The number of bounding boxes is data dependent.\nOne particularly important case of data-dependent shapes occurs when dealing with sparse representations, such as sparse tensors, jagged tensors, and graph neural networks. In all of these cases, the amount of data to be processed depends on the sparse structure of the problem, which will typically vary in a data-dependent way.\nIn supporting dynamic shapes, we chose not to support dynamic rank programs, e.g., programs whose inputs tensors change in dimensionality, as this pattern rarely occurs in real-world deep learning programs, and it avoids the need to reason inductively over symbolic lists of shapes.\n\n## Abridged public API#\n\nThe default dynamic behavior in PyTorch 2.1 is:\nPT2 assumes everything is static by default\nIf we recompile because a size changed, we will instead attempt to recompile\nthat size as being dynamic (sizes that have changed are likely to change in\nthe future). This generalization may fail (e.g., because user code does a\nconditional branch on the size in question or missing dynamic shapes support\nin PT2). If you are trying to understand why PT2 has overspecialized some\ncode, run withTORCH_LOGS=dynamicand look for \u201ceval\u201d entries that say\nwhen guards are added and why.\nTORCH_LOGS=dynamic\nIf you know ahead of time something will be dynamic, you can skip the first\nrecompile withtorch._dynamo.mark_dynamic(tensor,dim). If you know ahead of time\ntheminandmaxvalue this dimension can take, you can specifytorch._dynamo.mark_dynamic(tensor,dim,min=min,max=max)\ntorch._dynamo.mark_dynamic(tensor,dim)\nmin\nmax\ntorch._dynamo.mark_dynamic(tensor,dim,min=min,max=max)\nIf you saytorch.compile(dynamic=False), we will turn off automatic\ndynamic shapes on recompiles and always recompile for each distinct size.\nConversely, if you saytorch.compile(dynamic=True), we will try to make\neverything as dynamic as possible. This is mostly useful for small\noperators; if you try it on a big model it will (1) probably crash PT2 and (2) run slow for no good reason.\ntorch.compile(dynamic=False)\ntorch.compile(dynamic=True)\nYou can whitelist specific sources to be marked as dynamic using theTORCH_COMPILE_DYNAMIC_SOURCESenvironment variable or by settingtorch.compiler.config.dynamic_sources. This is particularly useful for large\nmodels with graph breaks, as you can maintain dynamism across graph breaks since\nsource names stay consistent. You can also use this to mark integers as dynamic.\nThe format is a comma-delimited list of source names, e.g.,\"L['x'],L['y']\".\nYou can also use regexes, e.g.,\"L\\['x.*'\\],L\\['y.*'\\]\").\nThis whitelist takes precedence over other flags likedynamic=False,force_nn_module_property_static_shapes, andforce_parameter_static_shapes.\nTORCH_COMPILE_DYNAMIC_SOURCES\ntorch.compiler.config.dynamic_sources\n\"L['x'],L['y']\"\n\"L\\['x.*'\\],L\\['y.*'\\]\")\ndynamic=False\nforce_nn_module_property_static_shapes\nforce_parameter_static_shapes\nSometimes it can be cumbersome to find the right inputs to mark as dynamic. If\nyou\u2019re willing to take a performance hit for the first batch, one other affordable\noption we have are the eager_then_compile stances which derive dynamism for you.\nSeetorch.compiler.set_stancefor more details.\n\n## The Guard Model#\n\nWhen considering how to add support for dynamic shapes to TorchDynamo and TorchInductor, we made a major design decision: in order to reuse decompositions and other preexisting code written in Python/C++ targeting the PyTorch API, we must be able to trace through dynamic shapes. Unlike a fully symbolic system which might capture both branches of a conditional, we always pick one branch and specialize our trace under the assumption that we only use this trace when we would have made the same choice for that branch in the future. To do this, we maintain a \u201chint\u201d for every symbolic size saying what its concrete value is at compile time (as TorchDynamo is a just-in-time compiler, it always knows what the actual input sizes are.) When we perform a condition on a tensor, we simply consult the hint to find out which branch to take.\nThis greatly simplifies the symbolic shape formulas we produce, but means we have a much more involved system for managing guards. Consider, for example, the following program:\n\n```python\ndef f(x, y):\n    z = torch.cat([x, y])\n    if z.size(0) > 2:\n        return z.mul(2)\n    else:\n        return z.add(2)\n\n```\n\nThe final IR we will compile with TorchInductor will either betorch.cat([x,y]).add(2)ortorch.cat([x,y]).mul(2)(with the condition flattened away), but to determine which branch we are in, we would need to know the size ofz, an intermediate. Because TorchDynamo must know upfront if a compiled trace is valid (we do not support bailouts, like some JIT compilers), we must be able to reducez.size(0)as an expression in terms of the inputs,x.size(0)+y.size(0). This is done by writing meta functions for all operators in PyTorch which can propagate size information to the output of a tensor without actually performing computation on the node.\ntorch.cat([x,y]).add(2)\ntorch.cat([x,y]).mul(2)\nz\nz.size(0)\nx.size(0)+y.size(0)\n\n## Overall architecture#\n\nSymbolic shapes workflow:\nWhen we start compiling a frame in Dynamo, we allocate a ShapeEnv (attached to FakeTensorMode) which keeps track of symbolic shapes state.\nWe allocate symbolic sizes for tensors on entry (what is static or dynamic is a policy decision, with some knobs).\nWe propagate the symbolic sizes through operators, maintaining both (1) FX IR so that we can faithfully export symbolic compute, and (2) Sympy expressions representing the size vars, so we can reason about them.\nWhen we condition on symbolic sizes, either in Dynamo tracing or in Inductor optimization, we add guards based on the conditional. These can be induced from both Python and C++.\nThese guards can induce further simplifications on symbolic variables. For example, if you asserts0==4, we can now replace all occurrences ofs0with4.\ns0==4\ns0\n4\nWhen we\u2019re done tracing and optimizing, we install all of these guards with the compiled code; the compiled code is only reusable if all the guards evaluate true.\nImportant files:\nC++ SymInt API:c10/core/SymInt.h,SymFloat.h,SymBool.h\nc10/core/SymInt.h\nSymFloat.h\nSymBool.h\nPython SymInt API:torch/__init__.py(look forSymInt/SymFloat/SymBool)\ntorch/__init__.py\nSymInt/SymFloat/SymBool\nC++ plumbing:c10/core/SymNodeImpl.h,torch/csrc/utils/python_symnode.h,torch/csrc/jit/python/init.cpp\nc10/core/SymNodeImpl.h\ntorch/csrc/utils/python_symnode.h\ntorch/csrc/jit/python/init.cpp\nPython infrastructure:torch/fx/experimental/symbolic_shapes.py\ntorch/fx/experimental/symbolic_shapes.py\nOther important files:torch/_subclasses/fake_tensor.py,torch/_meta_registrations.py, decomps, PrimTorch refs\ntorch/_subclasses/fake_tensor.py\ntorch/_meta_registrations.py\n\n## Abridged internal API#\n\nUnderstanding the Python class hierarchy:\nSymInt/SymFloat/SymBool: these are user-visible classes that simulate their int/float/bool counterparts. If you add two SymInts, we give you a new SymInt that symbolically tracks that the integer addition had occurred.\nSymNode: this is the internal structure (accessible via e.g.,symint.node) which holds the actual symbolic tracking info. SymNode is type erased; this makes it more convenient to represent mixed-type operations. Note that technically you don\u2019t have to call into Python SymNode from SymInt; for example, XLA\u2019s C++SymNodeImplwould take the place of SymNode.\nsymint.node\nSymNodeImpl\nShapeEnv: per-compile context state which keeps track of all the free symbols and guards we have accumulated so far. Every SymNode records its ShapeEnv (but not vice versa; SymNodes only get used if they participate in a guard).\nC++ is fairly similar:\nc10::SymInt/SymFloat/SymBool: user-visible classes that simulate int/float/bool.\nc10::SymNode/SymNodeImpl: analogous to SymNode\nThere is no ShapeEnv in C++; for ease of debugging, the entire symbolic reasoning apparatus is in Python.\nWhen you write code that is traceable withmake_fx, it must be able to deal with SymInt/SymFloat/SymBool flowing through it.The dynamic shapes manualgives some guidance for how to do this.\nmake_fx\n\n## DimDynamic policy#\n\nSymbolic reasoning:\nValue ranges\nSympy usage notes\nConstraints\nDimDynamic/Constraint\n\n## Unbacked SymInts#\n\nTo resolve control flow, we check the hint, aka actual value, of a symbolic integer to determine which branch to go. However, in some cases, we may not have a hint: so-called unbacked symbolic integers arise when a size variable emerges from a data-dependent operation like.nonzero()or.item(). It is illegal to perform control flow on these symbolic integers, so we must graph break on these operations.\n.nonzero()\n.item()\nNaively implemented, this is too restrictive: most PyTorch programs will immediately fail if you try to do anything with unbacked symbolic integers. Here are the most important enhancements to make this actually work:\nOn tensor creation, PyTorch precomputes a lot of data about a tensor; for example, if you useempty_stridedto create a tensor, we will eagerly sort the strides and determine if the tensor is non-overlapping and dense. Sorts produce a lot of guards. However, it is more common to produce a tensor directly with a higher-level API likeempty, which is guaranteed to produce a non-overlapping and dense tensor. We modified PyTorch to avoid needlessly recomputing these properties.\nempty_strided\nempty\nEven if nontrivial compute is needed, sometimes a property is never actually queried at all. Making these precomputed properties lazy allows us to avoid guarding on an unbacked symbolic integer unless it is actually needed.\nThe data in an integer tensor is generally not known to be non-negative. However, we provide an APIconstrain_rangewhereby a user can specify that a size is bounded above and below by known limits.\nconstrain_range\nSimilar to the dynamic APIs, there are corresponding unbacked APIs: namely you can use mark_unbacked instead ofmark_dynamicandTORCH_COMPILE_UNBACKED_SOURCESinstead ofTORCH_COMPILE_DYNAMIC_SOURCESto tell the compiler to mark an input as unbacked.\nmark_dynamic\nTORCH_COMPILE_UNBACKED_SOURCES\nTORCH_COMPILE_DYNAMIC_SOURCES\nIn future versions of PT2 (beyond PT2.1), we will extend our reasoning system\nto infer that an unbacked symbolic integer is size-like based on usage. For\nexample, if you pass the result of an.item()call to a factory function\nliketorch.empty, we will automatically infer that the result is a size\n(because if it was not, it would fail.) This assumption would get validated\nat runtime, raising an error if it was not fulfilled.\n.item()\ntorch.empty",
    "url": "https://pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e9774f8befa0c9c2b72e20e6baf958c2",
    "source": "pytorch_docs",
    "title": "DDP Communication Hooks \u2014 PyTorch 2.9 documentation",
    "text": "\n## DDP Communication Hooks#\n\nCreated On: Jun 06, 2025 | Last Updated On: Jun 06, 2025\nDDP communication hook is a generic interface to control how to communicate\ngradients across workers by overriding the vanilla allreduce inDistributedDataParallel.\nA few built-in communication hooks are provided,\nand users can easily apply any of these hooks to optimize communication.\nBesides, the hook interface can also support user-defined communication\nstrategies for more advanced use cases.\n\n## How to Use a Communication Hook?#\n\nTo use a communication hook, the user just needs to let the DDP model register\nthe hook before the training loop as below.\ntorch.nn.parallel.DistributedDataParallel.register_comm_hook()\ntorch.nn.parallel.DistributedDataParallel.register_comm_hook()\n\n## What Does a Communication Hook Operate On?#\n\nA communication hook provides a flexible way to allreduce gradients.\nTherefore, it mainly operates on the gradients on each replica before allreduce,\nwhich are bucketized to increase the overlap between communication and computation.\nParticularly,torch.distributed.GradBucketrepresents a bucket of gradient tensors to be allreduced.\ntorch.distributed.GradBucket\nThis class mainly passes a flattened gradient tensor\n(returned bybuffer())\nto DDP communication hook.\nThis tensor can be further decomposed into a list of per-parameter tensors within this bucket\n(returned byget_per_parameter_tensors())\nto apply layer-wise operations.\nbuffer()\nget_per_parameter_tensors()\nWarning\nSince the buckets are rebuilt after the first iteration, should not rely on the indices at the beginning of training.\nThe index of a bucket that stores gradients of a few contiguous layers.\nAll the gradients are bucketized.\nA flattened 1Dtorch.Tensorbuffer,\nwhich can be further decomposed into a list of per-parameter tensors within this bucket.\ntorch.Tensor\nA list oftorch.Tensor. Each tensor in the list corresponds to a gradient.\ntorch.Tensor\nWhether this bucket is the last bucket to allreduce in an iteration.\nThis also means that this bucket corresponds to the first few layers in the forward pass.\nReplaces the tensor in the bucket with the input tensor buffer.\nA list oftorch.Tensor. Each tensor in the list corresponds to a model\nparameter.\ntorch.Tensor\n\n## Default Communication Hooks#\n\nDefault communication hooks are simplestatelesshooks, so the input state\ninregister_comm_hookis either a process group orNone.\nThe inputbucketis atorch.distributed.GradBucketobject.\nregister_comm_hook\nNone\nbucket\ntorch.distributed.GradBucket\nCallallreduceusingGradBuckettensors.\nallreduce\nGradBucket\nOnce gradient tensors are aggregated across all workers, itsthencallback takes the mean and returns the result.\nthen\nIf user registers this DDP communication hook,\nDDP results is expected to be same as the case where no hook was registered.\nHence, this won\u2019t change behavior of DDP and user can use this as a reference\nor modify this hook to log useful information or any other purposes while\nunaffecting DDP behavior.\n\n```python\n>>> ddp_model.register_comm_hook(process_group, allreduce_hook)\n\n```\n\nFuture[Tensor]\nCompress by castingGradBuckettotorch.float16divided by process group size.\nGradBucket\ntorch.float16\nThis DDP communication hook implements a simple gradient compression\napproach that castsGradBuckettensor to half-precision floating-point format (torch.float16)\nand then divides it by the process group size.\nIt allreduces thosefloat16gradient tensors. Once compressed gradient\ntensors are allreduced, the chained callbackdecompresscasts it back to the input data type (such asfloat32).\nGradBucket\ntorch.float16\nfloat16\ndecompress\nfloat32\n\n```python\n>>> ddp_model.register_comm_hook(process_group, fp16_compress_hook)\n\n```\n\nFuture[Tensor]\nWarning: This API is experimental, and it requires NCCL version later than 2.9.6.\nThis DDP communication hook implements a simple gradient compression\napproach that castsGradBuckettensor to half-precisionBrain floating point format(torch.bfloat16)\nand then divides it by the process group size.\nIt allreduces thosebfloat16gradient tensors. Once compressed gradient\ntensors are allreduced, the chained callbackdecompresscasts it back to the input data type (such asfloat32).\nGradBucket\ntorch.bfloat16\nbfloat16\ndecompress\nfloat32\n\n```python\n>>> ddp_model.register_comm_hook(process_group, bf16_compress_hook)\n\n```\n\nFuture[Tensor]\nAdditionally, a communication hook wrapper is provided to supportfp16_compress_hook()orbf16_compress_hook()as a wrapper,\nwhich can be combined with other communication hooks.\nfp16_compress_hook()\nbf16_compress_hook()\nCast input tensor totorch.float16, cast result of hook back to input dtype.\ntorch.float16\nThis wrapper casts the input gradient tensor of a given DDP communication hook to half-precision\nfloating point format (torch.float16), and casts the resulting tensor of the given hook back to\nthe input data type, such asfloat32.\nTherefore,fp16_compress_hookis equivalent tofp16_compress_wrapper(allreduce_hook).\ntorch.float16\nfloat32\nfp16_compress_hook\nfp16_compress_wrapper(allreduce_hook)\n\n```python\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n>>> ddp_model.register_comm_hook(state, fp16_compress_wrapper(powerSGD_hook))\n\n```\n\nCallable[[Any,GradBucket],Future[Tensor]]\nWarning: This API is experimental, and it requires NCCL version later than 2.9.6.\nThis wrapper casts the input gradient tensor of a given DDP communication hook to half-precisionBrain floating point format(torch.bfloat16),\nand casts the resulting tensor of the given hook back to the input data type, such asfloat32.\ntorch.bfloat16\nfloat32\nTherefore,bf16_compress_hookis equivalent tobf16_compress_wrapper(allreduce_hook).\nbf16_compress_hook\nbf16_compress_wrapper(allreduce_hook)\n\n```python\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n>>> ddp_model.register_comm_hook(state, bf16_compress_wrapper(powerSGD_hook))\n\n```\n\nCallable[[Any,GradBucket],Future[Tensor]]\n\n## PowerSGD Communication Hook#\n\nPowerSGD (Vogels et al., NeurIPS 2019)\nis a gradient compression algorithm, which can provide very high compression\nrates and accelerate bandwidth-bound distributed training.\nThis algorithm needs to maintain both some hyperparameters and the internal\nstate. Therefore, PowerSGD communication hook is astatefulhook,\nand the user needs to provide a state object defined as below.\n\n## PowerSGD State#\n\nStore both the algorithm\u2019s hyperparameters and internal state for all gradients during training.\nParticularly,matrix_approximation_rankandstart_powerSGD_iterare the main hyperparameters that should be tuned by the user.\nFor performance, we suggest to keep binary hyperparametersuse_error_feedbackandwarm_starton.\nmatrix_approximation_rank\nstart_powerSGD_iter\nuse_error_feedback\nwarm_start\nmatrix_approximation_rankcontrols the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression.\nmatrix_approximation_rank\n1.1. Ifmatrix_approximation_rankis too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy.\nmatrix_approximation_rank\n1.2. The increase ofmatrix_approximation_rankcan substantially increase the computation costs of the compression, and the accuracy may not be further improved beyond a certainmatrix_approximation_rankthreshold.\nmatrix_approximation_rank\nmatrix_approximation_rank\nTo tunematrix_approximation_rank, we suggest to start from 1 and increase by factors of 2 (like an exponential grid search, 1, 2, 4, \u2026), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.\nmatrix_approximation_rank\nstart_powerSGD_iterdefers PowerSGD compression until stepstart_powerSGD_iter, and vanilla allreduce runs prior to stepstart_powerSGD_iter. This hybrid scheme ofvanilla allreduce + PowerSGDcan effectively improve the accuracy, even a relatively smallmatrix_approximation_rankis used. This is because that, the beginning of training phase is usually very sensitive to inaccurate gradients, and compressing gradients too early may make the training quickly take a suboptimal trajectory, which can result in an irrecoverable impact on the accuracy.\nstart_powerSGD_iter\nstart_powerSGD_iter\nstart_powerSGD_iter\nmatrix_approximation_rank\nTo tunestart_powerSGD_iter, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached. If there is a warm-up stage in the training,start_powerSGD_itertypically should be no less than the number of warm-up steps.\nstart_powerSGD_iter\nstart_powerSGD_iter\nmin_compression_rateis the minimum compression rate required when a layer is compressed. Due to the computation overheads incurred by the compression, a tensor is worth compressing only if there can be sufficient saving in bandwidth, where(num_rows+num_cols)*matrix_approximation_rank*min_compression_rate<num_rows*num_cols. If the specified compression rate threshold cannot be satisfied, the tensor will be directly allreduced without compression.\nmin_compression_rate\n(num_rows+num_cols)*matrix_approximation_rank*min_compression_rate<num_rows*num_cols\nCompression statistics are logged everycompression_stats_logging_frequencyiterations once PowerSGD compression starts.\ncompression_stats_logging_frequency\northogonalization_epsiloncan be a very small value (e.g., 1e-8) added to every normalized matrix column in orthogonalization step, to prevent div-by-zero error if any column has all 0s. If this can already be prevented (e.g., by batch normalization), an epsilon of 0 is recommended for accuracy.\northogonalization_epsilon\nbatch_tensors_with_same_shapecontrols whether to compress and decompress tensors with same shape in a batched operation to achieve higher parallelism. Note that you should also increase the bucket size (i.e.,bucket_cap_mbarg in DDP constructor) to make more same-shaped tensors appear in the same bucket, however this may reduce the overlap between computation and communication, and increase the memory footprint due to stacking the tensors of the same shape. Set toTrueif the compression / decompression computation is a bottleneck.\nbatch_tensors_with_same_shape\nbucket_cap_mb\nTrue\nWarning\nIf error feedback or warm-up is enabled, the minimum value ofstart_powerSGD_iterallowed in DDP is 2.\nThis is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP,\nand this can conflict with any tensor memorized before the rebuild process.\nstart_powerSGD_iter\n\n## PowerSGD Hooks#\n\nWarning\nPowerSGD typically requires extra memory of the same size as the model\u2019s\ngradients to enable error feedback, which can compensate for biased\ncompressed communication and improve accuracy.\nWarning\nPowerSGD hooks may conflict withApex automatic mixed precision package.\nPlease use PyTorchnative automatic mixed precision packageinstead.\nImplement PowerSGD algorithm.\nThis DDP communication hook implements PowerSGD gradient compression\nalgorithm described in thepaper.\nOnce gradient tensors are aggregated across all workers, this hook applies\ncompression as follows:\nViews the input flattened 1D gradient tensor as a list of per-parameter tensors, and divides all the tensors into two groups:\n1.1 The tensors that should be compressed before allreduce, because the compression can give enough saving in bandwidth.\n1.2 Rest of the tensors will be directly allreduced without compression, including all the vector tensors (for biases).\nHandles uncompressed tensors:\n2.1. Allocate contiguous memory for those uncompressed tensors, and allreduces all the uncompressed tensors as a batch, without compression;\n2.2. Copies the individual uncompressed tensors from the contiguous memory back to the input tensor.\nHandles the tensors that should be compressed by PowerSGD compression:\n3.1. For each tensor M, creates two low-rank tensors P and Q for decomposing M,\nsuch that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;\n3.2. Computes each P in Ps, which is equal to MQ;\n3.3. Allreduces Ps as a batch;\n3.4. Orthogonalizes each P in Ps;\n3.5. Computes each Q in Qs, which is approximately equal to M^TP;\n3.6. Allreduces Qs as a batch;\n3.7. Computes each M among all the compressed tensors, which is approximately equal to PQ^T.\nNote that this communication hook enforces vanilla allreduce for the firststate.start_powerSGD_iteriterations.\nThis not only gives the user more control over the tradeoff between speedup and accuracy,\nbut also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.\nstate.start_powerSGD_iter\nstate(PowerSGDState) \u2013 State information to configure the compression rate and support error feedback, warm start, etc.\nTo tune the compression configs, mainly need to tunematrix_approximation_rank,start_powerSGD_iterandmin_compression_rate.\nmatrix_approximation_rank\nstart_powerSGD_iter\nmin_compression_rate\nbucket(dist.GradBucket) \u2013 Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors.\nNote that since DDP comm hook only supports single process single device mode,\nonly exactly one tensor is stored in this bucket.\nFuture handler of the communication, which updates the gradients in place.\nFuture[Tensor]\n\n```python\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1,\n                          start_powerSGD_iter=10, min_compression_rate=0.5)\n>>> ddp_model.register_comm_hook(state, powerSGD_hook)\n\n```\n\nImplement simplified PowerSGD algorithm.\nThis DDP communication hook implements a simplified PowerSGD gradient compression\nalgorithm described in thepaper.\nThis variant does not compress the gradients layer by layer,\nbut instead compresses the flattened input tensor that batches all the gradients.\nTherefore, it isfasterthanpowerSGD_hook(),\nbut usually results in amuch lower accuracy, unlessmatrix_approximation_rankis 1.\npowerSGD_hook()\nmatrix_approximation_rank\nWarning\nIncreasingmatrix_approximation_rankhere may not necessarily increase the accuracy,\nbecause batching per-parameter tensors without column/row alignment can destroy low-rank structure.\nTherefore, the user should always considerpowerSGD_hook()first,\nand only consider this variant when a satisfactory accuracy can be achieved whenmatrix_approximation_rankis 1.\nmatrix_approximation_rank\npowerSGD_hook()\nmatrix_approximation_rank\nOnce gradient tensors are aggregated across all workers, this hook applies\ncompression as follows:\nViews the input flattened 1D gradient tensor as a square-shaped tensor M with 0 paddings;\nCreates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;\nComputes P, which is equal to MQ;\nAllreduces P;\nOrthogonalizes P;\nComputes Q, which is approximately equal to M^TP;\nAllreduces Q;\nComputes M, which is approximately equal to PQ^T.\nTruncates the input tensor to the original length.\nNote that this communication hook enforces vanilla allreduce for the firststate.start_powerSGD_iteriterations.\nThis not only gives the user more control over the tradeoff between speedup and accuracy,\nbut also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.\nstate.start_powerSGD_iter\nstate(PowerSGDState) \u2013 State information to configure the compression rate and support error feedback, warm start, etc.\nTo tune the compression configs, mainly need to tunematrix_approximation_rankandstart_powerSGD_iter.\nmatrix_approximation_rank\nstart_powerSGD_iter\nbucket(dist.GradBucket) \u2013 Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors.\nNote that since DDP comm hook only supports single process single device mode,\nonly exactly one tensor is stored in this bucket.\nFuture handler of the communication, which updates the gradients in place.\nFuture[Tensor]\n\n```python\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1)\n>>> ddp_model.register_comm_hook(state, batched_powerSGD_hook)\n\n```\n\n\n## Debugging Communication Hooks#\n\nAs the name implies, debugging communication hooks areonlyused for debugging and performance optimization purpose.\nWarning\nDebugging communication hooks do not necessarily output the correct results.\nReturn a future that wraps the input, so it is a no-op that does not incur any communication overheads.\nThis hook shouldonlybe used for headroom analysis of allreduce optimization,\ninstead of the normal gradient synchronization.\nFor example, if only less than 10% speedup of training time can be observed after this hook is registered,\nit usually implies that allreduce is not a performance bottleneck for this case.\nSuch instrumentation can be particularly useful\nif GPU traces cannot be easily retrieved or the trace analysis is complicated\nsome factors such as the overlap between allreduce and computation or the desynchronization across ranks.\n\n```python\n>>> ddp_model.register_comm_hook(None, noop_hook)\n\n```\n\nFuture[Tensor]\n\n## Checkpointing of Communication Hooks#\n\nA stateful communication hook can be saved as a part of model checkpointing to enable trainer restarts.\nTo make a hook serializable,__setstate__and__getstate__should be defined.\n__setstate__\n__getstate__\nWarning\n__getstate__should exclude non-serializable attributes from a returned dictionary.\n__getstate__\nWarning\n__setstate__should properly initialize non-serializable attributes, excluded from a providedstate.\n__setstate__\nstate\nPowerSGDStatehas__setstate__and__getstate__implemented and can be used as a reference.\nPowerSGDState\n__setstate__\n__getstate__\nReturn aDict[str,Any]which will be pickled and saved.\nDict[str,Any]\nprocess_groupis not serializable and excluded from\na returned state.\nprocess_group\nTake a providedstateand set to thisPowerSGDStateinstance.\nstate\nPowerSGDState\nprocess_groupis set to default.\nprocess_group\nHere is a simple, end-to-end example of saving and reloading PowerSGD state and hook.\n\n```python\n\nimport os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook as powerSGD\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(24,24)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(24,12)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(\n        demo_fn,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True)\n\ndef demo_serialization(rank, world_size):\n    setup(rank, world_size)\n\n    CHECKPOINT = tempfile.gettempdir() + \"/checkpoint.pt\"\n\n    model = SimpleModel().to(rank)\n    ddp_model = DistributedDataParallel(model, device_ids=[rank])\n\n    powersgd_hook = powerSGD.powerSGD_hook\n    powersgd_state = powerSGD.PowerSGDState(process_group=None)\n\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n    ddp_model.register_comm_hook(powersgd_state, powersgd_hook)\n\n    state = {\n        'state_dict': ddp_model.state_dict(),\n        'comm_hook': powersgd_hook,\n        'comm_hook_state': powersgd_state}\n\n    if rank == 0:\n        torch.save(state, CHECKPOINT)\n\n    dist.barrier()\n    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n    checkpoint = torch.load(CHECKPOINT, map_location=map_location)\n\n    new_ddp_model = DistributedDataParallel(SimpleModel().to(rank), device_ids=[rank])\n    new_ddp_model.load_state_dict(checkpoint['state_dict'])\n    powersgd_hook = checkpoint['comm_hook']\n    powersgd_state = checkpoint['comm_hook_state']\n\n    new_ddp_model.register_comm_hook(powersgd_state, powersgd_hook)\n\n    if rank == 0:\n        os.remove(CHECKPOINT)\n\n    cleanup()\n\nif __name__ == \"__main__\":\n    n_gpus = torch.cuda.device_count()\n    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n    world_size = n_gpus\n    run_demo(demo_serialization, world_size)\n\n```\n\n\n## Acknowledgements#\n\nMany thanks to PowerSGD paper authorThijs Vogelsfor the code review on\nPowerSGD communication hook, as well as thecomparison experiments,\nwhich show that the performance of PowerSGD communication hook is on par with\nthe implementation in the originalpaper.",
    "url": "https://pytorch.org/docs/stable/ddp_comm_hooks.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ff0fbd48c5565e457c42b88bd9927241",
    "source": "pytorch_docs",
    "title": "torch.Storage \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.Storage#\n\nCreated On: Dec 30, 2016 | Last Updated On: Apr 14, 2025\nIn PyTorch, a regular tensor is a multi-dimensional array that is defined by the following components:\nStorage: The actual data of the tensor, stored as a contiguous, one-dimensional array of bytes.\ndtype: The data type of the elements in the tensor, such as torch.float32 or torch.int64.\ndtype\nshape: A tuple indicating the size of the tensor in each dimension.\nshape\nStride: The step size needed to move from one element to the next in each dimension.\nOffset: The starting point in the storage from which the tensor data begins. This will usually be 0 for newly\ncreated tensors.\nThese components together define the structure and data of a tensor, with the storage holding the\nactual data and the rest serving as metadata.\n\n## Untyped Storage API#\n\nAtorch.UntypedStorageis a contiguous, one-dimensional array of elements. Its length is equal to the number of\nbytes of the tensor. The storage serves as the underlying data container for tensors.\nIn general, a tensor created in PyTorch using regular constructors such aszeros(),zeros_like()ornew_zeros()will produce tensors where there is a one-to-one correspondence between the tensor\nstorage and the tensor itself.\ntorch.UntypedStorage\nzeros()\nzeros_like()\nnew_zeros()\nHowever, a storage is allowed to be shared by multiple tensors.\nFor instance, any view of a tensor (obtained throughview()or some, but not all, kinds of indexing\nlike integers and slices) will point to the same underlying storage as the original tensor.\nWhen serializing and deserializing tensors that share a common storage, the relationship is preserved, and the tensors\ncontinue to point to the same storage. Interestingly, deserializing multiple tensors that point to a single storage\ncan be faster than deserializing multiple independent tensors.\nview()\nA tensor storage can be accessed through theuntyped_storage()method. This will return an object of\ntypetorch.UntypedStorage.\nFortunately, storages have a unique identifier accessed through thetorch.UntypedStorage.data_ptr()method.\nIn regular settings, two tensors with the same data storage will have the same storagedata_ptr.\nHowever, tensors themselves can point to two separate storages, one for its data attribute and another for its grad\nattribute. Each will require adata_ptr()of its own. In general, there is no guarantee that atorch.Tensor.data_ptr()andtorch.UntypedStorage.data_ptr()match and this should not be assumed to be true.\nuntyped_storage()\ntorch.UntypedStorage\ntorch.UntypedStorage.data_ptr()\ndata_ptr\ndata_ptr()\ntorch.Tensor.data_ptr()\ntorch.UntypedStorage.data_ptr()\nUntyped storages are somewhat independent of the tensors that are built on them. Practically, this means that tensors\nwith different dtypes or shape can point to the same storage.\nIt also implies that a tensor storage can be changed, as the following example shows:\n\n```python\n>>> t = torch.ones(3)\n>>> s0 = t.untyped_storage()\n>>> s0\n 0\n 0\n 128\n 63\n 0\n 0\n 128\n 63\n 0\n 0\n 128\n 63\n[torch.storage.UntypedStorage(device=cpu) of size 12]\n>>> s1 = s0.clone()\n>>> s1.fill_(0)\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n[torch.storage.UntypedStorage(device=cpu) of size 12]\n>>> # Fill the tensor with a zeroed storage\n>>> t.set_(s1, storage_offset=t.storage_offset(), stride=t.stride(), size=t.size())\ntensor([0., 0., 0.])\n\n```\n\nWarning\nPlease note that directly modifying a tensor\u2019s storage as shown in this example is not a recommended practice.\nThis low-level manipulation is illustrated solely for educational purposes, to demonstrate the relationship between\ntensors and their underlying storages. In general, it\u2019s more efficient and safer to use standardtorch.Tensormethods, such asclone()andfill_(), to achieve the same results.\ntorch.Tensor\nclone()\nfill_()\nOther thandata_ptr, untyped storage also have other attributes such asfilename(in case the storage points to a file on disk),deviceoris_cudafor device checks. A storage can also be manipulated in-place or\nout-of-place with methods likecopy_,fill_orpin_memory. FOr more information, check the API\nreference below. Keep in mind that modifying storages is a low-level API and comes with risks!\nMost of these APIs also exist on the tensor level: if present, they should be prioritized over their storage\ncounterparts.\ndata_ptr\nfilename\ndevice\nis_cuda\ncopy_\nfill_\npin_memory\n\n## Special cases#\n\nWe mentioned that a tensor that has a non-Nonegradattribute has actually two pieces of data within it.\nIn this case,untyped_storage()will return the storage of thedataattribute,\nwhereas the storage of the gradient can be obtained throughtensor.grad.untyped_storage().\ngrad\nuntyped_storage()\ndata\ntensor.grad.untyped_storage()\n\n```python\n>>> t = torch.zeros(3, requires_grad=True)\n>>> t.sum().backward()\n>>> assert list(t.untyped_storage()) == [0] * 12  # the storage of the tensor is just 0s\n>>> assert list(t.grad.untyped_storage()) != [0] * 12  # the storage of the gradient isn't\n\n```\n\nTensors on\"meta\"device: Tensors on the\"meta\"device are used for shape inference\nand do not hold actual data.\n\"meta\"\n\"meta\"\nFake Tensors: Another internal tool used by PyTorch\u2019s compiler isFakeTensorwhich is based on a similar idea.\nTensor subclasses or tensor-like objects can also display unusual behaviours. In general, we do not\nexpect many use cases to require operating at the Storage level!\nCasts this storage to bfloat16 type.\nCasts this storage to bool type.\nCasts this storage to byte type.\nSwap bytes in underlying data.\nCasts this storage to char type.\nReturn a copy of this storage.\nCasts this storage to complex double type.\nCasts this storage to complex float type.\nReturn a CPU copy of this storage if it\u2019s not already on the CPU.\nReturns a copy of this object in CUDA memory.\nIf this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned.\ndevice(int) \u2013 The destination GPU id. Defaults to the current device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nUnion[_StorageBase,TypedStorage]\nCasts this storage to double type.\nReturns the file name associated with this storage.\nThe file name will be a string if the storage is on CPU and was created viafrom_file()withsharedasTrue. This attribute isNoneotherwise.\nfrom_file()\nshared\nTrue\nNone\nCasts this storage to float type.\nCasts this storage to float8_e4m3fn type\nCasts this storage to float8_e4m3fnuz type\nCasts this storage to float8_e5m2 type\nCasts this storage to float8_e5m2fnuz type\nCreates a CPU storage backed by a memory-mapped file.\nIfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.\nshared\nTrue\nshared\nFalse\nnbytesis the number of bytes of storage. IfsharedisFalse,\nthen the file must contain at leastnbytesbytes. IfsharedisTruethe file will be created if needed. (Note that forUntypedStoragethis argument differs from that ofTypedStorage.from_file)\nnbytes\nshared\nFalse\nnbytes\nshared\nTrue\nUntypedStorage\nTypedStorage.from_file\nfilename(str) \u2013 file name to map\nshared(bool) \u2013 whether to share memory (whetherMAP_SHAREDorMAP_PRIVATEis passed to the\nunderlyingmmap(2) call)\nMAP_SHARED\nMAP_PRIVATE\nnbytes(int) \u2013 number of bytes of storage\nint\nCasts this storage to half type.\nReturns a copy of this object in HPU memory.\nIf this object is already in HPU memory and on the correct device, then\nno copy is performed and the original object is returned.\ndevice(int) \u2013 The destination HPU id. Defaults to the current device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nUnion[_StorageBase,TypedStorage]\nCasts this storage to int type.\nDetermine whether the CPU storage is already pinned on device.\ndevice(strortorch.device) \u2013 The device to pin memory on (default:'cuda').\nThis argument is discouraged and subject to deprecated.\n'cuda'\nA boolean variable.\nCasts this storage to long type.\nReturn a MPS copy of this storage if it\u2019s not already on the MPS.\nCopy the CPU storage to pinned memory, if it\u2019s not already pinned.\ndevice(strortorch.device) \u2013 The device to pin memory on (default:'cuda').\nThis argument is discouraged and subject to deprecated.\n'cuda'\nA pinned CPU storage.\nMoves the storage to shared memory.\nThis is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized.\nNote that to mitigate issues likethisit is thread safe to call this function from multiple threads on the same object.\nIt is NOT thread safe though to call any other function on self without proper\nsynchronization. Please seeMultiprocessing best practicesfor more details.\nNote\nWhen all references to a storage in shared memory are deleted, the associated shared memory\nobject will also be deleted. PyTorch has a special cleanup process to ensure that this happens\neven if the current process exits unexpectedly.\nIt is worth noting the difference betweenshare_memory_()andfrom_file()withshared=True\nshare_memory_()\nfrom_file()\nshared=True\nshare_memory_usesshm_open(3)to create a\nPOSIX shared memory object whilefrom_file()usesopen(2)to open the filename passed by the user.\nshare_memory_\nfrom_file()\nBoth use anmmap(2) callwithMAP_SHAREDto map the file/object into the current virtual address space\nMAP_SHARED\nshare_memory_will callshm_unlink(3)on the object after mapping it to make sure the shared memory\nobject is freed when no process has the object open.torch.from_file(shared=True)does not unlink the\nfile. This file is persistent and will remain until it is deleted by the user.\nshare_memory_\nshm_unlink(3)\ntorch.from_file(shared=True)\nself\nself\nCasts this storage to short type.\nint\nReturn a list containing the elements of this storage.\nUnion[_StorageBase,TypedStorage]\n\n## Legacy Typed Storage#\n\nWarning\nFor historical context, PyTorch previously used typed storage classes, which are\nnow deprecated and should be avoided. The following details this API in case you\nshould encounter it, although its usage is highly discouraged.\nAll storage classes except fortorch.UntypedStoragewill be removed\nin the future, andtorch.UntypedStoragewill be used in all cases.\ntorch.UntypedStorage\ntorch.UntypedStorage\ntorch.Storageis an alias for the storage class that corresponds with\nthe default data type (torch.get_default_dtype()). For example, if the\ndefault data type istorch.float,torch.Storageresolves totorch.FloatStorage.\ntorch.Storage\ntorch.get_default_dtype()\ntorch.float\ntorch.Storage\ntorch.FloatStorage\nThetorch.<type>Storageandtorch.cuda.<type>Storageclasses,\nliketorch.FloatStorage,torch.IntStorage, etc., are not\nactually ever instantiated. Calling their constructors creates\natorch.TypedStoragewith the appropriatetorch.dtypeandtorch.device.torch.<type>Storageclasses have all of the\nsame class methods thattorch.TypedStoragehas.\ntorch.<type>Storage\ntorch.cuda.<type>Storage\ntorch.FloatStorage\ntorch.IntStorage\ntorch.TypedStorage\ntorch.dtype\ntorch.device\ntorch.<type>Storage\ntorch.TypedStorage\nAtorch.TypedStorageis a contiguous, one-dimensional array of\nelements of a particulartorch.dtype. It can be given anytorch.dtype, and the internal data will be interpreted appropriately.torch.TypedStoragecontains atorch.UntypedStoragewhich\nholds the data as an untyped array of bytes.\ntorch.TypedStorage\ntorch.dtype\ntorch.dtype\ntorch.TypedStorage\ntorch.UntypedStorage\nEvery stridedtorch.Tensorcontains atorch.TypedStorage,\nwhich stores all of the data that thetorch.Tensorviews.\ntorch.Tensor\ntorch.TypedStorage\ntorch.Tensor\nCasts this storage to bfloat16 type.\nCasts this storage to bool type.\nCasts this storage to byte type.\nCasts this storage to char type.\nReturn a copy of this storage.\nCasts this storage to complex double type.\nCasts this storage to complex float type.\nReturn a CPU copy of this storage if it\u2019s not already on the CPU.\nReturns a copy of this object in CUDA memory.\nIf this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned.\ndevice(int) \u2013 The destination GPU id. Defaults to the current device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nSelf\nCasts this storage to double type.\nReturns the file name associated with this storage if the storage was memory mapped from a file.\norNoneif the storage was not created by memory mapping a file.\nNone\nCasts this storage to float type.\nCasts this storage to float8_e4m3fn type\nCasts this storage to float8_e4m3fnuz type\nCasts this storage to float8_e5m2 type\nCasts this storage to float8_e5m2fnuz type\nCreates a CPU storage backed by a memory-mapped file.\nIfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.\nshared\nTrue\nshared\nFalse\nsizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize*sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be created if needed.\nsize\nshared\nFalse\nsize*sizeof(Type)\nType\nshared\nTrue\nfilename(str) \u2013 file name to map\nshared(bool) \u2013whether to share memory (whetherMAP_SHAREDorMAP_PRIVATEis passed to the\nunderlyingmmap(2) call)\nwhether to share memory (whetherMAP_SHAREDorMAP_PRIVATEis passed to the\nunderlyingmmap(2) call)\nMAP_SHARED\nMAP_PRIVATE\nsize(int) \u2013 number of elements in the storage\nint\nCasts this storage to half type.\nReturns a copy of this object in HPU memory.\nIf this object is already in HPU memory and on the correct device, then\nno copy is performed and the original object is returned.\ndevice(int) \u2013 The destination HPU id. Defaults to the current device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nSelf\nCasts this storage to int type.\nDetermine whether the CPU TypedStorage is already pinned on device.\ndevice(strortorch.device) \u2013 The device to pin memory on (default:'cuda').\nThis argument is discouraged and subject to deprecated.\n'cuda'\nA boolean variable.\nCasts this storage to long type.\nCopy the CPU TypedStorage to pinned memory, if it\u2019s not already pinned.\ndevice(strortorch.device) \u2013 The device to pin memory on (default:'cuda').\nThis argument is discouraged and subject to deprecated.\n'cuda'\nA pinned CPU storage.\nSeetorch.UntypedStorage.share_memory_()\ntorch.UntypedStorage.share_memory_()\nCasts this storage to short type.\nReturns a copy of this object in device memory.\nIf this object is already on the correct device, then no copy is performed\nand the original object is returned.\ndevice(int) \u2013 The destination device.\nnon_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.\nTrue\nSelf\nReturn a list containing the elements of this storage.\nReturns the type ifdtypeis not provided, else casts this object to\nthe specified type.\nIf this is already of the correct type, no copy is performed and the\noriginal object is returned.\ndtype(typeorstring) \u2013 The desired type\nnon_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.\nTrue\n**kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated.\nasync\nnon_blocking\nasync\nUnion[_StorageBase,TypedStorage,str]\nReturn the internaltorch.UntypedStorage.\ntorch.UntypedStorage",
    "url": "https://pytorch.org/docs/stable/storage.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c9563940f68d4676767f71cb2db8f1c1",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/torch.escape-hatch.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1f0c2a082220d93b4222db6590779088",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/cuda_environment_variables.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "cd296afffe5de7ec59a3a92abadcbfbf",
    "source": "pytorch_docs",
    "title": "Threading Environment Variables \u2014 PyTorch 2.9 documentation",
    "text": "\n## Threading Environment Variables#\n\nCreated On: Jun 10, 2025 | Last Updated On: Jun 10, 2025\nVariable\nDescription\nOMP_NUM_THREADS\nOMP_NUM_THREADS\nSets the maximum number of threads to use for OpenMP parallel regions.\nMKL_NUM_THREADS\nMKL_NUM_THREADS\nSets the maximum number of threads to use for the Intel MKL library. Note that MKL_NUM_THREADS takes precedence overOMP_NUM_THREADS.\nOMP_NUM_THREADS",
    "url": "https://pytorch.org/docs/stable/threading_environment_variables.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "deb948dc92d7b7b15ebeddac8146a2e8",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2a8c12896f4d55378a381190bb043ef8",
    "source": "pytorch_docs",
    "title": "python.builtin \u2014 PyTorch 2.9 documentation",
    "text": "\n## python.builtin#\n\n\n## dynamic_shape_round#\n\nNote\nTags:python.builtin,torch.dynamic-shape\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch._export.db.case import SupportLevel\nfrom torch.export import Dim\n\nclass DynamicShapeRound(torch.nn.Module):\n    \"\"\"\n    Calling round on dynamic shapes is not supported.\n    \"\"\"\n\n    def forward(self, x):\n        return x[: round(x.shape[0] / 2)]\n\nx = torch.randn(3, 2)\ndim0_x = Dim(\"dim0_x\")\nexample_args = (x,)\ntags = {\"torch.dynamic-shape\", \"python.builtin\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\ndynamic_shapes = {\"x\": {0: dim0_x}}\nmodel = DynamicShapeRound()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nUnsupported: Constraints violated (dim0_x)! For more information, run with TORCH_LOGS=\"+dynamic\".\n\n```\n\n\n## tensor_setattr#\n\nNote\nTags:python.builtin\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass TensorSetattr(torch.nn.Module):\n    \"\"\"\n    setattr() call onto tensors is not supported.\n    \"\"\"\n    def forward(self, x, attr):\n        setattr(x, attr, torch.randn(3, 2))\n        return x + 4\n\nexample_args = (torch.randn(3, 2), \"attr\")\ntags = {\"python.builtin\"}\nmodel = TensorSetattr()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", attr):\n                 randn: \"f32[3, 2]\" = torch.ops.aten.randn.default([3, 2], device = device(type='cpu'), pin_memory = False);  randn = None\n\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    attr: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## type_reflection_method#\n\nNote\nTags:python.builtin\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass A:\n    @classmethod\n    def func(cls, x):\n        return 1 + x\n\nclass TypeReflectionMethod(torch.nn.Module):\n    \"\"\"\n    type() calls on custom objects followed by attribute accesses are not allowed\n    due to its overly dynamic nature.\n    \"\"\"\n\n    def forward(self, x):\n        a = A()\n        return type(a).func(x)\n\n\nexample_args = (torch.randn(3, 4),)\ntags = {\"python.builtin\"}\nmodel = TypeReflectionMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 4]\"):\n                 add: \"f32[3, 4]\" = torch.ops.aten.add.Tensor(x, 1);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/python.builtin.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9f7de6819c549d27694e6ddfb1e552bf",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/special.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e1f0afa1c1dc33a13af1b90b678262cf",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/add_image.png",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2135a7d1f705bad69b6099f2fb7722e2",
    "source": "pytorch_docs",
    "title": "Frequently Asked Questions \u2014 PyTorch 2.9 documentation",
    "text": "\n## Frequently Asked Questions#\n\nCreated On: Feb 15, 2018 | Last Updated On: Aug 05, 2021\n\n## My model reports \u201ccuda runtime error(2): out of memory\u201d#\n\nAs the error message suggests, you have run out of memory on your\nGPU.  Since we often deal with large amounts of data in PyTorch,\nsmall mistakes can rapidly cause your program to use up all of your\nGPU; fortunately, the fixes in these cases are often simple.\nHere are a few common things to check:\nDon\u2019t accumulate history across your training loop.By default, computations involving variables that require gradients\nwill keep history.  This means that you should avoid using such\nvariables in computations which will live beyond your training loops,\ne.g., when tracking statistics. Instead, you should detach the variable\nor access its underlying data.\nSometimes, it can be non-obvious when differentiable variables can\noccur.  Consider the following training loop (abridged fromsource):\n\n```python\ntotal_loss = 0\nfor i in range(10000):\n    optimizer.zero_grad()\n    output = model(input)\n    loss = criterion(output)\n    loss.backward()\n    optimizer.step()\n    total_loss += loss\n\n```\n\nHere,total_lossis accumulating history across your training loop, sincelossis a differentiable variable with autograd history. You can fix this by\nwritingtotal_loss += float(loss)instead.\ntotal_loss\nloss\nOther instances of this problem:1.\nDon\u2019t hold onto tensors and variables you don\u2019t need.If you assign a Tensor or Variable to a local, Python will not\ndeallocate until the local goes out of scope.  You can free\nthis reference by usingdelx.  Similarly, if you assign\na Tensor or Variable to a member variable of an object, it will\nnot deallocate until the object goes out of scope.  You will\nget the best memory usage if you don\u2019t hold onto temporaries\nyou don\u2019t need.\ndelx\nThe scopes of locals can be larger than you expect.  For example:\n\n```python\nfor i in range(5):\n    intermediate = f(input[i])\n    result += g(intermediate)\noutput = h(result)\nreturn output\n\n```\n\nHere,intermediateremains live even whilehis executing,\nbecause its scope extrudes past the end of the loop.  To free it\nearlier, you shoulddelintermediatewhen you are done with it.\nintermediate\nh\ndelintermediate\nAvoid running RNNs on sequences that are too large.The amount of memory required to backpropagate through an RNN scales\nlinearly with the length of the RNN input; thus, you will run out of memory\nif you try to feed an RNN a sequence that is too long.\nThe technical term for this phenomenon isbackpropagation through time,\nand there are plenty of references for how to implement truncated\nBPTT, including in theword language modelexample; truncation is handled by therepackagefunction as described inthis forum post.\nrepackage\nDon\u2019t use linear layers that are too large.A linear layernn.Linear(m,n)usesO(nm)O(nm)O(nm)memory: that is to say,\nthe memory requirements of the weights\nscales quadratically with the number of features.  It is very easy\ntoblow through your memorythis way (and remember that you will need at least twice the size of the\nweights, since you also need to store the gradients.)\nnn.Linear(m,n)\nConsider checkpointing.You can trade-off memory for compute by usingcheckpoint.\n\n## My GPU memory isn\u2019t freed properly#\n\nPyTorch uses a caching memory allocator to speed up memory allocations. As a\nresult, the values shown innvidia-smiusually don\u2019t reflect the true\nmemory usage. SeeMemory managementfor more details about GPU\nmemory management.\nnvidia-smi\nIf your GPU memory isn\u2019t freed even after Python quits, it is very likely that\nsome Python subprocesses are still alive. You may find them viaps-elf|greppythonand manually kill them withkill-9[pid].\nps-elf|greppython\nkill-9[pid]\n\n## My out of memory exception handler can\u2019t allocate memory#\n\nYou may have some code that tries to recover from out of memory errors.\n\n```python\ntry:\n    run_model(batch_size)\nexcept RuntimeError: # Out of memory\n    for _ in range(batch_size):\n        run_model(1)\n\n```\n\nBut find that when you do run out of memory, your recovery code can\u2019t allocate\neither. That\u2019s because the python exception object holds a reference to the\nstack frame where the error was raised. Which prevents the original tensor\nobjects from being freed. The solution is to move you OOM recovery code outside\nof theexceptclause.\nexcept\n\n```python\noom = False\ntry:\n    run_model(batch_size)\nexcept RuntimeError: # Out of memory\n    oom = True\n\nif oom:\n    for _ in range(batch_size):\n        run_model(1)\n\n```\n\n\n## My data loader workers return identical random numbers#\n\nYou are likely using other libraries to generate random numbers in the dataset\nand worker subprocesses are started viafork. Seetorch.utils.data.DataLoader\u2019s documentation for how to\nproperly set up random seeds in workers with itsworker_init_fnoption.\nfork\ntorch.utils.data.DataLoader\nworker_init_fn\n\n## My recurrent network doesn\u2019t work with data parallelism#\n\nThere is a subtlety in using thepacksequence->recurrentnetwork->unpacksequencepattern in aModulewithDataParallelordata_parallel(). Input to each theforward()on\neach device will only be part of the entire input. Because the unpack operationtorch.nn.utils.rnn.pad_packed_sequence()by default only pads up to the\nlongest input it sees, i.e., the longest on that particular device, size\nmismatches will happen when results are gathered together. Therefore, you can\ninstead take advantage of thetotal_lengthargument ofpad_packed_sequence()to make sure that theforward()calls return sequences of same length. For example, you can\nwrite:\npacksequence->recurrentnetwork->unpacksequence\nModule\nDataParallel\ndata_parallel()\nforward()\ntorch.nn.utils.rnn.pad_packed_sequence()\ntotal_length\npad_packed_sequence()\nforward()\n\n```python\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass MyModule(nn.Module):\n    # ... __init__, other methods, etc.\n\n    # padded_input is of shape [B x T x *] (batch_first mode) and contains\n    # the sequences sorted by lengths\n    #   B is the batch size\n    #   T is max sequence length\n    def forward(self, padded_input, input_lengths):\n        total_length = padded_input.size(1)  # get the max sequence length\n        packed_input = pack_padded_sequence(padded_input, input_lengths,\n                                            batch_first=True)\n        packed_output, _ = self.my_lstm(packed_input)\n        output, _ = pad_packed_sequence(packed_output, batch_first=True,\n                                        total_length=total_length)\n        return output\n\n\nm = MyModule().cuda()\ndp_m = nn.DataParallel(m)\n\n```\n\nAdditionally, extra care needs to be taken when batch dimension is dim1(i.e.,batch_first=False) with data parallelism. In this case, the first\nargument of pack_padded_sequencepadding_inputwill be of shape[TxBx*]and should be scattered along dim1, but the second argumentinput_lengthswill be of shape[B]and should be scattered along dim0. Extra code to manipulate the tensor shapes will be needed.\n1\nbatch_first=False\npadding_input\n[TxBx*]\n1\ninput_lengths\n[B]\n0",
    "url": "https://pytorch.org/docs/stable/notes/faq.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "617f377e86ebf418191f8546de72a689",
    "source": "pytorch_docs",
    "title": "Expiration Timers \u2014 PyTorch 2.9 documentation",
    "text": "\n## Expiration Timers#\n\nCreated On: May 04, 2021 | Last Updated On: Apr 25, 2024\nExpiration timers are set up on the same process as the agent and\nused from your script to deal with stuck workers. When you go into\na code-block that has the potential to get stuck you can acquire\nan expiration timer, which instructs the timer server to kill the\nprocess if it does not release the timer by the self-imposed expiration\ndeadline.\nUsage:\n\n```python\nimport torchelastic.timer as timer\nimport torchelastic.agent.server as agent\n\ndef main():\n    start_method = \"spawn\"\n    message_queue = mp.get_context(start_method).Queue()\n    server = timer.LocalTimerServer(message, max_interval=0.01)\n    server.start() # non-blocking\n\n    spec = WorkerSpec(\n                fn=trainer_func,\n                args=(message_queue,),\n                ...<OTHER_PARAMS...>)\n    agent = agent.LocalElasticAgent(spec, start_method)\n    agent.run()\n\ndef trainer_func(message_queue):\n    timer.configure(timer.LocalTimerClient(message_queue))\n    with timer.expires(after=60): # 60 second expiry\n        # do some work\n\n```\n\nIn the example above iftrainer_functakes more than 60 seconds to\ncomplete, then the worker process is killed and the agent retries the worker group.\ntrainer_func\n\n## Client Methods#\n\nConfigures a timer client. Must be called before usingexpires.\nexpires\nAcquires a countdown timer that expires inafterseconds from now,\nunless the code-block that it wraps is finished within the timeframe.\nWhen the timer expires, this worker is eligible to be reaped. The\nexact meaning of \u201creaped\u201d depends on the client implementation. In\nmost cases, reaping means to terminate the worker process.\nNote that the worker is NOT guaranteed to be reaped at exactlytime.now()+after, but rather the worker is \u201celigible\u201d for being\nreaped and theTimerServerthat the client talks to will ultimately\nmake the decision when and how to reap the workers with expired timers.\nafter\ntime.now()+after\nTimerServer\nUsage:\n\n```python\ntorch.distributed.elastic.timer.configure(LocalTimerClient())\nwith expires(after=10):\n    torch.distributed.all_reduce(...)\n\n```\n\n\n## Server/Client Implementations#\n\nBelow are the timer server and client pairs that are provided by torchelastic.\nNote\nTimer server and clients always have to be implemented and used\nin pairs since there is a messaging protocol between the server\nand client.\nBelow is a pair of timer server and client that is implemented based on\namultiprocess.Queue.\nmultiprocess.Queue\nServer that works withLocalTimerClient. Clients are expected to be\nsubprocesses to the parent process that is running this server. Each host\nin the job is expected to start its own timer server locally and each\nserver instance manages timers for local workers (running on processes\non the same host).\nLocalTimerClient\nClient side ofLocalTimerServer. This client is meant to be used\non the same host that theLocalTimerServeris running on and uses\npid to uniquely identify a worker. This is particularly useful in situations\nwhere one spawns a subprocess (trainer) per GPU on a host with multiple\nGPU devices.\nLocalTimerServer\nLocalTimerServer\nBelow is another pair of timer server and client that is implemented\nbased on a named pipe.\nServer that works withFileTimerClient. Clients are expected to be\nrunning on the same host as the process that is running this server.\nEach host in the job is expected to start its own timer server locally\nand each server instance manages timers for local workers (running on\nprocesses on the same host).\nFileTimerClient\nfile_path(str) \u2013 str, the path of a FIFO special file to be created.\nmax_interval(float) \u2013 float, max interval in seconds for each watchdog loop.\ndaemon(bool) \u2013 bool, running the watchdog thread in daemon mode or not.\nA daemon thread will not block a process to stop.\nlog_event(Optional[Callable[[str,Optional[FileTimerRequest]],None]]) \u2013 Callable[[Dict[str, str]], None], an optional callback for\nlogging the events in JSON format.\nClient side ofFileTimerServer. This client is meant to be used\non the same host that theFileTimerServeris running on and uses\npid to uniquely identify a worker.\nThis client uses a named_pipe to send timer requests to theFileTimerServer. This client is a producer while theFileTimerServeris a consumer. Multiple clients can work with\nthe sameFileTimerServer.\nFileTimerServer\nFileTimerServer\nFileTimerServer\nFileTimerServer\nFileTimerServer\nfile_path(str) \u2013 str, the path of a FIFO special file.FileTimerServermust have created it by calling os.mkfifo().\nFileTimerServer\nsignal\u2013 signal, the signal to use to kill the process. Using a\nnegative or zero signal will not kill the process.\n\n## Writing a custom timer server/client#\n\nTo write your own timer server and client extend thetorch.distributed.elastic.timer.TimerServerfor the server andtorch.distributed.elastic.timer.TimerClientfor the client. TheTimerRequestobject is used to pass messages between\nthe server and client.\ntorch.distributed.elastic.timer.TimerServer\ntorch.distributed.elastic.timer.TimerClient\nTimerRequest\nData object representing a countdown timer acquisition and release\nthat is used between theTimerClientandTimerServer.\nA negativeexpiration_timeshould be interpreted as a \u201crelease\u201d\nrequest.\nTimerClient\nTimerServer\nexpiration_time\nNote\nthe type ofworker_idis implementation specific.\nIt is whatever the TimerServer and TimerClient implementations\nhave on to uniquely identify a worker.\nworker_id\nEntity that monitors active timers and expires them\nin a timely fashion. This server is responsible for\nreaping workers that have expired timers.\nClears all timers for the givenworker_ids.\nworker_ids\nReturns all expired timers for each worker_id. An expired timer\nis a timer for which the expiration_time is less than or equal to\nthe provided deadline.\ndict[str,list[torch.distributed.elastic.timer.api.TimerRequest]]\nProcesses the incoming timer requests and registers them with the server.\nThe timer request can either be a acquire-timer or release-timer request.\nTimer requests with a negative expiration_time should be interpreted\nas a release-timer request.\nClient library to acquire and release countdown timers by communicating\nwith the TimerServer.\nAcquires a timer for the worker that holds this client object\ngiven the scope_id and expiration_time. Typically registers\nthe timer with the TimerServer.\nReleases the timer for thescope_idon the worker this\nclient represents. After this method is\ncalled, the countdown timer on the scope is no longer in effect.\nscope_id\n\n## Debug info logging#\n",
    "url": "https://pytorch.org/docs/stable/elastic/timer.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "320973fee640375481da64f85445d720",
    "source": "pytorch_docs",
    "title": "torch.func API Reference \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.func API Reference#\n\nCreated On: Jun 11, 2025 | Last Updated On: Jun 11, 2025\n\n## Function Transforms#\n\nvmap\n\nvmap\nvmap is the vectorizing map;vmap(func)returns a new function that mapsfuncover some dimension of the inputs.\nvmap(func)\nfunc\ngrad\n\ngrad\ngradoperator helps computing gradients offuncwith respect to the input(s) specified byargnums.\ngrad\nfunc\nargnums\ngrad_and_value\n\ngrad_and_value\nReturns a function to compute a tuple of the gradient and primal, or forward, computation.\nvjp\n\nvjp\nStanding for the vector-Jacobian product, returns a tuple containing the results offuncapplied toprimalsand a function that, when givencotangents, computes the reverse-mode Jacobian offuncwith respect toprimalstimescotangents.\nfunc\nprimals\ncotangents\nfunc\nprimals\ncotangents\njvp\n\njvp\nStanding for the Jacobian-vector product, returns a tuple containing the output offunc(*primals)and the \"Jacobian offuncevaluated atprimals\" timestangents.\nfunc\nprimals\ntangents\nlinearize\n\nlinearize\nReturns the value offuncatprimalsand linear approximation atprimals.\nfunc\nprimals\nprimals\njacrev\n\njacrev\nComputes the Jacobian offuncwith respect to the arg(s) at indexargnumusing reverse mode autodiff\nfunc\nargnum\njacfwd\n\njacfwd\nComputes the Jacobian offuncwith respect to the arg(s) at indexargnumusing forward-mode autodiff\nfunc\nargnum\nhessian\n\nhessian\nComputes the Hessian offuncwith respect to the arg(s) at indexargnumvia a forward-over-reverse strategy.\nfunc\nargnum\nfunctionalize\n\nfunctionalize\nfunctionalize is a transform that can be used to remove (intermediate) mutations and aliasing from a function, while preserving the function's semantics.\n\n## Utilities for working with torch.nn.Modules#\n\nIn general, you can transform over a function that calls atorch.nn.Module.\nFor example, the following is an example of computing a jacobian of a function\nthat takes three values and returns three values:\ntorch.nn.Module\n\n```python\nmodel = torch.nn.Linear(3, 3)\n\ndef f(x):\n    return model(x)\n\nx = torch.randn(3)\njacobian = jacrev(f)(x)\nassert jacobian.shape == (3, 3)\n\n```\n\nHowever, if you want to do something like compute a jacobian over the parameters of the model, then there needs to be a way to construct a function where the parameters are the inputs to the function. That\u2019s whatfunctional_call()is for: it accepts an nn.Module, the transformedparameters, and the inputs to the Module\u2019s forward pass. It returns the value of running the Module\u2019s forward pass with the replaced parameters.\nfunctional_call()\nparameters\nHere\u2019s how we would compute the Jacobian over the parameters\n\n```python\nmodel = torch.nn.Linear(3, 3)\n\ndef f(params, x):\n    return torch.func.functional_call(model, params, x)\n\nx = torch.randn(3)\njacobian = jacrev(f)(dict(model.named_parameters()), x)\n\n```\n\nfunctional_call\n\nfunctional_call\nPerforms a functional call on the module by replacing the module parameters and buffers with the provided ones.\nstack_module_state\n\nstack_module_state\nPrepares a list of torch.nn.Modules for ensembling withvmap().\nvmap()\nreplace_all_batch_norm_modules_\n\nreplace_all_batch_norm_modules_\nIn place updatesrootby setting therunning_meanandrunning_varto be None and setting track_running_stats to be False for any nn.BatchNorm module inroot\nroot\nrunning_mean\nrunning_var\nroot\nIf you\u2019re looking for information on fixing Batch Norm modules, please follow the\nguidance here\n\n## Debug utilities#\n\ndebug_unwrap\n\ndebug_unwrap\nUnwraps a functorch tensor (e.g.",
    "url": "https://pytorch.org/docs/stable/func.api.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5d3bcb1a99badf6483145db0c4fefdeb",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/subprocess_handler.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e718867b6395eaf39813ea54122393a4",
    "source": "pytorch_docs",
    "title": "torch.utils.bottleneck \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.bottleneck#\n\nCreated On: Mar 23, 2018 | Last Updated On: Sep 28, 2022\ntorch.utils.bottleneckis a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch\u2019s autograd profiler.\nRun it on the command line with\n\n```python\npython -m torch.utils.bottleneck /path/to/source/script.py [args]\n\n```\n\nwhere [args] are any number of arguments toscript.py, or runpython-mtorch.utils.bottleneck-hfor more usage instructions.\npython-mtorch.utils.bottleneck-h\nWarning\nBecause your script will be profiled, please ensure that it exits in a\nfinite amount of time.\nWarning\nDue to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful.\nNote\nTo decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler.\nOf course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result oftorch.autograd.profiler.emit_nvtx()withnvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Similarly,Intel\u00aeVTune\u2122Profilerhelps to analyze performance on Intel platforms further withtorch.autograd.profiler.emit_itt().\ntorch.autograd.profiler.emit_nvtx()\nnvprof\nIntel\u00aeVTune\u2122Profiler\ntorch.autograd.profiler.emit_itt()\nWarning\nIf you are profiling CUDA code, the first profiler thatbottleneckruns\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time.\nbottleneck\nFor more complicated uses of the profilers (like in a multi-GPU case),\nplease seehttps://docs.python.org/3/library/profile.htmlortorch.autograd.profiler.profile()for more information.\ntorch.autograd.profiler.profile()",
    "url": "https://pytorch.org/docs/stable/bottleneck.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "85d3ec765d6a5cb245821829e6d4ed77",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_inductor_profiling.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c0d5954b42051e656d105925ceff9f9a",
    "source": "pytorch_docs",
    "title": "torch.fx \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.fx#\n\nCreated On: Dec 15, 2020 | Last Updated On: Jul 15, 2025\n\n## Overview#\n\nFX is a toolkit for developers to use to transformnn.Moduleinstances. FX consists of three main components: asymbolic tracer,anintermediate representation, andPython code generation. A\ndemonstration of these components in action:\nnn.Module\n\n```python\nimport torch\n\n\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced: torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph():\n    %x : [num_users=1] = placeholder[target=x]\n    %param : [num_users=1] = get_attr[target=param]\n    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n    %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n    %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n    return clamp\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n    param = self.param\n    add = x + param;  x = param = None\n    linear = self.linear(add);  add = None\n    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n    return clamp\n\"\"\"\n\n```\n\nThesymbolic tracerperforms \u201csymbolic execution\u201d of the Python\ncode. It feeds fake values, called Proxies, through the code. Operations\non these Proxies are recorded. More information about symbolic tracing\ncan be found in thesymbolic_trace()andTracerdocumentation.\nsymbolic_trace()\nTracer\nTheintermediate representationis the container for the operations\nthat were recorded during symbolic tracing. It consists of a list of\nNodes that represent function inputs, callsites (to functions, methods,\nortorch.nn.Moduleinstances), and return values. More information\nabout the IR can be found in the documentation forGraph. The\nIR is the format on which transformations are applied.\ntorch.nn.Module\nGraph\nPython code generationis what makes FX a Python-to-Python (or\nModule-to-Module) transformation toolkit. For each Graph IR, we can\ncreate valid Python code matching the Graph\u2019s semantics. This\nfunctionality is wrapped up inGraphModule, which is atorch.nn.Moduleinstance that holds aGraphas well as aforwardmethod generated from the Graph.\nGraphModule\ntorch.nn.Module\nGraph\nforward\nTaken together, this pipeline of components (symbolic tracing ->\nintermediate representation -> transforms -> Python code generation)\nconstitutes the Python-to-Python transformation pipeline of FX. In\naddition, these components can be used separately. For example,\nsymbolic tracing can be used in isolation to capture a form of\nthe code for analysis (and not transformation) purposes. Code\ngeneration can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX!\nSeveral example transformations can be found at theexamplesrepository.\n\n## Writing Transformations#\n\nWhat is an FX transform? Essentially, it\u2019s a function that looks like this.\n\n```python\n\nimport torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n                tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)\n\n```\n\nYour transform will take in atorch.nn.Module, acquire aGraphfrom it, do some modifications, and return a newtorch.nn.Module. You should think of thetorch.nn.Modulethat your FX\ntransform returns as identical to a regulartorch.nn.Module\u2013 you can pass it to another\nFX transform, or you can run it. Ensuring that the inputs and outputs of your FX transform are atorch.nn.Modulewill allow for composability.\ntorch.nn.Module\nGraph\ntorch.nn.Module\ntorch.nn.Module\ntorch.nn.Module\ntorch.nn.Module\nNote\nIt is also possible to modify an existingGraphModuleinstead of\ncreating a new one, like so:\nGraphModule\n\n```python\nimport torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm\n\n```\n\nNote that you MUST callGraphModule.recompile()to bring the generatedforward()method on theGraphModulein sync with the modifiedGraph.\nGraphModule.recompile()\nforward()\nGraphModule\nGraph\nGiven that you\u2019ve passed in atorch.nn.Modulethat has been traced into aGraph, there are now two primary approaches you can take to building a newGraph.\ntorch.nn.Module\nGraph\nGraph\n\n## A Quick Primer on Graphs#\n\nFull treatment of the semantics of graphs can be found in theGraphdocumentation, but we are going to cover the basics here. AGraphis\na data structure that represents a method on aGraphModule. The\ninformation that this requires is:\nGraph\nGraph\nGraphModule\nWhat are the inputs to the method?\nWhat are the operations that run inside the method?\nWhat is the output (i.e. return) value from the method?\nAll three of these concepts are represented withNodeinstances.\nLet\u2019s see what we mean by that with a short example:\nNode\n\n```python\n\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()\n\n```\n\nHere we define a moduleMyModulefor demonstration purposes, instantiate it,\nsymbolically trace it, then call theGraph.print_tabular()method to print\nout a table showing the nodes of thisGraph:\nMyModule\nGraph.print_tabular()\nGraph\nopcode\nname\ntarget\nargs\nkwargs\nplaceholder\nx\nx\n()\n{}\nget_attr\nlinear_weight\nlinear.weight\n()\n{}\ncall_function\nadd_1\n\n(x, linear_weight)\n{}\ncall_module\nlinear_1\nlinear\n(add_1,)\n{}\ncall_method\nrelu_1\nrelu\n(linear_1,)\n{}\ncall_function\nsum_1\n<built-in method sum \u2026>\n(relu_1,)\n{\u2018dim\u2019: -1}\ncall_function\ntopk_1\n<built-in method topk \u2026>\n(sum_1, 3)\n{}\noutput\noutput\noutput\n(topk_1,)\n{}\nWe can use this information to answer the questions we posed above.\nWhat are the inputs to the method? In FX, method inputs are specified\nvia specialplaceholdernodes. In this case, we have a singleplaceholdernode with atargetofx, meaning we have\na single (non-self) argument named x.\nplaceholder\nplaceholder\ntarget\nx\nWhat are the operations within the method? Theget_attr,call_function,call_module, andcall_methodnodes\nrepresent the operations in the method. A full treatment of\nthe semantics of all of these can be found in theNodedocumentation.\nget_attr\ncall_function\ncall_module\ncall_method\nNode\nWhat is the return value of the method? The return value in aGraphis specified by a specialoutputnode.\nGraph\noutput\nGiven that we now know the basics of how code is represented in\nFX, we can now explore how we would edit aGraph.\nGraph\n\n## Graph Manipulation#\n\nOne approach to building this newGraphis to directly manipulate your old\none. To aid in this, we can simply take theGraphwe obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replacetorch.add()calls withtorch.mul()calls.\nGraph\nGraph\ntorch.add()\ntorch.mul()\n\n```python\n\nimport torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n                tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                    # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)\n\n```\n\nWe can also do more involvedGraphrewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in theGraphdocumentation. An\nexample of using these APIs to append atorch.relu()call\ncan be found below.\nGraph\nGraph\ntorch.relu()\n\n```python\n\n# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)\n\n```\n\nFor simple transformations that only consist of substitutions, you can also\nmake use of thesubgraph rewriter.\nFX also provides another level of automation on top of direct graph manipulation.\nThereplace_pattern()API is essentially a \u201cfind/replace\u201d tool for editingGraph\\s. It allows you to specify apatternandreplacementfunction\nand it will trace through those functions, find instances of the group of operations\nin thepatterngraph, and replace those instances with copies of thereplacementgraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex.\nreplace_pattern()\nGraph\npattern\nreplacement\npattern\nreplacement\nReplace one\nop\nConv/Batch Norm\nfusion\nreplace_pattern: Basic usage\nQuantization\nInvert Transformation\n\n## Proxy/Retracing#\n\nAnother way of manipulatingGraph\\s is by reusing theProxymachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform everyF.relu(x)call into(x>0)*x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after theF.relu, and then clean up the originalF.relu. However, we can automate this process by usingProxyobjects to automatically record operations into theGraph.\nGraph\nProxy\nF.relu(x)\n(x>0)*x\nF.relu\nF.relu\nProxy\nGraph\nTo use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code withProxyobjects as arguments.\nTheseProxyobjects will capture the operations that are performed\non them and append them to theGraph.\nProxy\nProxy\nGraph\n\n```python\n\n# Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n                tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    tracer = torch.fx.proxy.GraphAppendingTracer(new_graph)\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)\n\n```\n\nIn addition to avoiding explicit graph manipulation, usingProxy\\s\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. Note that while callingProxywe also\npassed a tracer pointing to the underlying variablegraph. This is done so\nif in case the operations in graph are n-ary (e.g. add is a binary operator)\nthe call toProxydoes not create multiple instances of a graph\ntracer which can lead to unexpected runtime errors. We recommend this method\nof usingProxyespecially when the underlying operators can not be\nsafely assumed to be unary.\nProxy\nProxy\ngraph\nProxy\nProxy\nA worked example of usingProxy\\s forGraphmanipulation\ncan be foundhere.\nProxy\nGraph\n\n## The Interpreter Pattern#\n\nA useful code organizational pattern in FX is to loop over all theNode\\s\nin aGraphand execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing withProxy\\s. For example, suppose we want to run aGraphModuleand record thetorch.Tensorshape and dtype\nproperties on the nodes as we see them at runtime. That might look like:\nNode\nGraph\nProxy\nGraphModule\ntorch.Tensor\n\n```python\n\nimport torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistent target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)\n\n```\n\nAs you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\ntheInterpreterclass, which encompasses the above logic\nin a way that certain aspects of the interpreter\u2019s execution can\nbe overridden via method overrides.\nInterpreter\nIn addition to executing operations, we can also generate a newGraphby feedingProxyvalues through an interpreter.\nSimilarly, we provide theTransformerclass to encompass\nthis pattern.Transformerbehaves similarly toInterpreter, but instead of calling therunmethod to\nget a concrete output value from the Module, you would call theTransformer.transform()method to return a newGraphModulewhich was subject to any transformation rules\nyou installed as overridden methods.\nGraph\nProxy\nTransformer\nTransformer\nInterpreter\nrun\nTransformer.transform()\nGraphModule\nShapePropagation\nPerformance Profiler\n\n## Debugging#\n\n\n## Introduction#\n\nOften in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code.\nIf you\u2019re not familiar with debuggers, please see the auxiliary sectionAvailable Debuggers.\n\n## Common Pitfalls in Transform Authoring#\n\nNondeterministicsetiteration order. In Python, thesetdatatype is\nunordered. Usingsetto contain collections of objects likeNode\\ s,\nfor example, can cause unexpected nondeterminism. An example is iterating\nover a set ofNodes to insert them into aGraph. Because thesetdata type is unordered, the ordering of the operations in the output\nprogram will be nondeterministic and can change across program invocations.\nThe recommended alternative is to use adictdata type, which isinsertion orderedas of Python 3.7 (and as of cPython 3.6). Adictcan be used equivalently\nto a set by storing values to be deduplicated in the keys of thedict.\nset\nset\nset\nNode\nNode\nGraph\nset\ndict\ndict\ndict\n\n## Checking Correctness of Modules#\n\nBecause the output of most deep learning modules consists of floating\npointtorch.Tensorinstances, checking for equivalence between\nthe results of twotorch.nn.Moduleis not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample:\ntorch.Tensor\ntorch.nn.Module\n\n```python\n\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"\n\n```\n\nHere, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the==equality operator. However, this is not well-defined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (seeherefor more\ndetails). We can usetorch.allclose()instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold:\n==\ntorch.allclose()\n\n```python\nassert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n\n```\n\nThis is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.\n\n## Debugging the Generated Code#\n\nBecause FX generates theforward()function onGraphModule\\s, using\ntraditional debugging techniques likeprintstatements orpdbis\nnot as straightforward. Luckily, we have several techniques we can use\nfor debugging the generated code.\nforward()\nGraphModule\nprint\npdb\npdb\nInvokepdbto step into the running program. Although the code that\nrepresents theGraphis not in any source file, we can still step\ninto it manually usingpdbwhen the forward pass is invoked.\npdb\nGraph\npdb\n\n```python\n\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)\n\n```\n\nIf you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code withpdb. In that case, one\napproach is to simply copy-paste the generatedforwardpass into\nyour code and examine it from there.\npdb\nforward\n\n```python\n\n# Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()\n\n```\n\nto_folder\nGraphModule\nGraphModule.to_folder()is a method inGraphModulethat allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as inPrint the Generated Code,\nit may be easier to examine modules and parameters usingto_folder.\nGraphModule.to_folder()\nGraphModule\nto_folder\n\n```python\n\nm = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()\n\n```\n\nAfter running the above example, we can then look at the code withinfoo/module.pyand modify it as desired (e.g. addingprintstatements or usingpdb) to debug the generated code.\nfoo/module.py\nprint\npdb\n\n## Debugging the Transformation#\n\nNow that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\ntheLimitations of Symbolic Tracingsection in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during ourGraphModuletransformation. There may be a quick answer inWriting Transformations, but, if not, there are several ways to\nexamine our traced module:\nGraphModule\n\n```python\n\n# Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add = x + y;  x = y = None\n    return add\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph():\n    %x : [num_users=1] = placeholder[target=x]\n    %y : [num_users=1] = placeholder[target=y]\n    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})\n    return add\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args    kwargs\n-------------  ------  -----------------------  ------  --------\nplaceholder    x       x                        ()      {}\nplaceholder    y       y                        ()      {}\ncall_function  add     <built-in function add>  (x, y)  {}\noutput         output  output                   (add,)  {}\n\"\"\"\n\n```\n\nUsing the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger likepdbcan be a good\nnext step.\npdb\nGoing off of the example above, consider the following code:\n\n```python\n\n# Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)\n\n```\n\nUsing the above example, let\u2019s say that the call toprint(traced)showed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start apdbsession. We can see\nwhat\u2019s happening during the transform by breaking ontransform_graph(traced), then pressingsto \u201cstep into\u201d the call\ntotransform_graph(traced).\nprint(traced)\npdb\ntransform_graph(traced)\ns\ntransform_graph(traced)\nWe may also have good luck by editing theprint_tabularmethod to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node\u2019sinput_nodesandusers.)\nprint_tabular\ninput_nodes\nusers\n\n## Available Debuggers#\n\nThe most common Python debugger ispdb. You can start\nyour program in \u201cdebug mode\u201d withpdbby typingpython-mpdbFILENAME.pyinto the command line, whereFILENAMEis the name of the file you want to debug. After that, you can use thepdbdebugger commandsto move through your running program stepwise. It\u2019s common to set a\nbreakpoint (bLINE-NUMBER) when you startpdb, then callcto\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (usingsorn) to get to the part\nof the code you want to examine. Alternatively, you can writeimportpdb;pdb.set_trace()before the line you want to break at.\nIf you addpdb.set_trace(), your program will automatically start\nin debug mode when you run it. (In other words, you can just typepythonFILENAME.pyinto the command line instead ofpython-mpdbFILENAME.py.) Once you\u2019re running your file in\ndebug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent\ntutorials onpdbonline, including RealPython\u2019s\u201cPython Debugging With Pdb\u201d.\npdb\npython-mpdbFILENAME.py\nFILENAME\npdb\nbLINE-NUMBER\npdb\nc\ns\nn\nimportpdb;pdb.set_trace()\npdb.set_trace()\npythonFILENAME.py\npython-mpdbFILENAME.py\npdb\nIDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) usepdbby pulling up a terminal\nwindow in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper aroundpdb).\npdb\npdb\n\n## Limitations of Symbolic Tracing#\n\nFX uses a system ofsymbolic tracing(a.k.asymbolic\nexecution)\nto capture the semantics of programs in a transformable/analyzable form.\nThe system istracingin that it executes the program (really atorch.nn.Moduleor function) to record operations. It issymbolicin that the data flowing through the program during this\nexecution is not real data, but rather symbols (Proxyin FX parlance).\ntorch.nn.Module\nProxy\nAlthough symbolic tracing works for most neural net code, it has some\nlimitations.\n\n## Dynamic Control Flow#\n\nThe main limitation of symbolic tracing is it does not currently supportdynamic control flow. That is, loops orifstatements where the\ncondition may depend on the input values of the program.\nif\nFor example, let\u2019s examine the following program:\n\n```python\n\ndef func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n    <...>\n    File \"dyn.py\", line 6, in func_to_trace\n    if x.sum() > 0:\n    File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n    File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"\n\n```\n\nThe condition to theifstatement relies on the value ofx.sum(),\nwhich relies on the value ofx, a function input. Sincexcan change (i.e. if you pass a new input tensor to the traced\nfunction), this isdynamic control flow. The traceback walks back up\nthrough your code to show you where this situation happens.\nif\nx.sum()\nx\nx\n\n## Static Control Flow#\n\nOn the other hand, so-calledstatic control flowis supported. Static\ncontrol flow is loops orifstatements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:\nif\n\n```python\n\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"\n\n```\n\nThe if-statementifself.do_activationdoes not depend on any\nfunction inputs, thus it is static.do_activationcan be considered\nto be a hyper-parameter, and the traces of different instances ofMyModulewith different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.\nifself.do_activation\ndo_activation\nMyModule\nMany instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues toModuleattributes or by binding concrete values to arguments\nduring symbolic tracing:\nModule\n\n```python\n\ndef f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\nfx.symbolic_trace(f, concrete_args={'flag': True})\n\n```\n\nIn the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (seeCustomizing Tracing with the Tracer class) or function (seewrap()) rather than tracing through them.\nwrap()\n\n## Non-torchFunctions#\n\ntorch\nFX uses__torch_function__as the mechanism by which it intercepts\ncalls (see thetechnical\noverviewfor more information about this). Some functions, such as builtin Python\nfunctions or those in themathmodule, are not covered by__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example:\n__torch_function__\nmath\n__torch_function__\n\n```python\n\nimport torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n    <...>\n    File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n    File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"\n\n```\n\nThe error tells us that the built-in functionlenis not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using thewrap()API:\nlen\nwrap()\n\n```python\n\ntorch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"\n\n```\n\n\n## Customizing Tracing with theTracerclass#\n\nTracer\nTheTracerclass is the class that underlies the\nimplementation ofsymbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:\nTracer\nsymbolic_trace\n\n```python\n\nclass MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)\n\n```\n\n\n## Leaf Modules#\n\nLeaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standardtorch.nnmodule instances. For example:\ntorch.nn\n\n```python\n\nclass MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"\n\n```\n\nThe set of leaf modules can be customized by overridingTracer.is_leaf_module().\nTracer.is_leaf_module()\n\n## Miscellanea#\n\nTensor constructors (e.g.torch.zeros,torch.ones,torch.rand,torch.randn,torch.sparse_coo_tensor)\nare currently not traceable.\ntorch.zeros\ntorch.ones\ntorch.rand\ntorch.randn\ntorch.sparse_coo_tensor\nThe deterministic constructors (zeros,ones) can be used\nand the value they produce will be embedded in the trace as a\nconstant. This is only problematic if the arguments to these\nconstructors refers to dynamic input sizes. In this case,ones_likeorzeros_likemay be a viable substitute.\nzeros\nones\nones_like\nzeros_like\nNondeterministic constructors (rand,randn) will have a\nsingle random value embedded in the trace. This is likely not the\nintended behavior. One workaround is to wraptorch.randnin atorch.fx.wrapfunction and call that instead.\nrand\nrandn\ntorch.randn\ntorch.fx.wrap\n\n```python\n\n@torch.fx.wrap\ndef torch_randn(x, shape):\n    return torch.randn(shape)\n\ndef f(x):\n    return x + torch_randn(x, 5)\nfx.symbolic_trace(f)\n\n```\n\nThis behavior may be fixed in a future release.\nType annotations\nPython 3-style type annotations (e.g.func(x:torch.Tensor,y:int)->torch.Tensor) are supported\nand will be preserved by symbolic tracing.\nfunc(x:torch.Tensor,y:int)->torch.Tensor\nPython 2-style comment type annotations#type:(torch.Tensor,int)->torch.Tensorare not currently\nsupported.\n#type:(torch.Tensor,int)->torch.Tensor\nAnnotations on local names within a function are not currently\nsupported.\nGotcha aroundtrainingflag and submodules\ntraining\nWhen using functionals liketorch.nn.functional.dropout, it will be common for the training argument to be passed in asself.training. During FX tracing, this will likely be baked in as a constant value.\ntorch.nn.functional.dropout\nself.training\n\n```python\n\nimport torch\nimport torch.fx\n\nclass DropoutRepro(torch.nn.Module):\n    def forward(self, x):\n    return torch.nn.functional.dropout(x, training=self.training)\n\n\ntraced = torch.fx.symbolic_trace(DropoutRepro())\nprint(traced.code)\n\"\"\"\ndef forward(self, x):\n    dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False);  x = None\n    return dropout\n\"\"\"\n\ntraced.eval()\n\nx = torch.randn(5, 3)\ntorch.testing.assert_close(traced(x), x)\n\"\"\"\nAssertionError: Tensor-likes are not close!\n\nMismatched elements: 15 / 15 (100.0%)\nGreatest absolute difference: 1.6207983493804932 at index (0, 2) (up to 1e-05 allowed)\nGreatest relative difference: 1.0 at index (0, 0) (up to 0.0001 allowed)\n\"\"\"\n\n```\n\nHowever, when the standardnn.Dropout()submodule is used, the training flag is encapsulated and\u2013because of the preservation of thenn.Moduleobject model\u2013can be changed.\nnn.Dropout()\nnn.Module\n\n```python\n\nclass DropoutRepro2(torch.nn.Module):\n    def __init__(self):\n    super().__init__()\n    self.drop = torch.nn.Dropout()\n\n    def forward(self, x):\n    return self.drop(x)\n\ntraced = torch.fx.symbolic_trace(DropoutRepro2())\nprint(traced.code)\n\"\"\"\ndef forward(self, x):\n    drop = self.drop(x);  x = None\n    return drop\n\"\"\"\n\ntraced.eval()\n\nx = torch.randn(5, 3)\ntorch.testing.assert_close(traced(x), x)\n\n```\n\nBecause of this difference, consider marking modules that interact with thetrainingflag dynamically as leaf modules.\ntraining\n\n## API Reference#\n\nSymbolic tracing API\nGiven annn.Moduleor function instanceroot, this function will return aGraphModuleconstructed by recording operations seen while tracing throughroot.\nnn.Module\nroot\nGraphModule\nroot\nconcrete_argsallows you to partially specialize your function, whether it\u2019s to remove control flow or data structures.\nconcrete_args\nFor example:\n\n```python\ndef f(a, b):\n    if b == True:\n        return a\n    else:\n        return a * 2\n\n```\n\nFX can typically not trace through this due to the presence of control\nflow. However, we can useconcrete_argsto specialize on the value ofbto trace through this:\n\n```python\nf = fx.symbolic_trace(f, concrete_args={\"b\": False})\nassert f(3, False) == 6\n\n```\n\nNote that although you can still pass in different values ofb, they will be ignored.\nWe can also useconcrete_argsto eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass infx.PHfor values that shouldn\u2019t be\nspecialized. For example:\n\n```python\ndef f(x):\n    out = 0\n    for v in x.values():\n        out += v\n    return out\n\n\nf = fx.symbolic_trace(\n    f, concrete_args={\"x\": {\"a\": fx.PH, \"b\": fx.PH, \"c\": fx.PH}}\n)\nassert f({\"a\": 1, \"b\": 2, \"c\": 4}) == 7\n\n```\n\nroot(Union[torch.nn.Module,Callable]) \u2013 Module or function to be traced and converted\ninto a Graph representation.\nconcrete_args(Optional[Dict[str,any]]) \u2013 Inputs to be partially specialized\na Module created from the recorded operations fromroot.\nroot\nGraphModule\nNote\nBackwards-compatibility for this API is guaranteed.\nThis function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through:\n\n```python\n# foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\n\ntorch.fx.wrap(\"my_custom_function\")\n\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)\n\n```\n\nThis function can also equivalently be used as a decorator:\n\n```python\n# foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y\n\n```\n\nA wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of\n\u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace\nrather than traced through.\nfn_or_name(Union[str,Callable]) \u2013 The function or name of the global function to insert into the\ngraph when it\u2019s called\nNote\nBackwards-compatibility for this API is guaranteed.\nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has agraphattribute, as well ascodeandforwardattributes generated\nfrom thatgraph.\ngraph\ncode\nforward\ngraph\nWarning\nWhengraphis reassigned,codeandforwardwill be automatically\nregenerated. However, if you edit the contents of thegraphwithout reassigning\nthegraphattribute itself, you must callrecompile()to update the generated\ncode.\ngraph\ncode\nforward\ngraph\ngraph\nrecompile()\nNote\nBackwards-compatibility for this API is guaranteed.\nConstruct a GraphModule.\nroot(Union[torch.nn.Module,Dict[str,Any]) \u2013rootcan either be an nn.Module instance or a Dict mapping strings to any attribute type.\nIn the case thatrootis a Module, any references to Module-based objects (via qualified\nname) in the Graph\u2019s Nodes\u2019targetfield will be copied over from the respective place\nwithinroot\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy.\nIn the case thatrootis a dict, the qualified name found in a Node\u2019stargetwill be\nlooked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied\nover into the appropriate place within the GraphModule\u2019s module hierarchy.\nroot\nroot\ntarget\nroot\nroot\ntarget\ngraph(Graph) \u2013graphcontains the nodes this GraphModule should use for code generation\ngraph\nclass_name(str) \u2013namedenotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all\nerror messages will report as originating fromGraphModule. It may be helpful to set this\ntoroot\u2019s original name or a name that makes sense within the context of your transform.\nname\nGraphModule\nroot\nNote\nBackwards-compatibility for this API is guaranteed.\nAdds the given submodule toself.\nself\nThis installs empty Modules where none exist yet if they are\nsubpaths oftarget.\ntarget\ntarget(str) \u2013 The fully-qualified string name of the new submodule\n(See example innn.Module.get_submodulefor how to\nspecify a fully-qualified string.)\nnn.Module.get_submodule\nm(Module) \u2013 The submodule itself; the actual object we want to\ninstall in the current Module\nWhether or not the submodule could be inserted. Forthis method to return True, each object in the chain\ndenoted bytargetmust either a) not exist yet,\nor b) reference annn.Module(not a parameter or\nother attribute)\nthis method to return True, each object in the chain\ndenoted bytargetmust either a) not exist yet,\nor b) reference annn.Module(not a parameter or\nother attribute)\ntarget\nnn.Module\nbool\nNote\nBackwards-compatibility for this API is guaranteed.\nReturn the Python code generated from theGraphunderlying thisGraphModule.\nGraph\nGraphModule\nDeletes all unused submodules fromself.\nself\nA Module is considered \u201cused\u201d if any one of the following is\ntrue:\n1. It has children that are used\n2. Its forward is called directly via acall_modulenode\n3. It has a non-Module attribute that is used from aget_attrnode\ncall_module\nget_attr\nThis method can be called to clean up annn.Modulewithout\nmanually callingdelete_submoduleon each unused submodule.\nnn.Module\ndelete_submodule\nNote\nBackwards-compatibility for this API is guaranteed.\nDeletes the given submodule fromself.\nself\nThe module will not be deleted iftargetis not a valid\ntarget.\ntarget\ntarget(str) \u2013 The fully-qualified string name of the new submodule\n(See example innn.Module.get_submodulefor how to\nspecify a fully-qualified string.)\nnn.Module.get_submodule\nWhether or not the target string referenced asubmodule we want to delete. A return value ofFalsemeans that thetargetwas not a valid reference to\na submodule.\nsubmodule we want to delete. A return value ofFalsemeans that thetargetwas not a valid reference to\na submodule.\nFalse\ntarget\nbool\nNote\nBackwards-compatibility for this API is guaranteed.\nReturn theGraphunderlying thisGraphModule\nGraph\nGraphModule\nReturn the Python code generated for current GraphModule and its children GraphModules\nWarning\nThis API is experimental and isNOTbackward-compatible.\nRecompile this GraphModule from itsgraphattribute. This should be\ncalled after editing the containedgraph, otherwise the generated\ncode of thisGraphModulewill be out of date.\ngraph\ngraph\nGraphModule\nNote\nBackwards-compatibility for this API is guaranteed.\nPythonCode\nfolder\nmodule_name\nimported withfrom<folder>import<module_name>\nfrom<folder>import<module_name>\nArgs:\nfolder (Union[str, os.PathLike]): The folder to write the code out to\nModule\nwriting out the code\nWarning\nThis API is experimental and isNOTbackward-compatible.\nGraphis the main data structure used in the FX Intermediate Representation.\nIt consists of a series ofNodes, each representing callsites (or other\nsyntactic constructs). The list ofNodes, taken together, constitute a\nvalid Python function.\nGraph\nNode\nNode\nFor example, the following code\n\n```python\nimport torch\nimport torch.fx\n\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(\n            torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3\n        )\n\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\n```\n\nWill produce the following Graph:\n\n```python\nprint(gm.graph)\n\n```\n\n\n```python\ngraph(x):\n    %linear_weight : [num_users=1] = self.linear.weight\n    %add_1 : [num_users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [num_users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [num_users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [num_users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [num_users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1\n\n```\n\nFor the semantics of operations represented in theGraph, please seeNode.\nGraph\nNode\nNote\nBackwards-compatibility for this API is guaranteed.\nConstruct an empty Graph.\nNote\nBackwards-compatibility for this API is guaranteed.\nInsert acall_functionNodeinto theGraph. Acall_functionnode\nrepresents a call to a Python callable, specified bythe_function.\ncall_function\nNode\nGraph\ncall_function\nthe_function\nthe_function(Callable[...,Any]) \u2013 The function to be called. Can be any PyTorch\noperator, Python function, or member of thebuiltinsoroperatornamespaces.\nbuiltins\noperator\nargs(Optional[Tuple[Argument,...]]) \u2013 The positional arguments to be passed\nto the called function.\nkwargs(Optional[Dict[str,Argument]]) \u2013 The keyword arguments to be passed\nto the called function\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nname(Optional[str]) \u2013 The name of the node. If not specified, set to None\nThe newly created and insertedcall_functionnode.\ncall_function\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node().\nGraph.create_node()\nNote\nBackwards-compatibility for this API is guaranteed.\nInsert acall_methodNodeinto theGraph. Acall_methodnode\nrepresents a call to a given method on the 0th element ofargs.\ncall_method\nNode\nGraph\ncall_method\nargs\nmethod_name(str) \u2013 The name of the method to apply to the self argument.\nFor example, if args[0] is aNoderepresenting aTensor,\nthen to callrelu()on thatTensor, passrelutomethod_name.\nNode\nTensor\nrelu()\nTensor\nrelu\nmethod_name\nargs(Optional[Tuple[Argument,...]]) \u2013 The positional arguments to be passed\nto the called method. Note that thisshouldinclude aselfargument.\nself\nkwargs(Optional[Dict[str,Argument]]) \u2013 The keyword arguments to be passed\nto the called method\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nThe newly created and insertedcall_methodnode.\ncall_method\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node().\nGraph.create_node()\nNote\nBackwards-compatibility for this API is guaranteed.\nInsert acall_moduleNodeinto theGraph. Acall_modulenode\nrepresents a call to the forward() function of aModulein theModulehierarchy.\ncall_module\nNode\nGraph\ncall_module\nModule\nModule\nmodule_name(str) \u2013 The qualified name of theModulein theModulehierarchy to be called. For example, if the tracedModulehas a\nsubmodule namedfoo, which has a submodule namedbar, the\nqualified namefoo.barshould be passed asmodule_nameto\ncall that module.\nModule\nModule\nModule\nfoo\nbar\nfoo.bar\nmodule_name\nargs(Optional[Tuple[Argument,...]]) \u2013 The positional arguments to be passed\nto the called method. Note that this shouldnotinclude aselfargument.\nself\nkwargs(Optional[Dict[str,Argument]]) \u2013 The keyword arguments to be passed\nto the called method\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nThe newly-created and insertedcall_modulenode.\ncall_module\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node().\nGraph.create_node()\nNote\nBackwards-compatibility for this API is guaranteed.\nCreate aNodeand add it to theGraphat the current insert-point.\nNote that the current insert-point can be set viaGraph.inserting_before()andGraph.inserting_after().\nNode\nGraph\nGraph.inserting_before()\nGraph.inserting_after()\nop(str) \u2013 the opcode for this Node. One of \u2018call_function\u2019, \u2018call_method\u2019, \u2018get_attr\u2019,\n\u2018call_module\u2019, \u2018placeholder\u2019, or \u2018output\u2019. The semantics of these opcodes are\ndescribed in theGraphdocstring.\nGraph\nargs(Optional[Tuple[Argument,...]]) \u2013 is a tuple of arguments to this node.\nkwargs(Optional[Dict[str,Argument]]) \u2013 the kwargs of this Node\nname(Optional[str]) \u2013 an optional string name for theNode.\nThis will influence the name of the value assigned to in the\nPython generated code.\nNode\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nThe newly-created and inserted node.\nNode\nNote\nBackwards-compatibility for this API is guaranteed.\nRemove all dead code from the graph, based on each node\u2019s number of\nusers, and whether the nodes have any side effects. The graph must be\ntopologically sorted before calling.\nis_impure_node(Optional[Callable[[Node],bool]]) \u2013 A function that returns\nNone(whether a node is impure. If this is) \u2013\nto(then the default behavior is) \u2013\nNode.is_impure.(use) \u2013\nWhether the graph was changed as a result of the pass.\nbool\nExample:\nBefore dead code is eliminated,afroma = x + 1below has no users\nand thus can be eliminated from the graph without having an effect.\n\n```python\ndef forward(self, x):\n    a = x + 1\n    return x + self.attr_1\n\n```\n\nAfter dead code is eliminated,a = x + 1has been removed, and the rest\nofforwardremains.\n\n```python\ndef forward(self, x):\n    return x + self.attr_1\n\n```\n\nWarning\nDead code elimination has some heuristics to avoid removing\nside-effectful nodes (see Node.is_impure) but in general coverage\nis very bad, so you should assume that this method is not sound\nto call unless you know that your FX graph consists entirely\nof functional operations or you supply your own custom\nfunction for detecting side-effectful nodes.\nNote\nBackwards-compatibility for this API is guaranteed.\nErases aNodefrom theGraph. Throws an exception if\nthere are still users of that node in theGraph.\nNode\nGraph\nGraph\nto_erase(Node) \u2013 TheNodeto erase from theGraph.\nNode\nGraph\nNote\nBackwards-compatibility for this API is guaranteed.\nAllows for fast query of nodes\nop(str) \u2013 the name of the operation\ntarget(Optional[Target]) \u2013 the target of the node. For call_function,\nthe target is required. For other ops, the target is optional.\nsort(bool) \u2013 whether to return nodes in the order they appear on\non the graph.\nIterable of nodes with the requested op and target.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInsert aget_attrnode into the Graph. Aget_attrNoderepresents the\nfetch of an attribute from theModulehierarchy.\nget_attr\nget_attr\nNode\nModule\nqualified_name(str) \u2013 the fully-qualified name of the attribute to be retrieved.\nFor example, if the traced Module has a submodule namedfoo, which has a\nsubmodule namedbar, which has an attribute namedbaz, the qualified\nnamefoo.bar.bazshould be passed asqualified_name.\nfoo\nbar\nbaz\nfoo.bar.baz\nqualified_name\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nThe newly-created and insertedget_attrnode.\nget_attr\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node.\nGraph.create_node\nNote\nBackwards-compatibility for this API is guaranteed.\nCopy all nodes from a given graph intoself.\nself\ng(Graph) \u2013 The source graph from which to copy Nodes.\nval_map(Dict[Node,Node]) \u2013 a dictionary that will be populated with a mapping\nfrom nodes ingto nodes inself. Note thatval_mapcan be passed\nin with values in it already to override copying of certain values.\ng\nself\nval_map\nThe value inselfthat is now equivalent to the output value ing,\nifghad anoutputnode.Noneotherwise.\nself\ng\ng\noutput\nNone\nOptional[Union[tuple[\u2018Argument\u2019, \u2026],Sequence[Argument],Mapping[str, Argument],slice,range,Node,str,int,float,bool,complex,dtype,Tensor,device,memory_format,layout,OpOverload,SymInt,SymBool,SymFloat]]\nNote\nBackwards-compatibility for this API is guaranteed.\nWhen used within a \u2018with\u2019 statement, this will temporary set the insert point and\nthen restore it when the with statement exits:\n\n```python\nwith g.inserting_after(n):\n    ...  # inserting after node n\n...  # insert point restored to what it was previously\ng.inserting_after(n)  #  set the insert point permanently\n\n```\n\nArgs:\nthe beginning of the entire graph.\nA resource manager that will restore the insert point on__exit__.\n__exit__\nNote\nBackwards-compatibility for this API is guaranteed.\nWhen used within a \u2018with\u2019 statement, this will temporary set the insert point and\nthen restore it when the with statement exits:\n\n```python\nwith g.inserting_before(n):\n    ...  # inserting before node n\n...  # insert point restored to what it was previously\ng.inserting_before(n)  #  set the insert point permanently\n\n```\n\nArgs:\nthe beginning of the entire graph.\nA resource manager that will restore the insert point on__exit__.\n__exit__\nNote\nBackwards-compatibility for this API is guaranteed.\nRuns various checks on this Graph to make sure it is well-formed. In\nparticular:\n- Checks Nodes have correct ownership (owned by this graph)\n- Checks Nodes appear in topological order\n- If this Graph has an owning GraphModule, checks that targets\nexist in that GraphModule\nNote\nBackwards-compatibility for this API is guaranteed.\nCopy a node from one graph into another.arg_transformneeds to transform arguments from\nthe graph of node to the graph of self. Example:\narg_transform\n\n```python\n# Copying all the nodes in `g` into `new_graph`\ng: torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n: value_remap[n])\n\n```\n\nnode(Node) \u2013 The node to copy intoself.\nself\narg_transform(Callable[[Node],Argument]) \u2013 A function that transformsNodearguments in node\u2019sargsandkwargsinto the\nequivalent argument inself. In the simplest case, this should\nretrieve a value out of a table mapping Nodes in the original\ngraph toself.\nNode\nargs\nkwargs\nself\nself\nNode\nNote\nBackwards-compatibility for this API is guaranteed.\nGet the list of Nodes that constitute this Graph.\nNote that thisNodelist representation is a doubly-linked list. Mutations\nduring iteration (e.g. delete a Node, add a Node) are safe.\nNode\nA doubly-linked list of Nodes. Note thatreversedcan be called on\nthis list to switch iteration order.\nreversed\nRegister a transformer function when python code is generated\na function that returns a code transformer to be registered.\nThis function is called byon_generate_codeto obtain the\ncode transformer.\nThis function is also given as its input the currently\nregistered code transformer (or None if nothing is registered),\nin case it is not desirable to overwrite it. This is useful to\nchain code transformers together.\na context manager that when used in awithstatement, to automatically\nrestore the previously registered code transformer.\nExample:\n\n```python\ngm: fx.GraphModule = ...\n\n\n# This is a code transformer we want to register. This code\n# transformer prepends a pdb import and trace statement at the very\n# beginning of the generated torch.fx code to allow for manual\n# debugging with the PDB library.\ndef insert_pdb(body):\n    return [\"import pdb; pdb.set_trace()\\n\", *body]\n\n\n# Registers `insert_pdb`, and overwrites the current registered\n# code transformer (given by `_` to the lambda):\ngm.graph.on_generate_code(lambda _: insert_pdb)\n\n# Or alternatively, registers a code transformer which first\n# runs `body` through existing registered transformer, then\n# through `insert_pdb`:\ngm.graph.on_generate_code(\n    lambda current_trans: (\n        lambda body: insert_pdb(\n            current_trans(body) if current_trans else body\n        )\n    )\n)\n\ngm.recompile()\ngm(*inputs)  # drops into pdb\n\n```\n\nThis function can also be used as a context manager, with the benefit to\nautomatically restores the previously registered code transformer:\n\n```python\n# ... continue from previous example\n\nwith gm.graph.on_generate_code(lambda _: insert_pdb):\n    # do more stuff with `gm`...\n    gm.recompile()\n    gm(*inputs)  # drops into pdb\n\n# now previous code transformer is restored (but `gm`'s code with pdb\n# remains - that means you can run `gm` with pdb here too, until you\n# run next `recompile()`).\n\n```\n\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInsert anoutputNodeinto theGraph. Anoutputnode represents\nareturnstatement in Python code.resultis the value that should\nbe returned.\noutput\nNode\nGraph\noutput\nreturn\nresult\nresult(Argument) \u2013 The value to be returned.\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have.\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node.\nGraph.create_node\nNote\nBackwards-compatibility for this API is guaranteed.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nNode\nInsert aplaceholdernode into the Graph. Aplaceholderrepresents\na function input.\nplaceholder\nplaceholder\nname(str) \u2013 A name for the input value. This corresponds to the name\nof the positional argument to the function thisGraphrepresents.\nGraph\ntype_expr(Optional[Any]) \u2013 an optional type annotation representing the\nPython type the output of this node will have. This is needed in some\ncases for proper code generation (e.g. when the function is used\nsubsequently in TorchScript compilation).\ndefault_value(Any) \u2013 The default value this function argument should take\non. NOTE: to allow forNoneas a default value,inspect.Signature.emptyshould be passed as this argument to specify that the parameter does _not_\nhave a default value.\nNode\nNote\nThe same insertion point and type expression rules apply for this method\nasGraph.create_node.\nGraph.create_node\nNote\nBackwards-compatibility for this API is guaranteed.\nPrints the intermediate representation of the graph in tabular\nformat. Note that this API requires thetabulatemodule to be\ninstalled.\ntabulate\nNote\nBackwards-compatibility for this API is guaranteed.\nProcesses args so that they can be passed to the FX graph.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nTurn thisGraphinto valid Python code.\nGraph\nroot_module(str) \u2013 The name of the root module on which to look-up\nqualified name targets. This is usually \u2018self\u2019.\nsrc: the Python source code representing the object\nglobals: a dictionary of global names insrc-> the objects that they reference.\nA PythonCode object, consisting of two fields\nNote\nBackwards-compatibility for this API is guaranteed.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nNodeis the data structure that represents individual operations within\naGraph. For the most part, Nodes represent callsites to various entities,\nsuch as operators, methods, and Modules (some exceptions include nodes that\nspecify function inputs and outputs). EachNodehas a function specified\nby itsopproperty. TheNodesemantics for each value ofopare as follows:\nNode\nGraph\nNode\nop\nNode\nop\nplaceholderrepresents a function input. Thenameattribute specifies the name this value will take on.targetis similarly the name of the argument.argsholds either: 1) nothing, or 2) a single argument\ndenoting the default parameter of the function input.kwargsis don\u2019t-care. Placeholders correspond to\nthe function parameters (e.g.x) in the graph printout.\nplaceholder\nname\ntarget\nargs\nkwargs\nx\nget_attrretrieves a parameter from the module hierarchy.nameis similarly the name the result of the\nfetch is assigned to.targetis the fully-qualified name of the parameter\u2019s position in the module hierarchy.argsandkwargsare don\u2019t-care\nget_attr\nname\ntarget\nargs\nkwargs\ncall_functionapplies a free function to some values.nameis similarly the name of the value to assign\nto.targetis the function to be applied.argsandkwargsrepresent the arguments to the function,\nfollowing the Python calling convention\ncall_function\nname\ntarget\nargs\nkwargs\ncall_moduleapplies a module in the module hierarchy\u2019sforward()method to given arguments.nameis\nas previous.targetis the fully-qualified name of the module in the module hierarchy to call.argsandkwargsrepresent the arguments to invoke the module on,excluding the self argument.\ncall_module\nforward()\nname\ntarget\nargs\nkwargs\ncall_methodcalls a method on a value.nameis as similar.targetis the string name of the method\nto apply to theselfargument.argsandkwargsrepresent the arguments to invoke the module on,including the self argument\ncall_method\nname\ntarget\nself\nargs\nkwargs\noutputcontains the output of the traced function in itsargs[0]attribute. This corresponds to the \u201creturn\u201d statement\nin the Graph printout.\noutput\nargs[0]\nNote\nBackwards-compatibility for this API is guaranteed.\nReturn all Nodes that are inputs to this Node. This is equivalent to\niterating overargsandkwargsand only collecting the values that\nare Nodes.\nargs\nkwargs\nList ofNodesthat appear in theargsandkwargsof thisNode, in that order.\nNodes\nargs\nkwargs\nNode\nInsertxafter this node in the list of nodes in the graph.\nEquivalent toself.next.prepend(x)\nx\nself.next.prepend(x)\nx(Node) \u2013 The node to put after this node. Must be a member of the same graph.\nNote\nBackwards-compatibility for this API is guaranteed.\nThe tuple of arguments to thisNode. The interpretation of arguments\ndepends on the node\u2019s opcode. See theNodedocstring for more\ninformation.\nNode\nNode\nAssignment to this property is allowed. All accounting of uses and users\nis updated automatically on assignment.\nReturn a descriptive string representation ofself.\nself\nThis method can be used with no arguments as a debugging\nutility.\nThis function is also used internally in the__str__method\nofGraph. Together, the strings inplaceholder_namesandmaybe_return_typenamemake up the signature of the\nautogeneratedforwardfunction in this Graph\u2019s surrounding\nGraphModule.placeholder_namesandmaybe_return_typenameshould not be used otherwise.\n__str__\nGraph\nplaceholder_names\nmaybe_return_typename\nforward\nplaceholder_names\nmaybe_return_typename\nplaceholder_names(Optional[list[str]]) \u2013 A list that will store formatted strings\nrepresenting the placeholders in the generatedforwardfunction. Internal use only.\nforward\nmaybe_return_typename(Optional[list[str]]) \u2013 A single-element list that will store\na formatted string representing the output of the\ngeneratedforwardfunction. Internal use only.\nforward\ninclude_tensor_metadata(bool) \u2013 Whether to include tensor metadata\nIf 1) we\u2019re usingformat_nodeas an internal helperin the__str__method ofGraph, and 2)selfis a placeholder Node, returnNone. Otherwise,\nreturn a  descriptive string representation of the\ncurrent Node.\nformat_node\nin the__str__method ofGraph, and 2)selfis a placeholder Node, returnNone. Otherwise,\nreturn a  descriptive string representation of the\ncurrent Node.\n__str__\nGraph\nself\nNone\nstr\nNote\nBackwards-compatibility for this API is guaranteed.\nInsert an positional argument to the argument list with given index.\nidx(int) \u2013 The index of the element inself.argsto be inserted before.\nself.args\narg(Argument) \u2013 The new argument value to insert intoargs\nargs\nNote\nBackwards-compatibility for this API is guaranteed.\nReturns whether this op is impure, i.e. if its op is a placeholder or\noutput, or if a call_function or call_module which is impure.\nimpure_random(bool) \u2013 Whether to treat rand op as impure.\nIf the op is impure or not.\nbool\nWarning\nThis API is experimental and isNOTbackward-compatible.\nThe dict of keyword arguments to thisNode. The interpretation of arguments\ndepends on the node\u2019s opcode. See theNodedocstring for more\ninformation.\nNode\nNode\nAssignment to this property is allowed. All accounting of uses and users\nis updated automatically on assignment.\nReturns the nextNodein the linked list of Nodes.\nNode\nThe nextNodein the linked list of Nodes.\nNode\nReturns normalized arguments to Python targets. This means thatargs/kwargswill be matched up to the module/functional\u2019s\nsignature and return exclusively kwargs in positional order\nifnormalize_to_only_use_kwargsis true.\nAlso populates default values. Does not support positional-only\nparameters or varargs parameters.\nSupports module calls.\nMay requirearg_typesandkwarg_typesin order to disambiguate overloads.\nroot(torch.nn.Module) \u2013 Module upon which to resolve module targets.\narg_types(Optional[Tuple[Any]]) \u2013 Tuple of arg types for the args\nkwarg_types(Optional[Dict[str,Any]]) \u2013 Dict of arg types for the kwargs\nnormalize_to_only_use_kwargs(bool) \u2013 Whether to normalize to only use kwargs.\nReturns NamedTuple ArgsKwargsPair, orNoneif not successful.\nOptional[ArgsKwargsPair]\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInsert x before this node in the list of nodes in the graph. Example:\n\n```python\nBefore: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n\n```\n\nx(Node) \u2013 The node to put before this node. Must be a member of the same graph.\nNote\nBackwards-compatibility for this API is guaranteed.\nReturns the previousNodein the linked list of Nodes.\nNode\nThe previousNodein the linked list of Nodes.\nNode\nReplace all uses ofselfin the Graph with the Nodereplace_with.\nself\nreplace_with\nreplace_with(Node) \u2013 The node to replace all uses ofselfwith.\nself\ndelete_user_cb(Callable) \u2013 Callback that is called to determine\nwhether a given user of the self node should be removed.\npropagate_meta(bool) \u2013 Whether or not to copy all properties\non the .meta field of the original node onto the replacement node.\nFor safety, this is only valid to do if the replacement node\ndoesn\u2019t already have an existing .meta field.\nThe list of Nodes on which this change was made.\nlist[\u2018Node\u2019]\nNote\nBackwards-compatibility for this API is guaranteed.\nLoop through input nodes ofself, and replace all instances ofold_inputwithnew_input.\nself\nold_input\nnew_input\nold_input(Node) \u2013 The old input node to be replaced.\nnew_input(Node) \u2013 The new input node to replaceold_input.\nold_input\nNote\nBackwards-compatibility for this API is guaranteed.\nReturn the Python stack trace that was recorded during tracing, if any.\nWhen traced with fx.Tracer, this property is usually populated byTracer.create_proxy. To record stack traces during tracing for debug purposes,\nsetrecord_stack_traces = Trueon theTracerinstance.\nWhen traced with dynamo, this property will be populated by default byOutputGraph.create_proxy.\nstack_trace would have the innermost frame at the end of the string.\nUpdate an existing positional argument to contain the new valuearg. After calling,self.args[idx]==arg.\narg\nself.args[idx]==arg\nidx(int) \u2013 The index intoself.argsof the element to update\nself.args\narg(Argument) \u2013 The new argument value to write intoargs\nargs\nNote\nBackwards-compatibility for this API is guaranteed.\nUpdate an existing keyword argument to contain the new valuearg. After calling,self.kwargs[key]==arg.\narg\nself.kwargs[key]==arg\nkey(str) \u2013 The key inself.kwargsof the element to update\nself.kwargs\narg(Argument) \u2013 The new argument value to write intokwargs\nkwargs\nNote\nBackwards-compatibility for this API is guaranteed.\nTraceris the class that implements the symbolic tracing functionality\noftorch.fx.symbolic_trace. A call tosymbolic_trace(m)is equivalent\ntoTracer().trace(m).\nTracer\ntorch.fx.symbolic_trace\nsymbolic_trace(m)\nTracer().trace(m)\nTracer can be subclassed to override various behaviors of the tracing\nprocess. The different behaviors that can be overridden are described\nin the docstrings of the methods on this class.\nNote\nBackwards-compatibility for this API is guaranteed.\nMethod that specifies the behavior of thisTracerwhen it encounters\na call to annn.Moduleinstance.\nTracer\nnn.Module\nBy default, the behavior is to check if the called module is a leaf module\nviais_leaf_module. If it is, emit acall_modulenode referring tomin theGraph. Otherwise, call theModulenormally, tracing through\nthe operations in itsforwardfunction.\nis_leaf_module\ncall_module\nm\nGraph\nModule\nforward\nThis method can be overridden to\u2013for example\u2013create nested traced\nGraphModules, or any other behavior you would want while tracing acrossModuleboundaries.\nModule\nm(Module) \u2013 The module for which a call is being emitted\nforward(Callable) \u2013 The forward() method of theModuleto be invoked\nModule\nargs(Tuple) \u2013 args of the module callsite\nkwargs(Dict) \u2013 kwargs of the module callsite\nThe return value from the Module call. In the case that acall_modulenode was emitted, this is aProxyvalue. Otherwise, it is whatever\nvalue was returned from theModuleinvocation.\ncall_module\nProxy\nModule\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nA method to specify the behavior of tracing when preparing values to\nbe used as arguments to nodes in theGraph.\nGraph\nBy default, the behavior includes:\nIterate through collection types (e.g. tuple, list, dict) and recursively\ncallcreate_argson the elements.\ncreate_args\nGiven a Proxy object, return a reference to the underlying IRNode\nNode\nGiven a non-Proxy Tensor object, emit IR for various cases:\nFor a Parameter, emit aget_attrnode referring to that Parameter\nget_attr\nFor a non-Parameter Tensor, store the Tensor away in a special\nattribute referring to that attribute.\nThis method can be overridden to support more types.\na(Any) \u2013 The value to be emitted as anArgumentin theGraph.\nArgument\nGraph\nThe valueaconverted into the appropriateArgument\na\nArgument\nArgument\nNote\nBackwards-compatibility for this API is guaranteed.\nCreateplaceholdernodes corresponding to the signature of therootModule. This method introspects root\u2019s signature and emits those\nnodes accordingly, also supporting*argsand**kwargs.\nplaceholder\nroot\n*args\n**kwargs\nWarning\nThis API is experimental and isNOTbackward-compatible.\nInserts a graph node given target, args, kwargs, and name.\nThis method can be overridden to do extra checking, validation, or\nmodification of values used in node creation. For example, one might\nwant to disallow in-place operations from being recorded.\nNote\nBackwards-compatibility for this API is guaranteed.\nNode\nCreate a Node from the given arguments, then return the Node\nwrapped in a Proxy object.\nIf kind = \u2018placeholder\u2019, then we\u2019re creating a Node that\nrepresents the parameter of a function. If we need to encode\na default parameter, we use theargstuple.argsis\notherwise empty forplaceholderNodes.\nargs\nargs\nplaceholder\nNote\nBackwards-compatibility for this API is guaranteed.\nGets a fresh name for a prefix and returns it. This function ensures\nthat it will not clash with an existing attribute on the graph.\nNote\nBackwards-compatibility for this API is guaranteed.\nstr\nMethod that specifies the behavior of thisTracerwhen we call getattr\non a call to annn.Moduleinstance.\nTracer\nnn.Module\nBy default, the behavior is to return a proxy value for the attribute. It\nalso stores the proxy value in theparameter_proxy_cache, so that future\ncalls will reuse the proxy rather than creating a new one.\nparameter_proxy_cache\nThis method can be overridden to \u2013for example\u2013 not return proxies when\nquerying parameters.\nattr(str) \u2013 The name of the attribute being queried\nattr_val(Any) \u2013 The value of the attribute\nparameter_proxy_cache(Dict[str,Any]) \u2013 A cache of attr names to proxies\nThe return value from the getattr call.\nWarning\nThis API is experimental and isNOTbackward-compatible.\nA method to specify whether a givennn.Moduleis a \u201cleaf\u201d module.\nnn.Module\nLeaf modules are the atomic units that appear in\nthe IR, referenced bycall_modulecalls. By default,\nModules in the PyTorch standard library namespace (torch.nn)\nare leaf modules. All other modules are traced through and\ntheir constituent ops are recorded, unless specified otherwise\nvia this parameter.\ncall_module\nm(Module) \u2013 The module being queried about\nmodule_qualified_name(str) \u2013 The path to root of this module. For example,\nif you have a module hierarchy where submodulefoocontains\nsubmodulebar, which contains submodulebaz, that module will\nappear with the qualified namefoo.bar.bazhere.\nfoo\nbar\nbaz\nfoo.bar.baz\nbool\nNote\nBackwards-compatibility for this API is guaranteed.\nwhen used in control flow.  Normally we don\u2019t know what to do because\nwe don\u2019t know the value of the proxy, but a custom tracer can attach more\ninformation to the graph node using create_node and can choose to return an iterator.\nNote\nBackwards-compatibility for this API is guaranteed.\nIterator\nThis is what happens when ** is called on a proxy. This should return an\niterator it ** is suppose to work in your custom tracer.\nNote\nBackwards-compatibility for this API is guaranteed.\nAny\nHelper method to find the qualified name ofmodin the Module hierarchy\nofroot. For example, ifroothas a submodule namedfoo, which has\na submodule namedbar, passingbarinto this function will return\nthe string \u201cfoo.bar\u201d.\nmod\nroot\nroot\nfoo\nbar\nbar\nmod(str) \u2013 TheModuleto retrieve the qualified name for.\nModule\nstr\nNote\nBackwards-compatibility for this API is guaranteed.\nNote\nBackwards-compatibility for this API is guaranteed.\nProxy\nwhen used in control flow.  Normally we don\u2019t know what to do because\nwe don\u2019t know the value of the proxy, but a custom tracer can attach more\ninformation to the graph node using create_node and can choose to return a value.\nNote\nBackwards-compatibility for this API is guaranteed.\nbool\nTracerootand return the corresponding FXGraphrepresentation.rootcan either be annn.Moduleinstance or a Python callable.\nroot\nGraph\nroot\nnn.Module\nNote that after this call,self.rootmay be different from therootpassed\nin here. For example, when a free function is passed totrace(), we will\ncreate annn.Moduleinstance to use as the root and add embedded constants\nto.\nself.root\nroot\ntrace()\nnn.Module\nroot(Union[Module,Callable]) \u2013 Either aModuleor a function to be\ntraced through. Backwards-compatibility for this parameter is\nguaranteed.\nModule\nconcrete_args(Optional[Dict[str,any]]) \u2013 Concrete arguments that should\nnot be treated as Proxies. This parameter is experimental and\nits backwards-compatibility isNOTguaranteed.\nAGraphrepresenting the semantics of the passed-inroot.\nGraph\nroot\nGraph\nNote\nBackwards-compatibility for this API is guaranteed.\nProxyobjects areNodewrappers that flow through the\nprogram during symbolic tracing and record all the operations\n(torchfunction calls, method calls, operators) that they touch\ninto the growing FX Graph.\nProxy\nNode\ntorch\nIf you\u2019re doing graph transforms, you can wrap your ownProxymethod around a rawNodeso that you can use the overloaded\noperators to add additional things to aGraph.\nProxy\nNode\nGraph\nProxyobjects cannot be iterated. In other words, the symbolic\ntracer will throw an error if aProxyis used in a loop or as\nan*args/**kwargsfunction argument.\nProxy\nProxy\n*args\n**kwargs\nThere are two main ways around this:\n1. Factor out the untraceable logic into a top-level function and\nusefx.wrapon it.\n2. If the control flow is static (i.e. the loop trip count is\nbased on some hyperparameter), the code can be kept in its original\nposition and refactored into something like:\nfx.wrap\n\n```python\nfor i in range(self.some_hyperparameter):\n    indexed_item = proxied_value[i]\n\n```\n\nFor a more detailed description into the Proxy internals, check out\nthe \u201cProxy\u201d section intorch/fx/README.md\nNote\nBackwards-compatibility for this API is guaranteed.\nAn Interpreter executes an FX graph Node-by-Node. This pattern\ncan be useful for many things, including writing code\ntransformations as well as analysis passes.\nMethods in the Interpreter class can be overridden to customize\nthe behavior of execution. The map of overridable methods\nin terms of call hierarchy:\n\n```python\nrun()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()\n\n```\n\nExample\nSuppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclass Interpreter like so:\ntorch.neg\ntorch.sigmoid\nTensor\n\n```python\nclass NegSigmSwapInterpreter(Interpreter):\n    def call_function(\n        self, target: Target, args: Tuple, kwargs: Dict\n    ) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(target, args, kwargs)\n\n    def call_method(self, target: Target, args: Tuple, kwargs: Dict) -> Any:\n        if target == \"neg\":\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(target, args, kwargs)\n\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_close(result, torch.neg(input).sigmoid())\n\n```\n\nmodule(torch.nn.Module) \u2013 The module to be executed\ngarbage_collect_values(bool) \u2013 Whether to delete values after their last\nuse within the Module\u2019s execution. This ensures optimal memory usage during\nexecution. This can be disabled to, for example, examine all of the intermediate\nvalues in the execution by looking at theInterpreter.envattribute.\nInterpreter.env\ngraph(Optional[Graph]) \u2013 If passed, the interpreter will execute this\ngraph instead ofmodule.graph, using the providedmoduleargument to satisfy any requests for state.\nNote\nBackwards-compatibility for this API is guaranteed.\nRunmodulevia interpretation and return the result.  This uses the \u201cboxed\u201d\ncalling convention, where you pass a list of arguments, which will be cleared\nby the interpreter.  This ensures that input tensors are promptly deallocated.\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute acall_functionnode and return the result.\ncall_function\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nAny\nAny: The value returned by the function invocation\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute acall_methodnode and return the result.\ncall_method\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nAny\nAny: The value returned by the method invocation\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute acall_modulenode and return the result.\ncall_module\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nAny\nAny: The value returned by the module invocation\nNote\nBackwards-compatibility for this API is guaranteed.\nFetch the concrete values ofargsandkwargsof nodenfrom the current execution environment.\nargs\nkwargs\nn\nn(Node) \u2013 The node for whichargsandkwargsshould be fetched.\nargs\nkwargs\nargsandkwargswith concrete values forn.\nargs\nkwargs\nn\nTuple[Tuple, Dict]\nNote\nBackwards-compatibility for this API is guaranteed.\nFetch an attribute from theModulehierarchy ofself.module.\nModule\nself.module\ntarget(str) \u2013 The fully-qualified name of the attribute to fetch\nThe value of the attribute.\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute aget_attrnode. Will retrieve an attribute\nvalue from theModulehierarchy ofself.module.\nget_attr\nModule\nself.module\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nThe value of the attribute that was retrieved\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nRecursively descend throughargsand look up the concrete value\nfor eachNodein the current execution environment.\nargs\nNode\nargs(Argument) \u2013 Data structure within which to look up concrete values\nn(Node) \u2013 Node to whichargsbelongs. This is only used for error reporting.\nargs\nOptional[Union[tuple[\u2018Argument\u2019, \u2026],Sequence[Argument],Mapping[str, Argument],slice,range,Node,str,int,float,bool,complex,dtype,Tensor,device,memory_format,layout,OpOverload,SymInt,SymBool,SymFloat]]\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute anoutputnode. This really just retrieves\nthe value referenced by theoutputnode and returns it.\noutput\noutput\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nThe return value referenced by the output node\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute aplaceholdernode. Note that this is stateful:Interpretermaintains an internal iterator over\narguments passed torunand this method returns\nnext() on that iterator.\nplaceholder\nInterpreter\nrun\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nThe argument value that was retrieved.\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nRunmodulevia interpretation and return the result.\n*args\u2013 The arguments to the Module to run, in positional order\ninitial_env(Optional[Dict[Node,Any]]) \u2013 An optional starting environment for execution.\nThis is a dict mappingNodeto any value. This can be used, for example, to\npre-populate results for certainNodesso as to do only partial evaluation within\nthe interpreter.\nenable_io_processing(bool) \u2013 If true, we process the inputs and outputs with graph\u2019s process_inputs and\nprocess_outputs function first before using them.\nThe value returned from executing the Module\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nRun a specific nodenand return the result.\nCalls into placeholder, get_attr, call_function,\ncall_method, call_module, or output depending\nonnode.op\nn\nnode.op\nn(Node) \u2013 The Node to execute\nThe result of executingn\nn\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nTransformeris a special type of interpreter that produces a\nnewModule. It exposes atransform()method that returns\nthe transformedModule.Transformerdoes not require\narguments to run, asInterpreterdoes.Transformerworks\nentirely symbolically.\nTransformer\nModule\ntransform()\nModule\nTransformer\nInterpreter\nTransformer\nExample\nSuppose we want to swap all instances oftorch.negwithtorch.sigmoidand vice versa (including theirTensormethod equivalents). We could subclassTransformerlike so:\ntorch.neg\ntorch.sigmoid\nTensor\nTransformer\n\n```python\nclass NegSigmSwapXformer(Transformer):\n    def call_function(\n        self,\n        target: \"Target\",\n        args: Tuple[Argument, ...],\n        kwargs: Dict[str, Any],\n    ) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(target, args, kwargs)\n\n    def call_method(\n        self,\n        target: \"Target\",\n        args: Tuple[Argument, ...],\n        kwargs: Dict[str, Any],\n    ) -> Any:\n        if target == \"neg\":\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(target, args, kwargs)\n\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed: torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_close(transformed(input), torch.neg(input).sigmoid())\n\n```\n\nmodule(GraphModule) \u2013 TheModuleto be transformed.\nModule\nNote\nBackwards-compatibility for this API is guaranteed.\nNote\nBackwards-compatibility for this API is guaranteed.\nAny\nNote\nBackwards-compatibility for this API is guaranteed.\nAny\nExecute aget_attrnode. InTransformer, this is\noverridden to insert a newget_attrnode into the output\ngraph.\nget_attr\nTransformer\nget_attr\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nProxy\nNote\nBackwards-compatibility for this API is guaranteed.\nExecute aplaceholdernode. InTransformer, this is\noverridden to insert a newplaceholderinto the output\ngraph.\nplaceholder\nTransformer\nplaceholder\ntarget(Target) \u2013 The call target for this node. SeeNodefor\ndetails on semantics\nargs(Tuple) \u2013 Tuple of positional args for this invocation\nkwargs(Dict) \u2013 Dict of keyword arguments for this invocation\nProxy\nNote\nBackwards-compatibility for this API is guaranteed.\nTransformself.moduleand return the transformedGraphModule.\nself.module\nGraphModule\nNote\nBackwards-compatibility for this API is guaranteed.\nGraphModule\nMatches all possible non-overlapping sets of operators and their\ndata dependencies (pattern) in the Graph of a GraphModule\n(gm), then replaces each of these matched subgraphs with another\nsubgraph (replacement).\npattern\ngm\nreplacement\ngm(GraphModule) \u2013 The GraphModule that wraps the Graph to operate on\npattern(Union[Callable,GraphModule]) \u2013 The subgraph to match ingmfor replacement\ngm\nreplacement(Union[Callable,GraphModule]) \u2013 The subgraph to replacepatternwith\npattern\nA list ofMatchobjects representing the places\nin the original graph thatpatternwas matched to. The list\nis empty if there are no matches.Matchis defined as:classMatch(NamedTuple):# Node from which the match was foundanchor:Node# Maps nodes in the pattern subgraph to nodes in the larger graphnodes_map:Dict[Node,Node]\nA list ofMatchobjects representing the places\nin the original graph thatpatternwas matched to. The list\nis empty if there are no matches.Matchis defined as:\nMatch\npattern\nMatch\n\n```python\nclass Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n\n```\n\nList[Match]\nExamples:\n\n```python\nimport torch\nfrom torch.fx import symbolic_trace, subgraph_rewriter\n\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x, w1, w2):\n        m1 = torch.cat([w1, w2]).sum()\n        m2 = torch.cat([w1, w2]).sum()\n        return x + torch.max(m1) + torch.max(m2)\n\n\ndef pattern(w1, w2):\n    return torch.cat([w1, w2])\n\n\ndef replacement(w1, w2):\n    return torch.stack([w1, w2])\n\n\ntraced_module = symbolic_trace(M())\n\nsubgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n\n```\n\nThe above code will first matchpatternin theforwardmethod oftraced_module. Pattern-matching is done based on\nuse-def relationships, not node names. For example, if you hadp=torch.cat([a,b])inpattern, you could matchm=torch.cat([a,b])in the originalforwardfunction,\ndespite the variable names being different (pvsm).\npattern\nforward\ntraced_module\np=torch.cat([a,b])\npattern\nm=torch.cat([a,b])\nforward\np\nm\nThereturnstatement inpatternis matched based on its\nvalue only; it may or may not match to thereturnstatement in\nthe larger graph. In other words, the pattern doesn\u2019t have to extend\nto the end of the larger graph.\nreturn\npattern\nreturn\nWhen the pattern is matched, it will be removed from the larger\nfunction and replaced byreplacement. If there are multiple\nmatches forpatternin the larger function, each non-overlapping\nmatch will be replaced. In the case of a match overlap, the first\nfound match in the set of overlapping matches will be replaced.\n(\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node\nis the parameter that appears directly afterself, while the\nlast Node is whatever the function returns.)\nreplacement\npattern\nself\nOne important thing to note is that the parameters of thepatternCallable must be used in the Callable itself,\nand the parameters of thereplacementCallable must match\nthe pattern. The first rule is why, in the above code block, theforwardfunction has parametersx,w1,w2, but thepatternfunction only has parametersw1,w2.patterndoesn\u2019t usex, so it shouldn\u2019t specifyxas a parameter.\nAs an example of the second rule, consider replacing\npattern\nreplacement\nforward\nx,w1,w2\npattern\nw1,w2\npattern\nx\nx\n\n```python\ndef pattern(x, y):\n    return torch.neg(x) + torch.relu(y)\n\n```\n\nwith\n\n```python\ndef replacement(x, y):\n    return torch.relu(x)\n\n```\n\nIn this case,replacementneeds the same number of parameters\naspattern(bothxandy), even though the parameteryisn\u2019t used inreplacement.\nreplacement\npattern\nx\ny\ny\nreplacement\nAfter callingsubgraph_rewriter.replace_pattern, the generated\nPython code looks like this:\nsubgraph_rewriter.replace_pattern\n\n```python\ndef forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2\n\n```\n\nNote\nBackwards-compatibility for this API is guaranteed.",
    "url": "https://pytorch.org/docs/stable/fx.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "78265f10dad38969b35de0d0b2212b90",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/fft.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "34f3a7bbf472f009473751e90b4278f4",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/index.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7ada5ac0251919cc7a35ac876705a525",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch_environment_variables.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ea54a74a40205038c5f72f785962eb7a",
    "source": "pytorch_docs",
    "title": "Windows FAQ \u2014 PyTorch 2.9 documentation",
    "text": "\n## Windows FAQ#\n\nCreated On: Apr 23, 2018 | Last Updated On: May 20, 2025\n\n## Building from source#\n\n\n## Include optional components#\n\nThere are two supported components for Windows PyTorch:\nMKL and MAGMA. Here are the steps to build with them.\n\n```python\nREM Make sure you have 7z and curl installed.\n\nREM Download MKL files\ncurl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O\n7z x -aoa mkl_2020.2.254.7z -omkl\n\nREM Download MAGMA files\nREM version available:\nREM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release)\nREM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release)\nREM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\nREM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\nset \"CUDA_PREFIX=cuda102\"\nset \"CONFIG=release\"\nset \"HOST=https://s3.amazonaws.com/ossci-windows\"\ncurl -k \"%HOST%/magma_2.5.4_%CUDA_PREFIX%_%CONFIG%.7z\" -o magma.7z\n7z x -aoa magma.7z -omagma\n\nREM Setting essential environment variables\nset \"CMAKE_INCLUDE_PATH=%cd%\\mkl\\include\"\nset \"LIB=%cd%\\mkl\\lib;%LIB%\"\nset \"MAGMA_HOME=%cd%\\magma\"\n\n```\n\n\n## Speeding CUDA build for Windows#\n\nVisual Studio doesn\u2019t support parallel custom task currently.\nAs an alternative, we can useNinjato parallelize CUDA\nbuild tasks. It can be used by typing only a few lines of code.\nNinja\n\n```python\nREM Let's install ninja first.\npip install ninja\n\nREM Set it as the cmake generator\nset CMAKE_GENERATOR=Ninja\n\n```\n\n\n## One key install script#\n\nYou can take a look atthis set of scripts.\nIt will lead the way for you.\n\n## Extension#\n\n\n## CFFI Extension#\n\nThe support for CFFI Extension is very experimental. You must specify\nadditionallibrariesinExtensionobject to make it build on\nWindows.\nlibraries\nExtension\n\n```python\nffi = create_extension(\n    '_ext.my_lib',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_compile_args=[\"-std=c99\"],\n    libraries=['ATen', '_C'] # Append cuda libraries when necessary, like cudart\n)\n\n```\n\n\n## Cpp Extension#\n\nThis type of extension has better support compared with\nthe previous one. However, it still needs some manual\nconfiguration. First, you should open thex86_x64 Cross Tools Command Prompt for VS 2017.\nAnd then, you can start your compiling process.\n\n## Installation#\n\n\n## Package not found in win-32 channel.#\n\n\n```python\nSolving environment: failed\n\nPackagesNotFoundError: The following packages are not available from current channels:\n\n- pytorch\n\nCurrent channels:\n- https://repo.continuum.io/pkgs/main/win-32\n- https://repo.continuum.io/pkgs/main/noarch\n- https://repo.continuum.io/pkgs/free/win-32\n- https://repo.continuum.io/pkgs/free/noarch\n- https://repo.continuum.io/pkgs/r/win-32\n- https://repo.continuum.io/pkgs/r/noarch\n- https://repo.continuum.io/pkgs/pro/win-32\n- https://repo.continuum.io/pkgs/pro/noarch\n- https://repo.continuum.io/pkgs/msys2/win-32\n- https://repo.continuum.io/pkgs/msys2/noarch\n\n```\n\nPyTorch doesn\u2019t work on 32-bit system. Please use Windows and\nPython 64-bit version.\n\n## Import error#\n\n\n```python\nfrom torch._C import *\n\nImportError: DLL load failed: The specified module could not be found.\n\n```\n\nThe problem is caused by the missing of the essential files.\nFor the wheels package, since we didn\u2019t pack some libraries and VS2017\nredistributable files in, please make sure you install them manually.\nTheVS 2017 redistributable installercan be downloaded.\nAnd you should also pay attention to your installation of Numpy. Make sure it\nuses MKL instead of OpenBLAS. You may type in the following command.\n\n```python\npip install numpy mkl intel-openmp mkl_fft\n\n```\n\n\n## Usage (multiprocessing)#\n\n\n## Multiprocessing error without if-clause protection#\n\n\n```python\nRuntimeError:\n       An attempt has been made to start a new process before the\n       current process has finished its bootstrapping phase.\n\n   This probably means that you are not using fork to start your\n   child processes and you have forgotten to use the proper idiom\n   in the main module:\n\n       if __name__ == '__main__':\n           freeze_support()\n           ...\n\n   The \"freeze_support()\" line can be omitted if the program\n   is not going to be frozen to produce an executable.\n\n```\n\nThe implementation ofmultiprocessingis different on Windows, which\nusesspawninstead offork. So we have to wrap the code with an\nif-clause to protect the code from executing multiple times. Refactor\nyour code into the following structure.\nmultiprocessing\nspawn\nfork\n\n```python\nimport torch\n\ndef main()\n    for i, data in enumerate(dataloader):\n        # do something here\n\nif __name__ == '__main__':\n    main()\n\n```\n\n\n## Multiprocessing error \u201cBroken pipe\u201d#\n\n\n```python\nForkingPickler(file, protocol).dump(obj)\n\nBrokenPipeError: [Errno 32] Broken pipe\n\n```\n\nThis issue happens when the child process ends before the parent process\nfinishes sending data. There may be something wrong with your code. You\ncan debug your code by reducing thenum_workerofDataLoaderto zero and see if the issue persists.\nnum_worker\nDataLoader\n\n## Multiprocessing error \u201cdriver shut down\u201d#\n\n\n```python\nCouldn\u2019t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\\lib\\TH\\THAllocator.c:154\n\n[windows] driver shut down\n\n```\n\nPlease update your graphics driver. If this persists, this may be that your\ngraphics card is too old or the calculation is too heavy for your card. Please\nupdate the TDR settings according to thispost.\n\n## CUDA IPC operations#\n\n\n```python\nTHCudaCheck FAIL file=torch\\csrc\\generic\\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS\n\n```\n\nThey are not supported on Windows. Something like doing multiprocessing on CUDA\ntensors cannot succeed, there are two alternatives for this.\n1. Don\u2019t usemultiprocessing. Set thenum_workerofDataLoaderto zero.\nmultiprocessing\nnum_worker\nDataLoader\n2. Share CPU tensors instead. Make sure your customDataSetreturns CPU tensors.\nDataSet",
    "url": "https://pytorch.org/docs/stable/notes/windows.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3397f590061a692c3b07c2343248f8b9",
    "source": "pytorch_docs",
    "title": "torch.nn.attention \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.nn.attention#\n\nCreated On: Jan 24, 2024 | Last Updated On: Oct 29, 2024\nThis module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention\n\n## Utils#\n\nsdpa_kernel\n\nsdpa_kernel\nContext manager to select which backend to use for scaled dot product attention.\nSDPBackend\n\nSDPBackend\nAn enum-like class that contains the different backends for scaled dot product attention.\n\n## Submodules#\n\nflex_attention\nflex_attention\nThis module implements the user facing API for flex_attention in PyTorch.\nbias\nbias\nDefines bias subclasses that work with scaled_dot_product_attention\nexperimental\nexperimental\n",
    "url": "https://pytorch.org/docs/stable/nn.attention.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "004deaf085f0163db23ce58659e5bdef",
    "source": "pytorch_docs",
    "title": "PyTorch Custom Operators Landing Page \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch Custom Operators Landing Page#\n\nCreated On: May 29, 2024 | Last Updated On: Nov 04, 2024\nThis page has moved.\nRedirecting to the new page\u2026",
    "url": "https://pytorch.org/docs/stable/notes/custom_operators.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "efb670d9a53ac3fb58c9b7189298376e",
    "source": "pytorch_docs",
    "title": "PyTorch 2.0 Troubleshooting (old) \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch 2.0 Troubleshooting (old)#\n\nCreated On: Jun 06, 2025 | Last Updated On: Aug 02, 2025\nAuthor:Michael Lazos\nNote\nThis document is outdated and is now mainly a primary resource on how to run thetorch.compileminifier.\nPlease see theupdated troubleshooting document.\nThere is also a morecomprehensive manual for torch.compileavailable.\ntorch.compile\nWe are actively developing debug tools, profilers, and improving our\nerror and warning messages. Below is a table of the available\ntools and their typical usage. For additional help seeDiagnosing Runtime Errors.\nTool\nPurpose\nUsage\nInfo logging\nView summarized steps of compilation\ntorch._logging.set_logs(dynamo=logging.INFO)orTORCH_LOGS=\"dynamo\"\ntorch._logging.set_logs(dynamo=logging.INFO)\nTORCH_LOGS=\"dynamo\"\nDebug logging\nView detailed steps of compilation (print every instruction traced)\ntorch._logging.set_logs(dynamo=logging.DEBUG)andtorch._dynamo.config.verbose=True, orTORCH_LOGS=\"+dynamo\"TORCHDYNAMO_VERBOSE=1\ntorch._logging.set_logs(dynamo=logging.DEBUG)\ntorch._dynamo.config.verbose=True\nTORCH_LOGS=\"+dynamo\"TORCHDYNAMO_VERBOSE=1\nMinifier for any backend\nFind smallest subgraph which reproduces errors for any backend\nset environment variableTORCHDYNAMO_REPRO_AFTER=\"dynamo\"\nTORCHDYNAMO_REPRO_AFTER=\"dynamo\"\nMinifier forTorchInductor\nTorchInductor\nIf the error is known to occur afterAOTAutogradfind\nsmallest subgraph which reproduces errors duringTorchInductorlowering\nAOTAutograd\nTorchInductor\nset environment variableTORCHDYNAMO_REPRO_AFTER=\"aot\"\nTORCHDYNAMO_REPRO_AFTER=\"aot\"\nDynamo accuracy minifier\nFinds the smallest subgraph which reproduces an accuracy issue\nbetween an eager mode model and optimized model, when you\nsuspect the problem is inAOTAutograd\nAOTAutograd\nTORCHDYNAMO_REPRO_AFTER=\"dynamo\"TORCHDYNAMO_REPRO_LEVEL=4\nTORCHDYNAMO_REPRO_AFTER=\"dynamo\"TORCHDYNAMO_REPRO_LEVEL=4\nInductor accuracy minifier\nFinds the smallest subgraph which reproduces an accuracy issue\nbetween an eager mode model and optimized model, when you\nsuspect the problem is in the backend (e.g., inductor).\nIf this doesn\u2019t work, try the Dynamo accuracy minifier\ninstead.\nTORCHDYNAMO_REPRO_AFTER=\"aot\"TORCHDYNAMO_REPRO_LEVEL=4\nTORCHDYNAMO_REPRO_AFTER=\"aot\"TORCHDYNAMO_REPRO_LEVEL=4\ntorch._dynamo.explain\ntorch._dynamo.explain\nFind graph breaks and display reasoning for them\ntorch._dynamo.explain(fn)(*inputs)\ntorch._dynamo.explain(fn)(*inputs)\nRecord/Replay\nRecord and replay frames which to reproduce errors during graph capture\ntorch._dynamo.config.replay_record_enabled=True\ntorch._dynamo.config.replay_record_enabled=True\nTorchDynamo function name filtering\nOnly compile functions with the given name to reduce noise when\ndebugging an issue\nset environment variableTORCHDYNAMO_DEBUG_FUNCTION=<name>\nTORCHDYNAMO_DEBUG_FUNCTION=<name>\nTorchInductor Debug logging\nPrint general TorchInductor debug info and generated Triton/C++ code\ntorch._inductor.config.debug=True\ntorch._inductor.config.debug=True\nTorchInductor Tracing\nShow time taken in each TorchInductor stage + output code and graph\nvisualization\nset the environment variable TORCH_COMPILE_DEBUG=1 ortorch._inductor.config.trace.enabled=True\ntorch._inductor.config.trace.enabled=True\nIn addition to info and debug logging,\nyou can usetorch._loggingfor more fine-grained logging.\n\n## Diagnosing Runtime Errors#\n\nAt a high level, the TorchDynamo stack consists of a graph capture from\nPython code (TorchDynamo) and a backend compiler. For example, a\nbackend compiler may consist of backward graph tracing (AOTAutograd) and\ngraph lowering (TorchInductor)*. Errors can occur in any component of\nthe stack and will provide full stack traces.\nTo determine in which component an error occurred,\nyou may use info-level loggingtorch._logging.set_logs(dynamo=logging.INFO)orTORCH_LOGS=\"dynamo\"and look forStep#:...outputs. Logs are made at the beginning and end of\neach step, so the step that an error should correspond to is the most recently\nlogged step whose end has not yet been logged. The steps correspond to the\nfollowing parts of the stack:\ntorch._logging.set_logs(dynamo=logging.INFO)\nTORCH_LOGS=\"dynamo\"\nStep#:...\nStep\nComponent\n1\nTorchDynamo\n2\nCompiler Backend\n3\nTorchInductor\nIf info logging is insufficient, you can use available backend\noptions. These options include:\n\"eager\": only runs TorchDynamo forward graph capture and then\nruns the captured graph with PyTorch. This provides an indication as\nto whether TorchDynamo is raising the error.\n\"eager\"\n\"aot_eager\": runs TorchDynamo to capture a forward graph, and\nthen AOTAutograd to trace the backward graph without any additional\nbackend compiler steps. PyTorch eager will then be used to run the\nforward and backward graphs. This is useful to narrow down the issue\nto AOTAutograd.\n\"aot_eager\"\nThe general procedure to narrow down an issue is the following:\nRun your program with the\"eager\"backend. If the error no longer\noccurs, the issue is in the backend compiler that is being used (if\nusing TorchInductor, proceed to step 2. If not, seeMinifying Backend Compiler Errors). If the error still\noccurs with the\"eager\"backend, it is due toTorchdynamo Errors.\n\"eager\"\n\"eager\"\nThis step is only necessary ifTorchInductoris used as the backend\ncompiler. Run the model with the\"aot_eager\"backend. If this\nbackend raises an error then the error is occurring during\nAOTAutograd tracing. If the error no longer occurs with this backend,\nthenMinifying TorchInductor Errors.\nTorchInductor\n\"aot_eager\"\nEach of these cases are analyzed in the following sections.\nNote\nThe TorchInductor backend consists of\nboth AOTAutograd tracing and the TorchInductor compiler itself. We will\ndisambiguate by referring toTorchInductoras the backend, and\nTorchInductor lowering as the phase which lowers the graph traced by\nAOTAutograd.\nTorchInductor\n\n## Torchdynamo Errors#\n\nIf the error that is generated occurs with the\"eager\"backend, then\nTorchDynamo is most likely the source of the error. Here is a sample code\nwhich will generate an error.\n\"eager\"\n\n```python\nimport torch\n\nimport torch._dynamo as dynamo\n\n\ndef test_assertion_error():\n    y = torch.ones(200, 200)\n    z = {y: 5}\n    return z\n\ncompiled_test_assertion_error = torch.compile(test_assertion_error, backend=\"eager\")\n\ncompiled_test_assertion_error()\n\n```\n\nThe code above generates the following error:\n\n```python\ntorch._dynamo.convert_frame: [ERROR] WON'T CONVERT test_assertion_error /scratch/mlazos/torchdynamo/../test/errors.py line 26\ndue to:\nTraceback (most recent call last):\n  File \"/scratch/mlazos/torchdynamo/torchdynamo/symbolic_convert.py\", line 837, in BUILD_MAP\n    assert isinstance(k, ConstantVariable) or (\nAssertionError\n\nfrom user code:\n   File \"/scratch/mlazos/torchdynamo/../test/errors.py\", line 34, in test_assertion_error\n    z = {y: 5}\n\nSet torch._dynamo.config.verbose=True for more information\n==========\n\n```\n\nAs the message suggests you can settorch._dynamo.config.verbose=Trueto get a full stack trace to both\nthe error in TorchDynamo and the user code. In addition to this flag,\nyou can also set thelog_levelof TorchDynamo throughtorch._logging.set_logs(dynamo=logging.INFO)orTORCH_LOGS=\"dynamo\". These levels include:\ntorch._dynamo.config.verbose=True\nlog_level\ntorch._logging.set_logs(dynamo=logging.INFO)\nTORCH_LOGS=\"dynamo\"\nlogging.DEBUGorTORCH_LOGS=\"+dynamo\": Print every instruction that is\nencountered in addition to all the log levels listed below.\nlogging.DEBUG\nTORCH_LOGS=\"+dynamo\"\nlogging.INFO:\nPrint each function that is compiled (original and modified bytecode)\nand the graph that is captured in addition to all the log levels listed below.\nlogging.INFO\nlogging.WARNING(default): Print graph breaks in addition to all\nthe log levels listed below.\nlogging.WARNING\nlogging.ERROR: Print errors only.\nlogging.ERROR\nIf a model is very large, the logs can become overwhelming. If\nan error occurs deep within a model\u2019s Python code, it can be useful to\nexecute only the frame in which the error occurs to enable easier\ndebugging. There are two tools available to enable this:\nSetting the environment variableTORCHDYNAMO_DEBUG_FUNCTIONto the desired function name will only run torchdynamo on functions with that\nname.\nTORCHDYNAMO_DEBUG_FUNCTION\nEnabling the record/replay tool (settorch._dynamo.config.replay_record_enabled=True)\nwhich dumps an execution record when an error is encountered. This record can\nthen be replayed to run only the frame where an error occurred.\ntorch._dynamo.config.replay_record_enabled=True\n\n## Diagnosing TorchInductor Errors#\n\nIf the error does not occur with the\"eager\"backend, then the\nbackend compiler is the source of the error (example\nerror).\nThere aredifferent choicesfor backend compilers for TorchDynamo, with TorchInductor\nfitting the needs of most users. This section focuses on TorchInductor\nas the motivating example, but some tools can also be used with other\nbackend compilers.\n\"eager\"\nBelow is the portion of the stack which we are focusing on:\nWith TorchInductor as the chosen backend, AOTAutograd is used to\ngenerate the backward graph from the forward graph captured by\ntorchdynamo. It is important to note that errors can occur during this\ntracing and also while TorchInductor lowers the forward and backward\ngraphs to GPU code or C++. A model can often consist of hundreds or\nthousands of FX nodes, so narrowing the exact nodes where this problem\noccurred can be very difficult. Fortunately, there are tools available to\nautomatically minify these input graphs to the nodes which are causing\nthe issue. The first step is to determine whether the error occurs\nduring tracing of the backward graph with AOTAutograd or during\nTorchInductor lowering. As mentioned above in step 2, the\"aot_eager\"backend can be used to run only AOTAutograd in isolation\nwithout lowering. If the error still occurs with this backend, this\nindicates that the error is occurring during AOTAutograd tracing.\n\"aot_eager\"\nHere is an example:\n\n```python\nimport torch\n\nimport torch._dynamo as dynamo\n\nmodel = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n\ndef test_backend_error():\n\n    y = torch.ones(200, 200)\n    x = torch.ones(200, 200)\n    z = x + y\n    a = torch.ops.aten._foobar(z)  # dummy function which errors\n    return model(a)\n\n\ncompiled_test_backend_error = torch.compile(test_backend_error, backend=\"inductor\")\ncompiled_test_backend_error()\n\n```\n\nRunning this should give you this error with a longer stack trace below\nit:\n\n```python\nTraceback (most recent call last):\n  File \"/scratch/mlazos/torchdynamo/torchinductor/graph.py\", line 246, in call_function\n    return lowerings[target](*args, **kwargs)\n  File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 185, in wrapped\n    return decomp_fn(*args, **kwargs)\n  File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 810, in _foobar\n    assert False\nAssertionError\n...\n\n```\n\nerror with full stack\ntrace\nIf you then changetorch.compile(backend=\"inductor\")totorch.compile(backend=\"aot_eager\"), it will run without error, becausethe\nissueis in the TorchInductor lowering process, not in AOTAutograd.\ntorch.compile(backend=\"inductor\")\ntorch.compile(backend=\"aot_eager\")\n\n## Minifying TorchInductor Errors#\n\nFrom here, let\u2019s run the minifier to get a minimal repro. Setting the\nenvironment variableTORCHDYNAMO_REPRO_AFTER=\"aot\"(or settingtorch._dynamo.config.repro_after=\"aot\"directly) will generate a\nPython program which reduces the graph produced by AOTAutograd to the\nsmallest subgraph which reproduces the error. (See below for an example\nwhere we minify the graph produced by TorchDynamo) Running the program\nwith this environment variable should show nearlyidentical\noutput,\nwith an additional line indicating whereminifier_launcher.pyhas\nbeen written to. The output directory is configurable by settingtorch._dynamo.config.base_dirto a valid directory name. The final\nstep is to run the minifier and check that it runs successfully. A\nsuccessful run looks likethis.\nIf the minifier runs successfully, it generates runnable python code\nwhich reproduces the exact error. For our example this is the following\ncode:\nTORCHDYNAMO_REPRO_AFTER=\"aot\"\ntorch._dynamo.config.repro_after=\"aot\"\nminifier_launcher.py\ntorch._dynamo.config.base_dir\n\n```python\nimport torch\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nfrom torch.fx.experimental.proxy_tensor import make_fx\n\n# torch version: 1.13.0a0+gitfddfc44\n# torch cuda version: 11.6\n# torch git version: fddfc4488afb207971c54ad4bf58130fdc8a4dc5\n\n\n# CUDA Info:\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2022 NVIDIA Corporation\n# Built on Thu_Feb_10_18:23:41_PST_2022\n# Cuda compilation tools, release 11.6, V11.6.112\n# Build cuda_11.6.r11.6/compiler.30978841_0\n\n# GPU Hardware Info:\n# NVIDIA A100-SXM4-40GB : 8\n\nfrom torch.nn import *\n\nclass Repro(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, add):\n        _foobar = torch.ops.aten._foobar.default(add);  add = None\n        return (_foobar,)\n\nargs = [((200, 200), (200, 1), torch.float32, 'cpu')]\nargs = [rand_strided(shape, stride, dtype, device) for shape, stride, dtype, device in args]\nmod = make_fx(Repro())(*args)\nfrom torch._inductor.compile_fx import compile_fx_inner\n\ncompiled = compile_fx_inner(mod, args)\ncompiled(*args)\n\n```\n\nTheforwardmethod of theRepromodule contains the exact op\nwhich causes the issue. When filing an issue, please include any\nminified repros to aid in debugging.\nforward\nRepro\n\n## Minifying Backend Compiler Errors#\n\nWith backend compilers other than TorchInductor the process for finding\nthe subgraph causing the error is nearly identical to the procedure inMinifying TorchInductor Errorswith one important\ncaveat. Namely, that the minifier will now be run on the graph that is\ntraced by TorchDynamo, not the output graph of AOTAutograd. Let\u2019s walk\nthrough an example.\n\n```python\nimport torch\n\nimport torch._dynamo as dynamo\n\nmodel = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n# toy compiler which fails if graph contains relu\ndef toy_compiler(gm: torch.fx.GraphModule, _):\n    for node in gm.graph.nodes:\n        if node.target == torch.relu:\n            assert False\n\n    return gm\n\n\ndef test_backend_error():\n    y = torch.ones(200, 200)\n    x = torch.ones(200, 200)\n    z = x + y\n    a = torch.relu(z)\n    return model(a)\n\n\ncompiled_test_backend_error = torch.compile(test_backend_error, backend=toy_compiler)\ncompiled_test_backend_error()\n\n```\n\nIn order to run the code after TorchDynamo has traced the forward graph,\nyou can use theTORCHDYNAMO_REPRO_AFTERenvironment variable. Running\nthis program withTORCHDYNAMO_REPRO_AFTER=\"dynamo\"(ortorch._dynamo.config.repro_after=\"dynamo\") should producethis\noutputand\nthe following code in{torch._dynamo.config.base_dir}/repro.py.\nTORCHDYNAMO_REPRO_AFTER\nTORCHDYNAMO_REPRO_AFTER=\"dynamo\"\ntorch._dynamo.config.repro_after=\"dynamo\"\n{torch._dynamo.config.base_dir}/repro.py\nNote\nThe other option for TORCHDYNAMO_REPRO_AFTER is\"aot\", which\nwill run the minifier after the backward graph has been generated.\n\"aot\"\n\n```python\nimport torch\nimport torch._dynamo as dynamo\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nfrom torch._dynamo.debug_utils import run_fwd_maybe_bwd\n\nfrom torch.nn import *\n\nclass Repro(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, add):\n        relu = torch.relu(add);  add = None\n        return (relu,)\n\n\nmod = Repro().cuda()\nopt_mod = torch.compile(mod, backend=\"None\")\n\n\nargs = [((200, 200), (200, 1), torch.float32, 'cpu', False)]\nargs = [rand_strided(sh, st, dt, dev).requires_grad_(rg) for (sh, st, dt, dev, rg) in args]\n\n\nwith torch.cuda.amp.autocast(enabled=False):\n    ref = run_fwd_maybe_bwd(mod, args)\n    res = run_fwd_maybe_bwd(opt_mod, args)\n\n```\n\nThe minifier successfully reduced the graph to the op that raises the\nerror intoy_compiler. The other difference from the procedure inMinifying TorchInductor Errorsis that the minifier is\nautomatically run after encountering a backend compiler error. After a\nsuccessful run, the minifier writesrepro.pytotorch._dynamo.config.base_dir.\ntoy_compiler\nrepro.py\ntorch._dynamo.config.base_dir\n\n## Performance Profiling#\n\n\n## Accessing TorchDynamo Profiler#\n\nTorchDynamo has a built-in stats function for collecting and displaying\nthe time spent in each compilation phase. These stats can be accessed by\ncallingtorch._dynamo.utils.compile_times()after executing\nTorch._Dynamo. By default, this returns a string representation of the\ncompile times spent in each TorchDynamo function by name.\ntorch._dynamo.utils.compile_times()\n\n## TorchInductor Debugging using TORCH_COMPILE_DEBUG#\n\nTorchInductor has a builtin stats and trace function for displaying time\nspent in each compilation phase, output code, output graph visualization\nand IR dump. This is a debugging tool designed to make it easier to\nunderstand and troubleshoot the internals of TorchInductor.\nLet\u2019s run an example with the following test program (repro.py):\nrepro.py\n\n```python\nimport torch\n\n@torch.compile()\ndef test_model(x):\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 10),\n        torch.nn.LayerNorm(10),\n        torch.nn.ReLU(),\n    )\n    return model(x)\n\n\ny = test_model(torch.ones(10, 10))\n\n```\n\nSetting the environment variableTORCH_COMPILE_DEBUG=1will cause a\ndebug trace directory to be created, by default this directory will be in the\ncurrent directory and named torch_compile_debug (this can be overridden in\nthe torchdynamo configuration fielddebug_dir_rootand also theenvvarTORCH_COMPILE_DEBUG_DIR). Inside this directory, each run will\nhave a separate folder named with the timestamp and process id of the run:\nTORCH_COMPILE_DEBUG=1\ndebug_dir_root\nenvvarTORCH_COMPILE_DEBUG_DIR\n\n```python\n$ env TORCH_COMPILE_DEBUG=1 python repro.py\n$ cd torch_compile_debug\n$ ls\nrun_2023_03_01_08_20_52_143510-pid_180167\n\n```\n\nIn the run folder there will be atorchdynamodirectory which contains\ndebug logs, and antorchinductorfolder which contains a subfolder for each\ncompiled kernel with inductor debug artifacts.\ntorchdynamo\ntorchinductor\n\n```python\n$ cd\nrun_2023_03_01_08_20_52_143510-pid_180167\n$ ls\ntorchinductor  torchdynamo\n\n```\n\nMoving further into thetorchinductordirectory, the\\*.logfiles are\nlogs from the AOT Autograd phase of compilation,model__0_forward_1.0contains\nthe inductor debug artifacts.\ntorchinductor\n\\*.log\nmodel__0_forward_1.0\n\n```python\n$ cd torchinductor\n$ ls\naot_model___0_debug.log  model__0_forward_1.0\n$ cd model__0_forward_1.0\n$ ls\ndebug.log  fx_graph_readable.py  fx_graph_runnable.py  fx_graph_transformed.py  ir_post_fusion.txt  ir_pre_fusion.txt  output_code.py\n\n```\n\nHere is a summary of the contents:\nfx_graph_readable.pyandfx_graph_runnable.pyare the readable and\nrunnable versions of thefx_graphreceived by inductor.\nfx_graph_readable.py\nfx_graph_runnable.py\nfx_graph\nfx_graph_transformed.pyis the fx graph after inductor has run all fx passes.\nfx_graph_transformed.py\nir\\*.txtis the inductor ir pre and post fusion.\nir\\*.txt\noutput_code.pyis the compiled triton kernel for the subgraph.\noutput_code.py\nHere areexample debug directory contentsfor the test program:\n\n```python\nimport torch\n\n@torch.compile()\ndef test_model(x):\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 10),\n        torch.nn.LayerNorm(10),\n        torch.nn.ReLU(),\n    )\n    return model(x)\n\n\ny = test_model(torch.ones(10, 10))\n\n```\n\nEach file in that debug trace can be enabled and disabled throughtorch._inductor.config.trace.*. The profile and the diagram are both\ndisabled by default since they are expensive to generate.\ntorch._inductor.config.trace.*\nA single node in this new debug format looks like:\n\n```python\nbuf1: SchedulerNode(ComputedBuffer)\nbuf1.writes =\n    {   MemoryDep(name='buf1', index=0, size=()),\n        MemoryDep(name='buf1', index=0, size=(s0,))}\nbuf1.unmet_dependencies = {MemoryDep(name='buf0', index=c0, size=(s0,))}\nbuf1.met_dependencies = {MemoryDep(name='primals_2', index=c0, size=(s0,))}\nbuf1.group.device = cuda:0\nbuf1.group.iteration = (1, s0)\nbuf1.sizes = ([], [s0])\nclass buf1_loop_body:\n    var_ranges = {z0: s0}\n    index0 = z0\n    index1 = 0\n    def body(self, ops):\n        get_index = self.get_index('index0')\n        load = ops.load('buf0', get_index, False)\n        get_index_1 = self.get_index('index0')\n        load_1 = ops.load('primals_2', get_index_1, False)\n        add = ops.add(load, load_1)\n        get_index_2 = self.get_index('index1')\n        reduction = ops.reduction('buf1', torch.float32, torch.float32, 'sum', get_index_2, add)\n        return reduction\n\n```\n\nSee theexample debug directory\noutputfor more examples.\n\n## Graph Breaks#\n\nGiven a program like this:\n\n```python\ndef some_fun(x):\n    ...\n\ncompiled_fun = torch.compile(some_fun, ...)\n...\n\n```\n\nTorchDynamo will attempt to compile all of the torch/tensor operations\nwithin some_fun into a single FX graph, but it may fail to capture\neverything into one graph.\nSome graph break reasons are insurmountable to TorchDynamo, and can\u2019t be\neasily fixed. - calling into a C extension other than torch is invisible\nto torchdynamo, and could do arbitrary things without TorchDynamo being\nable to introduce necessary guards (seeMaking Dynamo Sound: Guards)\nto ensure that the compiled program would be safe to reuse. Graph breaks\ncan hinder performance if the resulting fragments are small. To maximize\nperformance, it\u2019s important to have as few graph breaks as possible.\n\n## Identifying the Cause of a Graph Break#\n\nTo identify all graph breaks in a program and the associated reasons for\nthe breaks,torch._dynamo.explaincan be used. This tool runs\nTorchDynamo on the supplied function and aggregates the graph breaks\nthat are encountered. Here is an example usage:\ntorch._dynamo.explain\n\n```python\nimport torch\nimport torch._dynamo as dynamo\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    print(\"woo\")\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nexplanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))\nprint(explanation_verbose)\n\"\"\"\nGraph Count: 3\nGraph Break Count: 2\nOp Count: 5\nBreak Reasons:\n  Break Reason 1:\n    Reason: builtin: print [<class 'torch._dynamo.variables.constant.ConstantVariable'>] False\n    User Stack:\n      <FrameSummary file foo.py, line 5 in toy_example>\n  Break Reason 2:\n    Reason: generic_jump TensorVariable()\n    User Stack:\n      <FrameSummary file foo.py, line 6 in torch_dynamo_resume_in_toy_example_at_5>\nOps per Graph:\n  ...\nOut Guards:\n  ...\n\"\"\"\n\n```\n\nOutputs include:\nout_guards- a list of lists where each sublist contains the guards that must pass to ensure the traced graphs are valid.\nout_guards\ngraphs- a list of graph modules which were successfully traced.\ngraphs\nops_per_graph- a list of lists where each sublist contains the ops that are run in the graph.\nops_per_graph\nTo throw an error on the first graph break encountered, use thefullgraphmode. This mode disables TorchDynamo\u2019s Python fallback, and only\nsucceeds if the entire program is convertible into a single graph. Example\nusage:\nfullgraph\n\n```python\ndef toy_example(a, b):\n   ...\n\ncompiled_toy = torch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)\n\n```\n\n\n## Excessive Recompilation#\n\nWhen TorchDynamo compiles a function (or part of one), it makes certain\nassumptions about locals and globals in order to allow compiler\noptimizations, and expresses these assumptions as guards that check\nparticular values at runtime. If any of these guards fail, Dynamo will\nrecompile that function (or part) up totorch._dynamo.config.recompile_limittimes. If your program is\nhitting the cache limit, you will first need to determine which guard is\nfailing and what part of your program is triggering it.\ntorch._dynamo.config.recompile_limit\nIf your program exhibits a bounded amount of dynamism, you may be able\nto tune the TorchDynamo cache limit to allow for each variation to be\ncompiled and cached, but if the cache limit is too high you may find the\ncost of recompilation outweighs any optimization benefits.\n\n```python\ntorch._dynamo.config.recompile_limit = <your desired cache limit>\n\n```\n\nTorchDynamo plans to support many common cases of dynamic tensor shapes,\nsuch as varying batch size or sequence length. It does not plan to\nsupport rank-dynamism. In the meantime, setting a specific cache limit\ncan be used in coordination with bucketing techniques to achieve an\nacceptable number of recompilations for some dynamic models.\n\n## Accuracy Debugging#\n\nAccuracy issues can also be minified if you set the environment variableTORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect\nmodel and a full repro might be something likeTORCHDYNAMO_REPRO_AFTER=\"aot\"TORCHDYNAMO_REPRO_LEVEL=4the reason\nwe need this is downstream compilers will codegen code whether it\u2019s\nTriton code or the C++ backend, the numerics from those downstream\ncompilers can be different in subtle ways yet have dramatic impact on\nyour training stability. So the accuracy debugger is very useful for us\nto detect bugs in our codegen or with a backend compiler.\nTORCHDYNAMO_REPRO_LEVEL=4\nTORCHDYNAMO_REPRO_AFTER=\"aot\"TORCHDYNAMO_REPRO_LEVEL=4\nIf you\u2019d like to ensure that random number generation is the same across both torch\nand triton then you can enabletorch._inductor.config.fallback_random=True\ntorch._inductor.config.fallback_random=True\n\n## Extended Debugging#\n\nExtended debugging can be enabled by using the following experimental flags.\nTORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED- provides extended debug information if the\nstring representation of a guard matches this flag value. For example, set it to\n\u201cNe(s0, 10)\u201d to generate full Python and C++ backtrace whenever guard was issued.TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL- provides extended debug information when\na particular symbol is allocated. For example, set this to \u201cu2\u201d to generate full Python\nand C++ backtrace whenever this symbol was created.TORCHDYNAMO_EXTENDED_DEBUG_CPP- provides extended debug information (C++ backtrace)\nfor all extended debug settings as well as errors. For example, set this to \u201c1\u201d. The C++\nbacktrace is slow and very spammy so it is not included by default with extended debugging.\nTORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED\nTORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL\nTORCHDYNAMO_EXTENDED_DEBUG_CPP\n\n## Cold Start Timing and Cache Corruption Debugging#\n\nIn order to measure the cold start compilation time or debug a cache corruption,\nit is possible passTORCHINDUCTOR_FORCE_DISABLE_CACHES=1or settorch.compiler.config.force_disable_caches=Truewhich will override any\nother caching config option and disable all compile time caching.\nTORCHINDUCTOR_FORCE_DISABLE_CACHES=1\ntorch.compiler.config.force_disable_caches=True",
    "url": "https://pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e0a01e4be2726810d4144c3ce04b9d42",
    "source": "pytorch_docs",
    "title": "torch.mtia.memory \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.mtia.memory#\n\nCreated On: Dec 09, 2024 | Last Updated On: Jul 28, 2025\nThe MTIA backend is implemented out of the tree, only interfaces are be defined here.\nThis package adds support for device memory management implemented in MTIA.\nmemory_stats\n\nmemory_stats\nReturn a dictionary of MTIA memory allocator statistics for a given device.\nmemory_allocated\n\nmemory_allocated\nReturn the current MTIA memory occupied by tensors in bytes for a given device.",
    "url": "https://pytorch.org/docs/stable/mtia.memory.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0da84cb441e2e3c7d7f8601ec16d74bc",
    "source": "pytorch_docs",
    "title": "torch.export-based ONNX Exporter \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.export-based ONNX Exporter#\n\nCreated On: Jun 10, 2025 | Last Updated On: Aug 22, 2025\nOverview\nDependencies\nA simple example\nInspecting the ONNX model using GUI\nWhen the conversion fails\nMetadata\nAPI Reference\n\n## Overview#\n\ntorch.exportengine is leveraged to produce a traced graph representing only the Tensor computation of the function in an\nAhead-of-Time (AOT) fashion. The resulting traced graph (1) produces normalized operators in the functional\nATen operator set (as well as any user-specified custom operators), (2) has eliminated all Python control\nflow and data structures (with certain exceptions), and (3) records the set of shape constraints needed to\nshow that this normalization and control-flow elimination is sound for future inputs, before it is finally\ntranslated into an ONNX graph.\nIn addition, during the export process, memory usage is significantly reduced.\n\n## Dependencies#\n\nThe ONNX exporter depends on extra Python packages:\nONNX\nONNX Script\nThey can be installed throughpip:\n\n```python\n  pip install --upgrade onnx onnxscript\n\n```\n\nonnxruntimecan then be used to execute the model\non a large variety of processors.\n\n## A simple example#\n\nSee below a demonstration of exporter API in action with a simple Multilayer Perceptron (MLP) as example:\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass MLPModel(nn.Module):\n  def __init__(self):\n      super().__init__()\n      self.fc0 = nn.Linear(8, 8, bias=True)\n      self.fc1 = nn.Linear(8, 4, bias=True)\n      self.fc2 = nn.Linear(4, 2, bias=True)\n      self.fc3 = nn.Linear(2, 2, bias=True)\n      self.fc_combined = nn.Linear(8 + 8 + 8, 8, bias=True)  # Combine all inputs\n\n  def forward(self, tensor_x: torch.Tensor, input_dict: dict, input_list: list):\n      \"\"\"\n      Forward method that requires all inputs:\n      - tensor_x: A direct tensor input.\n      - input_dict: A dictionary containing the tensor under the key 'tensor_x'.\n      - input_list: A list where the first element is the tensor.\n      \"\"\"\n      # Extract tensors from inputs\n      dict_tensor = input_dict['tensor_x']\n      list_tensor = input_list[0]\n\n      # Combine all inputs into a single tensor\n      combined_tensor = torch.cat([tensor_x, dict_tensor, list_tensor], dim=1)\n\n      # Process the combined tensor through the layers\n      combined_tensor = self.fc_combined(combined_tensor)\n      combined_tensor = torch.sigmoid(combined_tensor)\n      combined_tensor = self.fc0(combined_tensor)\n      combined_tensor = torch.sigmoid(combined_tensor)\n      combined_tensor = self.fc1(combined_tensor)\n      combined_tensor = torch.sigmoid(combined_tensor)\n      combined_tensor = self.fc2(combined_tensor)\n      combined_tensor = torch.sigmoid(combined_tensor)\n      output = self.fc3(combined_tensor)\n      return output\n\nmodel = MLPModel()\n\n# Example inputs\ntensor_input = torch.rand((97, 8), dtype=torch.float32)\ndict_input = {'tensor_x': torch.rand((97, 8), dtype=torch.float32)}\nlist_input = [torch.rand((97, 8), dtype=torch.float32)]\n\n# The input_names and output_names are used to identify the inputs and outputs of the ONNX model\ninput_names = ['tensor_input', 'tensor_x', 'list_input_index_0']\noutput_names = ['output']\n\n# Exporting the model with all required inputs\nonnx_program = torch.onnx.export(model,(tensor_input, dict_input, list_input), dynamic_shapes=({0: \"batch_size\"},{\"tensor_x\": {0: \"batch_size\"}},[{0: \"batch_size\"}]), input_names=input_names, output_names=output_names, dynamo=True,)\n\n# Check the exported ONNX model is dynamic\nassert onnx_program.model.graph.inputs[0].shape == (\"batch_size\", 8)\nassert onnx_program.model.graph.inputs[1].shape == (\"batch_size\", 8)\nassert onnx_program.model.graph.inputs[2].shape == (\"batch_size\", 8)\n\n```\n\nAs the code above shows, all you need is to providetorch.onnx.export()with an instance of the model and its input.\nThe exporter will then return an instance oftorch.onnx.ONNXProgramthat contains the exported ONNX graph along with extra information.\ntorch.onnx.export()\ntorch.onnx.ONNXProgram\nThe in-memory model available throughonnx_program.model_protois anonnx.ModelProtoobject in compliance with theONNX IR spec.\nThe ONNX model may then be serialized into aProtobuf fileusing thetorch.onnx.ONNXProgram.save()API.\nonnx_program.model_proto\nonnx.ModelProto\ntorch.onnx.ONNXProgram.save()\n\n```python\n  onnx_program.save(\"mlp.onnx\")\n\n```\n\n\n## Inspecting the ONNX model using GUI#\n\nYou can view the exported model usingNetron.\n\n## When the conversion fails#\n\nFunctiontorch.onnx.export()should be called a second time with\nparameterreport=True. A markdown report is generated to help the user\nto resolve the issue.\ntorch.onnx.export()\nreport=True\n\n## Metadata#\n\nDuring ONNX export, each ONNX node is annotated with metadata that helps trace its origin and context from the original PyTorch model. This metadata is useful for debugging, model inspection, and understanding the mapping between PyTorch and ONNX graphs.\nThe following metadata fields are added to each ONNX node:\nnamespace\nA string representing the hierarchical namespace of the node, consisting of a stack trace of modules/methods.\nExample:__main__.SimpleAddModel/add:aten.add.Tensor\n__main__.SimpleAddModel/add:aten.add.Tensor\npkg.torch.onnx.class_hierarchy\nA list of class names representing the hierarchy of modules leading to this node.\nExample:['__main__.SimpleAddModel','aten.add.Tensor']\n['__main__.SimpleAddModel','aten.add.Tensor']\npkg.torch.onnx.fx_node\nThe string representation of the original FX node, including its name, number of consumers, the targeted torch op, arguments, and keyword arguments.\nExample:%cat:[num_users=1]=call_function[target=torch.ops.aten.cat.default](args=([%tensor_x,%input_dict_tensor_x,%input_list_0],1),kwargs={})\n%cat:[num_users=1]=call_function[target=torch.ops.aten.cat.default](args=([%tensor_x,%input_dict_tensor_x,%input_list_0],1),kwargs={})\npkg.torch.onnx.name_scopes\nA list of name scopes (methods) representing the path to this node in the PyTorch model.\nExample:['','add']\n['','add']\npkg.torch.onnx.stack_trace\nThe stack trace from the original code where this node was created, if available.\nExample:\n\n```python\nFile \"simpleadd.py\", line 7, in forward\n    return torch.add(x, y)\n\n```\n\nThese metadata fields are stored in the metadata_props attribute of each ONNX node and can be inspected using Netron or programmatically.\nThe overall ONNX graph has the followingmetadata_props:\nmetadata_props\npkg.torch.export.ExportedProgram.graph_signature\nThis property contains a string representation of the graph_signature from the original PyTorch ExportedProgram. The graph signature describes the structure of the model\u2019s inputs and outputs and how they map to the ONNX graph. The inputs are defined asInputSpecobjects, which include the kind of input (e.g.,InputKind.PARAMETERfor parameters,InputKind.USER_INPUTfor user-defined inputs), the argument name, the target (which can be a specific node in the model), and whether the input is persistent. The outputs are defined asOutputSpecobjects, which specify the kind of output (e.g.,OutputKind.USER_OUTPUT) and the argument name.\nInputSpec\nInputKind.PARAMETER\nInputKind.USER_INPUT\nOutputSpec\nOutputKind.USER_OUTPUT\nTo read more about the graph signature, please see thetorch.exportfor more information.\npkg.torch.export.ExportedProgram.range_constraints\nThis property contains a string representation of any range constraints that were present in the original PyTorch ExportedProgram. Range constraints specify valid ranges for symbolic shapes or values in the model, which can be important for models that use dynamic shapes or symbolic dimensions.\nExample:s0:VR[2,int_oo], which indicates that the size of the input tensor must be at least 2.\ns0:VR[2,int_oo]\nTo read more about range constraints, please see thetorch.exportfor more information.\nEach input value in the ONNX graph may have the following metadata property:\npkg.torch.export.graph_signature.InputSpec.kind\nThe kind of input, as defined by PyTorch\u2019s InputKind enum.\nExample values:\n\u201cUSER_INPUT\u201d: A user-provided input to the model.\n\u201cPARAMETER\u201d: A model parameter (e.g., weight).\n\u201cBUFFER\u201d: A model buffer (e.g., running mean in BatchNorm).\n\u201cCONSTANT_TENSOR\u201d: A constant tensor argument.\n\u201cCUSTOM_OBJ\u201d: A custom object input.\n\u201cTOKEN\u201d: A token input.\npkg.torch.export.graph_signature.InputSpec.persistent\nIndicates whether the input is persistent (i.e., should be saved as part of the model\u2019s state).\nExample values:\n\u201cTrue\u201d\n\u201cFalse\u201d\nEach output value in the ONNX graph may have the following metadata property:\npkg.torch.export.graph_signature.OutputSpec.kind\nThe kind of input, as defined by PyTorch\u2019s OutputKind enum.\nExample values:\n\u201cUSER_OUTPUT\u201d: A user-visible output.\n\u201cLOSS_OUTPUT\u201d: A loss value output.\n\u201cBUFFER_MUTATION\u201d: Indicates a buffer was mutated.\n\u201cGRADIENT_TO_PARAMETER\u201d: Gradient output for a parameter.\n\u201cGRADIENT_TO_USER_INPUT\u201d: Gradient output for a user input.\n\u201cUSER_INPUT_MUTATION\u201d: Indicates a user input was mutated.\n\u201cTOKEN\u201d: A token output.\nEach initialized value, input, output has the following metadata:\npkg.torch.onnx.original_node_name\nThe original name of the node in the PyTorch FX graph that produced this value in the case where the value was renamed. This helps trace initializers back to their source in the original model.\nExample:fc1.weight\nfc1.weight\n\n## API Reference#\n\nExports a model into ONNX format.\nSettingdynamo=Trueenables the new ONNX export logic\nwhich is based ontorch.export.ExportedProgramand a more modern\nset of translation logic. This is the recommended and default way to export models\nto ONNX.\ndynamo=True\ntorch.export.ExportedProgram\nWhendynamo=True:\ndynamo=True\nThe exporter tries the following strategies to get an ExportedProgram for conversion to ONNX.\nIf the model is already an ExportedProgram, it will be used as-is.\nUsetorch.export.export()and setstrict=False.\ntorch.export.export()\nstrict=False\nUsetorch.export.export()and setstrict=True.\ntorch.export.export()\nstrict=True\nmodel(torch.nn.Module|torch.export.ExportedProgram|torch.jit.ScriptModule|torch.jit.ScriptFunction) \u2013 The model to be exported.\nargs(tuple[Any,...]) \u2013 Example positional inputs. Any non-Tensor arguments will be hard-coded into the\nexported model; any Tensor arguments will become inputs of the exported model,\nin the order they occur in the tuple.\nf(str|os.PathLike|None) \u2013 Path to the output ONNX model file. E.g. \u201cmodel.onnx\u201d. This argument is kept for\nbackward compatibility. It is recommended to leave unspecified (None)\nand use the returnedtorch.onnx.ONNXProgramto serialize the model\nto a file instead.\ntorch.onnx.ONNXProgram\nkwargs(dict[str,Any]|None) \u2013 Optional example keyword inputs.\nverbose(bool|None) \u2013 Whether to enable verbose logging.\ninput_names(Sequence[str]|None) \u2013 names to assign to the input nodes of the graph, in order.\noutput_names(Sequence[str]|None) \u2013 names to assign to the output nodes of the graph, in order.\nopset_version(int|None) \u2013 The version of thedefault (ai.onnx) opsetto target. You should setopset_versionaccording to the supported opset versions\nof the runtime backend or compiler you want to run the exported model with.\nLeave as default (None) to use the recommended version, or refer to\nthe ONNX operators documentation for more information.\nopset_version\nNone\ndynamo(bool) \u2013 Whether to export the model withtorch.exportExportedProgram instead of TorchScript.\ntorch.export\nexternal_data(bool) \u2013 Whether to save the model weights as an external data file.\nThis is required for models with large weights that exceed the ONNX file size limit (2GB).\nWhen False, the weights are saved in the ONNX file with the model architecture.\ndynamic_shapes(dict[str,Any]|tuple[Any,...]|list[Any]|None) \u2013 A dictionary or a tuple of dynamic shapes for the model inputs. Refer totorch.export.export()for more details. This is only used (and preferred) when dynamo is True.\nNote that dynamic_shapes is designed to be used when the model is exported with dynamo=True, while\ndynamic_axes is used when dynamo=False.\ntorch.export.export()\ncustom_translation_table(dict[Callable,Callable|Sequence[Callable]]|None) \u2013 A dictionary of custom decompositions for operators in the model.\nThe dictionary should have the callable target in the fx Node as the key (e.g.torch.ops.aten.stft.default),\nand the value should be a function that builds that graph using ONNX Script. This option\nis only valid when dynamo is True.\ntorch.ops.aten.stft.default\nreport(bool) \u2013 Whether to generate a markdown report for the export process. This option\nis only valid when dynamo is True.\noptimize(bool) \u2013 Whether to optimize the exported model. This option\nis only valid when dynamo is True. Default is True.\nverify(bool) \u2013 Whether to verify the exported model using ONNX Runtime. This option\nis only valid when dynamo is True.\nprofile(bool) \u2013 Whether to profile the export process. This option\nis only valid when dynamo is True.\ndump_exported_program(bool) \u2013 Whether to dump thetorch.export.ExportedProgramto a file.\nThis is useful for debugging the exporter. This option is only valid when dynamo is True.\ntorch.export.ExportedProgram\nartifacts_dir(str|os.PathLike) \u2013 The directory to save the debugging artifacts like the report and the serialized\nexported program. This option is only valid when dynamo is True.\nfallback(bool) \u2013 Whether to fallback to the TorchScript exporter if the dynamo exporter fails.\nThis option is only valid when dynamo is True. When fallback is enabled, It is\nrecommended to set dynamic_axes even when dynamic_shapes is provided.\nexport_params(bool) \u2013When ``f`` is specified: If false, parameters (weights) will not be exported.You can also leave it unspecified and use the returnedtorch.onnx.ONNXProgramto control how initializers are treated when serializing the model.\nWhen ``f`` is specified: If false, parameters (weights) will not be exported.\nYou can also leave it unspecified and use the returnedtorch.onnx.ONNXProgramto control how initializers are treated when serializing the model.\ntorch.onnx.ONNXProgram\nkeep_initializers_as_inputs(bool) \u2013When ``f`` is specified: If True, all the\ninitializers (typically corresponding to model weights) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe user inputs are added as inputs.Set this to True if you intend to supply model weights at runtime.\nSet it to False if the weights are static to allow for better optimizations\n(e.g. constant folding) by backends/runtimes.You can also leave it unspecified and use the returnedtorch.onnx.ONNXProgramto control how initializers are treated when serializing the model.\nWhen ``f`` is specified: If True, all the\ninitializers (typically corresponding to model weights) in the\nexported graph will also be added as inputs to the graph. If False,\nthen initializers are not added as inputs to the graph, and only\nthe user inputs are added as inputs.\nSet this to True if you intend to supply model weights at runtime.\nSet it to False if the weights are static to allow for better optimizations\n(e.g. constant folding) by backends/runtimes.\nYou can also leave it unspecified and use the returnedtorch.onnx.ONNXProgramto control how initializers are treated when serializing the model.\ntorch.onnx.ONNXProgram\ndynamic_axes(Mapping[str,Mapping[int,str]]|Mapping[str,Sequence[int]]|None) \u2013Prefer specifyingdynamic_shapeswhendynamo=Trueand whenfallbackis not enabled.By default the exported model will have the shapes of all input and output tensors\nset to exactly match those given inargs. To specify axes of tensors as\ndynamic (i.e. known only at run-time), setdynamic_axesto a dict with schema:KEY (str): an input or output name. Each name must also be provided ininput_namesoroutput_names.VALUE (dict or list): If a dict, keys are axis indices and values are axis names. If alist, each element is an axis index.For example:classSumModule(torch.nn.Module):defforward(self,x):returntorch.sum(x,dim=1)torch.onnx.export(SumModule(),(torch.ones(2,2),),\"onnx.pb\",input_names=[\"x\"],output_names=[\"sum\"],)Produces:input{name:\"x\"...shape{dim{dim_value:2# axis 0}dim{dim_value:2# axis 1...output{name:\"sum\"...shape{dim{dim_value:2# axis 0...While:torch.onnx.export(SumModule(),(torch.ones(2,2),),\"onnx.pb\",input_names=[\"x\"],output_names=[\"sum\"],dynamic_axes={# dict value: manually named axes\"x\":{0:\"my_custom_axis_name\"},# list value: automatic names\"sum\":[0],},)Produces:input{name:\"x\"...shape{dim{dim_param:\"my_custom_axis_name\"# axis 0}dim{dim_value:2# axis 1...output{name:\"sum\"...shape{dim{dim_param:\"sum_dynamic_axes_1\"# axis 0...\nPrefer specifyingdynamic_shapeswhendynamo=Trueand whenfallbackis not enabled.\ndynamic_shapes\ndynamo=True\nfallback\nBy default the exported model will have the shapes of all input and output tensors\nset to exactly match those given inargs. To specify axes of tensors as\ndynamic (i.e. known only at run-time), setdynamic_axesto a dict with schema:\nargs\ndynamic_axes\ninput_names\noutput_names.\noutput_names\nlist, each element is an axis index.\nFor example:\n\n```python\nclass SumModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.sum(x, dim=1)\n\n\ntorch.onnx.export(\n    SumModule(),\n    (torch.ones(2, 2),),\n    \"onnx.pb\",\n    input_names=[\"x\"],\n    output_names=[\"sum\"],\n)\n\n```\n\nProduces:\n\n```python\ninput {\n  name: \"x\"\n  ...\n      shape {\n        dim {\n          dim_value: 2  # axis 0\n        }\n        dim {\n          dim_value: 2  # axis 1\n...\noutput {\n  name: \"sum\"\n  ...\n      shape {\n        dim {\n          dim_value: 2  # axis 0\n...\n\n```\n\nWhile:\n\n```python\ntorch.onnx.export(\n    SumModule(),\n    (torch.ones(2, 2),),\n    \"onnx.pb\",\n    input_names=[\"x\"],\n    output_names=[\"sum\"],\n    dynamic_axes={\n        # dict value: manually named axes\n        \"x\": {0: \"my_custom_axis_name\"},\n        # list value: automatic names\n        \"sum\": [0],\n    },\n)\n\n```\n\nProduces:\n\n```python\ninput {\n  name: \"x\"\n  ...\n      shape {\n        dim {\n          dim_param: \"my_custom_axis_name\"  # axis 0\n        }\n        dim {\n          dim_value: 2  # axis 1\n...\noutput {\n  name: \"sum\"\n  ...\n      shape {\n        dim {\n          dim_param: \"sum_dynamic_axes_1\"  # axis 0\n...\n\n```\n\ntraining(_C_onnx.TrainingMode) \u2013 Deprecated option. Instead, set the training mode of the model before exporting.\noperator_export_type(_C_onnx.OperatorExportTypes) \u2013 Deprecated option. Only ONNX is supported.\ndo_constant_folding(bool) \u2013 Deprecated option.\ncustom_opsets(Mapping[str,int]|None) \u2013 Deprecated option.\nexport_modules_as_functions(bool|Collection[type[torch.nn.Module]]) \u2013 Deprecated option.\nautograd_inlining(bool) \u2013 Deprecated option.\ntorch.onnx.ONNXProgramif dynamo is True, otherwise None.\ntorch.onnx.ONNXProgram\nONNXProgram| None\nChanged in version 2.6:trainingis now deprecated. Instead, set the training mode of the model before exporting.operator_export_typeis now deprecated. Only ONNX is supported.do_constant_foldingis now deprecated. It is always enabled.export_modules_as_functionsis now deprecated.autograd_inliningis now deprecated.\nChanged in version 2.7:optimizeis now True by default.\nChanged in version 2.9:dynamois now True by default.\nA class to represent an ONNX program that is callable with torch tensors.\nmodel\u2013 The ONNX model as an ONNX IR model object.\nexported_program\u2013 The exported program that produced the ONNX model.\nApply the weights from the specified state dict to the ONNX model.\nUse this method to replace FakeTensors or other weights.\nstate_dict(dict[str,torch.Tensor]) \u2013 The state dict containing the weights to apply to the ONNX model.\nRun the ONNX model using the reference backend.\nSequence[Tensor]\nCompute the values of the specified names in the ONNX model.\nThis method is used to compute the values of the specified names in the ONNX model.\nThe values are returned as a dictionary mapping names to tensors.\nvalue_names(Sequence[str]) \u2013 The names of the values to compute.\nA dictionary mapping names to tensors.\nSequence[Tensor]\nInitialize the ONNX Runtime inference session.\ninitializer(Callable[[str|bytes],ort.InferenceSession]) \u2013 The function to initialize the ONNX Runtime inference\nsession with the specified model. By default, it uses the_ort_session_initializer()function.\n_ort_session_initializer()\nReturn the ONNXModelProtoobject.\nModelProto\nOptimize the ONNX model.\nThis method optimizes the ONNX model by performing constant folding and\neliminating redundancies in the graph. The optimization is done in-place.\nRelease the inference session.\nYou may call this method to release the resources used by the inference session.\nSave the ONNX model to the specified destination.\nWhenexternal_dataisTrueor the model is larger than 2GB,\nthe weights are saved as external data in a separate file.\nexternal_data\nTrue\nInitializer (model weights) serialization behaviors:\ninclude_initializers=True,keep_initializers_as_inputs=False(default):\nThe initializers are included in the saved model.\ninclude_initializers=True\nkeep_initializers_as_inputs=False\ninclude_initializers=True,keep_initializers_as_inputs=True:\nThe initializers are included in the saved model and kept as model inputs.\nChoose this option if you want the ability to override the model weights\nduring inference.\ninclude_initializers=True\nkeep_initializers_as_inputs=True\ninclude_initializers=False,keep_initializers_as_inputs=False:\nThe initializers are not included in the saved model and are not listed\nas model inputs. Choose this option if you want to attach the initializers\nto the ONNX model in a separate, post-processing, step.\ninclude_initializers=False\nkeep_initializers_as_inputs=False\ninclude_initializers=False,keep_initializers_as_inputs=True:\nThe initializers are not included in the saved model but are listed as model\ninputs. Choose this option if you want to supply the initializers during\ninference and want to minimize the size of the saved model.\ninclude_initializers=False\nkeep_initializers_as_inputs=True\ndestination(str|os.PathLike) \u2013 The path to save the ONNX model to.\ninclude_initializers(bool) \u2013 Whether to include the initializers in the saved model.\nkeep_initializers_as_inputs(bool) \u2013 Whether to keep the initializers as inputs in the saved model.\nIfTrue, the initializers are added as inputs to the model which means they can be overwritten.\nby providing the initializers as model inputs.\nexternal_data(Optional[bool]) \u2013 Whether to save the weights as external data in a separate file.\nTypeError\u2013 Ifexternal_dataisTrueanddestinationis not a file path.\nexternal_data\nTrue\ndestination\nReturns whether it is in the middle of ONNX export.\nbool\nErrors raised by the ONNX exporter. This is the base class for all exporter errors.",
    "url": "https://pytorch.org/docs/stable/onnx_export.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "dc05bc37cace212b3ec5d7b17c3a5186",
    "source": "pytorch_docs",
    "title": "Reproducibility \u2014 PyTorch 2.9 documentation",
    "text": "\n## Reproducibility#\n\nCreated On: Sep 11, 2018 | Last Updated On: Nov 26, 2024\nCompletely reproducible results are not guaranteed across PyTorch releases,\nindividual commits, or different platforms. Furthermore, results may not be\nreproducible between CPU and GPU executions, even when using identical seeds.\nHowever, there are some steps you can take to limit the number of sources of\nnondeterministic behavior for a specific platform, device, and PyTorch release.\nFirst, you can control sources of randomness that can cause multiple executions\nof your application to behave differently. Second, you can configure PyTorch\nto avoid using nondeterministic algorithms for some operations, so that multiple\ncalls to those operations, given the same inputs, will produce the same result.\nWarning\nDeterministic operations are often slower than nondeterministic operations, so\nsingle-run performance may decrease for your model. However, determinism may\nsave time in development by facilitating experimentation, debugging, and\nregression testing.\n\n## Controlling sources of randomness#\n\n\n## PyTorch random number generator#\n\nYou can usetorch.manual_seed()to seed the RNG for all devices (both\nCPU and CUDA):\ntorch.manual_seed()\n\n```python\nimport torch\ntorch.manual_seed(0)\n\n```\n\nSome PyTorch operations may use random numbers internally.torch.svd_lowrank()does this, for instance. Consequently, calling it\nmultiple times back-to-back with the same input arguments may give different\nresults. However, as long astorch.manual_seed()is set to a constant\nat the beginning of an application and all other sources of nondeterminism have\nbeen eliminated, the same series of random numbers will be generated each time\nthe application is run in the same environment.\ntorch.svd_lowrank()\ntorch.manual_seed()\nIt is also possible to obtain identical results from an operation that uses\nrandom numbers by settingtorch.manual_seed()to the same value between\nsubsequent calls.\ntorch.manual_seed()\n\n## Python#\n\nFor custom operators, you might need to set python seed as well:\n\n```python\nimport random\nrandom.seed(0)\n\n```\n\n\n## Random number generators in other libraries#\n\nIf you or any of the libraries you are using rely on NumPy, you can seed the global\nNumPy RNG with:\n\n```python\nimport numpy as np\nnp.random.seed(0)\n\n```\n\nHowever, some applications and libraries may use NumPy Random Generator objects,\nnot the global RNG\n(https://numpy.org/doc/stable/reference/random/generator.html), and those will\nneed to be seeded consistently as well.\nIf you are using any other libraries that use random number generators, refer to\nthe documentation for those libraries to see how to set consistent seeds for them.\n\n## CUDA convolution benchmarking#\n\nThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism\nacross multiple executions of an application. When a cuDNN convolution is called with a\nnew set of size parameters, an optional feature can run multiple convolution algorithms,\nbenchmarking them to find the fastest one. Then, the fastest algorithm will be used\nconsistently during the rest of the process for the corresponding set of size parameters.\nDue to benchmarking noise and different hardware, the benchmark may select different\nalgorithms on subsequent runs, even on the same machine.\nDisabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced\nperformance.\ntorch.backends.cudnn.benchmark=False\nHowever, if you do not need reproducibility across multiple executions of your application,\nthen performance might improve if the benchmarking feature is enabled withtorch.backends.cudnn.benchmark=True.\ntorch.backends.cudnn.benchmark=True\nNote that this setting is different from thetorch.backends.cudnn.deterministicsetting discussed below.\ntorch.backends.cudnn.deterministic\n\n## Avoiding nondeterministic algorithms#\n\ntorch.use_deterministic_algorithms()lets you configure PyTorch to use\ndeterministic algorithms instead of nondeterministic ones where available, and\nto throw an error if an operation is known to be nondeterministic (and without\na deterministic alternative).\ntorch.use_deterministic_algorithms()\nPlease check the documentation fortorch.use_deterministic_algorithms()for a full list of affected operations. If an operation does not act correctly\naccording to the documentation, or if you need a deterministic implementation\nof an operation that does not have one, please submit an issue:pytorch/pytorch#issues\ntorch.use_deterministic_algorithms()\nFor example, running the nondeterministic CUDA implementation oftorch.Tensor.index_add_()will throw an error:\ntorch.Tensor.index_add_()\n\n```python\n>>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\n'torch.use_deterministic_algorithms(True)'. ...\n\n```\n\nWhentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a\nnondeterministic algorithm, but when the deterministic flag is turned on, its alternate\ndeterministic implementation will be used:\ntorch.bmm()\n\n```python\n>>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\ntensor([[[ 1.1900, -2.3409],\n         [ 0.4796,  0.8003]],\n        [[ 0.1509,  1.8027],\n         [ 0.0333, -1.1444]]], device='cuda:0')\n\n```\n\nFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you\nshould set the environment variableCUBLAS_WORKSPACE_CONFIGaccording to CUDA documentation:https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n\n## CUDA convolution determinism#\n\nWhile disabling CUDA convolution benchmarking (discussed above) ensures that\nCUDA selects the same algorithm each time an application is run, that algorithm\nitself may be nondeterministic, unless eithertorch.use_deterministic_algorithms(True)ortorch.backends.cudnn.deterministic=Trueis set. The latter setting\ncontrols only this behavior, unliketorch.use_deterministic_algorithms()which will make other PyTorch operations behave deterministically, too.\ntorch.use_deterministic_algorithms(True)\ntorch.backends.cudnn.deterministic=True\ntorch.use_deterministic_algorithms()\n\n## CUDA RNN and LSTM#\n\nIn some versions of CUDA, RNNs and LSTM networks may have non-deterministic behavior.\nSeetorch.nn.RNN()andtorch.nn.LSTM()for details and workarounds.\ntorch.nn.RNN()\ntorch.nn.LSTM()\n\n## Filling uninitialized memory#\n\nOperations liketorch.empty()andtorch.Tensor.resize_()can return\ntensors with uninitialized memory that contain undefined values. Using such a\ntensor as an input to another operation is invalid if determinism is required,\nbecause the output will be nondeterministic. But there is nothing to actually\nprevent such invalid code from being run. So for safety,torch.utils.deterministic.fill_uninitialized_memoryis set toTrueby default, which will fill the uninitialized memory with a known value iftorch.use_deterministic_algorithms(True)is set. This will prevent the\npossibility of this kind of nondeterministic behavior.\ntorch.empty()\ntorch.Tensor.resize_()\ntorch.utils.deterministic.fill_uninitialized_memory\nTrue\ntorch.use_deterministic_algorithms(True)\nHowever, filling uninitialized memory is detrimental to performance. So if your\nprogram is valid and does not use uninitialized memory as the input to an\noperation, then this setting can be turned off for better performance.\n\n## DataLoader#\n\nDataLoader will reseed workers followingRandomness in multi-process data loadingalgorithm.\nUseworker_init_fn()andgeneratorto preserve reproducibility:\nworker_init_fn()\n\n```python\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker,\n    generator=g,\n)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/notes/randomness.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "15ecbb44094de90860e4c681f8608bd2",
    "source": "pytorch_docs",
    "title": "PyTorch documentation \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch documentation#\n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\nFeatures described in this documentation are classified by release status:\nStable (API-Stable):These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).\nUnstable (API-Unstable):Encompasses all features that are under active development where APIs may change based on user feedback, requisite performance improvements or because coverage across operators is not yet complete.\nThe APIs and performance characteristics of these features may change.\n\n## Indices and tables#\n\nIndex\nModule Index",
    "url": "https://pytorch.org/docs/stable/",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c74f4c60b05d87bd7208dccace928b9a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/threading_environment_variables.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "37f9149fd05e28f7f67b2679930ded42",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/mtia.memory.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "44e5c0e7315ac8842c7ce907d7b0db14",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/python.builtin.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b08a89c20d822b24cc95bbbc50220b33",
    "source": "pytorch_docs",
    "title": "Fake tensor \u2014 PyTorch 2.9 documentation",
    "text": "\n## Fake tensor#\n\nCreated On: May 19, 2023 | Last Updated On: Jun 13, 2025\nCode:fake_tensor.py\n\n## Motivation#\n\nWhen doing Dynamo symbolic evaluation and compiler passes, we often want to be able to run tensor operations to understand what output sizes/dtypes/devices are, without actually running those operations (or trashing preexisting tensors), which would be slower (if you\u2019re doing a lot of compute) and take a lot of memory (it\u2019s bad if your compiler needs to use GPU memory while you are compiling the program). A fake tensor is like a real tensor in all respects, except that it doesn\u2019t actually have any data. For example, when we do Dynamo tracing, we need to trace through user Tensor code and answer questions about intermediates (e.g., if a user does a conditional on an intermediate tensor). Without fake tensor, we would not have accurate information for these queries.\nSimilarly, suppose you want to store metadata for a tensor, e.g., on an FX IR node (meta[\u2018val\u2019]). You can instead store a fake tensor directly on the node, which will give you all the metadata you need for the tensor, including subtle stuff that you probably wouldn\u2019t have handled (e.g., aliasing relationships).\n\n## Related work#\n\nA meta tensor is a tensor with device=\u2019meta\u2019. This is actually a lot of what you want for fake tensor, but meta tensors don\u2019t model devices, and sometimes stride behavior varies depending on your device, so fake tensors really can get a lot more accurate info this way. Also, meta tensors are \u201cglobal\u201d (they exist on their own, similar to how a CPU/CUDA tensor exist on their own), whereas fake tensors are scoped to a FakeTensorMode.\nA tensor subclass lets you subclass torch.Tensor and customize their behavior. Fake tensors are implemented as a tensor subclass; that means almost all of its implementation lives in Python! For more simple examples of tensor subclasses check outsubclass_zoo.\nDynamic shapes allow you to create tensors with symbolic sizes rather than only concrete sizes, and propagate these sizes symbolically through operations. Dynamic shapes maintain state in a ShapeEnv, which is always associated with a FakeTensorMode (so fake tensors also are responsible for managing symbolic sizes.) In general, whenever we compile a subgraph with PT2, there is a tracing context associated with this compilation, which contains, among other things, a FakeTensorMode and (possibly) a ShapeEnv.\n\n## Overall architecture#\n\nAll fake tensors are associated with a FakeTensorMode. Because fake tensor\u2019s primary use case is to do analysis on real tensors, the general workflow is you have a bunch of real tensors, you allocate a FakeTensorMode, and then you use from_real_tensor to convert all those real tensors into fake tensors, and then you do things to the fake tensors. In particular, the FakeTensorMode maintains a memo table persistently mapping tensors (and storages) to the same storages. If you fakeify the same tensor multiple times, you will get the same fake tensor; if you fakeify two tensors which alias each other, you will get two fake tensors which alias the same fake storage. FakeTensors are tensor subclasses, so if you do operations on them, you\u2019ll automatically get a fake tensor, but in general you will want to do operations on fake tensors (e.g., if you\u2019re running an FX pass) with the FakeTensorMode active; what a tensor operation will do is automatically turn on the fake tensor mode and try again.\nA fake tensor is represented as a __torch_dispatch__ tensor subclass of a meta tensor. This means under the hood, fake tensors are meta device tensors; they then use extra extensibility hooks, specifically dispatch_device, to lie about what the actual device of the tensor is. This was one of the more error-prone parts of fake tensors in the early days: sometimes, fake tensors were too good at lying about being CPU/CUDA whatever, and you\u2019d end up with a CPU kernel getting called with a fake tensor trying to dereference the data pointer, which obviously won\u2019t work. If you are segfaulting in fake tensor code, this is the first thing you should check: is the C++ backtrace in a CPU kernel (unexpected!) or a meta kernel (expected!) A meta kernel is like a real kernel, but all it does is allocate the outputs, it doesn\u2019t do any data compute.\nA tensor subclass has to define how to implement various operations. Here is the general fake tensor recipe:\nRun the meta kernel on the input fake tensors, reinterpreting them as meta tensors. This is done via a magic context manager in_kernel_invocation_manager which instructs all of PyTorch to view fake tensors as their underlying meta tensors, rather than \u201cunwrapping\u201d fake tensors into meta tensors (a fake tensor is a meta tensor). Fake tensors are represented this way to avoid having to keep two sets of metadata in sync (the meta tensor\u2019s metadata, and the fake tensor\u2019s metadata); the \u201cis a\u201d relationship ensures there is only one canonical copy of metadata.\nIf you\u2019re a factory function, you\u2019ll instead call the underlying factory function with device=\u2019meta\u2019.\nConvert the resulting meta tensor into a fake tensor, computing what the output device of the tensor should be (this is usually trivial, but sometimes it is not, e.g., cpu scalar promotion, or device-converting operations.)\n\n## API: the important bits#\n\nNon-PT2 usage (check out test/test_fake_tensor.py for more examples):\n\n```python\n# Create a fake mode\nfrom torch._subclasses.fake_tensor import FakeTensorMode\nfake_mode = FakeTensorMode()\nconverter = fake_mode.fake_tensor_converter\n# Fakeify some real tensors\nfake_x = converter.from_real_tensor(fake_mode, x)\nwith fake_mode:\n    # Do some operations on the fake tensors\n    fake_y = fake_x * 2\n    # Factory operations automatically get fakeified in the context manager\n    fake_z = torch.empty(20)\n\n```\n\nQ: Why do you have real tensors as inputs?\nA: In a PT2 context, this is because you typically are compiling just-in-time, so for all the inputs to a graph you\u2019re compiling, you already have the \u201creal\u201d inputs, because you\u2019re compiling while you\u2019re executing the program.\nPT2 pre-AOTAutograd usage (this is unusual, you probably don\u2019t want to do this):\n\n```python\n# Fake mode is not enabled!\nfrom torch._guards import detect_fake_mode\nfake_mode = detect_fake_mode(args)\n# if fake_mode isn't None\nconverter = fake_mode.fake_tensor_converter\nfake_args = [converter.from_real_tensor(fake_mode, arg) for arg in args]\nwith fake_mode:\n    ... # do stuff with the fake args, if needed ...\n\n```\n\ndetect_fake_mode will search a number of locations to try to find \u201cthe\u201d fake tensor mode associated with the lifecycle. Typically it will be pulled off of the tracing context.\nPT2 post-AOTAutograd usage:\n\n```python\n# Fake mode is enabled! example_inputs is typically fake already\n# TODO: we probably want to change this\n# Still do this to access fake mode\nfake_mode = detect_fake_mode(example_inputs)\n# But in general you don't have to turn it on\n\n```\n\nOther useful stuff:\n\n```python\nfrom torch._subclasses.fake_tensor import unset_fake_temporarily\nwith unset_fake_temporarily():\n    ... # fake mode is disabled here, you can do real tensor compute\n\n```\n\nWhen might you want to disable fake tensor mode? Usually you don\u2019t want to do this. One niche case where we\u2019ve found it useful is to implement constant propagation on fake tensors: in this case, we need to do some actual tensor computation even though we\u2019re in a fake tensor mode.\n\n```python\nimport FakeTensorProp from torch.fx.passes.fake_tensor_prop\ngm: GraphModule\nreal_inputs: List[Tensor]\nFakeTensorProp(gm).propagate(*real_inputs)\n# This will populate meta['val'] on all the FX nodes with a fake tensor\n# or if you have a preexisting fake mode, you should use it\nFakeTensorProp(gm, mode=fake_mode).propagate(*real_inputs)\n# There is also propagate_dont_convert_inputs if your inputs are already fake\nfake_inputs: List[FakeTensor]\nFakeTensorProp(gm, mode=fake_mode).propagate_dont_convert_inputs(*fake_inputs)\n\n```\n\n\n## Details#\n\nAuto-convert or not?\nOriginally, FakeTensorMode would not automatically fakeify real tensors if you tried to do compute on them inside a FakeTensorMode region. The motivation behind this was to prevent the following footgun:\n\n```python\nwith FakeTensorMode():\n    real_tensor.t_()\n\n```\n\nWhat should this code do? It would be surprising if we actually modified the metadata on the real tensor. But at the same time, there isn\u2019t any obvious opportunity to create a FakeTensor. So we conservatively decided to make this raise an error: \u201cInvoking operators with non-Fake Tensor inputs in FakeTensorMode is not yet supported. Please convert all Tensors to FakeTensors first.\u201d\nThis error is pretty annoying in practice. For example, suppose you have a real nn.Module and you want to feed fake tensors through it. You need to somehow fakeify the nn.Module. This motivated FakeCopyMode.\nEventually, we gave up and added automatic fakeification. However, this is still not yet enabled by default in many uses of FakeTensorMode.\nMetadata mutation on fake tensor\nIf you have a fake tensor, and you t_() it, the metadata on the fake tensor changes. This is reasonable on its face, but sometimes you want to also store fake tensors as metadata on FX nodes; mutating a fake tensor is bad because this will invalidate old metadata!\nIn fact, there is a fundamental tension here, which is that fake tensors maintain extremely accurate metadata about tensors, up to and including object identity. If object metadata changes over time in an FX graph, there is not actually any way to represent this change over time. Most of the time, our serious FX analyses are done on functionalized graphs, which don\u2019t have this, but occasionally you need to do an analysis on a non-functionalized graph. Maybe it was a mistake to put fake tensor in meta[\u2018val\u2019]\n\n## About the tensor subclass#\n\nFake tensor uses both a subclass and a mode tensor subclass pattern, where FakeTensor.__torch_dispatch__ enables the FakeTensorMode associated with the fake tensor, and then redispatches (relying on FakeTensorMode to do the heavy lifting). If fake tensor operations get a subclass argument it doesn\u2019t recognize, it will return NotImplemented, giving the other subclass a chance to run first (hopefully desugaring into plain tensor operations), before it tries again. This can cause infinite loops.\n\n## How is each individual operator implemented?#\n\nUnfortunately, there is a pretty complicated set of places where any given operator may be implemented. Some important cases to know about:\nTensor subclasses support limited constant propagation if the number of elements is very small (this helps deal with some cases where we immediately call item() on such tensors.)\nWe have some fastpath implementations for certain operators, which are done entirely in fake tensor, for performance reasons.\nIf you use @custom_op to generate a custom tensor, these will register impl_abstract directly to fake tensor.\nFake tensor itself has some hardcoded special cases for device-converting operations.\nIf there is no meta implementation nor any decomposition, we will generate real zero-filled tensors and attempt to run the operator directly to find out what the results will be. This can cause segfaults if the operator attempts to do indexing with data, so we don\u2019t turn this on by default for custom ops.\n\n## How does the converter work?#\n\nBecause fake tensors are used in situations that are very sensitive to the exact properties of a tensor, fake tensors do conversion very carefully, preserving leaf-ness, requires_grad\u2019ness, aliasing, and a whole host of other properties. The bulk of the heavy lifting is in MetaConverter.\n\n## Performance characteristics#\n\nYou would think fake tensors are fast because they don\u2019t do any tensor compute. But at small tensor sizes we are actually entirely overhead bound, and, well, fake tensor is in Python, and we often do a LOT of work to do a single tensor operation (because they are implemented as decompositions). So fake tensors are actually pretty slow in practice, especially when symbolic shapes are involved. There are two important fastpaths we currently have in fake tensor that make a big difference in practice:\nPointwise ops don\u2019t go through PrimTorch decomps, instead we\u2019ve hand-coded their propagation rule.\nIf possible, we should.\n\n## Fake tensor of fake tensor?#\n\nThere is interest in sending fake tensors as user inputs into the PT2 stack, which would imply we would need to be able to create a fake tensor of a fake tensor. This isn\u2019t really supported right now, but maybe it would not be too difficult to do.\n\n## Interaction with dynamic shapes#\n\nEvery FakeTensorMode contains a ShapeEnv, which tracks all symbolic shapes information. Their lifetimes are typically tied: they live and die together.\nBecause FakeTensorMode has a ShapeEnv (but meta implementations do not), meta functions that are data-dependent and require allocating an unbacked SymInt live in fake tensor. Fake tensor also takes care of memoizing unbacked SymInts, so that, e.g., if you call nonzero() on the same fake tensor twice, you get the same symbolic size.\n\n## Other resources#\n\nColab Tutorial On Using FakeTensor To Determine Max Batch Size",
    "url": "https://pytorch.org/docs/stable/torch.compiler_fake_tensor.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8de8da9d65e5419ef29868dc44307be2",
    "source": "pytorch_docs",
    "title": "PYTORCH ProcessGroupNCCL Environment Variables \u2014 PyTorch 2.9 documentation",
    "text": "\n## PYTORCH ProcessGroupNCCL Environment Variables#\n\nCreated On: Jun 10, 2025 | Last Updated On: Jun 10, 2025\nFor more information on the environment variables, seeProcessGroupNCCL Environment Variables.\nVariable\nDescription\nTORCH_NCCL_ASYNC_ERROR_HANDLING\nTORCH_NCCL_ASYNC_ERROR_HANDLING\nControl how we perform Async Error Handling with NCCL when an exception is observed in watchdog. If set to 0, no handling of asynchronous NCCL errors. If set to 1, aborting NCCL communicator and tearing down process upon error. If set to 2, only abort NCCL communicator and if set to 3, tearing down process without aborting NCCL communicator. By default, it is set to 3.\nTORCH_NCCL_HIGH_PRIORITY\nTORCH_NCCL_HIGH_PRIORITY\nControl whether to use high priority stream for the NCCL communicator.\nTORCH_NCCL_BLOCKING_WAIT\nTORCH_NCCL_BLOCKING_WAIT\nControl whether or not wait() is blocking or non-blocking.\nTORCH_NCCL_DUMP_ON_TIMEOUT\nTORCH_NCCL_DUMP_ON_TIMEOUT\nControl whether dumping debug info on watchdog timeout or exception is detected. This variable must be set together with TORCH_NCCL_TRACE_BUFFER_SIZE larger than 0.\nTORCH_NCCL_DESYNC_DEBUG\nTORCH_NCCL_DESYNC_DEBUG\nControl whether Desync Debug is enabled. This is helpful in figuring out the culprit rank of collective desync.\nTORCH_NCCL_ENABLE_TIMING\nTORCH_NCCL_ENABLE_TIMING\nIf set to1, enable recording start-events for all ProcessGroupNCCL collectives, and compute accurate collective timing per-collective.\n1\nTORCH_NCCL_ENABLE_MONITORING\nTORCH_NCCL_ENABLE_MONITORING\nIf set to1,enable monitoring thread which aborts the process when the ProcessGroupNCCL Watchdog thread gets stuck and no heartbeat is detected after TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC. This can happen due to calling CUDA/NCCL APIs that may hang. It is Useful to prevent jobs being stuck for a prolonged time than necessary tying up cluster resources.\n1\nTORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\nTORCH_NCCL_HEARTBEAT_TIMEOUT_SEC\nControl the watchdog heartbeat timeout period after which the monitoring thread will abort the process.\nTORCH_NCCL_TRACE_BUFFER_SIZE\nTORCH_NCCL_TRACE_BUFFER_SIZE\nThe maximum number of events we store in the flight recorder\u2019s ring buffer. One event could be the start or end of a collective, for example. Set to 0 to disable the tracebuffer and debugging info dump.\nTORCH_NCCL_TRACE_CPP_STACK\nTORCH_NCCL_TRACE_CPP_STACK\nWhether to collect cpp stack traces for flight recorder. Default value is False.\nTORCH_NCCL_COORD_CHECK_MILSEC\nTORCH_NCCL_COORD_CHECK_MILSEC\nControl the interval inside the monitoring thread to check the coordinated signal from other ranks, e.g. to dump the debugging information. Default value is 1000 ms.\nTORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC\nTORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC\nControl how much extra time we will wait for dumping the debugging info before we exit and throws timeout exception.\nTORCH_NCCL_DEBUG_INFO_TEMP_FILE\nTORCH_NCCL_DEBUG_INFO_TEMP_FILE\nThe file into which the debugging info would be dumped.\nTORCH_NCCL_DEBUG_INFO_PIPE_FILE\nTORCH_NCCL_DEBUG_INFO_PIPE_FILE\nThe pipe file to trigger debugging dump manually, write anything into the pipe would trigger the dump.\nTORCH_NCCL_NAN_CHECK\nTORCH_NCCL_NAN_CHECK\nControl whether to enable NAN check for the input, Error would be thrown if NAN is detected.",
    "url": "https://pytorch.org/docs/stable/torch_nccl_environment_variables.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5e8eef5ab1d8cb78a0cefe80309691b7",
    "source": "pytorch_docs",
    "title": "Type Info \u2014 PyTorch 2.9 documentation",
    "text": "\n## Type Info#\n\nCreated On: Jun 06, 2025 | Last Updated On: Aug 14, 2025\nThe numerical properties of atorch.dtypecan be accessed through either thetorch.finfoor thetorch.iinfo.\ntorch.dtype\ntorch.finfo\ntorch.iinfo\n\n## torch.finfo#\n\nAtorch.finfois an object that represents the numerical properties of a floating pointtorch.dtype, (i.e.torch.float32,torch.float64,torch.float16, andtorch.bfloat16).\nThis is similar tonumpy.finfo.\ntorch.finfo\ntorch.dtype\ntorch.float32\ntorch.float64\ntorch.float16\ntorch.bfloat16\nAtorch.finfoprovides the following attributes:\ntorch.finfo\nName\nType\nDescription\nbits\nint\nThe number of bits occupied by the type.\neps\nfloat\nThe difference between 1.0 and the next smallest representable float larger than 1.0.\nmax\nfloat\nThe largest representable number.\nmin\nfloat\nThe smallest representable number (typically-max).\n-max\ntiny\nfloat\nThe smallest positive normal number. Equivalent tosmallest_normal.\nsmallest_normal\nsmallest_normal\nfloat\nThe smallest positive normal number. See notes.\nresolution\nfloat\nThe approximate decimal resolution of this type, i.e.,10**-precision.\n10**-precision\nNote\nThe constructor oftorch.finfocan be called without argument,\nin which case the class is created for the pytorch default dtype (as returned bytorch.get_default_dtype()).\ntorch.finfo\ntorch.get_default_dtype()\nNote\nsmallest_normalreturns the smallestnormalnumber, but there are smaller\nsubnormal numbers. See https://en.wikipedia.org/wiki/Denormal_number\nfor more information.\nsmallest_normal\n\n## torch.iinfo#\n\nAtorch.iinfois an object that represents the numerical properties of a integertorch.dtype(i.e.torch.uint8,torch.int8,torch.int16,torch.int32, andtorch.int64).\nThis is similar tonumpy.iinfo.\ntorch.iinfo\ntorch.dtype\ntorch.uint8\ntorch.int8\ntorch.int16\ntorch.int32\ntorch.int64\nAtorch.iinfoprovides the following attributes:\ntorch.iinfo\nName\nType\nDescription\nbits\nint\nThe number of bits occupied by the type.\nmax\nint\nThe largest representable number.\nmin\nint\nThe smallest representable number.",
    "url": "https://pytorch.org/docs/stable/type_info.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "50303d1e3d355a14591d9cdbc7cbac3c",
    "source": "pytorch_docs",
    "title": "torch.utils.module_tracker \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.module_tracker#\n\nCreated On: May 04, 2024 | Last Updated On: Jun 11, 2025\nThis utility can be used to track the current position inside antorch.nn.Modulehierarchy.\nIt can be used within other tracking tools to be able to easily associate measured quantities to user-friendly names. This is used in particular in the FlopCounterMode today.\ntorch.nn.Module\nModuleTrackeris a context manager that tracks the nn.Module hierarchy during execution\nso that other system can query which Module is currently being executed (or its backward is being\nexecuted).\nModuleTracker\nYou can access theparentsattribute on this context manager to get the set of all the\nModules currently being executed via their fqn (fully qualified name, also used as the key within\nthe state_dict).\nYou can access theis_bwattribute to know if you are currently running in backward or not.\nparents\nis_bw\nNote thatparentsis never empty and always contains the \u201cGlobal\u201d key. Theis_bwflag\nwill remainTrueafter the forward until another Module is executed. If you need it to be\nmore accurate, please submit an issue requesting this. Adding a map from fqn to the module instance\nis possible but not done yet, please submit an issue requesting this if you need it.\nparents\nis_bw\nTrue\nExample usage\n\n```python\nmod = torch.nn.Linear(2, 2)\n\nwith ModuleTracker() as tracker:\n    # Access anything during the forward pass\n    def my_linear(m1, m2, bias):\n        print(f\"Current modules: {tracker.parents}\")\n        return torch.mm(m1, m2.t()) + bias\n\n    torch.nn.functional.linear = my_linear\n\n    mod(torch.rand(2, 2))\n\n```\n",
    "url": "https://pytorch.org/docs/stable/module_tracker.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9f00baa79c2a78499b190eac60cba802",
    "source": "pytorch_docs",
    "title": "TorchInductor GPU Profiling \u2014 PyTorch 2.9 documentation",
    "text": "\n## TorchInductor GPU Profiling#\n\nCreated On: Jul 28, 2023 | Last Updated On: Jun 10, 2025\nThis section lists useful commands and workflows that can help\nyou dive into a model\u2019s performance in TorchInductor. When a model is not\nrunning as fast as expected, you may want to check individual kernels of the\nmodel. Usually, those kernels taking the majority of the\nGPU time are the most interesting ones. After that, you\nmay also want to run individual kernels directly and inspect its perf.\nPyTorch provides tools to cover everything mentioned above.\n\n## Relevant Environment Variables#\n\nYou can use the following environment variables in your analysis:\nTORCHINDUCTOR_UNIQUE_KERNEL_NAMES\nTORCHINDUCTOR_UNIQUE_KERNEL_NAMES\nBy default, TorchInductor names a Triton kernel as\u2018triton\\_\u2019. When\nthis environmental variable is enabled, inductor generates a more\nmeaningful kernel name in the trace, for example,triton_poi_fused_cat_155which contains the kernel category\n(poifor pointwise) and original ATen\noperator. This config is disabled by default to improve the chance of\ncompilation cache hit.\n\u2018triton\\_\u2019\ntriton_poi_fused_cat_155\npoi\nTORCHINDUCTOR_BENCHMARK_KERNEL\nTORCHINDUCTOR_BENCHMARK_KERNEL\nEnabling this will make inductor codegen harness to benchmark\nindividual triton kernels.\nTORCHINDUCTOR_MAX_AUTOTUNE\nTORCHINDUCTOR_MAX_AUTOTUNE\nInductor autotuner will benchmark moretriton.Configsand pick the\none with the best performance results. This will increase compilation\ntime with the hope to improve performance.\ntriton.Configs\n\n## Breakdown Model GPU Time#\n\nBelow are the steps to breakdown execution time of a model into\nindividual kernels. We takemixnet_las an example.\nmixnet_l\nRun the benchmark script for the model:\n\n```python\n   TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1\n   python -u benchmarks/dynamo/timm_models.py \u2013backend inductor \u2013amp\n   \u2013performance \u2013dashboard \u2013only mixnet_l \u2013disable-cudagraphs \u2013training\n\n```\n\nNote\nThe tool relies on kernel name to decide its category. EnablingTORCHINDUCTOR_UNIQUE_KERNEL_NAMESis crucial for that.\nTORCHINDUCTOR_UNIQUE_KERNEL_NAMES\nIn the output log, look for lines:\n\n```python\n   **Compiled module path:\n   /tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py**\n\n```\n\nWe have one line for each compiled module. If there are no extra graph\nbreaks, we would see 2 such lines in the log, one for the forward graph\nand one for the backward graph.\nFor our example command, we get the following compiled module for the\nforward and backward graphs respectively:\nForward graph compiled module\nBackward graph compiled module\nNow we can dive into the perf for each individual compiled module.\nLet\u2019s pick the one for the forward graph for illustration purposes.\nI\u2019ll name itfwd.pyfor convenience. Run it directly with the-pargument:\nfwd.py\n-p\n\n```python\n   **> python fwd.py -p**\n\n```\n\nSee the full output log in thisexample gist\nIn the output, you can notice the following:\nWe write a chrome trace file for the profile so we can load the trace and interact with it. In the log, look for lines as follows to find the path of the trace file.\nChrome trace for the profile is written to /tmp/compiled_module_profile.json\nLoading the trace into Chrome (visit chrome://tracing in the chrome browser and load the file as the UI suggested) will show UI as follows:\nYou can zoom in and out to check the profile.\nWe report the percent of GPU time regarding to the wall time by log line like:\nPercent of time when GPU is busy: 102.88%\nSometimes you may see a value larger than 100%. The reason is because PyTorch\nuses the kernel execution time with profiling enabled while using wall time\nwith profiling disabled. Profiling may distort the kernel execution time a\nbit. But overall it should not be a big deal.\nIf we run the model likedensenet121with a small batch size, we would see\nlow percent of time when GPU is busy:\ndensenet121\n\n```python\n  (Forward graph) Percent of time when GPU is busy: 32.69%\n\n```\n\nThis means the model has a lot of CPU overhead. This is consistent with\nthe fact that enabling cudagraphs improve densenet121\u2019s perf a lot.\nWe can break down the GPU time to different categories of kernels.\nIn themixnet_lexample, we see\nmixnet_l\npointwise kernel takes 28.58%\nreduction kernel takes 13.85%\npersistent reduction kernel takes 3.89%\nthe rest are cutlass/cudnn kernels for mm/conv which takes 56.57%\nThis information can be found in the summary line (last line)\nof the report for each kernel category.\nWe also call zoom into a certain category of kernels. For example,\nlet\u2019s check reduction kernels:\nWe can see an ordered table of execution time for each individual\nreduction kernel. We also see how many times a kernel is executed. This\nis helpful for a few reasons:\nIf a kernel only takes a tiny amount of time, for example, 0.1%,\nimproving it will at most bring 0.1% overall gain. It is not\nworth spending a lot of effort on it.\nFf a kernel takes 2% of time, improving it by 2x will bring in 1%\noverall gain which justifies the effort.\n\n## Benchmark Individual Triton Kernel#\n\nLet\u2019s say we want to take a closer look attriton_red_fused\\__native_batch_norm_legit_functional_16which is the\nmost expensive reduction kernel and takes 2.19% of overall wall time for\nthe forward graph.\ntriton_red_fused\\__native_batch_norm_legit_functional_16\nWe can lookup the kernel name in thefwd.py, and find comment like:\nfwd.py\n# kernel path:\n/tmp/torchinductor_shunting/jk/cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py\nI\u2019ll rename it k.py for convenience. Here is a paste for thisfile.\nk.pyis a standalone Python module containing the kernel code and its\nbenchmark.\nk.py\nRunk.pydirectly will report its execution time and bandwidth:\nk.py\nWe can check if max-autotune helps this kernel, by running:\n\n```python\n   **TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py**\n\n```\n\nWe may also temporarily add more reduction heuristics and run the script\nagain to check how that helps with the kernel.",
    "url": "https://pytorch.org/docs/stable/torch.compiler_inductor_profiling.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "78cf278e2474763aa553fb21f1897587",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/python.data-structure.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "daf01d4f26caac814f4e922789676098",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/mps.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "bbfac55e5e22c97a39b6c78ab74f50f4",
    "source": "pytorch_docs",
    "title": "python.context-manager \u2014 PyTorch 2.9 documentation",
    "text": "\n## python.context-manager#\n\n\n## null_context_manager#\n\nNote\nTags:python.context-manager\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport contextlib\n\nimport torch\n\nclass NullContextManager(torch.nn.Module):\n    \"\"\"\n    Null context manager in Python will be traced out.\n    \"\"\"\n\n    def forward(self, x):\n        \"\"\"\n        Null context manager in Python will be traced out.\n        \"\"\"\n        ctx = contextlib.nullcontext()\n        with ctx:\n            return x.sin() + x.cos()\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.context-manager\"}\nmodel = NullContextManager()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 sin: \"f32[3, 2]\" = torch.ops.aten.sin.default(x)\n            cos: \"f32[3, 2]\" = torch.ops.aten.cos.default(x);  x = None\n            add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(sin, cos);  sin = cos = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/python.context-manager.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "986538ea7d777b32a0d34a117403be67",
    "source": "pytorch_docs",
    "title": "TunableOp \u2014 PyTorch 2.9 documentation",
    "text": "\n## TunableOp#\n\nCreated On: Jun 03, 2024 | Last Updated On: Jun 13, 2025\n\n## Overview#\n\nThis module exposes a TunableOp interface.\nSome operations, such as GEMMs, could be implemented using more than one library\nor more than one technique. For example, a GEMM could be implemented for CUDA or\nROCm using either the blas or blasLt libraries. Further, ROCm\u2019s rocblas and\nhipblaslt libraries allow the user to query for all possible algorithms and then\nchoose one. How does one know which implementation is the fastest and should be\nchosen? That\u2019s what TunableOp provides.\n\n## Enabling TunableOp and Tuning Separately#\n\nThe TunableOp feature is enabled separately from enabling the tuning phase\nitself. Enabling TunableOp means that PyTorch will replace any standard\noperators with their Tunable implementations. Any call to a TunableOp first\nchecks whether it has already been tuned for the given operator inputs. If so,\nit will immediately call the tuned operation; no further tuning will take place\neven when the tuning setting is enabled. Instead if no tuning result is found,\nand tuning is enabled, the TunableOp will benchmark every registered\nimplementation of that operator for the given set of inputs and select the\nfastest.\n\n## File Input and Output#\n\nThe first time any TunableOp is invoked, the internal database of tuned\noperations will be prepared by attempting to read the results from the given\nfile. The default filename is \u2018tunableop_results.csv\u2019. To support tuning when\nmultiple GPUs are used across multiple processes, the GPU device ordinal is\nautomatically inserted into the filename to avoid multiple processes overwriting\nthe same file.\nIf tuning is enabled and new tunings are discovered during the course of your\nworkload, it will also write out to this same filename with all tunings, both\nthe ones it read in at startup as well as the new ones found at runtime. This\ncan be used, for example, to build up a tunings file across many workloads by\nreusing the same file. The output file is automatically created when the\napplication terminates. This behavior can be controlled by the C++ and Python\nAPIs but not the environment variables.\nAssuming you specified a filename, you\u2019ll end up with a CSV file with contents\nlike so:\n\n```python\nValidator,PT_VERSION,2.2.0\nValidator,ROCM_VERSION,6.0.0.0-12969-1544e39\nValidator,HIPBLASLT_VERSION,0.6.0-a9c5cc7\nValidator,ROCBLAS_VERSION,4.0.0-72e57364-dirty\nGemmTunableOp_float_NT,nt_25088_4096_64,Gemm_Hipblaslt_1219,1.262\nGemmTunableOp_float_NT,nt_4096_4096_64,Gemm_Rocblas_1216,0.033\n\n```\n\nNote the \u201cValidator\u201d lines. If you change a library version, or ROCm version, or\nPyTorch version, TunableOp will detect this and reject the tunings file because\nthe prior tunings are likely affected by other software changes.\nThe remaining lines are the tuned solutions for each TunableOp encountered\nduring your execution. Each line consists of 4 comma-separated fields: operator\nname, operator parameters, solution name, and average execution time. The\nexecution time is an optional field. The CSV file can be edited, but with\ncaution. For example, the solution name (field 3) can be changed to \u201cDefault\u201d\nand it will fall back to the original PyTorch untuned implementation. Or, in the\ncase of ROCm\u2019s hipBLAS or hipBLASLt libraries, if you know the specific solution\nindex you can override the solution that TunableOp selected by replacing the\nvalue. The operator name and parameters (fields 1 and 2) are internally named\nand should not be modified. In the case of GemmTunableOp, field 1 indicates the\ndatatype and whether the inputs are transposed (T) or not (N) and field 2\nindicates the M, N, K input shapes.\nThere is an option to enable verbose output but it is only recommended for\ndebugging purposes. This will produce a lot of diagnostic messages but may be\nuseful to see if TunableOp is being used at all. Otherwise, TunableOp is\ncompletely silent, besides file output, unless there is a warning or error\nduring its use. The verbose option is only available by setting the environment\nvariable PYTORCH_TUNABLEOP_VEROBSE=1.\n\n## A Note on Tuning Behavior, Warmup, and Cache Effects#\n\nTuning an operator consists of iterating through the list or registered\nimplementations and profiling each one. The profile is established by running a\nsingle implementation in a loop multiple times and taking the average execution\ntime. There is also an optional warmup phase prior to tuning that can help with\nreaching stable power states by the hardware. During tuning of a workload the\nvarious hardware caches will more likely produce hits than when not tuning.\nThere are options for flushing the instruction cache and rotate the input tensors\nwhich might help produce a more faithful profile of the tuned operator as if the\noperator were run within a larger workload instead of in a tight, repetitive loop.\nBy default, each possible solution for a given operator will be run for either\n100 iterations or as many iterations that can be run within 30ms, whichever is\nsmaller, and its average execution will be calculated. The fastest solution\namong all that were successfully profiled will be chosen. A profile might fail\nif the given solution doesn\u2019t achieve the same accuracy as the default\nimplementation or if the solution returns an error code.\n\n## Current Tunable Operators#\n\nCurrently only a TunableGemm for ROCm is implemented. Note that CUDA builds of\nPyTorch will function correctly when using TunableOp but the only solution\navailable to CUDA builds is the \u2018Default\u2019 implementation i.e. the original\ncuBLAS default, now called through TunableOp. Any call to at::cuda::blas::gemm()\nor ::bgemm() will be routed through TunableOp when enabled. Calling gemm() for a\ngiven set of input arguments (transa, transb, m, n, k) will attempt to use the\nfastest available implementation across both rocblas and hipblaslt.\n\n## Offline Tuning#\n\nThere are several use cases for offline tuning.\nOne use case involves a workload with a high-memory utilization, where regular tuning might lead to running out of memory.\nAnother use case is for compute-intensive workloads. In such cases, it is more resource-efficient to collect\nthe GEMMs for the workload once and then tune repeatedly with different tuning parameters or libraries.\nThere are basically two steps:\n1) Set the environment variables to collect the untuned GEMM and this will generatetunableop_untuned0.csv:\ntunableop_untuned0.csv\n\n```python\nexport PYTORCH_TUNABLEOP_ENABLED=1\nexport PYTORCH_TUNABLEOP_TUNING=0\nexport PYTORCH_TUNABLEOP_RECORD_UNTUNED=1\n...\n\n```\n\nRun a Python script that reads thetunableop_untuned0.csvand generates thetunableop_results0.csv, like this:\ntunableop_untuned0.csv\ntunableop_results0.csv\n\n```python\nimport torch.cuda.tunable as tunable\nimport os\n\nos.putenv(\"PYTORCH_TUNABLEOP_ENABLED\", \"1\")\nos.putenv(\"PYTORCH_TUNABLEOP_TUNING\", \"1\")\nos.putenv(\"PYTORCH_TUNABLEOP_RECORD_UNTUNED\", \"0\")\ntunable.tune_gemm_in_file(\"tunableop_untuned0.csv\")\n\n```\n\nIt is also possible to take multiple untuned files and distribute the GEMMs for tuning to multiple GPUs\nwithin a single node. In the first step, the GEMMs are first gathered and duplicate GEMMs are eliminated.\nNext, the GEMMs are distributed to different GPUs for tuning. After all GEMMs are tuned, the results from\nall the GPUs are then gathered into a single file whose base filename has_full0appended to it\n(for exampletunableop_results_full0.csv). Finally, this new file, containing the gathered results, will be\nduplicated N times, once for each GPU as convenience to the user will run the workload with the tuned\nconfiguration on N GPUs.\n_full0\ntunableop_results_full0.csv\n\n```python\nif __name__ == \"__main__\":\n    num_gpus = 8  # number of GPUs that will be used during the tuning process\n    tunable.mgpu_tune_gemm_in_file(\"tunableop_untuned?.csv\", num_gpus)\n\n```\n\nNote that the usage of themgpu_tune_gemm_in_fileAPI is different from its single GPU counterpart\n(tune_gemm_in_file). The body of the Python script that calls the API must be wrapped inmain()as shown\ndue to the use of concurrent futures module. The argument tomgpu_tune_gemm_in_filemust contain a wild card\nexpression (?or*) to generate the list of untuned files containing the GEMMs to be processed. Thenum_gpusmust between 1 and the total number of GPUs available.\nmgpu_tune_gemm_in_file\ntune_gemm_in_file\nmain()\nmgpu_tune_gemm_in_file\n?\n*\nnum_gpus\n\n## Tuning Context#\n\nThe behavior of TunableOp is currently manipulated through environment\nvariables, the C++ interface of at::cuda::tunable::getTuningContext(), or the\ntorch.cuda.tunable python interfaces. The environment variables take precedence\nover any setting you manipulate using the C++ or Python APIs.\nEnvironment variables are cached the first time they are read. You cannot use the\nenvironment variable interface programmatically since the settings become fixed.\nUse the C++ or Python APIs instead.\n\n## API Reference#\n\nThis is the big on/off switch for all TunableOp implementations.\nReturns whether the TunableOp feature is enabled.\nbool\nEnable tuning of TunableOp implementations.\nWhen enabled, if a tuned entry isn\u2019t found, run the tuning step and record\nthe entry.\nReturns whether TunableOp implementations can be tuned.\nbool\nEnable recording untuned of TunableOp perations for offline tuning.\nWhen enabled, if a tuned entry isn\u2019t found, write it to the untuned file.\nReturns whether TunableOp operations are recorded for offline tuning.\nbool\nSet max time in milliseconds to spend tuning a given solution.\nIf both max tuning duration and iterations are set, the smaller of the two\nwill be honored. At minimum 1 tuning iteration will always be run.\nGet max time to spend tuning a given solution.\nint\nSet max number of iterations to spend tuning a given solution.\nIf both max tuning duration and iterations are set, the smaller of the two\nwill be honored. At minimum 1 tuning iteration will always be run.\nGet max iterations to spend tuning a given solution.\nint\nSet the filename to use for input/output of tuning results.\nIfinsert_device_ordinalisTruethen the current device ordinal\nwill be added to the given filename automatically. This can be used in a\n1-process-per-gpu scenario to ensure all processes write to a separate file.\ninsert_device_ordinal\nTrue\nGet the results filename.\nstr\nReturn all TunableOp results.\ntuple[str,str,str,float]\nReturn the TunableOp validators.\ntuple[str,str]\nDuring Tuning Context destruction, write file to disk.\nThis is useful as a final flush of your results to disk if your application\nterminates as result of normal operation or an error. Manual flushing of\nyour results can be achieved by manually callingwrite_file().\nwrite_file()\nWrite results to a CSV file.\nIffilenameis not given,get_filename()is called.\nfilename\nget_filename()\nbool\nRead results from a TunableOp CSV file.\nIffilenameis not given,get_filename()is called.\nfilename\nget_filename()\nbool\ntune GEMM in file.\nProcess one or more files and distribute work over one or more GPUs.\nSet rotating buffer size to this value in MB, if the buffer size is greater than zero.\nIf less than zero, query L2 cache size. If equal to zero, means deactivate rotating buffer.\nGet the rotating buffer size in kilobytes.\nint",
    "url": "https://pytorch.org/docs/stable/cuda.tunable.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "711d5c27fd7c5888f2a8740266d64d77",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/backends.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "c46c5214d0372c584735efdb3289041f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/nn.init.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e3661e7b9e8ec8ad9167c9ddd0fe2de2",
    "source": "pytorch_docs",
    "title": "torch.utils.deterministic \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.deterministic#\n\nCreated On: Oct 26, 2023 | Last Updated On: Jun 06, 2025\nAboolthat, if True, causes uninitialized memory to be filled with\na known value whentorch.use_deterministic_algorithms()is set toTrue. Floating point and complex values are set to NaN, and integer\nvalues are set to the maximum value.\nbool\ntorch.use_deterministic_algorithms()\nTrue\nDefault:True\nTrue\nFilling uninitialized memory is detrimental to performance. So if your\nprogram is valid and does not use uninitialized memory as the input to an\noperation, then this setting can be turned off for better performance and\nstill be deterministic.\nThe following operations will fill uninitialized memory when this setting is\nturned on:\ntorch.Tensor.resize_()when called with a tensor that is not\nquantized\ntorch.Tensor.resize_()\ntorch.empty()\ntorch.empty()\ntorch.empty_strided()\ntorch.empty_strided()\ntorch.empty_permuted()\ntorch.empty_permuted()\ntorch.empty_like()\ntorch.empty_like()",
    "url": "https://pytorch.org/docs/stable/deterministic.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b0e57c88f8281ad9804a062df878e0b2",
    "source": "pytorch_docs",
    "title": "torch.Tensor \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.Tensor#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jun 27, 2025\nAtorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Please seetorch.dtypefor more details about dtype support.\ntorch.Tensor\n\n## Initializing and basic operations#\n\nA tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor:\nlist\ntorch.tensor()\n\n```python\n>>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n\n```\n\nWarning\ntorch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor().\ntorch.tensor()\ndata\ndata\nrequires_grad\nrequires_grad_()\ndetach()\ntorch.as_tensor()\nA tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op:\ntorch.dtype\ntorch.device\n\n```python\n>>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n\n```\n\nFor more information about building Tensors, seeCreation Ops\nThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:\n\n```python\n>>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n\n```\n\nUsetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value:\ntorch.Tensor.item()\n\n```python\n>>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5\n\n```\n\nFor more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops\nA tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation.\nrequires_grad=True\ntorch.autograd\n\n```python\n>>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n\n```\n\nEach tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it.\ntorch.Storage\nNote\nFor more information on tensor views, seeTensor Views.\nNote\nFor more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes.\ntorch.dtype\ntorch.device\ntorch.layout\ntorch.Tensor\nNote\nMethods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor.\ntorch.FloatTensor.abs_()\ntorch.FloatTensor.abs()\nNote\nTo change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor.\ntorch.device\ntorch.dtype\nto()\nWarning\nCurrent implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.\ntorch.Tensor\n\n## Tensor class reference#\n\nThere are a few main ways to create a tensor, depending on your use case.\nTo create a tensor with pre-existing data, usetorch.tensor().\ntorch.tensor()\nTo create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops).\ntorch.*\nTo create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops).\ntorch.*_like\nTo create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops.\ntensor.new_*\nThere is a legacy constructortorch.Tensorwhose use is discouraged.\nUsetorch.tensor()instead.\ntorch.Tensor\ntorch.tensor()\nThis constructor is deprecated, we recommend usingtorch.tensor()instead.\nWhat this constructor does depends on the type ofdata.\ntorch.tensor()\ndata\nIfdatais a Tensor, returns an alias to the original Tensor.  Unliketorch.tensor(), this tracks autograd and will propagate gradients to\nthe original Tensor.devicekwarg is not supported for thisdatatype.\ndata\ntorch.tensor()\ndevice\ndata\nIfdatais a sequence or nested sequence, create a tensor of the default\ndtype (typicallytorch.float32) whose data is the values in the\nsequences, performing coercions if necessary.  Notably, this differs fromtorch.tensor()in that this constructor will always construct a float\ntensor, even if the inputs are all integers.\ndata\ntorch.float32\ntorch.tensor()\nIfdatais atorch.Size, returns an empty tensor of that size.\ndata\ntorch.Size\nThis constructor does not support explicitly specifyingdtypeordeviceof\nthe returned tensor.  We recommend usingtorch.tensor()which provides this\nfunctionality.\ndtype\ndevice\ntorch.tensor()\ndata (array_like): The tensor to construct from.\ntorch.device\nDefault: if None, sametorch.deviceas this tensor.\ntorch.device\nReturns a view of this tensor with its dimensions reversed.\nIfnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0).\nn\nx\nx.T\nx.permute(n-1,n-2,...,0)\nWarning\nThe use ofTensor.T()on tensors of dimension other than 2 to reverse their shape\nis deprecated and it will throw an error in a future release. ConsidermTto transpose batches of matrices orx.permute(*torch.arange(x.ndim - 1, -1, -1))to reverse\nthe dimensions of a tensor.\nTensor.T()\nmT\nReturns a view of a matrix (2-D tensor) conjugated and transposed.\nx.His equivalent tox.transpose(0,1).conj()for complex matrices andx.transpose(0,1)for real matrices.\nx.H\nx.transpose(0,1).conj()\nx.transpose(0,1)\nSee also\nmH: An attribute that also works on batches of matrices.\nmH\nReturns a view of this tensor with the last two dimensions transposed.\nx.mTis equivalent tox.transpose(-2,-1).\nx.mT\nx.transpose(-2,-1)\nAccessing this property is equivalent to callingadjoint().\nadjoint()\nTensor.new_tensor\nTensor.new_tensor\nReturns a new Tensor withdataas the tensor data.\ndata\nTensor.new_full\nTensor.new_full\nReturns a Tensor of sizesizefilled withfill_value.\nsize\nfill_value\nTensor.new_empty\nTensor.new_empty\nReturns a Tensor of sizesizefilled with uninitialized data.\nsize\nTensor.new_ones\nTensor.new_ones\nReturns a Tensor of sizesizefilled with1.\nsize\n1\nTensor.new_zeros\nTensor.new_zeros\nReturns a Tensor of sizesizefilled with0.\nsize\n0\nTensor.is_cuda\nTensor.is_cuda\nIsTrueif the Tensor is stored on the GPU,Falseotherwise.\nTrue\nFalse\nTensor.is_quantized\nTensor.is_quantized\nIsTrueif the Tensor is quantized,Falseotherwise.\nTrue\nFalse\nTensor.is_meta\nTensor.is_meta\nIsTrueif the Tensor is a meta tensor,Falseotherwise.\nTrue\nFalse\nTensor.device\nTensor.device\nIs thetorch.devicewhere this Tensor is.\ntorch.device\nTensor.grad\nTensor.grad\nThis attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself.\nNone\nbackward()\nself\nTensor.ndim\nTensor.ndim\nAlias fordim()\ndim()\nTensor.real\nTensor.real\nReturns a new tensor containing real values of theselftensor for a complex-valued input tensor.\nself\nTensor.imag\nTensor.imag\nReturns a new tensor containing imaginary values of theselftensor.\nself\nTensor.nbytes\nTensor.nbytes\nReturns the number of bytes consumed by the \"view\" of elements of the Tensor if the Tensor does not use sparse storage layout.\nTensor.itemsize\nTensor.itemsize\nAlias forelement_size()\nelement_size()\nTensor.abs\nTensor.abs\nSeetorch.abs()\ntorch.abs()\nTensor.abs_\nTensor.abs_\nIn-place version ofabs()\nabs()\nTensor.absolute\nTensor.absolute\nAlias forabs()\nabs()\nTensor.absolute_\nTensor.absolute_\nIn-place version ofabsolute()Alias forabs_()\nabsolute()\nabs_()\nTensor.acos\nTensor.acos\nSeetorch.acos()\ntorch.acos()\nTensor.acos_\nTensor.acos_\nIn-place version ofacos()\nacos()\nTensor.arccos\nTensor.arccos\nSeetorch.arccos()\ntorch.arccos()\nTensor.arccos_\nTensor.arccos_\nIn-place version ofarccos()\narccos()\nTensor.add\nTensor.add\nAdd a scalar or tensor toselftensor.\nself\nTensor.add_\nTensor.add_\nIn-place version ofadd()\nadd()\nTensor.addbmm\nTensor.addbmm\nSeetorch.addbmm()\ntorch.addbmm()\nTensor.addbmm_\nTensor.addbmm_\nIn-place version ofaddbmm()\naddbmm()\nTensor.addcdiv\nTensor.addcdiv\nSeetorch.addcdiv()\ntorch.addcdiv()\nTensor.addcdiv_\nTensor.addcdiv_\nIn-place version ofaddcdiv()\naddcdiv()\nTensor.addcmul\nTensor.addcmul\nSeetorch.addcmul()\ntorch.addcmul()\nTensor.addcmul_\nTensor.addcmul_\nIn-place version ofaddcmul()\naddcmul()\nTensor.addmm\nTensor.addmm\nSeetorch.addmm()\ntorch.addmm()\nTensor.addmm_\nTensor.addmm_\nIn-place version ofaddmm()\naddmm()\nTensor.sspaddmm\nTensor.sspaddmm\nSeetorch.sspaddmm()\ntorch.sspaddmm()\nTensor.addmv\nTensor.addmv\nSeetorch.addmv()\ntorch.addmv()\nTensor.addmv_\nTensor.addmv_\nIn-place version ofaddmv()\naddmv()\nTensor.addr\nTensor.addr\nSeetorch.addr()\ntorch.addr()\nTensor.addr_\nTensor.addr_\nIn-place version ofaddr()\naddr()\nTensor.adjoint\nTensor.adjoint\nAlias foradjoint()\nadjoint()\nTensor.allclose\nTensor.allclose\nSeetorch.allclose()\ntorch.allclose()\nTensor.amax\nTensor.amax\nSeetorch.amax()\ntorch.amax()\nTensor.amin\nTensor.amin\nSeetorch.amin()\ntorch.amin()\nTensor.aminmax\nTensor.aminmax\nSeetorch.aminmax()\ntorch.aminmax()\nTensor.angle\nTensor.angle\nSeetorch.angle()\ntorch.angle()\nTensor.apply_\nTensor.apply_\nApplies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable.\ncallable\ncallable\nTensor.argmax\nTensor.argmax\nSeetorch.argmax()\ntorch.argmax()\nTensor.argmin\nTensor.argmin\nSeetorch.argmin()\ntorch.argmin()\nTensor.argsort\nTensor.argsort\nSeetorch.argsort()\ntorch.argsort()\nTensor.argwhere\nTensor.argwhere\nSeetorch.argwhere()\ntorch.argwhere()\nTensor.asin\nTensor.asin\nSeetorch.asin()\ntorch.asin()\nTensor.asin_\nTensor.asin_\nIn-place version ofasin()\nasin()\nTensor.arcsin\nTensor.arcsin\nSeetorch.arcsin()\ntorch.arcsin()\nTensor.arcsin_\nTensor.arcsin_\nIn-place version ofarcsin()\narcsin()\nTensor.as_strided\nTensor.as_strided\nSeetorch.as_strided()\ntorch.as_strided()\nTensor.atan\nTensor.atan\nSeetorch.atan()\ntorch.atan()\nTensor.atan_\nTensor.atan_\nIn-place version ofatan()\natan()\nTensor.arctan\nTensor.arctan\nSeetorch.arctan()\ntorch.arctan()\nTensor.arctan_\nTensor.arctan_\nIn-place version ofarctan()\narctan()\nTensor.atan2\nTensor.atan2\nSeetorch.atan2()\ntorch.atan2()\nTensor.atan2_\nTensor.atan2_\nIn-place version ofatan2()\natan2()\nTensor.arctan2\nTensor.arctan2\nSeetorch.arctan2()\ntorch.arctan2()\nTensor.arctan2_\nTensor.arctan2_\natan2_(other) -> Tensor\nTensor.all\nTensor.all\nSeetorch.all()\ntorch.all()\nTensor.any\nTensor.any\nSeetorch.any()\ntorch.any()\nTensor.backward\nTensor.backward\nComputes the gradient of current tensor wrt graph leaves.\nTensor.baddbmm\nTensor.baddbmm\nSeetorch.baddbmm()\ntorch.baddbmm()\nTensor.baddbmm_\nTensor.baddbmm_\nIn-place version ofbaddbmm()\nbaddbmm()\nTensor.bernoulli\nTensor.bernoulli\nReturns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]).\nTensor.bernoulli_\nTensor.bernoulli_\nFills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p).\nself\nTensor.bfloat16\nTensor.bfloat16\nself.bfloat16()is equivalent toself.to(torch.bfloat16).\nself.bfloat16()\nself.to(torch.bfloat16)\nTensor.bincount\nTensor.bincount\nSeetorch.bincount()\ntorch.bincount()\nTensor.bitwise_not\nTensor.bitwise_not\nSeetorch.bitwise_not()\ntorch.bitwise_not()\nTensor.bitwise_not_\nTensor.bitwise_not_\nIn-place version ofbitwise_not()\nbitwise_not()\nTensor.bitwise_and\nTensor.bitwise_and\nSeetorch.bitwise_and()\ntorch.bitwise_and()\nTensor.bitwise_and_\nTensor.bitwise_and_\nIn-place version ofbitwise_and()\nbitwise_and()\nTensor.bitwise_or\nTensor.bitwise_or\nSeetorch.bitwise_or()\ntorch.bitwise_or()\nTensor.bitwise_or_\nTensor.bitwise_or_\nIn-place version ofbitwise_or()\nbitwise_or()\nTensor.bitwise_xor\nTensor.bitwise_xor\nSeetorch.bitwise_xor()\ntorch.bitwise_xor()\nTensor.bitwise_xor_\nTensor.bitwise_xor_\nIn-place version ofbitwise_xor()\nbitwise_xor()\nTensor.bitwise_left_shift\nTensor.bitwise_left_shift\nSeetorch.bitwise_left_shift()\ntorch.bitwise_left_shift()\nTensor.bitwise_left_shift_\nTensor.bitwise_left_shift_\nIn-place version ofbitwise_left_shift()\nbitwise_left_shift()\nTensor.bitwise_right_shift\nTensor.bitwise_right_shift\nSeetorch.bitwise_right_shift()\ntorch.bitwise_right_shift()\nTensor.bitwise_right_shift_\nTensor.bitwise_right_shift_\nIn-place version ofbitwise_right_shift()\nbitwise_right_shift()\nTensor.bmm\nTensor.bmm\nSeetorch.bmm()\ntorch.bmm()\nTensor.bool\nTensor.bool\nself.bool()is equivalent toself.to(torch.bool).\nself.bool()\nself.to(torch.bool)\nTensor.byte\nTensor.byte\nself.byte()is equivalent toself.to(torch.uint8).\nself.byte()\nself.to(torch.uint8)\nTensor.broadcast_to\nTensor.broadcast_to\nSeetorch.broadcast_to().\ntorch.broadcast_to()\nTensor.cauchy_\nTensor.cauchy_\nFills the tensor with numbers drawn from the Cauchy distribution:\nTensor.ceil\nTensor.ceil\nSeetorch.ceil()\ntorch.ceil()\nTensor.ceil_\nTensor.ceil_\nIn-place version ofceil()\nceil()\nTensor.char\nTensor.char\nself.char()is equivalent toself.to(torch.int8).\nself.char()\nself.to(torch.int8)\nTensor.cholesky\nTensor.cholesky\nSeetorch.cholesky()\ntorch.cholesky()\nTensor.cholesky_inverse\nTensor.cholesky_inverse\nSeetorch.cholesky_inverse()\ntorch.cholesky_inverse()\nTensor.cholesky_solve\nTensor.cholesky_solve\nSeetorch.cholesky_solve()\ntorch.cholesky_solve()\nTensor.chunk\nTensor.chunk\nSeetorch.chunk()\ntorch.chunk()\nTensor.clamp\nTensor.clamp\nSeetorch.clamp()\ntorch.clamp()\nTensor.clamp_\nTensor.clamp_\nIn-place version ofclamp()\nclamp()\nTensor.clip\nTensor.clip\nAlias forclamp().\nclamp()\nTensor.clip_\nTensor.clip_\nAlias forclamp_().\nclamp_()\nTensor.clone\nTensor.clone\nSeetorch.clone()\ntorch.clone()\nTensor.contiguous\nTensor.contiguous\nReturns a contiguous in memory tensor containing the same data asselftensor.\nself\nTensor.copy_\nTensor.copy_\nCopies the elements fromsrcintoselftensor and returnsself.\nsrc\nself\nself\nTensor.conj\nTensor.conj\nSeetorch.conj()\ntorch.conj()\nTensor.conj_physical\nTensor.conj_physical\nSeetorch.conj_physical()\ntorch.conj_physical()\nTensor.conj_physical_\nTensor.conj_physical_\nIn-place version ofconj_physical()\nconj_physical()\nTensor.resolve_conj\nTensor.resolve_conj\nSeetorch.resolve_conj()\ntorch.resolve_conj()\nTensor.resolve_neg\nTensor.resolve_neg\nSeetorch.resolve_neg()\ntorch.resolve_neg()\nTensor.copysign\nTensor.copysign\nSeetorch.copysign()\ntorch.copysign()\nTensor.copysign_\nTensor.copysign_\nIn-place version ofcopysign()\ncopysign()\nTensor.cos\nTensor.cos\nSeetorch.cos()\ntorch.cos()\nTensor.cos_\nTensor.cos_\nIn-place version ofcos()\ncos()\nTensor.cosh\nTensor.cosh\nSeetorch.cosh()\ntorch.cosh()\nTensor.cosh_\nTensor.cosh_\nIn-place version ofcosh()\ncosh()\nTensor.corrcoef\nTensor.corrcoef\nSeetorch.corrcoef()\ntorch.corrcoef()\nTensor.count_nonzero\nTensor.count_nonzero\nSeetorch.count_nonzero()\ntorch.count_nonzero()\nTensor.cov\nTensor.cov\nSeetorch.cov()\ntorch.cov()\nTensor.acosh\nTensor.acosh\nSeetorch.acosh()\ntorch.acosh()\nTensor.acosh_\nTensor.acosh_\nIn-place version ofacosh()\nacosh()\nTensor.arccosh\nTensor.arccosh\nacosh() -> Tensor\nTensor.arccosh_\nTensor.arccosh_\nacosh_() -> Tensor\nTensor.cpu\nTensor.cpu\nReturns a copy of this object in CPU memory.\nTensor.cross\nTensor.cross\nSeetorch.cross()\ntorch.cross()\nTensor.cuda\nTensor.cuda\nReturns a copy of this object in CUDA memory.\nTensor.logcumsumexp\nTensor.logcumsumexp\nSeetorch.logcumsumexp()\ntorch.logcumsumexp()\nTensor.cummax\nTensor.cummax\nSeetorch.cummax()\ntorch.cummax()\nTensor.cummin\nTensor.cummin\nSeetorch.cummin()\ntorch.cummin()\nTensor.cumprod\nTensor.cumprod\nSeetorch.cumprod()\ntorch.cumprod()\nTensor.cumprod_\nTensor.cumprod_\nIn-place version ofcumprod()\ncumprod()\nTensor.cumsum\nTensor.cumsum\nSeetorch.cumsum()\ntorch.cumsum()\nTensor.cumsum_\nTensor.cumsum_\nIn-place version ofcumsum()\ncumsum()\nTensor.chalf\nTensor.chalf\nself.chalf()is equivalent toself.to(torch.complex32).\nself.chalf()\nself.to(torch.complex32)\nTensor.cfloat\nTensor.cfloat\nself.cfloat()is equivalent toself.to(torch.complex64).\nself.cfloat()\nself.to(torch.complex64)\nTensor.cdouble\nTensor.cdouble\nself.cdouble()is equivalent toself.to(torch.complex128).\nself.cdouble()\nself.to(torch.complex128)\nTensor.data_ptr\nTensor.data_ptr\nReturns the address of the first element ofselftensor.\nself\nTensor.deg2rad\nTensor.deg2rad\nSeetorch.deg2rad()\ntorch.deg2rad()\nTensor.dequantize\nTensor.dequantize\nGiven a quantized Tensor, dequantize it and return the dequantized float Tensor.\nTensor.det\nTensor.det\nSeetorch.det()\ntorch.det()\nTensor.dense_dim\nTensor.dense_dim\nReturn the number of dense dimensions in asparse tensorself.\nself\nTensor.detach\nTensor.detach\nReturns a new Tensor, detached from the current graph.\nTensor.detach_\nTensor.detach_\nDetaches the Tensor from the graph that created it, making it a leaf.\nTensor.diag\nTensor.diag\nSeetorch.diag()\ntorch.diag()\nTensor.diag_embed\nTensor.diag_embed\nSeetorch.diag_embed()\ntorch.diag_embed()\nTensor.diagflat\nTensor.diagflat\nSeetorch.diagflat()\ntorch.diagflat()\nTensor.diagonal\nTensor.diagonal\nSeetorch.diagonal()\ntorch.diagonal()\nTensor.diagonal_scatter\nTensor.diagonal_scatter\nSeetorch.diagonal_scatter()\ntorch.diagonal_scatter()\nTensor.fill_diagonal_\nTensor.fill_diagonal_\nFill the main diagonal of a tensor that has at least 2-dimensions.\nTensor.fmax\nTensor.fmax\nSeetorch.fmax()\ntorch.fmax()\nTensor.fmin\nTensor.fmin\nSeetorch.fmin()\ntorch.fmin()\nTensor.diff\nTensor.diff\nSeetorch.diff()\ntorch.diff()\nTensor.digamma\nTensor.digamma\nSeetorch.digamma()\ntorch.digamma()\nTensor.digamma_\nTensor.digamma_\nIn-place version ofdigamma()\ndigamma()\nTensor.dim\nTensor.dim\nReturns the number of dimensions ofselftensor.\nself\nTensor.dim_order\nTensor.dim_order\nReturns the uniquely determined tuple of int describing the dim order or physical layout ofself.\nself\nTensor.dist\nTensor.dist\nSeetorch.dist()\ntorch.dist()\nTensor.div\nTensor.div\nSeetorch.div()\ntorch.div()\nTensor.div_\nTensor.div_\nIn-place version ofdiv()\ndiv()\nTensor.divide\nTensor.divide\nSeetorch.divide()\ntorch.divide()\nTensor.divide_\nTensor.divide_\nIn-place version ofdivide()\ndivide()\nTensor.dot\nTensor.dot\nSeetorch.dot()\ntorch.dot()\nTensor.double\nTensor.double\nself.double()is equivalent toself.to(torch.float64).\nself.double()\nself.to(torch.float64)\nTensor.dsplit\nTensor.dsplit\nSeetorch.dsplit()\ntorch.dsplit()\nTensor.element_size\nTensor.element_size\nReturns the size in bytes of an individual element.\nTensor.eq\nTensor.eq\nSeetorch.eq()\ntorch.eq()\nTensor.eq_\nTensor.eq_\nIn-place version ofeq()\neq()\nTensor.equal\nTensor.equal\nSeetorch.equal()\ntorch.equal()\nTensor.erf\nTensor.erf\nSeetorch.erf()\ntorch.erf()\nTensor.erf_\nTensor.erf_\nIn-place version oferf()\nerf()\nTensor.erfc\nTensor.erfc\nSeetorch.erfc()\ntorch.erfc()\nTensor.erfc_\nTensor.erfc_\nIn-place version oferfc()\nerfc()\nTensor.erfinv\nTensor.erfinv\nSeetorch.erfinv()\ntorch.erfinv()\nTensor.erfinv_\nTensor.erfinv_\nIn-place version oferfinv()\nerfinv()\nTensor.exp\nTensor.exp\nSeetorch.exp()\ntorch.exp()\nTensor.exp_\nTensor.exp_\nIn-place version ofexp()\nexp()\nTensor.expm1\nTensor.expm1\nSeetorch.expm1()\ntorch.expm1()\nTensor.expm1_\nTensor.expm1_\nIn-place version ofexpm1()\nexpm1()\nTensor.expand\nTensor.expand\nReturns a new view of theselftensor with singleton dimensions expanded to a larger size.\nself\nTensor.expand_as\nTensor.expand_as\nExpand this tensor to the same size asother.\nother\nTensor.exponential_\nTensor.exponential_\nFillsselftensor with elements drawn from the PDF (probability density function):\nself\nTensor.fix\nTensor.fix\nSeetorch.fix().\ntorch.fix()\nTensor.fix_\nTensor.fix_\nIn-place version offix()\nfix()\nTensor.fill_\nTensor.fill_\nFillsselftensor with the specified value.\nself\nTensor.flatten\nTensor.flatten\nSeetorch.flatten()\ntorch.flatten()\nTensor.flip\nTensor.flip\nSeetorch.flip()\ntorch.flip()\nTensor.fliplr\nTensor.fliplr\nSeetorch.fliplr()\ntorch.fliplr()\nTensor.flipud\nTensor.flipud\nSeetorch.flipud()\ntorch.flipud()\nTensor.float\nTensor.float\nself.float()is equivalent toself.to(torch.float32).\nself.float()\nself.to(torch.float32)\nTensor.float_power\nTensor.float_power\nSeetorch.float_power()\ntorch.float_power()\nTensor.float_power_\nTensor.float_power_\nIn-place version offloat_power()\nfloat_power()\nTensor.floor\nTensor.floor\nSeetorch.floor()\ntorch.floor()\nTensor.floor_\nTensor.floor_\nIn-place version offloor()\nfloor()\nTensor.floor_divide\nTensor.floor_divide\nSeetorch.floor_divide()\ntorch.floor_divide()\nTensor.floor_divide_\nTensor.floor_divide_\nIn-place version offloor_divide()\nfloor_divide()\nTensor.fmod\nTensor.fmod\nSeetorch.fmod()\ntorch.fmod()\nTensor.fmod_\nTensor.fmod_\nIn-place version offmod()\nfmod()\nTensor.frac\nTensor.frac\nSeetorch.frac()\ntorch.frac()\nTensor.frac_\nTensor.frac_\nIn-place version offrac()\nfrac()\nTensor.frexp\nTensor.frexp\nSeetorch.frexp()\ntorch.frexp()\nTensor.gather\nTensor.gather\nSeetorch.gather()\ntorch.gather()\nTensor.gcd\nTensor.gcd\nSeetorch.gcd()\ntorch.gcd()\nTensor.gcd_\nTensor.gcd_\nIn-place version ofgcd()\ngcd()\nTensor.ge\nTensor.ge\nSeetorch.ge().\ntorch.ge()\nTensor.ge_\nTensor.ge_\nIn-place version ofge().\nge()\nTensor.greater_equal\nTensor.greater_equal\nSeetorch.greater_equal().\ntorch.greater_equal()\nTensor.greater_equal_\nTensor.greater_equal_\nIn-place version ofgreater_equal().\ngreater_equal()\nTensor.geometric_\nTensor.geometric_\nFillsselftensor with elements drawn from the geometric distribution:\nself\nTensor.geqrf\nTensor.geqrf\nSeetorch.geqrf()\ntorch.geqrf()\nTensor.ger\nTensor.ger\nSeetorch.ger()\ntorch.ger()\nTensor.get_device\nTensor.get_device\nFor CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.\nTensor.gt\nTensor.gt\nSeetorch.gt().\ntorch.gt()\nTensor.gt_\nTensor.gt_\nIn-place version ofgt().\ngt()\nTensor.greater\nTensor.greater\nSeetorch.greater().\ntorch.greater()\nTensor.greater_\nTensor.greater_\nIn-place version ofgreater().\ngreater()\nTensor.half\nTensor.half\nself.half()is equivalent toself.to(torch.float16).\nself.half()\nself.to(torch.float16)\nTensor.hardshrink\nTensor.hardshrink\nSeetorch.nn.functional.hardshrink()\ntorch.nn.functional.hardshrink()\nTensor.heaviside\nTensor.heaviside\nSeetorch.heaviside()\ntorch.heaviside()\nTensor.histc\nTensor.histc\nSeetorch.histc()\ntorch.histc()\nTensor.histogram\nTensor.histogram\nSeetorch.histogram()\ntorch.histogram()\nTensor.hsplit\nTensor.hsplit\nSeetorch.hsplit()\ntorch.hsplit()\nTensor.hypot\nTensor.hypot\nSeetorch.hypot()\ntorch.hypot()\nTensor.hypot_\nTensor.hypot_\nIn-place version ofhypot()\nhypot()\nTensor.i0\nTensor.i0\nSeetorch.i0()\ntorch.i0()\nTensor.i0_\nTensor.i0_\nIn-place version ofi0()\ni0()\nTensor.igamma\nTensor.igamma\nSeetorch.igamma()\ntorch.igamma()\nTensor.igamma_\nTensor.igamma_\nIn-place version ofigamma()\nigamma()\nTensor.igammac\nTensor.igammac\nSeetorch.igammac()\ntorch.igammac()\nTensor.igammac_\nTensor.igammac_\nIn-place version ofigammac()\nigammac()\nTensor.index_add_\nTensor.index_add_\nAccumulate the elements ofalphatimessourceinto theselftensor by adding to the indices in the order given inindex.\nalpha\nsource\nself\nindex\nTensor.index_add\nTensor.index_add\nOut-of-place version oftorch.Tensor.index_add_().\ntorch.Tensor.index_add_()\nTensor.index_copy_\nTensor.index_copy_\nCopies the elements oftensorinto theselftensor by selecting the indices in the order given inindex.\ntensor\nself\nindex\nTensor.index_copy\nTensor.index_copy\nOut-of-place version oftorch.Tensor.index_copy_().\ntorch.Tensor.index_copy_()\nTensor.index_fill_\nTensor.index_fill_\nFills the elements of theselftensor with valuevalueby selecting the indices in the order given inindex.\nself\nvalue\nindex\nTensor.index_fill\nTensor.index_fill\nOut-of-place version oftorch.Tensor.index_fill_().\ntorch.Tensor.index_fill_()\nTensor.index_put_\nTensor.index_put_\nPuts values from the tensorvaluesinto the tensorselfusing the indices specified inindices(which is a tuple of Tensors).\nvalues\nself\nindices\nTensor.index_put\nTensor.index_put\nOut-place version ofindex_put_().\nindex_put_()\nTensor.index_reduce_\nTensor.index_reduce_\nAccumulate the elements ofsourceinto theselftensor by accumulating to the indices in the order given inindexusing the reduction given by thereduceargument.\nsource\nself\nindex\nreduce\nTensor.index_reduce\nTensor.index_reduce\n\nTensor.index_select\nTensor.index_select\nSeetorch.index_select()\ntorch.index_select()\nTensor.indices\nTensor.indices\nReturn the indices tensor of asparse COO tensor.\nTensor.inner\nTensor.inner\nSeetorch.inner().\ntorch.inner()\nTensor.int\nTensor.int\nself.int()is equivalent toself.to(torch.int32).\nself.int()\nself.to(torch.int32)\nTensor.int_repr\nTensor.int_repr\nGiven a quantized Tensor,self.int_repr()returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.\nself.int_repr()\nTensor.inverse\nTensor.inverse\nSeetorch.inverse()\ntorch.inverse()\nTensor.isclose\nTensor.isclose\nSeetorch.isclose()\ntorch.isclose()\nTensor.isfinite\nTensor.isfinite\nSeetorch.isfinite()\ntorch.isfinite()\nTensor.isinf\nTensor.isinf\nSeetorch.isinf()\ntorch.isinf()\nTensor.isposinf\nTensor.isposinf\nSeetorch.isposinf()\ntorch.isposinf()\nTensor.isneginf\nTensor.isneginf\nSeetorch.isneginf()\ntorch.isneginf()\nTensor.isnan\nTensor.isnan\nSeetorch.isnan()\ntorch.isnan()\nTensor.is_contiguous\nTensor.is_contiguous\nReturns True ifselftensor is contiguous in memory in the order specified by memory format.\nself\nTensor.is_complex\nTensor.is_complex\nReturns True if the data type ofselfis a complex data type.\nself\nTensor.is_conj\nTensor.is_conj\nReturns True if the conjugate bit ofselfis set to true.\nself\nTensor.is_floating_point\nTensor.is_floating_point\nReturns True if the data type ofselfis a floating point data type.\nself\nTensor.is_inference\nTensor.is_inference\nSeetorch.is_inference()\ntorch.is_inference()\nTensor.is_leaf\nTensor.is_leaf\nAll Tensors that haverequires_gradwhich isFalsewill be leaf Tensors by convention.\nrequires_grad\nFalse\nTensor.is_pinned\nTensor.is_pinned\nReturns true if this tensor resides in pinned memory.\nTensor.is_set_to\nTensor.is_set_to\nReturns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).\nTensor.is_shared\nTensor.is_shared\nChecks if tensor is in shared memory.\nTensor.is_signed\nTensor.is_signed\nReturns True if the data type ofselfis a signed data type.\nself\nTensor.is_sparse\nTensor.is_sparse\nIsTrueif the Tensor uses sparse COO storage layout,Falseotherwise.\nTrue\nFalse\nTensor.istft\nTensor.istft\nSeetorch.istft()\ntorch.istft()\nTensor.isreal\nTensor.isreal\nSeetorch.isreal()\ntorch.isreal()\nTensor.item\nTensor.item\nReturns the value of this tensor as a standard Python number.\nTensor.kthvalue\nTensor.kthvalue\nSeetorch.kthvalue()\ntorch.kthvalue()\nTensor.lcm\nTensor.lcm\nSeetorch.lcm()\ntorch.lcm()\nTensor.lcm_\nTensor.lcm_\nIn-place version oflcm()\nlcm()\nTensor.ldexp\nTensor.ldexp\nSeetorch.ldexp()\ntorch.ldexp()\nTensor.ldexp_\nTensor.ldexp_\nIn-place version ofldexp()\nldexp()\nTensor.le\nTensor.le\nSeetorch.le().\ntorch.le()\nTensor.le_\nTensor.le_\nIn-place version ofle().\nle()\nTensor.less_equal\nTensor.less_equal\nSeetorch.less_equal().\ntorch.less_equal()\nTensor.less_equal_\nTensor.less_equal_\nIn-place version ofless_equal().\nless_equal()\nTensor.lerp\nTensor.lerp\nSeetorch.lerp()\ntorch.lerp()\nTensor.lerp_\nTensor.lerp_\nIn-place version oflerp()\nlerp()\nTensor.lgamma\nTensor.lgamma\nSeetorch.lgamma()\ntorch.lgamma()\nTensor.lgamma_\nTensor.lgamma_\nIn-place version oflgamma()\nlgamma()\nTensor.log\nTensor.log\nSeetorch.log()\ntorch.log()\nTensor.log_\nTensor.log_\nIn-place version oflog()\nlog()\nTensor.logdet\nTensor.logdet\nSeetorch.logdet()\ntorch.logdet()\nTensor.log10\nTensor.log10\nSeetorch.log10()\ntorch.log10()\nTensor.log10_\nTensor.log10_\nIn-place version oflog10()\nlog10()\nTensor.log1p\nTensor.log1p\nSeetorch.log1p()\ntorch.log1p()\nTensor.log1p_\nTensor.log1p_\nIn-place version oflog1p()\nlog1p()\nTensor.log2\nTensor.log2\nSeetorch.log2()\ntorch.log2()\nTensor.log2_\nTensor.log2_\nIn-place version oflog2()\nlog2()\nTensor.log_normal_\nTensor.log_normal_\nFillsselftensor with numbers samples from the log-normal distribution parameterized by the given mean\u03bc\\mu\u03bcand standard deviation\u03c3\\sigma\u03c3.\nself\nTensor.logaddexp\nTensor.logaddexp\nSeetorch.logaddexp()\ntorch.logaddexp()\nTensor.logaddexp2\nTensor.logaddexp2\nSeetorch.logaddexp2()\ntorch.logaddexp2()\nTensor.logsumexp\nTensor.logsumexp\nSeetorch.logsumexp()\ntorch.logsumexp()\nTensor.logical_and\nTensor.logical_and\nSeetorch.logical_and()\ntorch.logical_and()\nTensor.logical_and_\nTensor.logical_and_\nIn-place version oflogical_and()\nlogical_and()\nTensor.logical_not\nTensor.logical_not\nSeetorch.logical_not()\ntorch.logical_not()\nTensor.logical_not_\nTensor.logical_not_\nIn-place version oflogical_not()\nlogical_not()\nTensor.logical_or\nTensor.logical_or\nSeetorch.logical_or()\ntorch.logical_or()\nTensor.logical_or_\nTensor.logical_or_\nIn-place version oflogical_or()\nlogical_or()\nTensor.logical_xor\nTensor.logical_xor\nSeetorch.logical_xor()\ntorch.logical_xor()\nTensor.logical_xor_\nTensor.logical_xor_\nIn-place version oflogical_xor()\nlogical_xor()\nTensor.logit\nTensor.logit\nSeetorch.logit()\ntorch.logit()\nTensor.logit_\nTensor.logit_\nIn-place version oflogit()\nlogit()\nTensor.long\nTensor.long\nself.long()is equivalent toself.to(torch.int64).\nself.long()\nself.to(torch.int64)\nTensor.lt\nTensor.lt\nSeetorch.lt().\ntorch.lt()\nTensor.lt_\nTensor.lt_\nIn-place version oflt().\nlt()\nTensor.less\nTensor.less\nlt(other) -> Tensor\nTensor.less_\nTensor.less_\nIn-place version ofless().\nless()\nTensor.lu\nTensor.lu\nSeetorch.lu()\ntorch.lu()\nTensor.lu_solve\nTensor.lu_solve\nSeetorch.lu_solve()\ntorch.lu_solve()\nTensor.as_subclass\nTensor.as_subclass\nMakes aclsinstance with the same data pointer asself.\ncls\nself\nTensor.map_\nTensor.map_\nAppliescallablefor each element inselftensor and the giventensorand stores the results inselftensor.\ncallable\nself\ntensor\nself\nTensor.masked_scatter_\nTensor.masked_scatter_\nCopies elements fromsourceintoselftensor at positions where themaskis True.\nsource\nself\nmask\nTensor.masked_scatter\nTensor.masked_scatter\nOut-of-place version oftorch.Tensor.masked_scatter_()\ntorch.Tensor.masked_scatter_()\nTensor.masked_fill_\nTensor.masked_fill_\nFills elements ofselftensor withvaluewheremaskis True.\nself\nvalue\nmask\nTensor.masked_fill\nTensor.masked_fill\nOut-of-place version oftorch.Tensor.masked_fill_()\ntorch.Tensor.masked_fill_()\nTensor.masked_select\nTensor.masked_select\nSeetorch.masked_select()\ntorch.masked_select()\nTensor.matmul\nTensor.matmul\nSeetorch.matmul()\ntorch.matmul()\nTensor.matrix_power\nTensor.matrix_power\nNotematrix_power()is deprecated, usetorch.linalg.matrix_power()instead.\nNote\nmatrix_power()is deprecated, usetorch.linalg.matrix_power()instead.\nmatrix_power()\ntorch.linalg.matrix_power()\nTensor.matrix_exp\nTensor.matrix_exp\nSeetorch.matrix_exp()\ntorch.matrix_exp()\nTensor.max\nTensor.max\nSeetorch.max()\ntorch.max()\nTensor.maximum\nTensor.maximum\nSeetorch.maximum()\ntorch.maximum()\nTensor.mean\nTensor.mean\nSeetorch.mean()\ntorch.mean()\nTensor.module_load\nTensor.module_load\nDefines how to transformotherwhen loading it intoselfinload_state_dict().\nother\nself\nload_state_dict()\nTensor.nanmean\nTensor.nanmean\nSeetorch.nanmean()\ntorch.nanmean()\nTensor.median\nTensor.median\nSeetorch.median()\ntorch.median()\nTensor.nanmedian\nTensor.nanmedian\nSeetorch.nanmedian()\ntorch.nanmedian()\nTensor.min\nTensor.min\nSeetorch.min()\ntorch.min()\nTensor.minimum\nTensor.minimum\nSeetorch.minimum()\ntorch.minimum()\nTensor.mm\nTensor.mm\nSeetorch.mm()\ntorch.mm()\nTensor.smm\nTensor.smm\nSeetorch.smm()\ntorch.smm()\nTensor.mode\nTensor.mode\nSeetorch.mode()\ntorch.mode()\nTensor.movedim\nTensor.movedim\nSeetorch.movedim()\ntorch.movedim()\nTensor.moveaxis\nTensor.moveaxis\nSeetorch.moveaxis()\ntorch.moveaxis()\nTensor.msort\nTensor.msort\nSeetorch.msort()\ntorch.msort()\nTensor.mul\nTensor.mul\nSeetorch.mul().\ntorch.mul()\nTensor.mul_\nTensor.mul_\nIn-place version ofmul().\nmul()\nTensor.multiply\nTensor.multiply\nSeetorch.multiply().\ntorch.multiply()\nTensor.multiply_\nTensor.multiply_\nIn-place version ofmultiply().\nmultiply()\nTensor.multinomial\nTensor.multinomial\nSeetorch.multinomial()\ntorch.multinomial()\nTensor.mv\nTensor.mv\nSeetorch.mv()\ntorch.mv()\nTensor.mvlgamma\nTensor.mvlgamma\nSeetorch.mvlgamma()\ntorch.mvlgamma()\nTensor.mvlgamma_\nTensor.mvlgamma_\nIn-place version ofmvlgamma()\nmvlgamma()\nTensor.nansum\nTensor.nansum\nSeetorch.nansum()\ntorch.nansum()\nTensor.narrow\nTensor.narrow\nSeetorch.narrow().\ntorch.narrow()\nTensor.narrow_copy\nTensor.narrow_copy\nSeetorch.narrow_copy().\ntorch.narrow_copy()\nTensor.ndimension\nTensor.ndimension\nAlias fordim()\ndim()\nTensor.nan_to_num\nTensor.nan_to_num\nSeetorch.nan_to_num().\ntorch.nan_to_num()\nTensor.nan_to_num_\nTensor.nan_to_num_\nIn-place version ofnan_to_num().\nnan_to_num()\nTensor.ne\nTensor.ne\nSeetorch.ne().\ntorch.ne()\nTensor.ne_\nTensor.ne_\nIn-place version ofne().\nne()\nTensor.not_equal\nTensor.not_equal\nSeetorch.not_equal().\ntorch.not_equal()\nTensor.not_equal_\nTensor.not_equal_\nIn-place version ofnot_equal().\nnot_equal()\nTensor.neg\nTensor.neg\nSeetorch.neg()\ntorch.neg()\nTensor.neg_\nTensor.neg_\nIn-place version ofneg()\nneg()\nTensor.negative\nTensor.negative\nSeetorch.negative()\ntorch.negative()\nTensor.negative_\nTensor.negative_\nIn-place version ofnegative()\nnegative()\nTensor.nelement\nTensor.nelement\nAlias fornumel()\nnumel()\nTensor.nextafter\nTensor.nextafter\nSeetorch.nextafter()\ntorch.nextafter()\nTensor.nextafter_\nTensor.nextafter_\nIn-place version ofnextafter()\nnextafter()\nTensor.nonzero\nTensor.nonzero\nSeetorch.nonzero()\ntorch.nonzero()\nTensor.norm\nTensor.norm\nSeetorch.norm()\ntorch.norm()\nTensor.normal_\nTensor.normal_\nFillsselftensor with elements samples from the normal distribution parameterized bymeanandstd.\nself\nmean\nstd\nTensor.numel\nTensor.numel\nSeetorch.numel()\ntorch.numel()\nTensor.numpy\nTensor.numpy\nReturns the tensor as a NumPyndarray.\nndarray\nTensor.orgqr\nTensor.orgqr\nSeetorch.orgqr()\ntorch.orgqr()\nTensor.ormqr\nTensor.ormqr\nSeetorch.ormqr()\ntorch.ormqr()\nTensor.outer\nTensor.outer\nSeetorch.outer().\ntorch.outer()\nTensor.permute\nTensor.permute\nSeetorch.permute()\ntorch.permute()\nTensor.pin_memory\nTensor.pin_memory\nCopies the tensor to pinned memory, if it's not already pinned.\nTensor.pinverse\nTensor.pinverse\nSeetorch.pinverse()\ntorch.pinverse()\nTensor.polygamma\nTensor.polygamma\nSeetorch.polygamma()\ntorch.polygamma()\nTensor.polygamma_\nTensor.polygamma_\nIn-place version ofpolygamma()\npolygamma()\nTensor.positive\nTensor.positive\nSeetorch.positive()\ntorch.positive()\nTensor.pow\nTensor.pow\nSeetorch.pow()\ntorch.pow()\nTensor.pow_\nTensor.pow_\nIn-place version ofpow()\npow()\nTensor.prod\nTensor.prod\nSeetorch.prod()\ntorch.prod()\nTensor.put_\nTensor.put_\nCopies the elements fromsourceinto the positions specified byindex.\nsource\nindex\nTensor.qr\nTensor.qr\nSeetorch.qr()\ntorch.qr()\nTensor.qscheme\nTensor.qscheme\nReturns the quantization scheme of a given QTensor.\nTensor.quantile\nTensor.quantile\nSeetorch.quantile()\ntorch.quantile()\nTensor.nanquantile\nTensor.nanquantile\nSeetorch.nanquantile()\ntorch.nanquantile()\nTensor.q_scale\nTensor.q_scale\nGiven a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().\nTensor.q_zero_point\nTensor.q_zero_point\nGiven a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().\nTensor.q_per_channel_scales\nTensor.q_per_channel_scales\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.\nTensor.q_per_channel_zero_points\nTensor.q_per_channel_zero_points\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.\nTensor.q_per_channel_axis\nTensor.q_per_channel_axis\nGiven a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.\nTensor.rad2deg\nTensor.rad2deg\nSeetorch.rad2deg()\ntorch.rad2deg()\nTensor.random_\nTensor.random_\nFillsselftensor with numbers sampled from the discrete uniform distribution over[from,to-1].\nself\n[from,to-1]\nTensor.ravel\nTensor.ravel\nseetorch.ravel()\ntorch.ravel()\nTensor.reciprocal\nTensor.reciprocal\nSeetorch.reciprocal()\ntorch.reciprocal()\nTensor.reciprocal_\nTensor.reciprocal_\nIn-place version ofreciprocal()\nreciprocal()\nTensor.record_stream\nTensor.record_stream\nMarks the tensor as having been used by this stream.\nTensor.register_hook\nTensor.register_hook\nRegisters a backward hook.\nTensor.register_post_accumulate_grad_hook\nTensor.register_post_accumulate_grad_hook\nRegisters a backward hook that runs after grad accumulation.\nTensor.remainder\nTensor.remainder\nSeetorch.remainder()\ntorch.remainder()\nTensor.remainder_\nTensor.remainder_\nIn-place version ofremainder()\nremainder()\nTensor.renorm\nTensor.renorm\nSeetorch.renorm()\ntorch.renorm()\nTensor.renorm_\nTensor.renorm_\nIn-place version ofrenorm()\nrenorm()\nTensor.repeat\nTensor.repeat\nRepeats this tensor along the specified dimensions.\nTensor.repeat_interleave\nTensor.repeat_interleave\nSeetorch.repeat_interleave().\ntorch.repeat_interleave()\nTensor.requires_grad\nTensor.requires_grad\nIsTrueif gradients need to be computed for this Tensor,Falseotherwise.\nTrue\nFalse\nTensor.requires_grad_\nTensor.requires_grad_\nChange if autograd should record operations on this tensor: sets this tensor'srequires_gradattribute in-place.\nrequires_grad\nTensor.reshape\nTensor.reshape\nReturns a tensor with the same data and number of elements asselfbut with the specified shape.\nself\nTensor.reshape_as\nTensor.reshape_as\nReturns this tensor as the same shape asother.\nother\nTensor.resize_\nTensor.resize_\nResizesselftensor to the specified size.\nself\nTensor.resize_as_\nTensor.resize_as_\nResizes theselftensor to be the same size as the specifiedtensor.\nself\ntensor\nTensor.retain_grad\nTensor.retain_grad\nEnables this Tensor to have theirgradpopulated duringbackward().\ngrad\nbackward()\nTensor.retains_grad\nTensor.retains_grad\nIsTrueif this Tensor is non-leaf and itsgradis enabled to be populated duringbackward(),Falseotherwise.\nTrue\ngrad\nbackward()\nFalse\nTensor.roll\nTensor.roll\nSeetorch.roll()\ntorch.roll()\nTensor.rot90\nTensor.rot90\nSeetorch.rot90()\ntorch.rot90()\nTensor.round\nTensor.round\nSeetorch.round()\ntorch.round()\nTensor.round_\nTensor.round_\nIn-place version ofround()\nround()\nTensor.rsqrt\nTensor.rsqrt\nSeetorch.rsqrt()\ntorch.rsqrt()\nTensor.rsqrt_\nTensor.rsqrt_\nIn-place version ofrsqrt()\nrsqrt()\nTensor.scatter\nTensor.scatter\nOut-of-place version oftorch.Tensor.scatter_()\ntorch.Tensor.scatter_()\nTensor.scatter_\nTensor.scatter_\nWrites all values from the tensorsrcintoselfat the indices specified in theindextensor.\nsrc\nself\nindex\nTensor.scatter_add_\nTensor.scatter_add_\nAdds all values from the tensorsrcintoselfat the indices specified in theindextensor in a similar fashion asscatter_().\nsrc\nself\nindex\nscatter_()\nTensor.scatter_add\nTensor.scatter_add\nOut-of-place version oftorch.Tensor.scatter_add_()\ntorch.Tensor.scatter_add_()\nTensor.scatter_reduce_\nTensor.scatter_reduce_\nReduces all values from thesrctensor to the indices specified in theindextensor in theselftensor using the applied reduction defined via thereduceargument (\"sum\",\"prod\",\"mean\",\"amax\",\"amin\").\nsrc\nindex\nself\nreduce\n\"sum\"\n\"prod\"\n\"mean\"\n\"amax\"\n\"amin\"\nTensor.scatter_reduce\nTensor.scatter_reduce\nOut-of-place version oftorch.Tensor.scatter_reduce_()\ntorch.Tensor.scatter_reduce_()\nTensor.select\nTensor.select\nSeetorch.select()\ntorch.select()\nTensor.select_scatter\nTensor.select_scatter\nSeetorch.select_scatter()\ntorch.select_scatter()\nTensor.set_\nTensor.set_\nSets the underlying storage, size, and strides.\nTensor.share_memory_\nTensor.share_memory_\nMoves the underlying storage to shared memory.\nTensor.short\nTensor.short\nself.short()is equivalent toself.to(torch.int16).\nself.short()\nself.to(torch.int16)\nTensor.sigmoid\nTensor.sigmoid\nSeetorch.sigmoid()\ntorch.sigmoid()\nTensor.sigmoid_\nTensor.sigmoid_\nIn-place version ofsigmoid()\nsigmoid()\nTensor.sign\nTensor.sign\nSeetorch.sign()\ntorch.sign()\nTensor.sign_\nTensor.sign_\nIn-place version ofsign()\nsign()\nTensor.signbit\nTensor.signbit\nSeetorch.signbit()\ntorch.signbit()\nTensor.sgn\nTensor.sgn\nSeetorch.sgn()\ntorch.sgn()\nTensor.sgn_\nTensor.sgn_\nIn-place version ofsgn()\nsgn()\nTensor.sin\nTensor.sin\nSeetorch.sin()\ntorch.sin()\nTensor.sin_\nTensor.sin_\nIn-place version ofsin()\nsin()\nTensor.sinc\nTensor.sinc\nSeetorch.sinc()\ntorch.sinc()\nTensor.sinc_\nTensor.sinc_\nIn-place version ofsinc()\nsinc()\nTensor.sinh\nTensor.sinh\nSeetorch.sinh()\ntorch.sinh()\nTensor.sinh_\nTensor.sinh_\nIn-place version ofsinh()\nsinh()\nTensor.asinh\nTensor.asinh\nSeetorch.asinh()\ntorch.asinh()\nTensor.asinh_\nTensor.asinh_\nIn-place version ofasinh()\nasinh()\nTensor.arcsinh\nTensor.arcsinh\nSeetorch.arcsinh()\ntorch.arcsinh()\nTensor.arcsinh_\nTensor.arcsinh_\nIn-place version ofarcsinh()\narcsinh()\nTensor.shape\nTensor.shape\nReturns the size of theselftensor.\nself\nTensor.size\nTensor.size\nReturns the size of theselftensor.\nself\nTensor.slogdet\nTensor.slogdet\nSeetorch.slogdet()\ntorch.slogdet()\nTensor.slice_scatter\nTensor.slice_scatter\nSeetorch.slice_scatter()\ntorch.slice_scatter()\nTensor.softmax\nTensor.softmax\nAlias fortorch.nn.functional.softmax().\ntorch.nn.functional.softmax()\nTensor.sort\nTensor.sort\nSeetorch.sort()\ntorch.sort()\nTensor.split\nTensor.split\nSeetorch.split()\ntorch.split()\nTensor.sparse_mask\nTensor.sparse_mask\nReturns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask.\nself\nmask\nTensor.sparse_dim\nTensor.sparse_dim\nReturn the number of sparse dimensions in asparse tensorself.\nself\nTensor.sqrt\nTensor.sqrt\nSeetorch.sqrt()\ntorch.sqrt()\nTensor.sqrt_\nTensor.sqrt_\nIn-place version ofsqrt()\nsqrt()\nTensor.square\nTensor.square\nSeetorch.square()\ntorch.square()\nTensor.square_\nTensor.square_\nIn-place version ofsquare()\nsquare()\nTensor.squeeze\nTensor.squeeze\nSeetorch.squeeze()\ntorch.squeeze()\nTensor.squeeze_\nTensor.squeeze_\nIn-place version ofsqueeze()\nsqueeze()\nTensor.std\nTensor.std\nSeetorch.std()\ntorch.std()\nTensor.stft\nTensor.stft\nSeetorch.stft()\ntorch.stft()\nTensor.storage\nTensor.storage\nReturns the underlyingTypedStorage.\nTypedStorage\nTensor.untyped_storage\nTensor.untyped_storage\nReturns the underlyingUntypedStorage.\nUntypedStorage\nTensor.storage_offset\nTensor.storage_offset\nReturnsselftensor's offset in the underlying storage in terms of number of storage elements (not bytes).\nself\nTensor.storage_type\nTensor.storage_type\nReturns the type of the underlying storage.\nTensor.stride\nTensor.stride\nReturns the stride ofselftensor.\nself\nTensor.sub\nTensor.sub\nSeetorch.sub().\ntorch.sub()\nTensor.sub_\nTensor.sub_\nIn-place version ofsub()\nsub()\nTensor.subtract\nTensor.subtract\nSeetorch.subtract().\ntorch.subtract()\nTensor.subtract_\nTensor.subtract_\nIn-place version ofsubtract().\nsubtract()\nTensor.sum\nTensor.sum\nSeetorch.sum()\ntorch.sum()\nTensor.sum_to_size\nTensor.sum_to_size\nSumthistensor tosize.\nthis\nsize\nTensor.svd\nTensor.svd\nSeetorch.svd()\ntorch.svd()\nTensor.swapaxes\nTensor.swapaxes\nSeetorch.swapaxes()\ntorch.swapaxes()\nTensor.swapdims\nTensor.swapdims\nSeetorch.swapdims()\ntorch.swapdims()\nTensor.t\nTensor.t\nSeetorch.t()\ntorch.t()\nTensor.t_\nTensor.t_\nIn-place version oft()\nt()\nTensor.tensor_split\nTensor.tensor_split\nSeetorch.tensor_split()\ntorch.tensor_split()\nTensor.tile\nTensor.tile\nSeetorch.tile()\ntorch.tile()\nTensor.to\nTensor.to\nPerforms Tensor dtype and/or device conversion.\nTensor.to_mkldnn\nTensor.to_mkldnn\nReturns a copy of the tensor intorch.mkldnnlayout.\ntorch.mkldnn\nTensor.take\nTensor.take\nSeetorch.take()\ntorch.take()\nTensor.take_along_dim\nTensor.take_along_dim\nSeetorch.take_along_dim()\ntorch.take_along_dim()\nTensor.tan\nTensor.tan\nSeetorch.tan()\ntorch.tan()\nTensor.tan_\nTensor.tan_\nIn-place version oftan()\ntan()\nTensor.tanh\nTensor.tanh\nSeetorch.tanh()\ntorch.tanh()\nTensor.tanh_\nTensor.tanh_\nIn-place version oftanh()\ntanh()\nTensor.atanh\nTensor.atanh\nSeetorch.atanh()\ntorch.atanh()\nTensor.atanh_\nTensor.atanh_\nIn-place version ofatanh()\natanh()\nTensor.arctanh\nTensor.arctanh\nSeetorch.arctanh()\ntorch.arctanh()\nTensor.arctanh_\nTensor.arctanh_\nIn-place version ofarctanh()\narctanh()\nTensor.tolist\nTensor.tolist\nReturns the tensor as a (nested) list.\nTensor.topk\nTensor.topk\nSeetorch.topk()\ntorch.topk()\nTensor.to_dense\nTensor.to_dense\nCreates a strided copy ofselfifselfis not a strided tensor, otherwise returnsself.\nself\nself\nself\nTensor.to_sparse\nTensor.to_sparse\nReturns a sparse copy of the tensor.\nTensor.to_sparse_csr\nTensor.to_sparse_csr\nConvert a tensor to compressed row storage format (CSR).\nTensor.to_sparse_csc\nTensor.to_sparse_csc\nConvert a tensor to compressed column storage (CSC) format.\nTensor.to_sparse_bsr\nTensor.to_sparse_bsr\nConvert a tensor to a block sparse row (BSR) storage format of given blocksize.\nTensor.to_sparse_bsc\nTensor.to_sparse_bsc\nConvert a tensor to a block sparse column (BSC) storage format of given blocksize.\nTensor.trace\nTensor.trace\nSeetorch.trace()\ntorch.trace()\nTensor.transpose\nTensor.transpose\nSeetorch.transpose()\ntorch.transpose()\nTensor.transpose_\nTensor.transpose_\nIn-place version oftranspose()\ntranspose()\nTensor.triangular_solve\nTensor.triangular_solve\nSeetorch.triangular_solve()\ntorch.triangular_solve()\nTensor.tril\nTensor.tril\nSeetorch.tril()\ntorch.tril()\nTensor.tril_\nTensor.tril_\nIn-place version oftril()\ntril()\nTensor.triu\nTensor.triu\nSeetorch.triu()\ntorch.triu()\nTensor.triu_\nTensor.triu_\nIn-place version oftriu()\ntriu()\nTensor.true_divide\nTensor.true_divide\nSeetorch.true_divide()\ntorch.true_divide()\nTensor.true_divide_\nTensor.true_divide_\nIn-place version oftrue_divide_()\ntrue_divide_()\nTensor.trunc\nTensor.trunc\nSeetorch.trunc()\ntorch.trunc()\nTensor.trunc_\nTensor.trunc_\nIn-place version oftrunc()\ntrunc()\nTensor.type\nTensor.type\nReturns the type ifdtypeis not provided, else casts this object to the specified type.\nTensor.type_as\nTensor.type_as\nReturns this tensor cast to the type of the given tensor.\nTensor.unbind\nTensor.unbind\nSeetorch.unbind()\ntorch.unbind()\nTensor.unflatten\nTensor.unflatten\nSeetorch.unflatten().\ntorch.unflatten()\nTensor.unfold\nTensor.unfold\nReturns a view of the original tensor which contains all slices of sizesizefromselftensor in the dimensiondimension.\nsize\nself\ndimension\nTensor.uniform_\nTensor.uniform_\nFillsselftensor with numbers sampled from the continuous uniform distribution:\nself\nTensor.unique\nTensor.unique\nReturns the unique elements of the input tensor.\nTensor.unique_consecutive\nTensor.unique_consecutive\nEliminates all but the first element from every consecutive group of equivalent elements.\nTensor.unsqueeze\nTensor.unsqueeze\nSeetorch.unsqueeze()\ntorch.unsqueeze()\nTensor.unsqueeze_\nTensor.unsqueeze_\nIn-place version ofunsqueeze()\nunsqueeze()\nTensor.values\nTensor.values\nReturn the values tensor of asparse COO tensor.\nTensor.var\nTensor.var\nSeetorch.var()\ntorch.var()\nTensor.vdot\nTensor.vdot\nSeetorch.vdot()\ntorch.vdot()\nTensor.view\nTensor.view\nReturns a new tensor with the same data as theselftensor but of a differentshape.\nself\nshape\nTensor.view_as\nTensor.view_as\nView this tensor as the same size asother.\nother\nTensor.vsplit\nTensor.vsplit\nSeetorch.vsplit()\ntorch.vsplit()\nTensor.where\nTensor.where\nself.where(condition,y)is equivalent totorch.where(condition,self,y).\nself.where(condition,y)\ntorch.where(condition,self,y)\nTensor.xlogy\nTensor.xlogy\nSeetorch.xlogy()\ntorch.xlogy()\nTensor.xlogy_\nTensor.xlogy_\nIn-place version ofxlogy()\nxlogy()\nTensor.xpu\nTensor.xpu\nReturns a copy of this object in XPU memory.\nTensor.zero_\nTensor.zero_\nFillsselftensor with zeros.\nself",
    "url": "https://pytorch.org/docs/stable/tensors.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "cbdb041ea20b4168efee1081ec3230e2",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/quickstart.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "65eb08b5848309eff93c7c8238f96316",
    "source": "pytorch_docs",
    "title": "AOTInductor Minifier \u2014 PyTorch 2.9 documentation",
    "text": "\n## AOTInductor Minifier#\n\nCreated On: Jun 13, 2025 | Last Updated On: Jun 13, 2025\nIf you encounter an error while using AOT Inductor APIs such astorch._inductor.aoti_compile_and_package,torch._indcutor.aoti_load_package,\nor running the loaded model ofaoti_load_packageon some inputs, you can use the AOTInductor Minifier\nto create a minimal nn.Module that reproduce the error by settingfromtorch._inductorimportconfig;config.aot_inductor.dump_aoti_minifier=True.\ntorch._inductor.aoti_compile_and_package\ntorch._indcutor.aoti_load_package\naoti_load_package\nfromtorch._inductorimportconfig;config.aot_inductor.dump_aoti_minifier=True\nOne a high-level, there are two steps in using the minifier:\nSetfromtorch._inductorimportconfig;config.aot_inductor.dump_aoti_minifier=Trueor set the environment variableDUMP_AOTI_MINIFIER=1. Then running the script that errors would produce aminifier_launcher.pyscript. The output directory is configurable by settingtorch._dynamo.config.debug_dir_rootto a valid directory name.\nfromtorch._inductorimportconfig;config.aot_inductor.dump_aoti_minifier=True\nDUMP_AOTI_MINIFIER=1\nminifier_launcher.py\ntorch._dynamo.config.debug_dir_root\nRun theminifier_launcher.pyscript. If the minifier runs successfully, it generates runnable python code inrepro.pywhich reproduces the exact error.\nminifier_launcher.py\nrepro.py\n\n## Example Code#\n\nHere is sample code which will generate an error because we injected an error on relu withtorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY=\"compile_error\".\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY=\"compile_error\"\n\n```python\nimport torch\nfrom torch._inductor import config as inductor_config\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 16)\n        self.relu = torch.nn.ReLU()\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.sigmoid(x)\n        return x\n\n\ninductor_config.aot_inductor.dump_aoti_minifier = True\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = \"compile_error\"\n\nwith torch.no_grad():\n    model = Model().to(\"cuda\")\n    example_inputs = (torch.randn(8, 10).to(\"cuda\"),)\n    ep = torch.export.export(model, example_inputs)\n    package_path = torch._inductor.aoti_compile_and_package(ep)\n    compiled_model = torch._inductor.aoti_load_package(package_path)\n    result = compiled_model(*example_inputs)\n\n```\n\nThe code above generates the following error:\n\n```python\nRuntimeError: Failed to import /tmp/torchinductor_shangdiy/fr/cfrlf4smkwe4lub4i4cahkrb3qiczhf7hliqqwpewbw3aplj5g3s.py\nSyntaxError: invalid syntax (cfrlf4smkwe4lub4i4cahkrb3qiczhf7hliqqwpewbw3aplj5g3s.py, line 29)\n\n```\n\nThis is because we injected an error on relu, and so the generated triton kernel looks like below. Note that we havecompileerror!instead ifrelu, so we get aSyntaxError.\ncompileerror!\nrelu\nSyntaxError\n\n```python\n@triton.jit\ndef triton_poi_fused_addmm_relu_sigmoid_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 16\n    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = compile error!\n    tmp4 = tl.sigmoid(tmp3)\n    tl.store(in_out_ptr0 + (x2), tmp4, xmask)\n\n```\n\nSince we havetorch._inductor.config.aot_inductor.dump_aoti_minifier=True, we also see an additional line indicating whereminifier_launcher.pyhas\nbeen written to. The output directory is configurable by settingtorch._dynamo.config.debug_dir_rootto a valid directory name.\ntorch._inductor.config.aot_inductor.dump_aoti_minifier=True\nminifier_launcher.py\ntorch._dynamo.config.debug_dir_root\n\n```python\nW1031 16:21:08.612000 2861654 pytorch/torch/_dynamo/debug_utils.py:279] Writing minified repro to:\nW1031 16:21:08.612000 2861654 pytorch/torch/_dynamo/debug_utils.py:279] /data/users/shangdiy/pytorch/torch_compile_debug/run_2024_10_31_16_21_08_602433-pid_2861654/minifier/minifier_launcher.py\n\n```\n\n\n## Minifier Launcher#\n\nTheminifier_launcher.pyfile has the following code. Theexported_programcontains the inputs totorch._inductor.aoti_compile_and_package.\nThecommand='minify'parameter means the script will run the minifier to create a minimal graph module that reproduce the error. Alternatively, you set\nusecommand='run'to just compile, load, and run the loaded model (without running the minifier).\nminifier_launcher.py\nexported_program\ntorch._inductor.aoti_compile_and_package\ncommand='minify'\ncommand='run'\n\n```python\nimport torch\nimport torch._inductor.inductor_prims\n\nimport torch._dynamo.config\nimport torch._inductor.config\nimport torch._functorch.config\nimport torch.fx.experimental._config\n\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error'\ntorch._inductor.config.aot_inductor.dump_aoti_minifier = True\n\n\n\n\nisolate_fails_code_str = None\n\n\n\n# torch version: 2.6.0a0+gitcd9c6e9\n# torch cuda version: 12.0\n# torch git version: cd9c6e9408dd79175712223895eed36dbdc84f84\n\n\n# CUDA Info:\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2023 NVIDIA Corporation\n# Built on Fri_Jan__6_16:45:21_PST_2023\n# Cuda compilation tools, release 12.0, V12.0.140\n# Build cuda_12.0.r12.0/compiler.32267302_0\n\n# GPU Hardware Info:\n# NVIDIA PG509-210 : 8\n\nexported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints/exported_program.pt2')\n# print(exported_program.graph)\nconfig_patches={}\nif __name__ == '__main__':\n    from torch._dynamo.repro.aoti import run_repro\n    with torch.no_grad():\n        run_repro(exported_program, config_patches=config_patches, accuracy=False, command='minify', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints', check_str=None)\n\n```\n\nSuppose we kept thecommand='minify'option, and run the script, we would get the following output:\ncommand='minify'\n\n```python\n...\nW1031 16:48:08.938000 3598491 torch/_dynamo/repro/aoti.py:89] Writing checkpoint with 3 nodes to /data/users/shangdiy/pytorch/torch_compile_debug/run_2024_10_31_16_48_02_720863-pid_3598491/minifier/checkpoints/3.py\nW1031 16:48:08.975000 3598491 torch/_dynamo/repro/aoti.py:101] Copying repro file for convenience to /data/users/shangdiy/pytorch/repro.py\nWrote minimal repro out to repro.py\n\n```\n\nIf you get anAOTIMinifierErrorwhen runningminifier_launcher.py, please report a bughere.\nAOTIMinifierError\nminifier_launcher.py\n\n## Minified Result#\n\nTherepro.pylooks like this. Notice that the exported program is printed at the top of the file, and it contains only the relu node. The minifier successfully reduced the graph to the op that raises the error.\nrepro.py\n\n```python\n# from torch.nn import *\n# class Repro(torch.nn.Module):\n#     def __init__(self) -> None:\n#         super().__init__()\n\n\n\n#     def forward(self, linear):\n#         relu = torch.ops.aten.relu.default(linear);  linear = None\n#         return (relu,)\n\nimport torch\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nimport torch._inductor.inductor_prims\n\nimport torch._dynamo.config\nimport torch._inductor.config\nimport torch._functorch.config\nimport torch.fx.experimental._config\n\ntorch._inductor.config.generate_intermediate_hooks = True\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error'\ntorch._inductor.config.aot_inductor.dump_aoti_minifier = True\n\n\n\n\nisolate_fails_code_str = None\n\n\n\n# torch version: 2.6.0a0+gitcd9c6e9\n# torch cuda version: 12.0\n# torch git version: cd9c6e9408dd79175712223895eed36dbdc84f84\n\n\n# CUDA Info:\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2023 NVIDIA Corporation\n# Built on Fri_Jan__6_16:45:21_PST_2023\n# Cuda compilation tools, release 12.0, V12.0.140\n# Build cuda_12.0.r12.0/compiler.32267302_0\n\n# GPU Hardware Info:\n# NVIDIA PG509-210 : 8\n\n\nexported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints/exported_program.pt2')\n# print(exported_program.graph)\nconfig_patches={'aot_inductor.package': True}\nif __name__ == '__main__':\n    from torch._dynamo.repro.aoti import run_repro\n    with torch.no_grad():\n        run_repro(exported_program, config_patches=config_patches, accuracy=False, command='run', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints', check_str=None)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/torch.compiler_aot_inductor_minifier.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "78fb4fedf103adc6b97789dd754f7d10",
    "source": "pytorch_docs",
    "title": "Toggling error_on_graph_break \u2014 PyTorch 2.9 documentation",
    "text": "\n## Togglingerror_on_graph_break#\n\nerror_on_graph_break\nCreated On: Sep 03, 2025 | Last Updated On: Sep 03, 2025\nSummary:\nWhenfullgraph=False, we can usetorch._dynamo.error_on_graph_break()for more flexibility in\ndealing with graph breaks.\nfullgraph=False\ntorch._dynamo.error_on_graph_break()\nSo far, we have introduced two ways in dealing with graph breaks intorch.compile:\ntorch.compile\nfullgraph=Trueerrors on the first graph break and additionally guarantees that only one graph is traced from the code.\nfullgraph=True\nfullgraph=Falsecontinues tracing even when encountering graph breaks.\nfullgraph=False\nWhat if we want to disallow graph breaks for most of the code, but there are a few problematic functions where the graph breaks are hard to remove,\nand we are okay with having those graph breaks? We can usetorch._dynamo.error_on_graph_break()to achieve this.\ntorch._dynamo.error_on_graph_break()\ntorch.compilehas anerror_on_graph_breaksetting (initially set toFalse).\nIf a graph break or compiler error occurs in code whileerror_on_graph_breakis set toFalse, thentorch.compilewill attempt to continue compilation after the graph break/error.\nIferror_on_graph_breakis set toTrue, thentorch.compilewill abort compilation and propagate the error to user code.\ntorch.compile\nerror_on_graph_break\nFalse\nerror_on_graph_break\nFalse\ntorch.compile\nerror_on_graph_break\nTrue\ntorch.compile\nA significant difference betweenerror_on_graph_break=Trueandfullgraph=Trueis that the formerdoes not guarantee that a single graph will be captured.error_on_graph_breakcan be arbitrarily toggled during compile timeby using thetorch._dynamo.error_on_graph_break()context manager/decorator.\nIn comparison, oncefullgraphis set toTrue, it cannot be set back toFalse.\nFinally,error_on_graph_breakhas lower precedence thanfullgraph-error_on_graph_breakonly takes effect whenfullgraph=False.\nerror_on_graph_break=True\nfullgraph=True\nerror_on_graph_break\ntorch._dynamo.error_on_graph_break()\nfullgraph\nTrue\nFalse\nerror_on_graph_break\nfullgraph\nerror_on_graph_break\nfullgraph=False\n\n## error_on_graph_break(False)example#\n\nerror_on_graph_break(False)\n\n```python\n@torch._dynamo.error_on_graph_break(False)\ndef code_with_a_difficult_graph_break(x):\n    x = x + 1\n    torch._dynamo.graph_break()\n    return x + 2\n\ndef inner(x):\n    return code_with_a_difficult_graph_break(x)\n\n# NOTE: fullgraph=False\n@torch._dynamo.error_on_graph_break(True)\n@torch.compile\ndef fn(x):\n    return inner(x)\n\n# No error, but there is a graph break\nfn(torch.randn(3))\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_357/1452578661.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_357/1452578661.py\", line 17, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_357/1452578661.py\", line 14, in fn\n    return inner(x)\n  File \"/tmp/ipykernel_357/1452578661.py\", line 8, in inner\n    return code_with_a_difficult_graph_break(x)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_357/1452578661.py\", line 4, in code_with_a_difficult_graph_break\n    torch._dynamo.graph_break()\n\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_357/1452578661.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_357/1452578661.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_357/1452578661.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\n```\n\n\n```python\ntensor([3.1074, 3.0004, 2.7473])\n\n```\n\nUsingerror_on_graph_break(False)undererror_on_graph_break(True)is helpful for when we want to minimize graph breaks (i.e. follow thefullgraph=Trueprogramming model),\nbut there are some sections of code with non-performance-critical graph breaks that are difficult to work around.\nerror_on_graph_break(False)\nerror_on_graph_break(True)\nfullgraph=True\nerror_on_graph_break()can be used as a context manager as well:\nerror_on_graph_break()\n\n```python\n# NOTE: fullgraph=False\n@torch._dynamo.error_on_graph_break(True)\n@torch.compile\ndef fn(x):\n    x = x + 1\n    with torch._dynamo.error_on_graph_break(False):\n        torch._dynamo.graph_break()  # no error\n    return x + 2\n\n# No error, but there is a graph break\nfn(torch.randn(3))\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_357/737485247.py:7\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_357/737485247.py\", line 11, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_357/737485247.py\", line 7, in fn\n    torch._dynamo.graph_break()  # no error\n\n```\n\n\n```python\ntensor([2.5366, 3.5258, 3.4748])\n\n```\n\nYou can use monkey patching to toggleerror_on_graph_breakfor code where you cannot edit the source (e.g. framework code):\nerror_on_graph_break\n\n```python\nclass ThirdPartyModule(torch.nn.Module):\n    def forward(self, x):\n        x = x + 1\n        torch._dynamo.graph_break()\n        return x + 2\n\ntp_mod = ThirdPartyModule()\ntp_mod.forward = torch._dynamo.error_on_graph_break(False)(tp_mod.forward)\n\n@torch._dynamo.error_on_graph_break(True)\n@torch.compile\ndef fn(x):\n    return tp_mod.forward(x)\n\n# No error, but there is a graph break\nfn(torch.randn(3))\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_357/2112598647.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_357/2112598647.py\", line 16, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_357/2112598647.py\", line 13, in fn\n    return tp_mod.forward(x)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_357/2112598647.py\", line 4, in forward\n    torch._dynamo.graph_break()\n\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_357/2112598647.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_357/2112598647.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\n```\n\n\n```python\ntensor([2.6542, 2.0017, 3.5263])\n\n```\n\n\n## error_on_graph_break(True)example#\n\nerror_on_graph_break(True)\n\n```python\n@torch._dynamo.error_on_graph_break(True)\ndef inner2(x):\n    x = x + 1\n    torch._dynamo.graph_break()  # error\n    return x + 2\n\ndef inner(x):\n    return inner2(x)\n\n# fullgraph=False, error_on_graph_break=False\n@torch.compile\ndef fn(x):\n    x = x + 4\n    torch._dynamo.graph_break()  # no error\n    return inner(x)\n\ntry:\n    fn(torch.randn(3))\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nCall to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_357/2379101916.py\", line 15, in torch_dynamo_resume_in_fn_at_14\n    return inner(x)\n  File \"/tmp/ipykernel_357/2379101916.py\", line 8, in inner\n    return inner2(x)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_357/2379101916.py\", line 4, in inner2\n    torch._dynamo.graph_break()  # error\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_357/2379101916.py:14\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_357/2379101916.py\", line 18, in <module>\n    fn(torch.randn(3))\n  File \"/tmp/ipykernel_357/2379101916.py\", line 14, in fn\n    torch._dynamo.graph_break()  # no error\n\n```\n\nUsingerror_on_graph_break(True)undererror_on_graph_break(False)is helpful for when we want to usetorch.compileflexibly (i.e. follow thefullgraph=Falseprogramming model),\nbut there are some sections of the code that are performance-critical and we want to ensure that those sections do not contain graph breaks.\nerror_on_graph_break(True)\nerror_on_graph_break(False)\ntorch.compile\nfullgraph=False\n\n## error_on_graph_breaknesting behavior#\n\nerror_on_graph_break\ntorch._dynamo.error_on_graph_break()affects theerror_on_graph_breaksetting of nested calls as well:\ntorch._dynamo.error_on_graph_break()\nerror_on_graph_break\n\n```python\ndef inner(x):\n    x = x + 1\n    torch._dynamo.graph_break()\n    return x + 2\n\ndef inner2(x):\n    with torch._dynamo.error_on_graph_break(False):\n        return inner(x)\n\n@torch._dynamo.error_on_graph_break(True)\n@torch.compile\ndef fn(x):\n    return inner2(x)\n\n# no error\nfn(torch.randn(3))\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_357/1007149706.py:3\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_357/1007149706.py\", line 16, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_357/1007149706.py\", line 13, in fn\n    return inner2(x)\n  File \"/tmp/ipykernel_357/1007149706.py\", line 8, in inner2\n    return inner(x)\n  File \"/tmp/ipykernel_357/1007149706.py\", line 3, in inner\n    torch._dynamo.graph_break()\n\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_357/1007149706.py:3\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_357/1007149706.py:3\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\n```\n\n\n```python\ntensor([2.3742, 1.1152, 2.7729])\n\n```\n\ntorch._dynamo.error_on_graph_break()can be used under anothertorch._dynamo.error_on_graph_break()region:\ntorch._dynamo.error_on_graph_break()\ntorch._dynamo.error_on_graph_break()\n\n```python\ndef inner(x):\n    x = x + 1\n    with torch._dynamo.error_on_graph_break(False):\n        torch._dynamo.graph_break()\n    return x + 2\n\ndef inner2(x):\n    with torch._dynamo.error_on_graph_break(True):\n        return inner(x)\n\n@torch.compile\ndef fn(x):\n    return inner2(x)\n\n# no error\nfn(torch.randn(3))\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_357/1343774799.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_357/1343774799.py\", line 16, in <module>\n    fn(torch.randn(3))\n  File \"/tmp/ipykernel_357/1343774799.py\", line 13, in fn\n    return inner2(x)\n  File \"/tmp/ipykernel_357/1343774799.py\", line 9, in inner2\n    return inner(x)\n  File \"/tmp/ipykernel_357/1343774799.py\", line 4, in inner\n    torch._dynamo.graph_break()\n\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_357/1343774799.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_357/1343774799.py:4\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\n```\n\n\n```python\ntensor([3.5156, 2.9280, 3.7220])\n\n```\n\n\n## Interaction withfullgraph#\n\nfullgraph\nfullgraph=Truetakes higher precedence thanerror_on_graph_break:\nfullgraph=True\nerror_on_graph_break\n\n```python\n@torch._dynamo.error_on_graph_break(False)\ndef inner(x):\n    x = x + 1\n    torch._dynamo.graph_break()\n    return x + 2\n\n@torch.compile(fullgraph=True)\ndef fn(x):\n    return inner(x)\n\ntry:\n    fn(torch.randn(3))\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nCall to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_357/2331424258.py\", line 9, in fn\n    return inner(x)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 209, in inner\n    return func(*args, **kwargs)\n  File \"/tmp/ipykernel_357/2331424258.py\", line 3, in inner\n    x = x + 1\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\nfullgraph=Truecannot be toggled back tofullgraph=False:\nfullgraph=True\nfullgraph=False\n\n```python\n@torch.compile(fullgraph=False)\ndef inner(x):\n    x = x + 1\n    torch._dynamo.graph_break()\n    return x + 2\n\n@torch.compile(fullgraph=True)\ndef fn(x):\n    return inner(x)\n\ntry:\n    fn(torch.randn(3))\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nCall to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_357/262151723.py\", line 9, in fn\n    return inner(x)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 264, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/tmp/ipykernel_357/262151723.py\", line 3, in inner\n    x = x + 1\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\n@torch.compile(fullgraph=True)\ndef inner(x):\n    x = x + 1\n    torch._dynamo.graph_break()\n    return x + 2\n\n@torch.compile(fullgraph=False)\ndef fn(x):\n    return inner(x)\n\ntry:\n    fn(torch.randn(3))\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nCall to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_357/2119173801.py\", line 3, in inner\n    x = x + 1\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_357/2119173801.py:3\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_357/2119173801.py\", line 12, in <module>\n    fn(torch.randn(3))\n  File \"/tmp/ipykernel_357/2119173801.py\", line 9, in fn\n    return inner(x)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py\", line 264, in getattr_and_trace\n    return fn(*args[2:], **kwargs)\n  File \"/tmp/ipykernel_357/2119173801.py\", line 3, in inner\n    x = x + 1\n\n```\n\n\n## Summary offullgraph=True/Falsevserror_on_graph_break#\n\nfullgraph=True/False\nerror_on_graph_break\nHere is a table summarizing the differences betweenfullgraph=True/Falseanderror_on_graph_break:\nfullgraph=True/False\nerror_on_graph_break\n\nerror_on_graph_break=True\nerror_on_graph_break=True\nerror_on_graph_break=False(default)\nerror_on_graph_break=False\nfullgraph=True\nfullgraph=True\nGraph breaks result in errors. Only the first graph break will be reported.One graph guarantee.fullgraphcannot be toggled toFalse.error_on_graph_breakhas no effect.User code must be fully compatible withtorch.compile. Guarantees no performance hits from graph breaks (because there are no graph breaks).Ideal for code sensitive to graph breaks: framework/library code or cases where getting maximum performance is required. Prevents downstream user code from inadvertently allowing graph breaks.\nfullgraph\nFalse\nerror_on_graph_break\ntorch.compile\nSame asfullgraph=Trueanderror_on_graph_break=Trueaserror_on_graph_breakhas no effect whenfullgraph=True.\nfullgraph=True\nerror_on_graph_break=True\nerror_on_graph_break\nfullgraph=True\nfullgraph=False(default)\nfullgraph=False\nGraph breaks result in errors. Only the first graph break will be reported.No one graph guarantee.error_on_graph_breakcan be toggled toFalse.User code must be fully compatible withtorch.compile. Guarantees no performance hits from graph breaks (because there are no graph breaks).Ideal for user code sensitive to graph breaks.error_on_graph_breakcan be toggled toFalseto deal with sections that have graph breaks that are difficult to work around.\nerror_on_graph_break\nFalse\ntorch.compile\nerror_on_graph_break\nFalse\nWill continue to compile after encountering graph breaks. All graph breaks will be reported.error_on_graph_breakcan be toggled toTrue.Doesn\u2019t require many user code changes to work. Performance may be negatively impacted due to graph breaks.Ideal for out-of-the-box use cases, on \u201cnon-weird\u201d code, or where squeezing maximal performance is not necessary\nerror_on_graph_break\nTrue",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.error_on_graph_break.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "28d8cab3dc6bdf9a27748cd56e4c035a",
    "source": "pytorch_docs",
    "title": "torch._logging \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch._logging#\n\nCreated On: Apr 24, 2023 | Last Updated On: Jun 17, 2025\nPyTorch has a configurable logging system, where different components can be\ngiven different log level settings. For instance, one component\u2019s log messages\ncan be completely disabled, while another component\u2019s log messages can be\nset to maximum verbosity.\nWarning\nThis feature is in beta and may have compatibility breaking\nchanges in the future.\nWarning\nThis feature has not been expanded to control the log messages of\nall components in PyTorch yet.\nThere are two ways to configure the logging system: through the environment variableTORCH_LOGSor the python API torch._logging.set_logs.\nTORCH_LOGS\nset_logs\n\nset_logs\nSets the log level for individual components and toggles individual log artifact types.\nThe environment variableTORCH_LOGSis a comma-separated list of[+-]<component>pairs, where<component>is a component specified below. The+prefix\nwill decrease the log level of the component, displaying more log messages while the-prefix\nwill increase the log level of the component and display fewer log messages. The default setting\nis the behavior when a component is not specified inTORCH_LOGS. In addition to components, there are\nalso artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed,\nso prefixing an artifact with+or-will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact,\nunless that artifact was specified to beoff_by_default. This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled.\nThe following components and artifacts are configurable through theTORCH_LOGSenvironment\nvariable (see torch._logging.set_logs for the python API):\nTORCH_LOGS\n[+-]<component>\n<component>\n+\n-\nTORCH_LOGS\n+\n-\noff_by_default\nTORCH_LOGS\nall\nSpecial component which configures the default log level of all components. Default:logging.WARN\nlogging.WARN\ndynamo\nThe log level for the TorchDynamo component. Default:logging.WARN\nlogging.WARN\naot\nThe log level for the AOTAutograd component. Default:logging.WARN\nlogging.WARN\ninductor\nThe log level for the TorchInductor component. Default:logging.WARN\nlogging.WARN\nyour.custom.module\nThe log level for an arbitrary unregistered module. Provide the fully qualified name and the module will be enabled. Default:logging.WARN\nlogging.WARN\nbytecode\nWhether to emit the original and generated bytecode from TorchDynamo.\nDefault:False\nFalse\naot_graphs\nWhether to emit the graphs generated by AOTAutograd. Default:False\nFalse\naot_joint_graph\nWhether to emit the joint forward-backward graph generated by AOTAutograd. Default:False\nFalse\ncompiled_autograd\nWhether to emit logs from compiled_autograd. Defaults:False\nFalse\nddp_graphs\nWhether to emit graphs generated by DDPOptimizer. Default:False\nFalse\ngraph\nWhether to emit the graph captured by TorchDynamo in tabular format.\nDefault:False\nFalse\ngraph_code\nWhether to emit the python source of the graph captured by TorchDynamo.\nDefault:False\nFalse\ngraph_breaks\nWhether to emit a message when a unique graph break is encountered during\nTorchDynamo tracing. Default:False\nFalse\nguards\nWhether to emit the guards generated by TorchDynamo for each compiled\nfunction. Default:False\nFalse\nrecompiles\nWhether to emit a guard failure reason and message every time\nTorchDynamo recompiles a function. Default:False\nFalse\noutput_code\nWhether to emit the TorchInductor output code. Default:False\nFalse\nschedule\nWhether to emit the TorchInductor schedule. Default:False\nFalse\nTORCH_LOGS=\"+dynamo,aot\"will set the log level of TorchDynamo tologging.DEBUGand  AOT tologging.INFO\nTORCH_LOGS=\"+dynamo,aot\"\nlogging.DEBUG\nlogging.INFO\nTORCH_LOGS=\"-dynamo,+inductor\"will set the log level of TorchDynamo tologging.ERRORand  TorchInductor tologging.DEBUG\nTORCH_LOGS=\"-dynamo,+inductor\"\nlogging.ERROR\nlogging.DEBUG\nTORCH_LOGS=\"aot_graphs\"will enable theaot_graphsartifact\nTORCH_LOGS=\"aot_graphs\"\naot_graphs\nTORCH_LOGS=\"+dynamo,schedule\"will enable set the log level of TorchDynamo tologging.DEBUGand enable thescheduleartifact\nTORCH_LOGS=\"+dynamo,schedule\"\nlogging.DEBUG\nschedule\nTORCH_LOGS=\"+some.random.module,schedule\"will set the log level of some.random.module tologging.DEBUGand enable thescheduleartifact\nTORCH_LOGS=\"+some.random.module,schedule\"\nlogging.DEBUG\nschedule",
    "url": "https://pytorch.org/docs/stable/logging.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "61c1e68ca34cea5bb4ac5b4ef87ba1d7",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.skipped_functions.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3024d91734ec9b5a240d94dae37f359f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/hub.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a929194818996a03baceef44af763089",
    "source": "pytorch_docs",
    "title": "Control Plane \u2014 PyTorch 2.9 documentation",
    "text": "\n## Control Plane#\n\nCreated On: May 30, 2024 | Last Updated On: Jun 04, 2024\nThis module contains optional helpers that add extra debug and control handlers\ninto your application.\nThis is a context manager that wraps your main entry function. This combines\nthe existingerrors.recordlogic as well as a new_WorkerServerthat\nexposes handlers via a unix socket specified byTorch_WORKER_SERVER_SOCKET.\nerrors.record\n_WorkerServer\nTorch_WORKER_SERVER_SOCKET\nExample\n\n```python\n@worker_main()\ndef main():\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\nGenerator[None, None, None]",
    "url": "https://pytorch.org/docs/stable/elastic/control_plane.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "baed15e46b17dfd5e648f605bb4650aa",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.error_on_graph_break.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7967b281938c0456198b2fc7bb91e0f8",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/torch.dynamic-shape.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "85626c10ded1d23d14a2547f3164f66a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler.config.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "36a510f52504d95672594594a384600f",
    "source": "pytorch_docs",
    "title": "torch.compiler.cudagraph_mark_step_begin \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.compiler.cudagraph_mark_step_begin#\n\nIndicates that a new iteration of inference or training is about to begin.\nCUDA Graphs will free tensors of a prior iteration. A new iteration is started on each invocation of\ntorch.compile, so long as there is not a pending backward that has not been called.\nIf that heuristic is wrong, such as in the following example, manually mark it with this api.\n\n```python\n@torch.compile(mode=\"reduce-overhead\")\ndef rand_foo():\n    return torch.rand([4], device=\"cuda\")\n\n\nfor _ in range(5):\n    torch.compiler.cudagraph_mark_step_begin()\n    rand_foo() + rand_foo()\n\n```\n\nFor more details, seetorch.compiler_cudagraph_trees",
    "url": "https://pytorch.org/docs/stable/generated/torch.compiler.cudagraph_mark_step_begin.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f230a6a7900cc7c9bf5158cd35ceab77",
    "source": "pytorch_docs",
    "title": "Nested Graph Breaks \u2014 PyTorch 2.9 documentation",
    "text": "\n## Nested Graph Breaks#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nSummary:\nGraph breaks in nested functions can result in hard-to-understand compiler behavior, which we document below\nA nested graph break results inO(N)\\mathcal O(N)O(N)duplicate graph break behavior\nRecall that whentorch.compileis applied to a function, any nested function calls are also traced.\nAnested graph breakrefers to any graph break that happens in a nested function call.\ntorch.compile\n\n```python\ndef inner(x):\n    ...\n    torch._dynamo.graph_break()  # nested graph break\n    ...\n\n@torch.compile\ndef outer(x):\n    ...\n    y = inner(x)\n    ...\n\n```\n\nThe resumption semantics around nested graph breaks can be confusing, so we describe the behavior here.\nRecall that infullgraph=False,graph breaks are handledby compiling the FX graph that has been determined so far,\nrunning the unsupported code in regular Python, then resuming tracing after the unsupported code with a new FX graph.\nResuming a function is actually a fairly complicated technical feat, so resuming tracing is only supported on top-level functions.\nfullgraph=False\nWe can therefore resume tracing after a nested graph break with this restriction in the following way:\nFirst, consider the below example wheretorch.compiletraces fromfand traces all the way until the\ngraph break ininner1is encountered.\ntorch.compile\nf\ninner1\n\n```python\ndef inner1(x):\n    x = x + 1\n    torch._dynamo.graph_break()  # stop tracing due to graph break\n    return x + 2\n\ndef inner2(x):\n    x = x + 4\n    x = inner1(x)\n    x = x + 8\n\n@torch.compile\ndef f(x):\n    # start tracing from here\n    x = x + 16\n    x = inner2(x)\n    x = x + 32\n\nf(torch.randn(3))\n\n```\n\nSince we can only resume from top-level functions, we graph break on theinner2call inf.\ninner2\nf\n\n```python\n# The semantics of torch.compile(f)(x) is roughly this:\ndef compiled_f_semantics(x):\n    y = x + 16\n    z = inner2(y)\n    return torch.compile(resume_f_semantics)(z)\n\ndef resume_f_semantics(x):\n    return x + 32\n\ncompiled_f_semantics(torch.randn(3))\n\n```\n\ninner2is then automatically compiled as a top-level function.\nWe trace all the way until the graph break ininner1is encountered again.\ninner2\ninner1\n\n```python\ndef inner1(x):\n    x = x + 1\n    torch._dynamo.graph_break()  # stop tracing due to graph break\n    return x + 2\n\n# this torch.compile is automatically applied\n@torch.compile\ndef inner2(x):\n    # start tracing from here\n    x = x + 4\n    x = inner1(x)\n    x = x + 8\n\ndef compiled_f_semantics(x):\n    y = x + 16\n    z = inner2(y)\n    return torch.compile(resume_f_semantics)(z)\n\ndef resume_f_semantics(x):\n    return x + 32\n\ncompiled_f_semantics(torch.randn(3))\n\n```\n\nThen we graph break on theinner1call ininner2.\ninner1\ninner2\n\n```python\ndef compiled_inner2_semantics(x):\n    y = x + 4\n    z = inner1(y)\n    return torch.compile(resume_inner2_semantics)(z)\n\ndef resume_inner2_semantics(x):\n    return x + 8\n\n```\n\ninner1is then automatically compiled as a top-level function.\nThe graph break is frominner1, so we handle the graph break normally.\ninner1\ninner1\n\n```python\n# this torch.compile is automatically applied\n@torch.compile\ndef inner1(x):\n    # start tracing from here\n    x = x + 1\n    torch._dynamo.graph_break()  # stop tracing due to graph break\n    return x + 2\n\ndef compiled_f_semantics(x):\n    y = x + 16\n    z = compiled_inner2_semantics(y)\n    return torch.compile(resume_f_semantics)(z)\n\ndef resume_f_semantics(x):\n    return x + 32\n\ndef compiled_inner2_semantics(x):\n    y = x + 4\n    z = inner1(y)\n    return torch.compile(resume_inner2_semantics)(z)\n\ndef resume_inner2_semantics(x):\n    return x + 8\n\ncompiled_f_semantics(torch.randn(3))\n\n```\n\ninner1is handled normally:\ninner1\n\n```python\ndef compiled_inner1_semantics(x):\n    y = x + 1\n    torch._dynamo.graph_break()\n    return torch.compile(resume_inner1_semantics)(y)\n\ndef resume_inner1_semantics(x):\n    return x + 2\n\n```\n\nSo the initial code is semantically equivalent to\n\n```python\ndef compiled_f_semantics(x):\n    y = x + 16\n    z = compiled_inner2_semantics(y)\n    return torch.compile(resume_f_semantics)(z)\n\ndef resume_f_semantics(x):\n    return x + 32\n\ndef compiled_inner2_semantics(x):\n    y = x + 4\n    z = compiled_inner1_semantics(y)\n    return torch.compile(resume_inner2_semantics)(z)\n\ndef resume_inner2_semantics(x):\n    return x + 8\n\ndef compiled_inner1_semantics(x):\n    y = x + 1\n    torch._dynamo.graph_break()\n    return torch.compile(resume_inner1_semantics)(y)\n\ndef resume_inner1_semantics(x):\n    return x + 2\n\ncompiled_f_semantics(torch.randn(3))\n\n```\n\nNote in particular that we traced 3 top-level functions, and that we traced the same graph break 3 times.This explains why you may encounter duplicate graph breaks when usingtorch.compile.\ntorch.compile\nIn summary, nested graph breaks are handled by:\nTracing from the top-level function all the way to the nested graph break\nGraph breaking on the top-level function at the call to the second-level function\nCompiling the PyTorch ops tracked so far and running the compiled graph\nCalling the second-level function, which gets automatically compiled as a top-level function\nResuming tracing after the second-level function call\nNote that the runtime of handling this graph break isO(NK)\\mathcal O(NK)O(NK), whereNNNis the nesting depth,\nandKKKis the number of instructions from the top-level function to the graph break.\nWe end up tracingO(N2)\\mathcal O(N^2)O(N2)frames, and we trace the same graph breakO(N)\\mathcal O(N)O(N)times.",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.nested_graph_breaks.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b319ad5e8668227ce8e06d71de408e4b",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/export/api_reference.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "927edc293273a0f549add6adf66b326a",
    "source": "pytorch_docs",
    "title": "torch.random \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.random#\n\nCreated On: Aug 07, 2019 | Last Updated On: Jun 18, 2025\nForks the RNG, so that when you return, the RNG is reset\nto the state that it was previously in.\ndevices(iterableofDevice IDs) \u2013 devices for which to fork\nthe RNG. CPU RNG state is always forked. By default,fork_rng()operates\non all devices, but will emit a warning if your machine has a lot\nof devices, since this function will run very slowly in that case.\nIf you explicitly specify devices, this warning will be suppressed\nfork_rng()\nenabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience\nargument for easily disabling the context manager without having\nto delete it and unindent your Python code under it.\nFalse\ndevice_type(str) \u2013 device type str, default iscuda. As for supported device,\nsee details inaccelerator\nGenerator\nReturns the random number generator state as atorch.ByteTensor.\nNote\nThe returned state is for the default generator on CPU only.\nSee also:torch.random.fork_rng().\ntorch.random.fork_rng()\nTensor\nReturns the initial seed for generating random numbers as a\nPythonlong.\nNote\nThe returned seed is for the default generator on CPU only.\nint\nSets the seed for generating random numbers on all devices. Returns atorch.Generatorobject.\nseed(int) \u2013 The desired seed. Value must be within the inclusive range[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula0xffff_ffff_ffff_ffff + seed.\nGenerator\n\nSets the seed for generating random numbers to a non-deterministic\nrandom number on all devices. Returns a 64 bit number used to seed the RNG.\nint\nSets the random number generator state.\nNote\nThis function only works for CPU. For CUDA, please usetorch.manual_seed(), which works for both CPU and CUDA.\ntorch.manual_seed()\nnew_state(torch.ByteTensor) \u2013 The desired state",
    "url": "https://pytorch.org/docs/stable/random.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "7cf8841874a2503aa063218ffbb5dd7a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_ir.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4807cd8f99668cbf1ce5f88e486c9bde",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/torch.mutation.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4d8306f510f12d6d582c442ebbc70848",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/debugging_environment_variables.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f1a6c06841ca38c3a025188bf2e2b712",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/multiprocessing.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "acf1b4782fe43cb63b57474e937a3e5c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/miscellaneous_environment_variables.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e91ed8a5cceda40f7d25f5b8ae782a42",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributed.pipelining.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e7bcb6b141422b934b01d096617726b2",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/sparse.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "24995e1650d4f37e14892c9a79c58bcb",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/kubernetes.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "091700cb6225093ea147183f0c5c3693",
    "source": "pytorch_docs",
    "title": "Probability distributions - torch.distributions \u2014 PyTorch 2.9 documentation",
    "text": "\n## Probability distributions - torch.distributions#\n\nCreated On: Oct 19, 2017 | Last Updated On: Jun 13, 2025\nThedistributionspackage contains parameterizable probability distributions\nand sampling functions. This allows the construction of stochastic computation\ngraphs and stochastic gradient estimators for optimization. This package\ngenerally follows the design of theTensorFlow Distributionspackage.\ndistributions\nIt is not possible to directly backpropagate through random samples. However,\nthere are two main methods for creating surrogate functions that can be\nbackpropagated through. These are the score function estimator/likelihood ratio\nestimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly\nseen as the basis for policy gradient methods in reinforcement learning, and the\npathwise derivative estimator is commonly seen in the reparameterization trick\nin variational autoencoders. Whilst the score function only requires the value\nof samplesf(x)f(x)f(x), the pathwise derivative requires the derivativef\u2032(x)f'(x)f\u2032(x). The next sections discuss these two in a reinforcement learning\nexample. For more details seeGradient Estimation Using Stochastic Computation Graphs.\n\n## Score function#\n\nWhen the probability density function is differentiable with respect to its\nparameters, we only needsample()andlog_prob()to implement REINFORCE:\nsample()\nlog_prob()\nwhere\u03b8\\theta\u03b8are the parameters,\u03b1\\alpha\u03b1is the learning rate,rrris the reward andp(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s))p(a\u2223\u03c0\u03b8(s))is the probability of\ntaking actionaaain statesssgiven policy\u03c0\u03b8\\pi^\\theta\u03c0\u03b8.\nIn practice we would sample an action from the output of a network, apply this\naction in an environment, and then uselog_probto construct an equivalent\nloss function. Note that we use a negative because optimizers use gradient\ndescent, whilst the rule above assumes gradient ascent. With a categorical\npolicy, the code for implementing REINFORCE would be as follows:\nlog_prob\n\n```python\nprobs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()\n\n```\n\n\n## Pathwise derivative#\n\nThe other way to implement these stochastic/policy gradients would be to use the\nreparameterization trick from thersample()method, where the\nparameterized random variable can be constructed via a parameterized\ndeterministic function of a parameter-free random variable. The reparameterized\nsample therefore becomes differentiable. The code for implementing the pathwise\nderivative would be as follows:\nrsample()\n\n```python\nparams = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()\n\n```\n\n\n## Distribution#\n\nBases:object\nobject\nDistribution is the abstract base class for probability distributions.\nbatch_shape(torch.Size) \u2013 The shape over which parameters are batched.\nevent_shape(torch.Size) \u2013 The shape of a single sample (without batching).\nvalidate_args(bool,optional) \u2013 Whether to validate arguments. Default: None.\nReturns a dictionary from argument names toConstraintobjects that\nshould be satisfied by each argument of this distribution. Args that\nare not tensors need not appear in this dict.\nConstraint\nReturns the shape over which parameters are batched.\nReturns the cumulative density/mass function evaluated atvalue.\nvalue(Tensor) \u2013\nTensor\nReturns entropy of distribution, batched over batch_shape.\nTensor of shape batch_shape.\nTensor\nReturns tensor containing all values supported by a discrete\ndistribution. The result will enumerate over dimension 0, so the shape\nof the result will be(cardinality,) + batch_shape + event_shape(whereevent_shape = ()for univariate distributions).\nNote that this enumerates over all batched tensors in lock-step[[0, 0], [1, 1], \u2026]. Withexpand=False, enumeration happens\nalong dim 0, but with the remaining batch dimensions being\nsingleton dimensions,[[0], [1], ...\nTo iterate over the full Cartesian product useitertools.product(m.enumerate_support()).\nexpand(bool) \u2013 whether to expand the support over the\nbatch dims to match the distribution\u2019sbatch_shape.\nTensor iterating over dimension 0.\nTensor\nReturns the shape of a single sample (without batching).\nReturns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded tobatch_shape. This method callsexpandon\nthe distribution\u2019s parameters. As such, this does not allocate new\nmemory for the expanded distribution instance. Additionally,\nthis does not repeat any args checking or parameter broadcasting in__init__.py, when an instance is first created.\nexpand\nbatch_shape(torch.Size) \u2013 the desired expanded size.\n_instance\u2013 new instance provided by subclasses that\nneed to override.expand.\nNew distribution instance with batch dimensions expanded tobatch_size.\nReturns the inverse cumulative density/mass function evaluated atvalue.\nvalue(Tensor) \u2013\nTensor\nReturns the log of the probability density/mass function evaluated atvalue.\nvalue(Tensor) \u2013\nTensor\nReturns the mean of the distribution.\nReturns the mode of the distribution.\nReturns perplexity of distribution, batched over batch_shape.\nTensor of shape batch_shape.\nTensor\nGenerates a sample_shape shaped reparameterized sample or sample_shape\nshaped batch of reparameterized samples if the distribution parameters\nare batched.\nTensor\nGenerates a sample_shape shaped sample or sample_shape shaped batch of\nsamples if the distribution parameters are batched.\nTensor\nGenerates n samples or n batches of samples if the distribution\nparameters are batched.\nTensor\nSets whether validation is enabled or disabled.\nThe default behavior mimics Python\u2019sassertstatement: validation\nis on by default, but is disabled if Python is run in optimized mode\n(viapython-O). Validation may be expensive, so you may want to\ndisable it once a model is working.\nassert\npython-O\nvalue(bool) \u2013 Whether to enable validation.\nReturns the standard deviation of the distribution.\nReturns aConstraintobject\nrepresenting this distribution\u2019s support.\nConstraint\nReturns the variance of the distribution.\n\n## ExponentialFamily#\n\nBases:Distribution\nDistribution\nExponentialFamily is the abstract base class for probability distributions belonging to an\nexponential family, whose probability mass/density function has the form is defined below\nwhere\u03b8\\theta\u03b8denotes the natural parameters,t(x)t(x)t(x)denotes the sufficient statistic,F(\u03b8)F(\\theta)F(\u03b8)is the log normalizer function for a given family andk(x)k(x)k(x)is the carrier\nmeasure.\nNote\nThis class is an intermediary between theDistributionclass and distributions which belong\nto an exponential family mainly to check the correctness of the.entropy()and analytic KL\ndivergence methods. We use this class to compute the entropy and KL divergence using the AD\nframework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and\nCross-entropies of Exponential Families).\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\n## Bernoulli#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Bernoulli distribution parameterized byprobsorlogits(but not both).\nprobs\nlogits\nSamples are binary (0 or 1). They take the value1with probabilitypand0with probability1 - p.\nExample:\n\n```python\n>>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n\n```\n\nprobs(Number,Tensor) \u2013 the probability of sampling1\nlogits(Number,Tensor) \u2013 the log-odds of sampling1\nvalidate_args(bool,optional) \u2013 whether to validate arguments, None by default\n\n## Beta#\n\nBases:ExponentialFamily\nExponentialFamily\nBeta distribution parameterized byconcentration1andconcentration0.\nconcentration1\nconcentration0\nExample:\n\n```python\n>>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n\n```\n\nconcentration1(floatorTensor) \u2013 1st concentration parameter of the distribution\n(often referred to as alpha)\nconcentration0(floatorTensor) \u2013 2nd concentration parameter of the distribution\n(often referred to as beta)\nTensor\n\n## Binomial#\n\nBases:Distribution\nDistribution\nCreates a Binomial distribution parameterized bytotal_countand\neitherprobsorlogits(but not both).total_countmust be\nbroadcastable withprobs/logits.\ntotal_count\nprobs\nlogits\ntotal_count\nprobs\nlogits\nExample:\n\n```python\n>>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n\n```\n\ntotal_count(intorTensor) \u2013 number of Bernoulli trials\nprobs(Tensor) \u2013 Event probabilities\nlogits(Tensor) \u2013 Event log-odds\n_DependentProperty\n\n## Categorical#\n\nBases:Distribution\nDistribution\nCreates a categorical distribution parameterized by eitherprobsorlogits(but not both).\nprobs\nlogits\nNote\nIt is equivalent to the distribution thattorch.multinomial()samples from.\ntorch.multinomial()\nSamples are integers from{0,\u2026,K\u22121}\\{0, \\ldots, K-1\\}{0,\u2026,K\u22121}whereKisprobs.size(-1).\nprobs.size(-1)\nIfprobsis 1-dimensional with length-K, each element is the relative probability\nof sampling the class at that index.\nIfprobsis N-dimensional, the first N-1 dimensions are treated as a batch of\nrelative probability vectors.\nNote\nTheprobsargument must be non-negative, finite and have a non-zero sum,\nand it will be normalized to sum to 1 along the last dimension.probswill return this normalized value.\nThelogitsargument will be interpreted as unnormalized log probabilities\nand can therefore be any real number. It will likewise be normalized so that\nthe resulting probabilities sum to 1 along the last dimension.logitswill return this normalized value.\nprobs\nlogits\nSee also:torch.multinomial()\ntorch.multinomial()\nExample:\n\n```python\n>>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n\n```\n\nprobs(Tensor) \u2013 event probabilities\nlogits(Tensor) \u2013 event log probabilities (unnormalized)\n_DependentProperty\n\n## Cauchy#\n\nBases:Distribution\nDistribution\nSamples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\nindependent normally distributed random variables with means0follows a\nCauchy distribution.\nExample:\n\n```python\n>>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n\n```\n\nloc(floatorTensor) \u2013 mode or median of the distribution.\nscale(floatorTensor) \u2013 half width at half maximum.\nTensor\n\n## Chi2#\n\nBases:Gamma\nGamma\nCreates a Chi-squared distribution parameterized by shape parameterdf.\nThis is exactly equivalent toGamma(alpha=0.5*df,beta=0.5)\ndf\nGamma(alpha=0.5*df,beta=0.5)\nExample:\n\n```python\n>>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n\n```\n\ndf(floatorTensor) \u2013 shape parameter of the distribution\n\n## ContinuousBernoulli#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a continuous Bernoulli distribution parameterized byprobsorlogits(but not both).\nprobs\nlogits\nThe distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to\nlog-odds, but the same names are used due to the similarity with the\nBernoulli. See [1] for more details.\nExample:\n\n```python\n>>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])\n\n```\n\nprobs(Number,Tensor) \u2013 (0,1) valued parameters\nlogits(Number,Tensor) \u2013 real valued parameters whose sigmoid matches \u2018probs\u2019\n[1] The continuous Bernoulli: fixing a pervasive error in variational\nautoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.https://arxiv.org/abs/1907.06845\nTensor\n\n## Dirichlet#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Dirichlet distribution parameterized by concentrationconcentration.\nconcentration\nExample:\n\n```python\n>>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentration [0.5, 0.5]\ntensor([ 0.1046,  0.8954])\n\n```\n\nconcentration(Tensor) \u2013 concentration parameter of the distribution\n(often referred to as alpha)\nTensor\n\n## Exponential#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Exponential distribution parameterized byrate.\nrate\nExample:\n\n```python\n>>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n\n```\n\nrate(floatorTensor) \u2013 rate = 1 / scale of the distribution\nTensor\n\n## FisherSnedecor#\n\nBases:Distribution\nDistribution\nCreates a Fisher-Snedecor distribution parameterized bydf1anddf2.\ndf1\ndf2\nExample:\n\n```python\n>>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n\n```\n\ndf1(floatorTensor) \u2013 degrees of freedom parameter 1\ndf2(floatorTensor) \u2013 degrees of freedom parameter 2\nTensor\n\n## Gamma#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Gamma distribution parameterized by shapeconcentrationandrate.\nconcentration\nrate\nExample:\n\n```python\n>>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n\n```\n\nconcentration(floatorTensor) \u2013 shape parameter of the distribution\n(often referred to as alpha)\nrate(floatorTensor) \u2013 rate parameter of the distribution\n(often referred to as beta), rate = 1 / scale\nTensor\n\n## GeneralizedPareto#\n\nBases:Distribution\nDistribution\nCreates a Generalized Pareto distribution parameterized byloc,scale, andconcentration.\nloc\nscale\nconcentration\nThe Generalized Pareto distribution is a family of continuous probability distributions on the real line.\nSpecial cases include Exponential (whenloc= 0,concentration= 0), Pareto (whenconcentration> 0,loc=scale/concentration), and Uniform (whenconcentration= -1).\nloc\nconcentration\nconcentration\nloc\nscale\nconcentration\nconcentration\nThis distribution is often used to model the tails of other distributions. This implementation is based on the\nimplementation in TensorFlow Probability.\nExample:\n\n```python\n>>> m = GeneralizedPareto(torch.tensor([0.1]), torch.tensor([2.0]), torch.tensor([0.4]))\n>>> m.sample()  # sample from a Generalized Pareto distribution with loc=0.1, scale=2.0, and concentration=0.4\ntensor([ 1.5623])\n\n```\n\nloc(floatorTensor) \u2013 Location parameter of the distribution\nscale(floatorTensor) \u2013 Scale parameter of the distribution\nconcentration(floatorTensor) \u2013 Concentration parameter of the distribution\n_DependentProperty\n\n## Geometric#\n\nBases:Distribution\nDistribution\nCreates a Geometric distribution parameterized byprobs,\nwhereprobsis the probability of success of Bernoulli trials.\nprobs\nprobs\nNote\ntorch.distributions.geometric.Geometric()(k+1)(k+1)(k+1)-th trial is the first success\nhence draws samples in{0,1,\u2026}\\{0, 1, \\ldots\\}{0,1,\u2026}, whereastorch.Tensor.geometric_()k-th trial is the first success hence draws samples in{1,2,\u2026}\\{1, 2, \\ldots\\}{1,2,\u2026}.\ntorch.distributions.geometric.Geometric()\ntorch.Tensor.geometric_()\nExample:\n\n```python\n>>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n\n```\n\nprobs(Number,Tensor) \u2013 the probability of sampling1. Must be in range (0, 1]\nlogits(Number,Tensor) \u2013 the log-odds of sampling1.\n\n## Gumbel#\n\nBases:TransformedDistribution\nTransformedDistribution\nSamples from a Gumbel Distribution.\nExamples:\n\n```python\n>>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n\n```\n\nloc(floatorTensor) \u2013 Location parameter of the distribution\nscale(floatorTensor) \u2013 Scale parameter of the distribution\n\n## HalfCauchy#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a half-Cauchy distribution parameterized byscalewhere:\n\n```python\nX ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n\n```\n\nExample:\n\n```python\n>>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n\n```\n\nscale(floatorTensor) \u2013 scale of the full Cauchy distribution\n\n## HalfNormal#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a half-normal distribution parameterized byscalewhere:\n\n```python\nX ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n\n```\n\nExample:\n\n```python\n>>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n\n```\n\nscale(floatorTensor) \u2013 scale of the full Normal distribution\n\n## Independent#\n\nBases:Distribution,Generic[D]\nDistribution\nGeneric\nD\nReinterprets some of the batch dims of a distribution as event dims.\nThis is mainly useful for changing the shape of the result oflog_prob(). For example to create a diagonal Normal distribution with\nthe same shape as a Multivariate Normal distribution (so they are\ninterchangeable), you can:\nlog_prob()\n\n```python\n>>> from torch.distributions.multivariate_normal import MultivariateNormal\n>>> from torch.distributions.normal import Normal\n>>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size([]), torch.Size([3])]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size([3]), torch.Size([])]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size([]), torch.Size([3])]\n\n```\n\nbase_distribution(torch.distributions.distribution.Distribution) \u2013 a\nbase distribution\nreinterpreted_batch_ndims(int) \u2013 the number of batch dims to\nreinterpret as event dims\nTensor\nTensor\n_DependentProperty\n\n## InverseGamma#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates an inverse gamma distribution parameterized byconcentrationandratewhere:\nconcentration\nrate\n\n```python\nX ~ Gamma(concentration, rate)\nY = 1 / X ~ InverseGamma(concentration, rate)\n\n```\n\nExample:\n\n```python\n>>> m = InverseGamma(torch.tensor([2.0]), torch.tensor([3.0]))\n>>> m.sample()\ntensor([ 1.2953])\n\n```\n\nconcentration(floatorTensor) \u2013 shape parameter of the distribution\n(often referred to as alpha)\nrate(floatorTensor) \u2013 rate = 1 / scale of the distribution\n(often referred to as beta)\n\n## Kumaraswamy#\n\nBases:TransformedDistribution\nTransformedDistribution\nSamples from a Kumaraswamy distribution.\nExample:\n\n```python\n>>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])\n\n```\n\nconcentration1(floatorTensor) \u2013 1st concentration parameter of the distribution\n(often referred to as alpha)\nconcentration0(floatorTensor) \u2013 2nd concentration parameter of the distribution\n(often referred to as beta)\n\n## LKJCholesky#\n\nBases:Distribution\nDistribution\nLKJ distribution for lower Cholesky factor of correlation matrices.\nThe distribution is controlled byconcentrationparameter\u03b7\\eta\u03b7to make the probability of the correlation matrixMMMgenerated from\na Cholesky factor proportional todet\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1}det(M)\u03b7\u22121. Because of that,\nwhenconcentration==1, we have a uniform distribution over Cholesky\nfactors of correlation matrices:\nconcentration\nconcentration==1\n\n```python\nL ~ LKJCholesky(dim, concentration)\nX = L @ L' ~ LKJCorr(dim, concentration)\n\n```\n\nNote that this distribution samples the\nCholesky factor of correlation matrices and not the correlation matrices\nthemselves and thereby differs slightly from the derivations in [1] for\ntheLKJCorrdistribution. For sampling, this uses the Onion method from\n[1] Section 3.\nExample:\n\n```python\n>>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])\n\n```\n\ndimension(dim) \u2013 dimension of the matrices\nconcentration(floatorTensor) \u2013 concentration/shape parameter of the\ndistribution (often referred to as eta)\nReferences\n[1]Generating random correlation matrices based on vines and extended onion method(2009),\nDaniel Lewandowski, Dorota Kurowicka, Harry Joe.\nJournal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008\n\n## Laplace#\n\nBases:Distribution\nDistribution\nCreates a Laplace distribution parameterized bylocandscale.\nloc\nscale\nExample:\n\n```python\n>>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n\n```\n\nloc(floatorTensor) \u2013 mean of the distribution\nscale(floatorTensor) \u2013 scale of the distribution\nTensor\n\n## LogNormal#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a log-normal distribution parameterized bylocandscalewhere:\nloc\nscale\n\n```python\nX ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n\n```\n\nExample:\n\n```python\n>>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n\n```\n\nloc(floatorTensor) \u2013 mean of log of distribution\nscale(floatorTensor) \u2013 standard deviation of log of the distribution\n\n## LowRankMultivariateNormal#\n\nBases:Distribution\nDistribution\nCreates a multivariate normal distribution with covariance matrix having a low-rank form\nparameterized bycov_factorandcov_diag:\ncov_factor\ncov_diag\n\n```python\ncovariance_matrix = cov_factor @ cov_factor.T + cov_diag\n\n```\n\nExample\n\n```python\n>>> m = LowRankMultivariateNormal(\n...     torch.zeros(2), torch.tensor([[1.0], [0.0]]), torch.ones(2)\n... )\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n\n```\n\nloc(Tensor) \u2013 mean of the distribution with shapebatch_shape + event_shape\ncov_factor(Tensor) \u2013 factor part of low-rank form of covariance matrix with shapebatch_shape + event_shape + (rank,)\ncov_diag(Tensor) \u2013 diagonal part of low-rank form of covariance matrix with shapebatch_shape + event_shape\nNote\nThe computation for determinant and inverse of covariance matrix is avoided whencov_factor.shape[1] << cov_factor.shape[0]thanks toWoodbury matrix identityandmatrix determinant lemma.\nThanks to these formulas, we just need to compute the determinant and inverse of\nthe small size \u201ccapacitance\u201d matrix:\n\n```python\ncapacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n\n```\n\nTensor\n\n## MixtureSameFamily#\n\nBases:Distribution\nDistribution\nTheMixtureSameFamilydistribution implements a (batch of) mixture\ndistribution where all component are from different parameterizations of\nthe same distribution type. It is parameterized by aCategorical\u201cselecting distribution\u201d (overkcomponent) and a component\ndistribution, i.e., aDistributionwith a rightmost batch shape\n(equal to[k]) which indexes each (batch of) component.\nExamples:\n\n```python\n>>> # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n>>> # weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n>>> # Construct Gaussian Mixture Model in 2D consisting of 5 equally\n>>> # weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n...          torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n>>> # Construct a batch of 3 Gaussian Mixture Models in 2D each\n>>> # consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n...         torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n```\n\nmixture_distribution(Categorical) \u2013torch.distributions.Categorical-like\ninstance. Manages the probability of selecting component.\nThe number of categories must match the rightmost batch\ndimension of thecomponent_distribution. Must have either\nscalarbatch_shapeorbatch_shapematchingcomponent_distribution.batch_shape[:-1]\ncomponent_distribution(Distribution) \u2013torch.distributions.Distribution-like\ninstance. Right-most batch dimension indexes component.\n_DependentProperty\n\n## Multinomial#\n\nBases:Distribution\nDistribution\nCreates a Multinomial distribution parameterized bytotal_countand\neitherprobsorlogits(but not both). The innermost dimension ofprobsindexes over categories. All other dimensions index over batches.\ntotal_count\nprobs\nlogits\nprobs\nNote thattotal_countneed not be specified if onlylog_prob()is\ncalled (see example below)\ntotal_count\nlog_prob()\nNote\nTheprobsargument must be non-negative, finite and have a non-zero sum,\nand it will be normalized to sum to 1 along the last dimension.probswill return this normalized value.\nThelogitsargument will be interpreted as unnormalized log probabilities\nand can therefore be any real number. It will likewise be normalized so that\nthe resulting probabilities sum to 1 along the last dimension.logitswill return this normalized value.\nprobs\nlogits\nsample()requires a single sharedtotal_countfor all\nparameters and samples.\nsample()\nlog_prob()allows differenttotal_countfor each parameter and\nsample.\nlog_prob()\nExample:\n\n```python\n>>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n\n```\n\ntotal_count(int) \u2013 number of trials\nprobs(Tensor) \u2013 event probabilities\nlogits(Tensor) \u2013 event log probabilities (unnormalized)\n_DependentProperty\n\n## MultivariateNormal#\n\nBases:Distribution\nDistribution\nCreates a multivariate normal (also called Gaussian) distribution\nparameterized by a mean vector and a covariance matrix.\nThe multivariate normal distribution can be parameterized either\nin terms of a positive definite covariance matrix\u03a3\\mathbf{\\Sigma}\u03a3or a positive definite precision matrix\u03a3\u22121\\mathbf{\\Sigma}^{-1}\u03a3\u22121or a lower-triangular matrixL\\mathbf{L}Lwith positive-valued\ndiagonal entries, such that\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\u03a3=LL\u22a4. This triangular matrix\ncan be obtained via e.g. Cholesky decomposition of the covariance.\nExample\n\n```python\n>>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n\n```\n\nloc(Tensor) \u2013 mean of the distribution\ncovariance_matrix(Tensor) \u2013 positive-definite covariance matrix\nprecision_matrix(Tensor) \u2013 positive-definite precision matrix\nscale_tril(Tensor) \u2013 lower-triangular factor of covariance, with positive-valued diagonal\nNote\nOnly one ofcovariance_matrixorprecision_matrixorscale_trilcan be specified.\ncovariance_matrix\nprecision_matrix\nscale_tril\nUsingscale_trilwill be more efficient: all computations internally\nare based onscale_tril. Ifcovariance_matrixorprecision_matrixis passed instead, it is only used to compute\nthe corresponding lower triangular matrices using a Cholesky decomposition.\nscale_tril\nscale_tril\ncovariance_matrix\nprecision_matrix\nTensor\n\n## NegativeBinomial#\n\nBases:Distribution\nDistribution\nCreates a Negative Binomial distribution, i.e. distribution\nof the number of successful independent and identical Bernoulli trials\nbeforetotal_countfailures are achieved. The probability\nof success of each Bernoulli trial isprobs.\ntotal_count\nprobs\ntotal_count(floatorTensor) \u2013 non-negative number of negative Bernoulli\ntrials to stop, although the distribution is still valid for real\nvalued count\nprobs(Tensor) \u2013 Event probabilities of success in the half open interval [0, 1)\nlogits(Tensor) \u2013 Event log-odds for probabilities of success\n\n## Normal#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a normal (also called Gaussian) distribution parameterized bylocandscale.\nloc\nscale\nExample:\n\n```python\n>>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n\n```\n\nloc(floatorTensor) \u2013 mean of the distribution (often referred to as mu)\nscale(floatorTensor) \u2013 standard deviation of the distribution\n(often referred to as sigma)\nTensor\n\n## OneHotCategorical#\n\nBases:Distribution\nDistribution\nCreates a one-hot categorical distribution parameterized byprobsorlogits.\nprobs\nlogits\nSamples are one-hot coded vectors of sizeprobs.size(-1).\nprobs.size(-1)\nNote\nTheprobsargument must be non-negative, finite and have a non-zero sum,\nand it will be normalized to sum to 1 along the last dimension.probswill return this normalized value.\nThelogitsargument will be interpreted as unnormalized log probabilities\nand can therefore be any real number. It will likewise be normalized so that\nthe resulting probabilities sum to 1 along the last dimension.logitswill return this normalized value.\nprobs\nlogits\nSee also:torch.distributions.Categorical()for specifications ofprobsandlogits.\ntorch.distributions.Categorical()\nprobs\nlogits\nExample:\n\n```python\n>>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n\n```\n\nprobs(Tensor) \u2013 event probabilities\nlogits(Tensor) \u2013 event log probabilities (unnormalized)\n\n## Pareto#\n\nBases:TransformedDistribution\nTransformedDistribution\nSamples from a Pareto Type 1 distribution.\nExample:\n\n```python\n>>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n\n```\n\nscale(floatorTensor) \u2013 Scale parameter of the distribution\nalpha(floatorTensor) \u2013 Shape parameter of the distribution\nTensor\nPareto\n_DependentProperty\n\n## Poisson#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Poisson distribution parameterized byrate, the rate parameter.\nrate\nSamples are nonnegative integers, with a pmf given by\nExample:\n\n```python\n>>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])\n\n```\n\nrate(Number,Tensor) \u2013 the rate parameter\n\n## RelaxedBernoulli#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a RelaxedBernoulli distribution, parametrized bytemperature, and eitherprobsorlogits(but not both). This is a relaxed version of theBernoullidistribution,\nso the values are in (0, 1), and has reparametrizable samples.\ntemperature\nprobs\nlogits\nExample:\n\n```python\n>>> m = RelaxedBernoulli(torch.tensor([2.2]),\n...                      torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n\n```\n\ntemperature(Tensor) \u2013 relaxation temperature\nprobs(Number,Tensor) \u2013 the probability of sampling1\nlogits(Number,Tensor) \u2013 the log-odds of sampling1\n\n## LogitRelaxedBernoulli#\n\nBases:Distribution\nDistribution\nCreates a LogitRelaxedBernoulli distribution parameterized byprobsorlogits(but not both), which is the logit of a RelaxedBernoulli\ndistribution.\nprobs\nlogits\nSamples are logits of values in (0, 1). See [1] for more details.\ntemperature(Tensor) \u2013 relaxation temperature\nprobs(Number,Tensor) \u2013 the probability of sampling1\nlogits(Number,Tensor) \u2013 the log-odds of sampling1\n[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random\nVariables (Maddison et al., 2017)\n[2] Categorical Reparametrization with Gumbel-Softmax\n(Jang et al., 2017)\nTensor\n\n## RelaxedOneHotCategorical#\n\nBases:TransformedDistribution\nTransformedDistribution\nCreates a RelaxedOneHotCategorical distribution parametrized bytemperature, and eitherprobsorlogits.\nThis is a relaxed version of theOneHotCategoricaldistribution, so\nits samples are on simplex, and are reparametrizable.\ntemperature\nprobs\nlogits\nOneHotCategorical\nExample:\n\n```python\n>>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n...                              torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n\n```\n\ntemperature(Tensor) \u2013 relaxation temperature\nprobs(Tensor) \u2013 event probabilities\nlogits(Tensor) \u2013 unnormalized log probability for each event\n\n## StudentT#\n\nBases:Distribution\nDistribution\nCreates a Student\u2019s t-distribution parameterized by degree of\nfreedomdf, meanlocand scalescale.\ndf\nloc\nscale\nExample:\n\n```python\n>>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n\n```\n\ndf(floatorTensor) \u2013 degrees of freedom\nloc(floatorTensor) \u2013 mean of the distribution\nscale(floatorTensor) \u2013 scale of the distribution\nTensor\n\n## TransformedDistribution#\n\nBases:Distribution\nDistribution\nExtension of the Distribution class, which applies a sequence of Transforms\nto a base distribution.  Let f be the composition of transforms applied:\n\n```python\nX ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n\n```\n\nNote that the.event_shapeof aTransformedDistributionis the\nmaximum shape of its base distribution and its transforms, since transforms\ncan introduce correlations among events.\n.event_shape\nTransformedDistribution\nAn example for the usage ofTransformedDistributionwould be:\nTransformedDistribution\n\n```python\n# Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n\n```\n\nFor more examples, please look at the implementations ofGumbel,HalfCauchy,HalfNormal,LogNormal,Pareto,Weibull,RelaxedBernoulliandRelaxedOneHotCategorical\nGumbel\nHalfCauchy\nHalfNormal\nLogNormal\nPareto\nWeibull\nRelaxedBernoulli\nRelaxedOneHotCategorical\nComputes the cumulative distribution function by inverting the\ntransform(s) and computing the score of the base distribution.\nComputes the inverse cumulative distribution function using\ntransform(s) and computing the score of the base distribution.\nScores the sample by inverting the transform(s) and computing the score\nusing the score of the base distribution and the log abs det jacobian.\nGenerates a sample_shape shaped reparameterized sample or sample_shape\nshaped batch of reparameterized samples if the distribution parameters\nare batched. Samples first from base distribution and appliestransform()for every transform in the list.\nTensor\nGenerates a sample_shape shaped sample or sample_shape shaped batch of\nsamples if the distribution parameters are batched. Samples first from\nbase distribution and appliestransform()for every transform in the\nlist.\n_DependentProperty\n\n## Uniform#\n\nBases:Distribution\nDistribution\nGenerates uniformly distributed random samples from the half-open interval[low,high).\n[low,high)\nExample:\n\n```python\n>>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n\n```\n\nlow(floatorTensor) \u2013 lower range (inclusive).\nhigh(floatorTensor) \u2013 upper range (exclusive).\nTensor\n_DependentProperty\n\n## VonMises#\n\nBases:Distribution\nDistribution\nA circular von Mises distribution.\nThis implementation uses polar coordinates. Thelocandvalueargs\ncan be any real number (to facilitate unconstrained optimization), but are\ninterpreted as angles modulo 2 pi.\nloc\nvalue\n\n```python\n>>> m = VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n\n```\n\nloc(torch.Tensor) \u2013 an angle in radians.\nconcentration(torch.Tensor) \u2013 concentration parameter\nThe provided mean is the circular one.\nThe sampling algorithm for the von Mises distribution is based on the\nfollowing paper: D.J. Best and N.I. Fisher, \u201cEfficient simulation of the\nvon Mises distribution.\u201d Applied Statistics (1979): 152-157.\nSampling is always done in double precision internally to avoid a hang\nin _rejection_sample() for small values of the concentration, which\nstarts to happen for single precision around 1e-4 (see issue #88443).\nThe provided variance is the circular one.\n\n## Weibull#\n\nBases:TransformedDistribution\nTransformedDistribution\nSamples from a two-parameter Weibull distribution.\nExample\n\n```python\n>>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n\n```\n\nscale(floatorTensor) \u2013 Scale parameter of distribution (lambda).\nconcentration(floatorTensor) \u2013 Concentration parameter of distribution (k/shape).\nvalidate_args(bool,optional) \u2013 Whether to validate arguments. Default: None.\n\n## Wishart#\n\nBases:ExponentialFamily\nExponentialFamily\nCreates a Wishart distribution parameterized by a symmetric positive definite matrix\u03a3\\Sigma\u03a3,\nor its Cholesky decomposition\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\u03a3=LL\u22a4\nExample\n\n```python\n>>> m = Wishart(torch.Tensor([2]), covariance_matrix=torch.eye(2))\n>>> m.sample()  # Wishart distributed with mean=`df * I` and\n>>> # variance(x_ij)=`df` for i != j and variance(x_ij)=`2 * df` for i == j\n\n```\n\ndf(floatorTensor) \u2013 real-valued parameter larger than the (dimension of Square matrix) - 1\ncovariance_matrix(Tensor) \u2013 positive-definite covariance matrix\nprecision_matrix(Tensor) \u2013 positive-definite precision matrix\nscale_tril(Tensor) \u2013 lower-triangular factor of covariance, with positive-valued diagonal\nNote\nOnly one ofcovariance_matrixorprecision_matrixorscale_trilcan be specified.\nUsingscale_trilwill be more efficient: all computations internally\nare based onscale_tril. Ifcovariance_matrixorprecision_matrixis passed instead, it is only used to compute\nthe corresponding lower triangular matrices using a Cholesky decomposition.\n\u2018torch.distributions.LKJCholesky\u2019 is a restricted Wishart distribution.[1]\ncovariance_matrix\nprecision_matrix\nscale_tril\nscale_tril\nscale_tril\ncovariance_matrix\nprecision_matrix\nReferences\n[1] Wang, Z., Wu, Y. and Chu, H., 2018.On equivalence of the LKJ distribution and the restricted Wishart distribution.\n[2] Sawyer, S., 2007.Wishart Distributions and Inverse-Wishart Sampling.\n[3] Anderson, T. W., 2003.An Introduction to Multivariate Statistical Analysis (3rd ed.).\n[4] Odell, P. L. & Feiveson, A. H., 1966.A Numerical Procedure to Generate a SampleCovariance Matrix. JASA, 61(313):199-203.\n[5] Ku, Y.-C. & Bloomfield, P., 2010.Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX.\nWarning\nIn some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples.\nSeveral tries to correct singular samples are performed by default, but it may end up returning\nsingular matrix samples. Singular samples may return-infvalues in.log_prob().\nIn those cases, the user should validate the samples and either fix the value ofdfor adjustmax_try_correctionvalue for argument in.rsampleaccordingly.\nTensor\n\n## KLDivergence#\n\nKLDivergence\nCompute Kullback-Leibler divergenceKL(p\u2225q)KL(p \\| q)KL(p\u2225q)between two distributions.\np(Distribution) \u2013 ADistributionobject.\nDistribution\nq(Distribution) \u2013 ADistributionobject.\nDistribution\nA batch of KL divergences of shapebatch_shape.\nTensor\nNotImplementedError\u2013 If the distribution types have not been registered viaregister_kl().\nregister_kl()\nBernoulliandBernoulli\nBernoulli\nBernoulli\nBernoulliandPoisson\nBernoulli\nPoisson\nBetaandBeta\nBeta\nBeta\nBetaandContinuousBernoulli\nBeta\nContinuousBernoulli\nBetaandExponential\nBeta\nExponential\nBetaandGamma\nBeta\nGamma\nBetaandNormal\nBeta\nNormal\nBetaandPareto\nBeta\nPareto\nBetaandUniform\nBeta\nUniform\nBinomialandBinomial\nBinomial\nBinomial\nCategoricalandCategorical\nCategorical\nCategorical\nCauchyandCauchy\nCauchy\nCauchy\nContinuousBernoulliandContinuousBernoulli\nContinuousBernoulli\nContinuousBernoulli\nContinuousBernoulliandExponential\nContinuousBernoulli\nExponential\nContinuousBernoulliandNormal\nContinuousBernoulli\nNormal\nContinuousBernoulliandPareto\nContinuousBernoulli\nPareto\nContinuousBernoulliandUniform\nContinuousBernoulli\nUniform\nDirichletandDirichlet\nDirichlet\nDirichlet\nExponentialandBeta\nExponential\nBeta\nExponentialandContinuousBernoulli\nExponential\nContinuousBernoulli\nExponentialandExponential\nExponential\nExponential\nExponentialandGamma\nExponential\nGamma\nExponentialandGumbel\nExponential\nGumbel\nExponentialandNormal\nExponential\nNormal\nExponentialandPareto\nExponential\nPareto\nExponentialandUniform\nExponential\nUniform\nExponentialFamilyandExponentialFamily\nExponentialFamily\nExponentialFamily\nGammaandBeta\nGamma\nBeta\nGammaandContinuousBernoulli\nGamma\nContinuousBernoulli\nGammaandExponential\nGamma\nExponential\nGammaandGamma\nGamma\nGamma\nGammaandGumbel\nGamma\nGumbel\nGammaandNormal\nGamma\nNormal\nGammaandPareto\nGamma\nPareto\nGammaandUniform\nGamma\nUniform\nGeometricandGeometric\nGeometric\nGeometric\nGumbelandBeta\nGumbel\nBeta\nGumbelandContinuousBernoulli\nGumbel\nContinuousBernoulli\nGumbelandExponential\nGumbel\nExponential\nGumbelandGamma\nGumbel\nGamma\nGumbelandGumbel\nGumbel\nGumbel\nGumbelandNormal\nGumbel\nNormal\nGumbelandPareto\nGumbel\nPareto\nGumbelandUniform\nGumbel\nUniform\nHalfNormalandHalfNormal\nHalfNormal\nHalfNormal\nIndependentandIndependent\nIndependent\nIndependent\nLaplaceandBeta\nLaplace\nBeta\nLaplaceandContinuousBernoulli\nLaplace\nContinuousBernoulli\nLaplaceandExponential\nLaplace\nExponential\nLaplaceandGamma\nLaplace\nGamma\nLaplaceandLaplace\nLaplace\nLaplace\nLaplaceandNormal\nLaplace\nNormal\nLaplaceandPareto\nLaplace\nPareto\nLaplaceandUniform\nLaplace\nUniform\nLowRankMultivariateNormalandLowRankMultivariateNormal\nLowRankMultivariateNormal\nLowRankMultivariateNormal\nLowRankMultivariateNormalandMultivariateNormal\nLowRankMultivariateNormal\nMultivariateNormal\nMultivariateNormalandLowRankMultivariateNormal\nMultivariateNormal\nLowRankMultivariateNormal\nMultivariateNormalandMultivariateNormal\nMultivariateNormal\nMultivariateNormal\nNormalandBeta\nNormal\nBeta\nNormalandContinuousBernoulli\nNormal\nContinuousBernoulli\nNormalandExponential\nNormal\nExponential\nNormalandGamma\nNormal\nGamma\nNormalandGumbel\nNormal\nGumbel\nNormalandLaplace\nNormal\nLaplace\nNormalandNormal\nNormal\nNormal\nNormalandPareto\nNormal\nPareto\nNormalandUniform\nNormal\nUniform\nOneHotCategoricalandOneHotCategorical\nOneHotCategorical\nOneHotCategorical\nParetoandBeta\nPareto\nBeta\nParetoandContinuousBernoulli\nPareto\nContinuousBernoulli\nParetoandExponential\nPareto\nExponential\nParetoandGamma\nPareto\nGamma\nParetoandNormal\nPareto\nNormal\nParetoandPareto\nPareto\nPareto\nParetoandUniform\nPareto\nUniform\nPoissonandBernoulli\nPoisson\nBernoulli\nPoissonandBinomial\nPoisson\nBinomial\nPoissonandPoisson\nPoisson\nPoisson\nTransformedDistributionandTransformedDistribution\nTransformedDistribution\nTransformedDistribution\nUniformandBeta\nUniform\nBeta\nUniformandContinuousBernoulli\nUniform\nContinuousBernoulli\nUniformandExponential\nUniform\nExponential\nUniformandGamma\nUniform\nGamma\nUniformandGumbel\nUniform\nGumbel\nUniformandNormal\nUniform\nNormal\nUniformandPareto\nUniform\nPareto\nUniformandUniform\nUniform\nUniform\nDecorator to register a pairwise function withkl_divergence().\nUsage:\nkl_divergence()\n\n```python\n@register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n\n```\n\nLookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, aRuntimeWarningis raised. For example to\nresolve the ambiguous situation:\n\n```python\n@register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n\n```\n\nyou should register a third most-specific implementation, e.g.:\n\n```python\nregister_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n\n```\n\ntype_p(type) \u2013 A subclass ofDistribution.\nDistribution\ntype_q(type) \u2013 A subclass ofDistribution.\nDistribution\n\n## Transforms#\n\nTransforms\nTransform via the mappingy=\u2223x\u2223y = |x|y=\u2223x\u2223.\nTransform via the pointwise affine mappingy=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times xy=loc+scale\u00d7x.\nloc(Tensororfloat) \u2013 Location parameter.\nscale(Tensororfloat) \u2013 Scale parameter.\nevent_dim(int) \u2013 Optional size ofevent_shape. This should be zero\nfor univariate random variables, 1 for distributions over vectors,\n2 for distributions over matrices, etc.\nTransform functor that applies a sequence of transformstseqcomponent-wise to each submatrix atdim, of lengthlengths[dim],\nin a way compatible withtorch.cat().\ntorch.cat()\nExample:\n\n```python\nx0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)\nx = torch.cat([x0, x0], dim=0)\nt0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])\nt = CatTransform([t0, t0], dim=0, lengths=[20, 20])\ny = t(x)\n\n```\n\nComposes multiple transforms in a chain.\nThe transforms being composed are responsible for caching.\nparts(list ofTransform) \u2013 A list of transforms to compose.\nTransform\ncache_size(int) \u2013 Size of cache. If zero, no caching is done. If one,\nthe latest single value is cached. Only 0 and 1 are supported.\nTransforms an unconstrained real vectorxxxwith lengthD\u2217(D\u22121)/2D*(D-1)/2D\u2217(D\u22121)/2into the\nCholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower\ntriangular matrix with positive diagonals and unit Euclidean norm for each row.\nThe transform is processed as follows:\nFirst we convert x into a lower triangular matrix in row order.\nFor each rowXiX_iXi\u200bof the lower triangular part, we apply asignedversion of\nclassStickBreakingTransformto transformXiX_iXi\u200binto a\nunit Euclidean length vector using the following steps:\n- Scales into the interval(\u22121,1)(-1, 1)(\u22121,1)domain:ri=tanh\u2061(Xi)r_i = \\tanh(X_i)ri\u200b=tanh(Xi\u200b).\n- Transforms into an unsigned domain:zi=ri2z_i = r_i^2zi\u200b=ri2\u200b.\n- Appliessi=StickBreakingTransform(zi)s_i = StickBreakingTransform(z_i)si\u200b=StickBreakingTransform(zi\u200b).\n- Transforms back into signed domain:yi=sign(ri)\u2217siy_i = sign(r_i) * \\sqrt{s_i}yi\u200b=sign(ri\u200b)\u2217si\u200b\u200b.\nStickBreakingTransform\nTransform via the cumulative distribution function of a probability distribution.\ndistribution(Distribution) \u2013 Distribution whose cumulative distribution function to use for\nthe transformation.\nExample:\n\n```python\n# Construct a Gaussian copula from a multivariate normal.\nbase_dist = MultivariateNormal(\n    loc=torch.zeros(2),\n    scale_tril=LKJCholesky(2).sample(),\n)\ntransform = CumulativeDistributionTransform(Normal(0, 1))\ncopula = TransformedDistribution(base_dist, [transform])\n\n```\n\nTransform via the mappingy=exp\u2061(x)y = \\exp(x)y=exp(x).\nWrapper around another transform to treatreinterpreted_batch_ndims-many extra of the right most dimensions as\ndependent. This has no effect on the forward or backward transforms, but\ndoes sum outreinterpreted_batch_ndims-many of the rightmost dimensions\ninlog_abs_det_jacobian().\nreinterpreted_batch_ndims\nreinterpreted_batch_ndims\nlog_abs_det_jacobian()\nbase_transform(Transform) \u2013 A base transform.\nTransform\nreinterpreted_batch_ndims(int) \u2013 The number of extra rightmost\ndimensions to treat as dependent.\nTransform from unconstrained matrices to lower-triangular matrices with\nnonnegative diagonal entries.\nThis is useful for parameterizing positive definite matrices in terms of\ntheir Cholesky factorization.\nTransform from unconstrained matrices to positive-definite matrices.\nTransform via the mappingy=xexponenty = x^{\\text{exponent}}y=xexponent.\nUnit Jacobian transform to reshape the rightmost part of a tensor.\nNote thatin_shapeandout_shapemust have the same number of\nelements, just as fortorch.Tensor.reshape().\nin_shape\nout_shape\ntorch.Tensor.reshape()\nin_shape(torch.Size) \u2013 The input event shape.\nout_shape(torch.Size) \u2013 The output event shape.\ncache_size(int) \u2013 Size of cache. If zero, no caching is done. If one,\nthe latest single value is cached. Only 0 and 1 are supported. (Default 0.)\nTransform via the mappingy=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)}y=1+exp(\u2212x)1\u200bandx=logit(y)x = \\text{logit}(y)x=logit(y).\nTransform via the mappingSoftplus(x)=log\u2061(1+exp\u2061(x))\\text{Softplus}(x) = \\log(1 + \\exp(x))Softplus(x)=log(1+exp(x)).\nThe implementation reverts to the linear function whenx>20x > 20x>20.\nTransform via the mappingy=tanh\u2061(x)y = \\tanh(x)y=tanh(x).\nIt is equivalent to\n\n```python\nComposeTransform(\n    [\n        AffineTransform(0.0, 2.0),\n        SigmoidTransform(),\n        AffineTransform(-1.0, 2.0),\n    ]\n)\n\n```\n\nHowever this might not be numerically stable, thus it is recommended to useTanhTransforminstead.\nNote that one should usecache_size=1when it comes toNaN/Infvalues.\nTransform from unconstrained space to the simplex viay=exp\u2061(x)y = \\exp(x)y=exp(x)then\nnormalizing.\nThis is not bijective and cannot be used for HMC. However this acts mostly\ncoordinate-wise (except for the final normalization), and thus is\nappropriate for coordinate-wise optimization algorithms.\nTransform functor that applies a sequence of transformstseqcomponent-wise to each submatrix atdimin a way compatible withtorch.stack().\ntorch.stack()\nExample:\n\n```python\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)\nt = StackTransform([ExpTransform(), identity_transform], dim=1)\ny = t(x)\n\n```\n\nTransform from unconstrained space to the simplex of one additional\ndimension via a stick-breaking process.\nThis transform arises as an iterated sigmoid transform in a stick-breaking\nconstruction of theDirichletdistribution: the first logit is\ntransformed via sigmoid to the first probability and the probability of\neverything else, and then the process recurses.\nThis is bijective and appropriate for use in HMC; however it mixes\ncoordinates together and is less appropriate for optimization.\nAbstract class for invertable transformations with computable log\ndet jacobians. They are primarily used intorch.distributions.TransformedDistribution.\ntorch.distributions.TransformedDistribution\nCaching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values\nsince the autograd graph may be reversed. For example while the following\nworks with or without caching:\n\n```python\ny = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n\n```\n\nHowever the following will error when caching due to dependency reversal:\n\n```python\ny = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n\n```\n\nDerived classes should implement one or both of_call()or_inverse(). Derived classes that setbijective=Trueshould also\nimplementlog_abs_det_jacobian().\n_call()\n_inverse()\nlog_abs_det_jacobian()\ncache_size(int) \u2013 Size of cache. If zero, no caching is done. If one,\nthe latest single value is cached. Only 0 and 1 are supported.\ndomain(Constraint) \u2013 The constraint representing valid inputs to this transform.\nConstraint\ncodomain(Constraint) \u2013 The constraint representing valid outputs to this transform\nwhich are inputs to the inverse transform.\nConstraint\nbijective(bool) \u2013 Whether this transform is bijective. A transformtis bijective ifft.inv(t(x))==xandt(t.inv(y))==yfor everyxin the domain andyin\nthe codomain. Transforms that are not bijective should at least\nmaintain the weaker pseudoinverse propertiest(t.inv(t(x))==t(x)andt.inv(t(t.inv(y)))==t.inv(y).\nt\nt.inv(t(x))==x\nt(t.inv(y))==y\nx\ny\nt(t.inv(t(x))==t(x)\nt.inv(t(t.inv(y)))==t.inv(y)\nsign(intorTensor) \u2013 For bijective univariate transforms, this\nshould be +1 or -1 depending on whether transform is monotone\nincreasing or decreasing.\nReturns the inverseTransformof this transform.\nThis should satisfyt.inv.invist.\nTransform\nt.inv.invist\nReturns the sign of the determinant of the Jacobian, if applicable.\nIn general this only makes sense for bijective transforms.\nComputes the log det jacobianlog |dy/dx|given input and output.\nInfers the shape of the forward computation, given the input shape.\nDefaults to preserving shape.\nInfers the shapes of the inverse computation, given the output shape.\nDefaults to preserving shape.\n\n## Constraints#\n\nConstraints\nAbstract base class for constraints.\nA constraint object represents a region over which a variable is valid,\ne.g. within which a variable can be optimized.\nis_discrete(bool) \u2013 Whether constrained space is discrete.\nDefaults to False.\nevent_dim(int) \u2013 Number of rightmost dimensions that together define\nan event. Thecheck()method will remove this many dimensions\nwhen computing validity.\ncheck()\nReturns a byte tensor ofsample_shape+batch_shapeindicating\nwhether each event in value satisfies this constraint.\nsample_shape+batch_shape\nalias of_Cat\n_Cat\nalias of_DependentProperty\n_DependentProperty\nalias of_GreaterThan\n_GreaterThan\nalias of_GreaterThanEq\n_GreaterThanEq\nalias of_IndependentConstraint\n_IndependentConstraint\nalias of_IntegerInterval\n_IntegerInterval\nalias of_Interval\n_Interval\nalias of_HalfOpenInterval\n_HalfOpenInterval\nChecks ifconstraintis a_Dependentobject.\nconstraint\n_Dependent\nconstraint\u2013 AConstraintobject.\nConstraint\nTrue ifconstraintcan be refined to the type_Dependent, False otherwise.\nconstraint\n_Dependent\nbool\nbool\nExamples\n\n```python\n>>> import torch\n>>> from torch.distributions import Bernoulli\n>>> from torch.distributions.constraints import is_dependent\n\n```\n\n\n```python\n>>> dist = Bernoulli(probs=torch.tensor([0.6], requires_grad=True))\n>>> constraint1 = dist.arg_constraints[\"probs\"]\n>>> constraint2 = dist.arg_constraints[\"logits\"]\n\n```\n\n\n```python\n>>> for constraint in [constraint1, constraint2]:\n>>>     if is_dependent(constraint):\n>>>         continue\n\n```\n\nalias of_LessThan\n_LessThan\nConstraint for theMixtureSameFamilydistribution that adds back the rightmost batch dimension before\nperforming the validity check with the component distribution\nconstraint.\nMixtureSameFamily\nbase_constraint\u2013 TheConstraintobject of\nthe component distribution of\ntheMixtureSameFamilydistribution.\nConstraint\nMixtureSameFamily\nCheck validity ofvalueas a possible outcome of sampling\ntheMixtureSameFamilydistribution.\nvalue\nMixtureSameFamily\nalias of_Multinomial\n_Multinomial\nalias of_Stack\n_Stack\n\n## ConstraintRegistry#\n\nConstraintRegistry\nPyTorch provides two globalConstraintRegistryobjects that linkConstraintobjects toTransformobjects. These objects both\ninput constraints and return transforms, but they have different guarantees on\nbijectivity.\nConstraintRegistry\nConstraint\nTransform\nbiject_to(constraint)looks up a bijectiveTransformfromconstraints.realto the givenconstraint. The returned transform is guaranteed to have.bijective=Trueand should implement.log_abs_det_jacobian().\nbiject_to(constraint)\nTransform\nconstraints.real\nconstraint\n.bijective=True\n.log_abs_det_jacobian()\ntransform_to(constraint)looks up a not-necessarily bijectiveTransformfromconstraints.realto the givenconstraint. The returned transform is not guaranteed to\nimplement.log_abs_det_jacobian().\ntransform_to(constraint)\nTransform\nconstraints.real\nconstraint\n.log_abs_det_jacobian()\nThetransform_to()registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s.arg_constraintsdict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:\ntransform_to()\n.arg_constraints\n\n```python\nloc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints[\"scale\"])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()\n\n```\n\nThebiject_to()registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained.supportare\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:\nbiject_to()\n.support\n\n```python\ndist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()\n\n```\n\nNote\nAn example wheretransform_toandbiject_todiffer isconstraints.simplex:transform_to(constraints.simplex)returns aSoftmaxTransformthat simply\nexponentiates and normalizes its inputs; this is a cheap and mostly\ncoordinate-wise operation appropriate for algorithms like SVI. In\ncontrast,biject_to(constraints.simplex)returns aStickBreakingTransformthat\nbijects its input down to a one-fewer-dimensional space; this a more\nexpensive less numerically stable transform but is needed for algorithms\nlike HMC.\ntransform_to\nbiject_to\nconstraints.simplex\ntransform_to(constraints.simplex)\nSoftmaxTransform\nbiject_to(constraints.simplex)\nStickBreakingTransform\nThebiject_toandtransform_toobjects can be extended by user-defined\nconstraints and transforms using their.register()method either as a\nfunction on singleton constraints:\nbiject_to\ntransform_to\n.register()\n\n```python\ntransform_to.register(my_constraint, my_transform)\n\n```\n\nor as a decorator on parameterized constraints:\n\n```python\n@transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)\n\n```\n\nYou can create your own registry by creating a newConstraintRegistryobject.\nConstraintRegistry\nRegistry to link constraints to transforms.\nRegisters aConstraintsubclass in this registry. Usage:\nConstraint\n\n```python\n@my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n\n```\n\nconstraint(subclass ofConstraint) \u2013 A subclass ofConstraint, or\na singleton object of the desired class.\nConstraint\nConstraint\nfactory(Callable) \u2013 A callable that inputs a constraint object and returns\naTransformobject.\nTransform",
    "url": "https://pytorch.org/docs/stable/distributions.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5ef6724221ff7de22e2665cfebab41c7",
    "source": "pytorch_docs",
    "title": "torch.optim \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.optim#\n\nCreated On: Jun 13, 2025 | Last Updated On: Aug 24, 2025\ntorch.optimis a package implementing various optimization algorithms.\ntorch.optim\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can also be easily integrated in the\nfuture.\n\n## How to use an optimizer#\n\nTo usetorch.optimyou have to construct an optimizer object that will hold\nthe current state and will update the parameters based on the computed gradients.\ntorch.optim\n\n## Constructing it#\n\nTo construct anOptimizeryou have to give it an iterable containing the\nparameters (all should beParameters) or named parameters\n(tuples of (str,Parameter)) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc.\nOptimizer\nParameter\nParameter\nExample:\n\n```python\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001)\n\n```\n\nNamed parameters example:\n\n```python\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([('layer0', var1), ('layer1', var2)], lr=0.0001)\n\n```\n\n\n## Per-parameter options#\n\nOptimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group.\nOptimizer\nVariable\ndict\nparams\nFor example, this is very useful when one wants to specify per-layer learning rates:\n\n```python\noptim.SGD([\n    {'params': model.base.parameters(), 'lr': 1e-2},\n    {'params': model.classifier.parameters()}\n], lr=1e-3, momentum=0.9)\n\noptim.SGD([\n    {'params': model.base.named_parameters(), 'lr': 1e-2},\n    {'params': model.classifier.named_parameters()}\n], lr=1e-3, momentum=0.9)\n\n```\n\nThis means thatmodel.base\u2019s parameters will use a learning rate of1e-2, whereasmodel.classifier\u2019s parameters will stick to the default learning rate of1e-3.\nFinally a momentum of0.9will be used for all parameters.\nmodel.base\n1e-2\nmodel.classifier\n1e-3\n0.9\nNote\nYou can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups.\nAlso consider the following example related to the distinct penalization of parameters.\nRemember thatparameters()returns an iterable that\ncontains all learnable parameters, including biases and other\nparameters that may prefer distinct penalization. To address this, one can specify\nindividual penalization weights for each parameter group:\nparameters()\n\n```python\nbias_params = [p for name, p in self.named_parameters() if 'bias' in name]\nothers = [p for name, p in self.named_parameters() if 'bias' not in name]\n\noptim.SGD([\n    {'params': others},\n    {'params': bias_params, 'weight_decay': 0}\n], weight_decay=1e-2, lr=1e-2)\n\n```\n\nIn this manner, bias terms are isolated from non-bias terms, and aweight_decayof0is set specifically for the bias terms, as to avoid any penalization for\nthis group.\nweight_decay\n0\n\n## Taking an optimization step#\n\nAll optimizers implement astep()method, that updates the\nparameters. It can be used in two ways:\nstep()\noptimizer.step()\nThis is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward().\nbackward()\nExample:\n\n```python\nfor input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n\n```\n\noptimizer.step(closure)\nSome optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it.\nExample:\n\n```python\nfor input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n\n```\n\n\n## Base class#\n\nBase class for all optimizers.\nWarning\nParameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.\nparams(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized.\ntorch.Tensor\ndict\ndefaults(dict[str,Any]) \u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them).\nOptimizer.add_param_group\nOptimizer.add_param_group\nAdd a param group to theOptimizersparam_groups.\nOptimizer\nOptimizer.load_state_dict\nOptimizer.load_state_dict\nLoad the optimizer state.\nOptimizer.register_load_state_dict_pre_hook\nOptimizer.register_load_state_dict_pre_hook\nRegister a load_state_dict pre-hook which will be called beforeload_state_dict()is called. It should have the following signature::.\nload_state_dict()\nOptimizer.register_load_state_dict_post_hook\nOptimizer.register_load_state_dict_post_hook\nRegister a load_state_dict post-hook which will be called afterload_state_dict()is called. It should have the following signature::.\nload_state_dict()\nOptimizer.state_dict\nOptimizer.state_dict\nReturn the state of the optimizer as adict.\ndict\nOptimizer.register_state_dict_pre_hook\nOptimizer.register_state_dict_pre_hook\nRegister a state dict pre-hook which will be called beforestate_dict()is called.\nstate_dict()\nOptimizer.register_state_dict_post_hook\nOptimizer.register_state_dict_post_hook\nRegister a state dict post-hook which will be called afterstate_dict()is called.\nstate_dict()\nOptimizer.step\nOptimizer.step\nPerform a single optimization step to update parameter.\nOptimizer.register_step_pre_hook\nOptimizer.register_step_pre_hook\nRegister an optimizer step pre hook which will be called before optimizer step.\nOptimizer.register_step_post_hook\nOptimizer.register_step_post_hook\nRegister an optimizer step post hook which will be called after optimizer step.\nOptimizer.zero_grad\nOptimizer.zero_grad\nReset the gradients of all optimizedtorch.Tensors.\ntorch.Tensor\n\n## Algorithms#\n\nAdadelta\n\nAdadelta\nImplements Adadelta algorithm.\nAdafactor\n\nAdafactor\nImplements Adafactor algorithm.\nAdagrad\n\nAdagrad\nImplements Adagrad algorithm.\nAdam\n\nAdam\nImplements Adam algorithm.\nAdamW\n\nAdamW\nImplements AdamW algorithm, where weight decay does not accumulate in the momentum nor variance.\nSparseAdam\n\nSparseAdam\nSparseAdam implements a masked version of the Adam algorithm suitable for sparse gradients.\nAdamax\n\nAdamax\nImplements Adamax algorithm (a variant of Adam based on infinity norm).\nASGD\n\nASGD\nImplements Averaged Stochastic Gradient Descent.\nLBFGS\n\nLBFGS\nImplements L-BFGS algorithm.\nMuon\n\nMuon\nImplements Muon algorithm.\nNAdam\n\nNAdam\nImplements NAdam algorithm.\nRAdam\n\nRAdam\nImplements RAdam algorithm.\nRMSprop\n\nRMSprop\nImplements RMSprop algorithm.\nRprop\n\nRprop\nImplements the resilient backpropagation algorithm.\nSGD\n\nSGD\nImplements stochastic gradient descent (optionally with momentum).\nMany of our algorithms have various implementations optimized for performance,\nreadability and/or generality, so we attempt to default to the generally fastest\nimplementation for the current device if no particular implementation has been\nspecified by the user.\nWe have 3 major categories of implementations: for-loop, foreach (multi-tensor), and\nfused. The most straightforward implementations are for-loops over the parameters with\nbig chunks of computation. For-looping is usually slower than our foreach\nimplementations, which combine parameters into a multi-tensor and run the big chunks\nof computation all at once, thereby saving many sequential kernel calls. A few of our\noptimizers have even faster fused implementations, which fuse the big chunks of\ncomputation into one kernel. We can think of foreach implementations as fusing\nhorizontally and fused implementations as fusing vertically on top of that.\nIn general, the performance ordering of the 3 implementations is fused > foreach > for-loop.\nSo when applicable, we default to foreach over for-loop. Applicable means the foreach\nimplementation is available, the user has not specified any implementation-specific kwargs\n(e.g., fused, foreach, differentiable), and all tensors are native. Note that while fused\nshould be even faster than foreach, the implementations are newer and we would like to give\nthem more bake-in time before flipping the switch everywhere. We summarize the stability status\nfor each implementation on the second table below, you are welcome to try them out though!\nBelow is a table showing the available and default implementations of each algorithm:\nAlgorithm\nDefault\nHas foreach?\nHas fused?\nAdadelta\n\nAdadelta\nforeach\nyes\nno\nAdafactor\n\nAdafactor\nfor-loop\nno\nno\nAdagrad\n\nAdagrad\nforeach\nyes\nyes (cpu only)\nAdam\n\nAdam\nforeach\nyes\nyes\nAdamW\n\nAdamW\nforeach\nyes\nyes\nSparseAdam\n\nSparseAdam\nfor-loop\nno\nno\nAdamax\n\nAdamax\nforeach\nyes\nno\nASGD\n\nASGD\nforeach\nyes\nno\nLBFGS\n\nLBFGS\nfor-loop\nno\nno\nMuon\n\nMuon\nfor-loop\nno\nno\nNAdam\n\nNAdam\nforeach\nyes\nno\nRAdam\n\nRAdam\nforeach\nyes\nno\nRMSprop\n\nRMSprop\nforeach\nyes\nno\nRprop\n\nRprop\nforeach\nyes\nno\nSGD\n\nSGD\nforeach\nyes\nyes\nBelow table is showing the stability status for fused implementations:\nAlgorithm\nCPU\nCUDA\nMPS\nAdadelta\n\nAdadelta\nunsupported\nunsupported\nunsupported\nAdafactor\n\nAdafactor\nunsupported\nunsupported\nunsupported\nAdagrad\n\nAdagrad\nbeta\nunsupported\nunsupported\nAdam\n\nAdam\nbeta\nstable\nbeta\nAdamW\n\nAdamW\nbeta\nstable\nbeta\nSparseAdam\n\nSparseAdam\nunsupported\nunsupported\nunsupported\nAdamax\n\nAdamax\nunsupported\nunsupported\nunsupported\nASGD\n\nASGD\nunsupported\nunsupported\nunsupported\nLBFGS\n\nLBFGS\nunsupported\nunsupported\nunsupported\nMuon\n\nMuon\nunsupported\nunsupported\nunsupported\nNAdam\n\nNAdam\nunsupported\nunsupported\nunsupported\nRAdam\n\nRAdam\nunsupported\nunsupported\nunsupported\nRMSprop\n\nRMSprop\nunsupported\nunsupported\nunsupported\nRprop\n\nRprop\nunsupported\nunsupported\nunsupported\nSGD\n\nSGD\nbeta\nbeta\nbeta\n\n## How to adjust learning rate#\n\ntorch.optim.lr_scheduler.LRSchedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements.\ntorch.optim.lr_scheduler.LRScheduler\ntorch.optim.lr_scheduler.ReduceLROnPlateau\nLearning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way:\nExample:\n\n```python\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = ExponentialLR(optimizer, gamma=0.9)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n\n```\n\nMost learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it.\nExample:\n\n```python\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler1.step()\n    scheduler2.step()\n\n```\n\nIn many places in the documentation, we will use the following template to refer to schedulers\nalgorithms.\n\n```python\n>>> scheduler = ...\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n\n```\n\nWarning\nPrior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time.\nscheduler.step()\noptimizer.step()\nscheduler.step()\nlr_scheduler.LRScheduler\nlr_scheduler.LRScheduler\nAdjusts the learning rate during optimization.\nlr_scheduler.LambdaLR\nlr_scheduler.LambdaLR\nSets the initial learning rate.\nlr_scheduler.MultiplicativeLR\nlr_scheduler.MultiplicativeLR\nMultiply the learning rate of each parameter group by the factor given in the specified function.\nlr_scheduler.StepLR\nlr_scheduler.StepLR\nDecays the learning rate of each parameter group by gamma every step_size epochs.\nlr_scheduler.MultiStepLR\nlr_scheduler.MultiStepLR\nDecays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.\nlr_scheduler.ConstantLR\nlr_scheduler.ConstantLR\nMultiply the learning rate of each parameter group by a small constant factor.\nlr_scheduler.LinearLR\nlr_scheduler.LinearLR\nDecays the learning rate of each parameter group by linearly changing small multiplicative factor.\nlr_scheduler.ExponentialLR\nlr_scheduler.ExponentialLR\nDecays the learning rate of each parameter group by gamma every epoch.\nlr_scheduler.PolynomialLR\nlr_scheduler.PolynomialLR\nDecays the learning rate of each parameter group using a polynomial function in the given total_iters.\nlr_scheduler.CosineAnnealingLR\nlr_scheduler.CosineAnnealingLR\nSet the learning rate of each parameter group using a cosine annealing schedule.\nlr_scheduler.ChainedScheduler\nlr_scheduler.ChainedScheduler\nChains a list of learning rate schedulers.\nlr_scheduler.SequentialLR\nlr_scheduler.SequentialLR\nContains a list of schedulers expected to be called sequentially during the optimization process.\nlr_scheduler.ReduceLROnPlateau\nlr_scheduler.ReduceLROnPlateau\nReduce learning rate when a metric has stopped improving.\nlr_scheduler.CyclicLR\nlr_scheduler.CyclicLR\nSets the learning rate of each parameter group according to cyclical learning rate policy (CLR).\nlr_scheduler.OneCycleLR\nlr_scheduler.OneCycleLR\nSets the learning rate of each parameter group according to the 1cycle learning rate policy.\nlr_scheduler.CosineAnnealingWarmRestarts\nlr_scheduler.CosineAnnealingWarmRestarts\nSet the learning rate of each parameter group using a cosine annealing schedule.\n\n## How to utilize named parameters to load optimizer state dict#\n\nThe functionload_state_dict()stores the optionalparam_namescontent from the\nloaded state dict if present. However, the process of loading the optimizer state is not affected,\nas the order of the parameters matters to maintain compatibility (in case of different ordering).\nTo utilize the loaded parameters names from the loaded state dict, a customregister_load_state_dict_pre_hookneeds to be implemented according to the desired behavior.\nload_state_dict()\nparam_names\nregister_load_state_dict_pre_hook\nThis can be useful, for instance, when the model architecture changes, but the weights and optimizer states need to\nremain unchanged. The following example demonstrates how to implement this customization.\nExample:\n\n```python\nclass OneLayerModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(3, 4)\n\n    def forward(self, x):\n        return self.fc(x)\n\nmodel = OneLayerModel()\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\n# training..\ntorch.save(optimizer.state_dict(), PATH)\n\n```\n\nLet\u2019s say thatmodelimplements an expert (MoE), and we want to duplicate it and resume training\nfor two experts, both initialized the same way as thefclayer. For the followingmodel2we create two layers identical tofcand resume training by loading the model weights and optimizer states frommodelinto bothfc1andfc2ofmodel2(and adjust them accordingly):\nmodel\nfc\nmodel2\nfc\nmodel\nfc1\nfc2\nmodel2\n\n```python\nclass TwoLayerModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(3, 4)\n        self.fc2 = nn.Linear(3, 4)\n\n    def forward(self, x):\n        return (self.fc1(x) + self.fc2(x)) / 2\n\nmodel2 = TwoLayerModel()\n# adapt and load model weights..\noptimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)\n\n```\n\nTo load the state dict foroptimizer2with the state dict of the previous optimizer such that bothfc1andfc2will be initialized with a copy offcoptimizer states\n(to resume training for each layer fromfc), we can use the following hook:\noptimizer2\nfc1\nfc2\nfc\nfc\n\n```python\ndef adapt_state_dict_ids(optimizer, state_dict):\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n    for k, v in state_dict['param_groups'][0].items():\n        if k not in ['params', 'param_names']:\n            adapted_state_dict['param_groups'][0][k] = v\n\n    lookup_dict = {\n        'fc1.weight': 'fc.weight',\n        'fc1.bias': 'fc.bias',\n        'fc2.weight': 'fc.weight',\n        'fc2.bias': 'fc.bias'\n    }\n    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}\n    for param_id, param_name in zip(\n            optimizer.state_dict()['param_groups'][0]['params'],\n            optimizer.state_dict()['param_groups'][0]['param_names']):\n        name_in_loaded = lookup_dict[param_name]\n        index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)\n        id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]\n        # Copy the state of the corresponding parameter\n        if id_in_loaded in state_dict['state']:\n            adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\noptimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)\noptimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict\n\n```\n\nThis ensures that the adapted state_dict with the correct states for the layers ofmodel2will be used\nduring model loading.\nNote that this code is designed specifically for this example (e.g., assuming a single parameter group),\nand other cases might require different adaptations.\nmodel2\nThe following example shows how to handle missing parameters in a loadedstatedictwhen the model structure changes.\nTheModel_bypassadds a newbypasslayer, which is not present in the originalModel1.\nTo resume training, a customadapt_state_dict_missing_paramhook is used to adapt the optimizer\u2019sstate_dict,\nensuring existing parameters are mapped correctly, while missing ones (like the bypass layer) remain unchanged\n(as initialized in this example).\nThis approach enables smooth loading and resuming of the optimizer state despite model changes.\nThe new bypass layer will be trained from scratch:\nstatedict\nModel_bypass\nbypass\nModel1\nadapt_state_dict_missing_param\nstate_dict\n\n```python\nclass Model1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(5, 5)\n\n    def forward(self, x):\n        return self.fc(x) + x\n\n\nmodel = Model1()\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\n# training..\ntorch.save(optimizer.state_dict(), PATH)\n\nclass Model_bypass(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(5, 5)\n        self.bypass = nn.Linear(5, 5, bias=False)\n        torch.nn.init.eye_(self.bypass.weight)\n\n    def forward(self, x):\n        return self.fc(x) + self.bypass(x)\n\nmodel2 = Model_bypass()\noptimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)\n\ndef adapt_state_dict_missing_param(optimizer, state_dict):\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n    for k, v in state_dict['param_groups'][0].items():\n        if k not in ['params', 'param_names']:\n            adapted_state_dict['param_groups'][0][k] = v\n\n    lookup_dict = {\n        'fc.weight': 'fc.weight',\n        'fc.bias': 'fc.bias',\n        'bypass.weight': None,\n    }\n\n    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}\n    for param_id, param_name in zip(\n            optimizer.state_dict()['param_groups'][0]['params'],\n            optimizer.state_dict()['param_groups'][0]['param_names']):\n        name_in_loaded = lookup_dict[param_name]\n        if name_in_loaded in state_dict['param_groups'][0]['param_names']:\n            index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)\n            id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]\n            # Copy the state of the corresponding parameter\n            if id_in_loaded in state_dict['state']:\n                adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\noptimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)\noptimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict\n\n```\n\nAs a third example, instead of loading a state according to the order of parameters (the default approach),\nthis hook can be used to load according to the parameters\u2019 names:\n\n```python\ndef names_matching(optimizer, state_dict):\n    assert len(state_dict['param_groups']) == len(optimizer.state_dict()['param_groups'])\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    for g_ind in range(len(state_dict['param_groups'])):\n        assert len(state_dict['param_groups'][g_ind]['params']) == len(\n            optimizer.state_dict()['param_groups'][g_ind]['params'])\n\n        for k, v in state_dict['param_groups'][g_ind].items():\n            if k not in ['params', 'param_names']:\n                adapted_state_dict['param_groups'][g_ind][k] = v\n\n        for param_id, param_name in zip(\n                optimizer.state_dict()['param_groups'][g_ind]['params'],\n                optimizer.state_dict()['param_groups'][g_ind]['param_names']):\n            index_in_loaded_list = state_dict['param_groups'][g_ind]['param_names'].index(param_name)\n            id_in_loaded = state_dict['param_groups'][g_ind]['params'][index_in_loaded_list]\n            # Copy the state of the corresponding parameter\n            if id_in_loaded in state_dict['state']:\n                adapted_state_dict['state'][param_id] = deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\n```\n\n\n## Weight Averaging (SWA and EMA)#\n\ntorch.optim.swa_utils.AveragedModelimplements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA),torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA/EMA batch\nnormalization statistics at the end of training.\ntorch.optim.swa_utils.AveragedModel\ntorch.optim.swa_utils.SWALR\ntorch.optim.swa_utils.update_bn()\nSWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization.\nEMA is a widely known technique to reduce the training time by reducing the number of weight updates needed.\nIt is a variation ofPolyak averaging, but using exponential weights instead of equal weights across iterations.\n\n## Constructing averaged models#\n\nTheAveragedModelclass serves to compute the weights of the SWA or EMA model.\nAveragedModel\nYou can create an SWA averaged model by running:\n\n```python\n>>> averaged_model = AveragedModel(model)\n\n```\n\nEMA models are constructed by specifying themulti_avg_fnargument as follows:\nmulti_avg_fn\n\n```python\n>>> decay = 0.999\n>>> averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))\n\n```\n\nDecay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided totorch.optim.swa_utils.get_ema_multi_avg_fn(), the default is 0.999. Decay value should be close to 1.0, as smaller values can cause optimization convergence issues.\ntorch.optim.swa_utils.get_ema_multi_avg_fn()\ntorch.optim.swa_utils.get_ema_multi_avg_fn()returns a function that applies the following EMA equation to the weights:\ntorch.optim.swa_utils.get_ema_multi_avg_fn()\nwhere alpha is the EMA decay.\nHere the modelmodelcan be an arbitrarytorch.nn.Moduleobject.averaged_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you should use theupdate_parameters()function after theoptimizer.step():\nmodel\ntorch.nn.Module\naveraged_model\nmodel\nupdate_parameters()\noptimizer.step()\n\n```python\n>>> averaged_model.update_parameters(model)\n\n```\n\nFor SWA and EMA, this call is usually done right after the optimizerstep(). In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training.\nstep()\n\n## Custom averaging strategies#\n\nBy default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnormulti_avg_fnparameters:\ntorch.optim.swa_utils.AveragedModel\navg_fn\nmulti_avg_fn\navg_fnallows defining a function operating on each parameter tuple (averaged parameter, model parameter) and should return the new averaged parameter.\navg_fn\nmulti_avg_fnallows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using thetorch._foreach*functions. This function must update the averaged parameters in-place.\nmulti_avg_fn\ntorch._foreach*\nIn the following exampleema_modelcomputes an exponential moving average using theavg_fnparameter:\nema_model\navg_fn\n\n```python\n>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.9 * averaged_model_parameter + 0.1 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)\n\n```\n\nIn the following exampleema_modelcomputes an exponential moving average using the more efficientmulti_avg_fnparameter:\nema_model\nmulti_avg_fn\n\n```python\n>>> ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9))\n\n```\n\n\n## SWA learning rate schedules#\n\nTypically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group:\nSWALR\n\n```python\n>>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\n\n```\n\nYou can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\".\nanneal_strategy=\"cos\"\n\n## Taking care of batch normalization#\n\nupdate_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training:\nupdate_bn()\nloader\n\n```python\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n\n```\n\nupdate_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model.\nupdate_bn()\nswa_model\nWarning\nupdate_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.\nupdate_bn()\nloader\nswa_model\nswa_model\nswa_model\n\n## Putting it all together: SWA#\n\nIn the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:\nswa_model\n\n```python\n>>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input)\n\n```\n\n\n## Putting it all together: EMA#\n\nIn the example below,ema_modelis the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999.\nWe train the model for a total of 300 epochs and start to collect EMA averages immediately.\nema_model\n\n```python\n>>> loader, optimizer, model, loss_fn = ...\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, \\\n>>>             multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>           ema_model.update_parameters(model)\n>>>\n>>> # Update bn statistics for the ema_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, ema_model)\n>>> # Use ema_model to make predictions on test data\n>>> preds = ema_model(test_input)\n\n```\n\nswa_utils.AveragedModel\nswa_utils.AveragedModel\nImplements averaged model for Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA).\nswa_utils.SWALR\nswa_utils.SWALR\nAnneals the learning rate in each parameter group to a fixed value.\nGet the function applying exponential moving average (EMA) across multiple params.\nUpdate BatchNorm running_mean, running_var buffers in the model.\nIt performs one pass over data inloaderto estimate the activation\nstatistics for BatchNorm layers in the model.\nloader(torch.utils.data.DataLoader) \u2013 dataset loader to compute the\nactivation statistics on. Each data batch should be either a\ntensor, or a list/tuple whose first element is a tensor\ncontaining data.\nmodel(torch.nn.Module) \u2013 model for which we seek to update BatchNorm\nstatistics.\ndevice(torch.device,optional) \u2013 If set, data will be transferred todevicebefore being passed intomodel.\ndevice\nmodel\nExample\n\n```python\n>>> loader, model = ...\n>>> torch.optim.swa_utils.update_bn(loader, model)\n\n```\n\nNote\nTheupdate_bnutility assumes that each data batch inloaderis either a tensor or a list or tuple of tensors; in the latter case it\nis assumed thatmodel.forward()should be called on the first\nelement of the list or tuple corresponding to the data batch.\nloader\nmodel.forward()",
    "url": "https://pytorch.org/docs/stable/optim.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ff4daa2622d561ab55ff0d48dd181db2",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.fullgraph_false.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5afc9d240e1329150c6cebf5b67b93b7",
    "source": "pytorch_docs",
    "title": "Patching Batch Norm \u2014 PyTorch 2.9 documentation",
    "text": "\n## Patching Batch Norm#\n\nCreated On: Jan 03, 2023 | Last Updated On: Jun 11, 2025\n\n## What\u2019s happening?#\n\nBatch Norm requires in-place updates to running_mean and running_var of the same size as the input.\nFunctorch does not support inplace update to a regular tensor that takes in a batched tensor (i.e.regular.add_(batched)is not allowed). So when vmapping over a batch of inputs to a single module,\nwe end up with this error\nregular.add_(batched)\n\n## How to fix#\n\nOne of the best supported ways is to switch BatchNorm for GroupNorm. Options 1 and 2 support this\nAll of these options assume that you don\u2019t need running stats. If you\u2019re using a module this means\nthat it\u2019s assumed you won\u2019t use batch norm in evaluation mode. If you have a use case that involves\nrunning batch norm with vmap in evaluation mode, please file an issue\n\n## Option 1: Change the BatchNorm#\n\nIf you want to change for GroupNorm, anywhere that you have BatchNorm, replace it with:\n\n```python\nBatchNorm2d(C, G, track_running_stats=False)\n\n```\n\nHereCis the sameCas in the original BatchNorm.Gis the number of groups to\nbreakCinto. As such,C%G==0and as a fallback, you can setC==G, meaning\neach channel will be treated separately.\nC\nC\nG\nC\nC%G==0\nC==G\nIf you must use BatchNorm and you\u2019ve built the module yourself, you can change the module to\nnot use running stats. In other words, anywhere that there\u2019s a BatchNorm module, set thetrack_running_statsflag to be False\ntrack_running_stats\n\n```python\nBatchNorm2d(64, track_running_stats=False)\n\n```\n\n\n## Option 2: torchvision parameter#\n\nSome torchvision models, like resnet and regnet, can take in anorm_layerparameter. These are\noften defaulted to be BatchNorm2d if they\u2019ve been defaulted.\nnorm_layer\nInstead you can set it to be GroupNorm.\n\n```python\nimport torchvision\nfrom functools import partial\ntorchvision.models.resnet18(norm_layer=lambda c: GroupNorm(num_groups=g, c))\n\n```\n\nHere, once again,c%g==0so as a fallback, setg=c.\nc%g==0\ng=c\nIf you are attached to BatchNorm, be sure to use a version that doesn\u2019t use running stats\n\n```python\nimport torchvision\nfrom functools import partial\ntorchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))\n\n```\n\n\n## Option 3: functorch\u2019s patching#\n\nfunctorch has added some functionality to allow for quick, in-place patching of the module to not\nuse running stats. Changing the norm layer is more fragile, so we have not offered that. If you\nhave a net where you want the BatchNorm to not use running stats, you can runreplace_all_batch_norm_modules_to update the module in-place to not use running stats\nreplace_all_batch_norm_modules_\n\n```python\nfrom torch.func import replace_all_batch_norm_modules_\nreplace_all_batch_norm_modules_(net)\n\n```\n\n\n## Option 4: eval mode#\n\nWhen run under eval mode, the running_mean and running_var will not be updated. Therefore, vmap can support this mode\n\n```python\nmodel.eval()\nvmap(model)(x)\nmodel.train()\n\n```\n",
    "url": "https://pytorch.org/docs/stable/func.batch_norm.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "622218891ff66cfb5c95c51a4f45d281",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e78c1463a0f331307845fa48a840bd83",
    "source": "pytorch_docs",
    "title": "torch.func Whirlwind Tour \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.func Whirlwind Tour#\n\nCreated On: Jun 12, 2025 | Last Updated On: Jun 12, 2025\n\n## What is torch.func?#\n\ntorch.func, previously known as functorch, is a library forJAX-like composable function transforms in\nPyTorch.\nA \u201cfunction transform\u201d is a higher-order function that accepts a numerical\nfunction and returns a new function that computes a different quantity.\ntorch.func has auto-differentiation transforms (grad(f)returns a function\nthat computes the gradient off), a vectorization/batching transform\n(vmap(f)returns a function that computesfover batches of inputs),\nand others.\ngrad(f)\nf\nvmap(f)\nf\nThese function transforms can compose with each other arbitrarily. For\nexample, composingvmap(grad(f))computes a quantity called\nper-sample-gradients that stock PyTorch cannot efficiently compute today.\nvmap(grad(f))\n\n## Why composable function transforms?#\n\nThere are a number of use cases that are tricky to do in PyTorch today:\ncomputing per-sample-gradients (or other per-sample quantities)\nrunning ensembles of models on a single machine\nefficiently batching together tasks in the inner-loop of MAML\nefficiently computing Jacobians and Hessians\nefficiently computing batched Jacobians and Hessians\nComposingvmap(),grad(),vjp(), andjvp()transforms\nallows us to express the above without designing a separate subsystem for each.\nvmap()\ngrad()\nvjp()\njvp()\n\n## What are the transforms?#\n\n\n## grad()(gradient computation)#\n\ngrad()\ngrad(func)is our gradient computation transform. It returns a new function\nthat computes the gradients offunc. It assumesfuncreturns a single-element\nTensor and by default it computes the gradients of the output offuncw.r.t.\nto the first input.\ngrad(func)\nfunc\nfunc\nfunc\n\n```python\nimport torch\nfrom torch.func import grad\nx = torch.randn([])\ncos_x = grad(lambda x: torch.sin(x))(x)\nassert torch.allclose(cos_x, x.cos())\n\n# Second-order gradients\nneg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)\nassert torch.allclose(neg_sin_x, -x.sin())\n\n```\n\n\n## vmap()(auto-vectorization)#\n\nvmap()\nNote:vmap()imposes restrictions on the code that it can be used on. For more\ndetails, please seeUX Limitations.\nvmap()\nvmap(func)(*inputs)is a transform that adds a dimension to all Tensor\noperations infunc.vmap(func)returns a new function that mapsfuncover some dimension (default: 0) of each Tensor in inputs.\nvmap(func)(*inputs)\nfunc\nvmap(func)\nfunc\nvmap is useful for hiding batch dimensions: one can write a function func that\nruns on examples and then lift it to a function that can take batches of\nexamples withvmap(func), leading to a simpler modeling experience:\nvmap(func)\n\n```python\nimport torch\nfrom torch.func import vmap\nbatch_size, feature_size = 3, 5\nweights = torch.randn(feature_size, requires_grad=True)\n\ndef model(feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\nexamples = torch.randn(batch_size, feature_size)\nresult = vmap(model)(examples)\n\n```\n\nWhen composed withgrad(),vmap()can be used to compute per-sample-gradients:\ngrad()\nvmap()\n\n```python\nfrom torch.func import vmap\nbatch_size, feature_size = 3, 5\n\ndef model(weights,feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\ndef compute_loss(weights, example, target):\n    y = model(weights, example)\n    return ((y - target) ** 2).mean()  # MSELoss\n\nweights = torch.randn(feature_size, requires_grad=True)\nexamples = torch.randn(batch_size, feature_size)\ntargets = torch.randn(batch_size)\ninputs = (weights,examples, targets)\ngrad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n\n```\n\n\n## vjp()(vector-Jacobian product)#\n\nvjp()\nThevjp()transform appliesfunctoinputsand returns a new function\nthat computes the vector-Jacobian product (vjp) given somecotangentsTensors.\nvjp()\nfunc\ninputs\ncotangents\n\n```python\nfrom torch.func import vjp\n\ninputs = torch.randn(3)\nfunc = torch.sin\ncotangents = (torch.randn(3),)\n\noutputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)\n\n```\n\n\n## jvp()(Jacobian-vector product)#\n\njvp()\nThejvp()transforms computes Jacobian-vector-products and is also known as\n\u201cforward-mode AD\u201d. It is not a higher-order function unlike most other transforms,\nbut it returns the outputs offunc(inputs)as well as the jvps.\njvp()\nfunc(inputs)\n\n```python\nfrom torch.func import jvp\nx = torch.randn(5)\ny = torch.randn(5)\nf = lambda x, y: (x * y)\n_, out_tangent = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))\nassert torch.allclose(out_tangent, x + y)\n\n```\n\n\n## jacrev(),jacfwd(), andhessian()#\n\njacrev()\njacfwd()\nhessian()\nThejacrev()transform returns a new function that takes inxand returns\nthe Jacobian of the function with respect toxusing reverse-mode AD.\njacrev()\nx\nx\n\n```python\nfrom torch.func import jacrev\nx = torch.randn(5)\njacobian = jacrev(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n\n```\n\njacrev()can be composed withvmap()to produce batched jacobians:\njacrev()\nvmap()\n\n```python\nx = torch.randn(64, 5)\njacobian = vmap(jacrev(torch.sin))(x)\nassert jacobian.shape == (64, 5, 5)\n\n```\n\njacfwd()is a drop-in replacement for jacrev that computes Jacobians using\nforward-mode AD:\njacfwd()\n\n```python\nfrom torch.func import jacfwd\nx = torch.randn(5)\njacobian = jacfwd(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n\n```\n\nComposingjacrev()with itself orjacfwd()can produce hessians:\njacrev()\njacfwd()\n\n```python\ndef f(x):\n    return x.sin().sum()\n\nx = torch.randn(5)\nhessian0 = jacrev(jacrev(f))(x)\nhessian1 = jacfwd(jacrev(f))(x)\n\n```\n\nhessian()is a convenience function that combines jacfwd and jacrev:\nhessian()\n\n```python\nfrom torch.func import hessian\n\ndef f(x):\n    return x.sin().sum()\n\nx = torch.randn(5)\nhess = hessian(f)(x)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/func.whirlwind_tour.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b3dbd2c080b85ee8ef05a91eb68fbe12",
    "source": "pytorch_docs",
    "title": "JIT Utils - torch.utils.jit \u2014 PyTorch 2.9 documentation",
    "text": "\n## JIT Utils - torch.utils.jit#\n\nCreated On: Jun 26, 2025 | Last Updated On: Jun 26, 2025",
    "url": "https://pytorch.org/docs/stable/jit_utils.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "45a9dbe77410a9362e85c2c5b25482f5",
    "source": "pytorch_docs",
    "title": "python.data-structure \u2014 PyTorch 2.9 documentation",
    "text": "\n## python.data-structure#\n\n\n## dictionary#\n\nNote\nTags:python.data-structure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass Dictionary(torch.nn.Module):\n    \"\"\"\n    Dictionary structures are inlined and flattened along tracing.\n    \"\"\"\n\n    def forward(self, x, y):\n        elements = {}\n        elements[\"x2\"] = x * x\n        y = y * elements[\"x2\"]\n        return {\"y\": y}\n\nexample_args = (torch.randn(3, 2), torch.tensor(4))\ntags = {\"python.data-structure\"}\nmodel = Dictionary()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\", y: \"i64[]\"):\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, x);  x = None\n\n                 mul_1: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(y, mul);  y = mul = None\n            return (mul_1,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    mul_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## fn_with_kwargs#\n\nNote\nTags:python.data-structure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass FnWithKwargs(torch.nn.Module):\n    \"\"\"\n    Keyword arguments are not supported at the moment.\n    \"\"\"\n\n    def forward(self, pos0, tuple0, *myargs, mykw0, **mykwargs):\n        out = pos0\n        for arg in tuple0:\n            out = out * arg\n        for arg in myargs:\n            out = out * arg\n        out = out * mykw0\n        out = out * mykwargs[\"input0\"] * mykwargs[\"input1\"]\n        return out\n\nexample_args = (\n    torch.randn(4),\n    (torch.randn(4), torch.randn(4)),\n    *[torch.randn(4), torch.randn(4)]\n)\nexample_kwargs = {\n    \"mykw0\": torch.randn(4),\n    \"input0\": torch.randn(4),\n    \"input1\": torch.randn(4),\n}\ntags = {\"python.data-structure\"}\nmodel = FnWithKwargs()\n\n\ntorch.export.export(model, example_args, example_kwargs)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pos0: \"f32[4]\", tuple0_0: \"f32[4]\", tuple0_1: \"f32[4]\", myargs_0: \"f32[4]\", myargs_1: \"f32[4]\", mykw0: \"f32[4]\", input0: \"f32[4]\", input1: \"f32[4]\"):\n                 mul: \"f32[4]\" = torch.ops.aten.mul.Tensor(pos0, tuple0_0);  pos0 = tuple0_0 = None\n            mul_1: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul, tuple0_1);  mul = tuple0_1 = None\n\n                 mul_2: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_1, myargs_0);  mul_1 = myargs_0 = None\n            mul_3: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_2, myargs_1);  mul_2 = myargs_1 = None\n\n                 mul_4: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_3, mykw0);  mul_3 = mykw0 = None\n\n                 mul_5: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_4, input0);  mul_4 = input0 = None\n            mul_6: \"f32[4]\" = torch.ops.aten.mul.Tensor(mul_5, input1);  mul_5 = input1 = None\n            return (mul_6,)\n\nGraph signature:\n    # inputs\n    pos0: USER_INPUT\n    tuple0_0: USER_INPUT\n    tuple0_1: USER_INPUT\n    myargs_0: USER_INPUT\n    myargs_1: USER_INPUT\n    mykw0: USER_INPUT\n    input0: USER_INPUT\n    input1: USER_INPUT\n\n    # outputs\n    mul_6: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_contains#\n\nNote\nTags:python.data-structure,torch.dynamic-shape,python.assert\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass ListContains(torch.nn.Module):\n    \"\"\"\n    List containment relation can be checked on a dynamic shape or constants.\n    \"\"\"\n\n    def forward(self, x):\n        assert x.size(-1) in [6, 2]\n        assert x.size(0) not in [4, 5, 6]\n        assert \"monkey\" not in [\"cow\", \"pig\"]\n        return x + x\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"}\nmodel = ListContains()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_unpack#\n\nNote\nTags:python.data-structure,python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\n\nimport torch\n\nclass ListUnpack(torch.nn.Module):\n    \"\"\"\n    Lists are treated as static construct, therefore unpacking should be\n    erased after tracing.\n    \"\"\"\n\n    def forward(self, args: list[torch.Tensor]):\n        \"\"\"\n        Lists are treated as static construct, therefore unpacking should be\n        erased after tracing.\n        \"\"\"\n        x, *y = args\n        return x + y[0]\n\nexample_args = ([torch.randn(3, 2), torch.tensor(4), torch.tensor(5)],)\ntags = {\"python.control-flow\", \"python.data-structure\"}\nmodel = ListUnpack()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, args_0: \"f32[3, 2]\", args_1: \"i64[]\", args_2: \"i64[]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(args_0, args_1);  args_0 = args_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    args_0: USER_INPUT\n    args_1: USER_INPUT\n    args_2: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/python.data-structure.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "646877e62e18cdd8986da10844664a69",
    "source": "pytorch_docs",
    "title": "Use fullgraph=True to Identify and Eliminate Graph Breaks \u2014 PyTorch 2.9 documentation",
    "text": "\n## Usefullgraph=Trueto Identify and Eliminate Graph Breaks#\n\nfullgraph=True\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nUsingtorch.compile(fullgraph=False)(the default) is a good way to get started withtorch.compile: it supports all Python programs out-of-the-box via the ability to graph break and gives good performance on common cases.\ntorch.compile(fullgraph=False)\ntorch.compile\nHowever, if you\u2019re trying to get more performance out of your model, you should explicitly think about what regions of code should be compiled:\nWe recommend usingtorch.compile(fullgraph=True)to find and eliminate graph breaks in your code.\ntorch.compile(fullgraph=True)\nIf you\u2019re a library developer (or testing if your code \u201cworks\u201d withtorch.compile), we recommend testing usingtorch.compile(fullgraph=True).\ntorch.compile\ntorch.compile(fullgraph=True)\ntorch.compile(fullgraph=True)offers stronger guarantees overfullgraph=False:\nwe will always capture a single FX graph to be compiled (or error if we cannot due to a graph break).In particular, you are forced to resolve every graph break that is encountered.\ntorch.compile(fullgraph=True)\nfullgraph=False\nThere are a number of strategies for resolving a graph break.\n\n## Strategy 1:  Rewrite the unsupported code to use features supported by Dynamo#\n\nMany graph break error messages will give some suggestions on how to rewrite code to avoid the graph break.\nIf the graph break is still difficult to resolve, then please move on to the next strategy\nor submit an issue to thePyTorch GitHub repo.\nMore graph break examples and how to resolve them can be found inCommon Graph Breaks.\nExample: Dynamo does not support callingnexton alist_iteratorobject that was an input to the function being compiled.\nnext\nlist_iterator\n\n```python\n@torch.compile(fullgraph=True)\ndef f(xs):\n    a = next(xs)\n    b = next(xs)\n    return a + b\n\nxs = [torch.tensor(1.), torch.tensor(2.)]\ntry:\n    out = f(iter(xs))\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nUnsupported method call\n  Explanation: Dynamo does not know how to trace method `__next__` of class `list_iterator`\n  Hint: Avoid calling `list_iterator.__next__` in your code.\n  Hint: Please report an issue to PyTorch.\n  Hint: Dynamo does not fully support tracing builtin iterators (e.g. `map`, `zip`, `enumerate`) passed in from uncompiled to compiled regions (e.g. `torch.compile(fn)(enumerate(...))`). This can happen unintentionally if a previous graph break happens with a builtin iterator in the local scope.\n  Hint: List/dict comprehensions in Python <= 3.11 result in implicit function calls, which Dynamo cannot trace as a top level frame. Possible workarounds are (1) use a loop instead of a comprehension, (2) fix any graph breaks in the function above the comprehension, (3) wrap the comprehension in a function, or (4) use Python 3.12+.\n\n  Developer debug context: call_method UserDefinedObjectVariable(list_iterator) __next__ [] {}\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0156.html\n\nfrom user code:\n   File \"/tmp/ipykernel_387/1195637716.py\", line 3, in f\n    a = next(xs)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\nInstead, rewrite the compiled function to accept a list.\n\n```python\n@torch.compile(fullgraph=True)\ndef f_rewritten(xs):\n    it = iter(xs)\n    a = next(it)\n    b = next(it)\n    return a + b\n\nf_rewritten(xs)\n\n```\n\n\n```python\ntensor(3.)\n\n```\n\n\n## Strategy 2: Pure functions can always be compiled via an escape hatch.#\n\nSummary: The space of all Python functions is vast and thus it is impractical for Dynamo to be able to trace\nthrough every Python function without graph breaks. For Python functions considered to be \u201cpure\u201d\nthat Dynamo cannot trace through without graph breaks, we provide some escape hatches to attempt\nto trace through these functions anyway:\nUsecustom_oportriton_opon pure triton kernels.\ncustom_op\ntriton_op\nUsenonstrict_tracefor pure functions that only use PyTorch Tensor ops.\nnonstrict_trace\nUsecustom_opfor all other pure functions.\ncustom_op\nA \u201cpure function\u201d is a function with the following properties:\nDeterminism. Given the same inputs, the pure function will always return the same output\nNo external side effects. A pure function does not have any externally-visible side effects,\nsuch as modifying external state or performing I/O operations.\nSide effects that remain internal to the function are allowed (e.g. mutating intermediate tensors).\nOne notable exception is that mutatingtorch.*ops on function input Tensors are generally allowed.\ntorch.*\nExplicit input/output. All the input data must be passed through the function parameters and all of the outputs are returned from the function.\nSeePure Functionsfor examples.\nDynamo is theoretically able to handle a wide variety of impure functions, but may be lacking coverage for specific\nPython language features. However, pure functions can always be compiled via an escape hatch.\nIf you have a graph break it may be possible to refactor the code around it into a pure function and use an escape hatch that bypasses Dynamo tracing:\nUsetorch._dynamo.nonstrict_traceif you want the Tensor operations in the function to show up in the Dynamo output graph (and therefore be optimizable).nonstrict_tracetells Dynamo to usenon-strict tracing.\ntorch._dynamo.nonstrict_trace\nnonstrict_trace\nUse custom operators if you want the function to be opaque w.r.t. totorch.compile(both the frontend Dynamo and the backend).\ntorch.compile\nNote that there is nothing preventing these escape hatches from being applied to impure functions,\nbutwe do not provide any soundness guarantees.\nExample: If Dynamo doesn\u2019t support some Python feature or API that is non-strict traceable (e.g. it uses PyTorch operations),usetorch._dynamo.nonstrict_traceto capture it instead.\ntorch._dynamo.nonstrict_trace\n\n```python\n# this is a function that Dynamo doesn't support (due to the graph_break() call).\ndef g(x):\n    y = x.sin()\n    torch._dynamo.graph_break()\n    z = y.sin()\n    return z\n\n@torch.compile(fullgraph=True)\ndef f(x):\n    w = x.sin()\n    return g(w)\n\nx = torch.randn(3)\ntry:\n    f(x)  # Graph Break: there was a call to torch._dynamo.graph_break()\nexcept Exception as e:\n    print(e)\n\n@torch.compile(fullgraph=True)\ndef f_rewritten(x):\n    w = x.sin()\n    return torch._dynamo.nonstrict_trace(g)(w)\nf_rewritten(x)  # works\n\n```\n\n\n```python\nCall to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_387/2422769198.py\", line 11, in f\n    return g(w)\n  File \"/tmp/ipykernel_387/2422769198.py\", line 4, in g\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\ntensor([ 0.4607, -0.7399, -0.5501])\n\n```\n\nExample: usecustom operatorsto create opaque functions w.r.t. totorch.compile\ntorch.compile\n\n```python\nfrom torch.utils.cpp_extension import load_inline\n\n# C++ source code for the square operation\ncpp_source = \"\"\"\ntorch::Tensor square_cpu(torch::Tensor input) {\n    // Check that input is a CPU tensor\n    TORCH_CHECK(input.device().is_cpu(), \"Input must be a CPU tensor\");\n\n    // Create output tensor with same shape and dtype as input\n    torch::Tensor output = torch::empty_like(input);\n\n    // Get data pointers\n    float* input_data = input.data_ptr<float>();\n    float* output_data = output.data_ptr<float>();\n\n    // Get total number of elements\n    int64_t numel = input.numel();\n\n    // For loop to compute square of each element\n    for (int64_t i = 0; i < numel; i++) {\n        output_data[i] = input_data[i] * input_data[i];\n    }\n\n    return output;\n}\n\"\"\"\n\n# Load the extension inline\nsquare_module = load_inline(\n    name=\"square_cpu_kernel\",\n    cpp_sources=cpp_source,\n    functions=[\"square_cpu\"],\n    verbose=True\n)\n\ndef square(x):\n    return square_module.square_cpu(x)\n\n@torch.compile(fullgraph=True)\ndef f(x):\n    return square(x)\n\ntry:\n    f(torch.randn(3, 3))  # graph break\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\n[1/2] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=square_cpu_kernel -DTORCH_API_INCLUDE_EXTENSION_H -isystem /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include -isystem /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/envs/py_3.10/include/python3.10 -fPIC -std=c++17 -c /var/lib/jenkins/.cache/torch_extensions/py310_cpu/square_cpu_kernel/main.cpp -o main.o \n[2/2] c++ main.o -shared -L/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o square_cpu_kernel.so\nAttempted to call function marked as skipped\n  Explanation: Dynamo does not know how to trace the builtin `square_cpu_kernel.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1.square_cpu.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).\n  Hint: If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.\n  Hint: If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.\n\n  Developer debug context: module: square_cpu_kernel, qualname: pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1.square_cpu, skip reason: <missing reason>\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html\n\nfrom user code:\n   File \"/tmp/ipykernel_387/2059008136.py\", line 41, in f\n    return square(x)\n  File \"/tmp/ipykernel_387/2059008136.py\", line 37, in square\n    return square_module.square_cpu(x)\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\n/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py:1598: UserWarning: Dynamo does not know how to trace the builtin `square_cpu_kernel.pybind11_detail_function_record_v1_system_libstdcpp_gxx_abi_1xxx_use_cxx11_abi_1.square_cpu.` This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind).\nIf it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround.\nIf it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use `torch.compiler.allow_in_graph`.\n  torch._dynamo.utils.warn_once(explanation + \"\\n\" + \"\\n\".join(hints))\n\n```\n\n\n```python\n# Use torch.library.custom_op to define a new custom operator.\n# Custom operators are opaque with respect to torch.compile:\n# that is, torch.compile does not peek into them.\n\n@torch.library.custom_op(\"mylib::square\", mutates_args=())\ndef square(x: torch.Tensor) -> torch.Tensor:\n    return square_module.square_cpu(x)\n\n# Use register_fake to add a ``FakeTensor`` kernel for the operator\n@square.register_fake\ndef _(x):\n    return x.new_empty(x.size())\n\nprint(f(torch.randn(3, 3)))  # no graph break\n\n```\n\n\n```python\ntensor([[5.2260e-01, 1.9521e+00, 8.7664e-01],\n        [6.8205e-01, 5.1948e-03, 2.5824e-03],\n        [5.2988e+00, 5.3917e-01, 2.5636e+00]])\n\n```\n\nFor more information ontriton_opfor custom triton kernels, see theuser-defined triton kernel tutorial.\ntriton_op\n\n## Strategy 3: Don\u2019t compile the code#\n\nNot all code is amenable to being compiled.torch.compileis a compiler for Tensor computation;\nit will not be able to optimize things like disk IO. Try to refactor the code such that the unsupported\ncode is not called in the compiled region.\ntorch.compile\n\n```python\n@torch.compile(fullgraph=True)\ndef f(x):\n   y = x ** 2  / 2\n   torch.save(y, \"foo.pt\")\n   z = y ** 3 / 6\n   return z\n\nx = torch.randn(3)\ntry:\n    f(x)  # Graph Break: torch.save not supported\nexcept Exception as e:\n    print(e)\n\n```\n\n\n```python\nAttempted to call function marked as skipped\n  Explanation: Dynamo developers have intentionally marked that the function `save` in file `/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/serialization.py` should not be traced.\n  Hint: Avoid calling the function `save`.\n  Hint: Apply `@torch._dynamo.dont_skip_tracing` to the function `save` to force tracing into the function. More graph breaks may occur as a result of attempting to trace into the function.\n  Hint: Please file an issue to PyTorch.\n\n  Developer debug context: module: torch.serialization, qualname: save, skip reason: <missing reason>\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html\n\nfrom user code:\n   File \"/tmp/ipykernel_387/150060719.py\", line 4, in f\n    torch.save(y, \"foo.pt\")\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\ndef f_rewritten(x):\n   y = g(x)\n   torch.save(y, \"foo.pt\")\n   z = h(y)\n   return z\n\n@torch.compile(fullgraph=True)\ndef g(x):\n   y = x ** 2  / 2\n   return y\n\n@torch.compile(fullgraph=True)\ndef h(y):\n   z = y ** 3 / 6\n   return z\n\nf_rewritten(x)\n\n```\n\n\n```python\ntensor([3.1630e-04, 2.0966e-05, 2.8076e-02])\n\n```\n",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.fullgraph_true.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0e87999bf3958d097ef062d05b0d0edf",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/futures.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "bff5145bb8dd57a93fa155897983b44f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/community/persons_of_interest.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "50c5c13b258a0bcc4e485b7bb24ee69b",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/agent.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "49a765ec3791b5727bb4e803e6178419",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.compiler_disable.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1329429f2238f9c5069112c5dca0bb06",
    "source": "pytorch_docs",
    "title": "Accelerator Integration \u2014 PyTorch 2.9 documentation",
    "text": "\n## Accelerator Integration#\n\nCreated On: Sep 02, 2025 | Last Updated On: Sep 02, 2025\nSince PyTorch 2.1, the community has made significant progress in streamlining the process of integrating new accelerators into the PyTorch ecosystem. These improvements include, but are not limited to: refinements to thePrivateUse1Dispatch Key, the introduction and enhancement of core subsystem extension mechanisms, and the device-agnostic refactoring of key modules (e.g.,torch.accelerator,memorymanagement). Taken together, these advances provide the foundation for arobust,flexible, anddeveloper-friendlypathway for accelerator integration.\nPrivateUse1\ntorch.accelerator\nmemorymanagement\n\n## Why Does This Matter?#\n\nThis integration pathway offers several major benefits:\nSpeed: Extensibility is built into all core PyTorch modules. Developers can integrate new accelerators into their downstream codebases independently\u2014without modifying upstream code and without being limited by community review bandwidth.\nFuture-proofing: This is the default integration path for all future PyTorch features, meaning that as new modules and features are added, they will automatically support scaling to new accelerators if this path is followed.\nAutonomy: Vendors maintain full control over their accelerator integration timelines, enabling fast iteration cycles and reducing reliance on upstream coordination.\n\n## About This Document#\n\nThis guide aims to provide acomprehensive overview of the modern integration pathwayfor new accelerator in PyTorch. It walks through the full integration surface, from low-level device primitives to higher-level domain modules like compilation and quantization. The structure follows amodular and scenario-driven approach, where each topic is paired with corresponding code examples fromtorch_openreg, an official reference implementation.\nThe goal is to help developers:\nUnderstand the full scope of accelerator integration;\nFollow best practices to quickly launch new accelerators;\nAvoid common pitfalls through clear, targeted examples.\n\n## Target Audience#\n\nThis document is intended for:\nAccelerator Developerswho are integrating accelerator into PyTorch;\nAdvanced PyTorch Usersinterested in the inner workings of key modules;\n\n## Quick Overview#\n\nThis document outlines the key processes and practical scenarios involved in integrating new devices into PyTorch, providing developers with a comprehensive and detailed guide for bringing up new backends. The discussion is structured around four major axes:\nRuntime: Covers core components such as Event, Stream, Memory, Generator, Guard, Hooks, as well as the supporting C++ scaffolding.\nOperators: Involve the minimum necessary set of operators, forward and backward operators, fallback operators, fallthroughs, STUBs, etc. in both C++ and Python implementations.\nPython Frontend: Focuses on Python bindings for modules and device-agnostic APIs.\nHigh-level Modules: Explores integration with major subsystems such asAMP,Compiler,ONNX, andDistributedand so on.\nAMP\nCompiler\nONNX\nDistributed\nNext, we will officially embark on the integration journey for a new PyTorch accelerator.\nNote\nThis guide is a work in progress. For more details, please refer to theroadmap.",
    "url": "https://pytorch.org/docs/stable/accelerator/index.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9ba806bf717056232c967724a3be3da2",
    "source": "pytorch_docs",
    "title": "UX Limitations \u2014 PyTorch 2.9 documentation",
    "text": "\n## UX Limitations#\n\nCreated On: Jun 12, 2025 | Last Updated On: Jun 12, 2025\ntorch.func, likeJAX, has restrictions around\nwhat can be transformed. In general, JAX\u2019s limitations are that transforms\nonly work with pure functions: that is, functions where the output is completely\ndetermined by the input and that do not involve side effects (like mutation).\nWe have a similar guarantee: our transforms work well with pure functions.\nHowever, we do support certain in-place operations. On one hand, writing code\ncompatible with function transforms may involve changing how you write PyTorch\ncode, on the other hand, you may find that our transforms let you express things\nthat were previously difficult to express in PyTorch.\n\n## General limitations#\n\nAll torch.func transforms share a limitation in that a function should not\nassign to global variables. Instead, all outputs to a function must be returned\nfrom the function. This restriction comes from how torch.func is implemented:\neach transform wraps Tensor inputs in special torch.func Tensor subclasses\nthat facilitate the transform.\nSo, instead of the following:\n\n```python\nimport torch\nfrom torch.func import grad\n\n# Don't do this\nintermediate = None\n\ndef f(x):\n  global intermediate\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z\n\nx = torch.randn([])\ngrad_x = grad(f)(x)\n\n```\n\nPlease rewritefto returnintermediate:\nf\nintermediate\n\n```python\ndef f(x):\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z, intermediate\n\ngrad_x, intermediate = grad(f, has_aux=True)(x)\n\n```\n\n\n## torch.autograd APIs#\n\nIf you are trying to use atorch.autogradAPI liketorch.autograd.gradortorch.autograd.backwardinside of a function being transformed byvmap()or one of torch.func\u2019s AD transforms (vjp(),jvp(),jacrev(),jacfwd()), the transform may not be able to transform over it.\nIf it is unable to do so, you\u2019ll receive an error message.\ntorch.autograd\ntorch.autograd.grad\ntorch.autograd.backward\nvmap()\nvjp()\njvp()\njacrev()\njacfwd()\nThis is a fundamental design limitation in how PyTorch\u2019s AD support is implemented\nand the reason why we designed the torch.func library. Please instead use the torch.func\nequivalents of thetorch.autogradAPIs:\ntorch.autograd\ntorch.autograd.grad,Tensor.backward->torch.func.vjportorch.func.grad\ntorch.autograd.grad\nTensor.backward\ntorch.func.vjp\ntorch.func.grad\ntorch.autograd.functional.jvp->torch.func.jvp\ntorch.autograd.functional.jvp\ntorch.func.jvp\ntorch.autograd.functional.jacobian->torch.func.jacrevortorch.func.jacfwd\ntorch.autograd.functional.jacobian\ntorch.func.jacrev\ntorch.func.jacfwd\ntorch.autograd.functional.hessian->torch.func.hessian\ntorch.autograd.functional.hessian\ntorch.func.hessian\n\n## vmap limitations#\n\nNote\nvmap()is our most restrictive transform.\nThe grad-related transforms (grad(),vjp(),jvp()) do not\nhave these limitations.jacfwd()(andhessian(), which is\nimplemented withjacfwd()) is a composition ofvmap()andjvp()so it also has these limitations.\nvmap()\ngrad()\nvjp()\njvp()\njacfwd()\nhessian()\njacfwd()\nvmap()\njvp()\nvmap(func)is a transform that returns a function that mapsfuncover\nsome new dimension of each input Tensor. The mental model for vmap is that it is\nlike running a for-loop: for pure functions (i.e. in the absence of side\neffects),vmap(f)(x)is equivalent to:\nvmap(func)\nfunc\nvmap(f)(x)\n\n```python\ntorch.stack([f(x_i) for x_i in x.unbind(0)])\n\n```\n\n\n## Mutation: Arbitrary mutation of Python data structures#\n\nIn the presence of side effects,vmap()no longer acts like it is running\na for-loop. For example, the following function:\nvmap()\n\n```python\ndef f(x, list):\n  list.pop()\n  print(\"hello!\")\n  return x.sum(0)\n\nx = torch.randn(3, 1)\nlst = [0, 1, 2, 3]\n\nresult = vmap(f, in_dims=(0, None))(x, lst)\n\n```\n\nwill print \u201chello!\u201d once and pop only one element fromlst.\nlst\nvmap()executesfa single time, so all side effects only happen once.\nvmap()\nf\nThis is a consequence of how vmap is implemented. torch.func has a special,\ninternal BatchedTensor class.vmap(f)(*inputs)takes all Tensor inputs,\nturns them into BatchedTensors, and callsf(*batched_tensor_inputs).\nBatchedTensor overrides the PyTorch API to produce batched (i.e. vectorized)\nbehavior for each PyTorch operator.\nvmap(f)(*inputs)\nf(*batched_tensor_inputs)\n\n## Mutation: in-place PyTorch Operations#\n\nYou might be here due to receiving an error about vmap-incompatible in-place\noperations.vmap()will raise an error if it encounters an unsupported PyTorch\nin-place operation and it will succeed otherwise. Unsupported operations\nare those that would cause a Tensor with more elements to be written to a\nTensor with fewer elements. Here\u2019s an example of how this can occur:\nvmap()\n\n```python\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(1)\ny = torch.randn(3, 1)  # When vmapped over, looks like it has shape [1]\n\n# Raises an error because `x` has fewer elements than `y`.\nvmap(f, in_dims=(None, 0))(x, y)\n\n```\n\nxis a Tensor with one element,yis a Tensor with three elements.x+yhas three elements (due to broadcasting), but attempting to write\nthree elements back intox, which only has one element, raises an error\ndue to attempting to write three elements into a Tensor with a single element.\nx\ny\nx+y\nx\nThere is no problem if the Tensor being written to is batched undervmap()(i.e. it is being vmapped over).\nvmap()\n\n```python\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(3, 1)\ny = torch.randn(3, 1)\nexpected = x + y\n\n# Does not raise an error because x is being vmapped over.\nvmap(f, in_dims=(0, 0))(x, y)\nassert torch.allclose(x, expected)\n\n```\n\nOne common fix for this is to replace calls to factory functions with\ntheir \u201cnew_*\u201d equivalent. For example:\nReplacetorch.zeros()withTensor.new_zeros()\ntorch.zeros()\nTensor.new_zeros()\nReplacetorch.empty()withTensor.new_empty()\ntorch.empty()\nTensor.new_empty()\nTo see why this helps, consider the following.\n\n```python\ndef diag_embed(vec):\n  assert vec.dim() == 1\n  result = torch.zeros(vec.shape[0], vec.shape[0])\n  result.diagonal().copy_(vec)\n  return result\n\nvecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\n\n# RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ...\nvmap(diag_embed)(vecs)\n\n```\n\nInside ofvmap(),resultis a Tensor of shape [3, 3].\nHowever, althoughveclooks like it has shape [3],vecactually has\nunderlying shape [2, 3].\nIt is not possible to copyvecintoresult.diagonal(), which has\nshape [3], because it has too many elements.\nvmap()\nresult\nvec\nvec\nvec\nresult.diagonal()\n\n```python\ndef diag_embed(vec):\n  assert vec.dim() == 1\n  result = vec.new_zeros(vec.shape[0], vec.shape[0])\n  result.diagonal().copy_(vec)\n  return result\n\nvecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\nvmap(diag_embed)(vecs)\n\n```\n\nReplacingtorch.zeros()withTensor.new_zeros()makes it so thatresulthas an underlying Tensor of shape [2, 3, 3], so it is now possible\nto copyvec, which has underlying shape [2, 3], intoresult.diagonal().\ntorch.zeros()\nTensor.new_zeros()\nresult\nvec\nresult.diagonal()\n\n## Mutation: out= PyTorch Operations#\n\nvmap()doesn\u2019t support theout=keyword argument in PyTorch operations.\nIt will error out gracefully if it encounters that in your code.\nvmap()\nout=\nThis is not a fundamental limitation; we could theoretically support this in the\nfuture but we have chosen not to for now.\n\n## Data-dependent Python control flow#\n\nWe don\u2019t yet supportvmapover data-dependent control flow. Data-dependent\ncontrol flow is when the condition of an if-statement, while-loop, or\nfor-loop is a Tensor that is beingvmap\u2019ed over. For example, the\nfollowing will raise an error message:\nvmap\nvmap\n\n```python\ndef relu(x):\n  if x > 0:\n    return x\n  return 0\n\nx = torch.randn(3)\nvmap(relu)(x)\n\n```\n\nHowever, any control flow that is not dependent on the values invmap\u2019ed\ntensors will work:\nvmap\n\n```python\ndef custom_dot(x):\n  if x.dim() == 1:\n    return torch.dot(x, x)\n  return (x * x).sum()\n\nx = torch.randn(3)\nvmap(custom_dot)(x)\n\n```\n\nJAX supports transforming overdata-dependent control flowusing special control flow operators (e.g.jax.lax.cond,jax.lax.while_loop).\nWe\u2019re investigating adding equivalents of those to PyTorch.\njax.lax.cond\njax.lax.while_loop\n\n## Data-dependent operations (.item())#\n\nWe do not (and will not) support vmap over a user-defined function that calls.item()on a Tensor. For example, the following will raise an error message:\n.item()\n\n```python\ndef f(x):\n  return x.item()\n\nx = torch.randn(3)\nvmap(f)(x)\n\n```\n\nPlease try to rewrite your code to not use.item()calls.\n.item()\nYou may also encounter an error message about using.item()but you might\nnot have used it. In those cases, it is possible that PyTorch internally is\ncalling.item()\u2013 please file an issue on GitHub and we\u2019ll fix\nPyTorch internals.\n.item()\n.item()\n\n## Dynamic shape operations (nonzero and friends)#\n\nvmap(f)requires thatfapplied to every \u201cexample\u201d in your input\nreturns a Tensor with the same shape. Operations such astorch.nonzero,torch.is_nonzeroare not supported and will error as a result.\nvmap(f)\nf\ntorch.nonzero\ntorch.is_nonzero\nTo see why, consider the following example:\n\n```python\nxs = torch.tensor([[0, 1, 2], [0, 0, 3]])\nvmap(torch.nonzero)(xs)\n\n```\n\ntorch.nonzero(xs[0])returns a Tensor of shape 2;\nbuttorch.nonzero(xs[1])returns a Tensor of shape 1.\nWe are unable to construct a single Tensor as an output;\nthe output would need to be a ragged Tensor (and PyTorch does not yet have\nthe concept of a ragged Tensor).\ntorch.nonzero(xs[0])\ntorch.nonzero(xs[1])\n\n## Randomness#\n\nThe user\u2019s intention when calling a random operation can be unclear. Specifically, some users may want\nthe random behavior to be the same across batches while others may want it to differ across batches.\nTo address this,vmaptakes a randomness flag.\nvmap\nThe flag can only be passed to vmap and can take on 3 values, \u201cerror,\u201d \u201cdifferent,\u201d or \u201csame,\u201d defaulting\nto error. Under \u201cerror\u201d mode, any call to a random function will produce an error asking the user to use\none of the other two flags based on their use case.\nUnder \u201cdifferent\u201d randomness, elements in a batch produce different random values. For instance,\n\n```python\ndef add_noise(x):\n  y = torch.randn(())  # y will be different across the batch\n  return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"different\")(x)  # we get 3 different values\n\n```\n\nUnder \u201csame\u201d randomness, elements in a batch produce same random values. For instance,\n\n```python\ndef add_noise(x):\n  y = torch.randn(())  # y will be the same across the batch\n  return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"same\")(x)  # we get the same value, repeated 3 times\n\n```\n\nWarning\nOur system only determine the randomness behavior of PyTorch operators and cannot control the\nbehavior of other libraries, like numpy. This is similar to JAX\u2019s limitations with their solutions\nNote\nMultiple vmap calls using either type of supported randomness will not produce\nthe same results. Like with standard PyTorch, a user can get randomness reproducibility through\neither usingtorch.manual_seed()outside of vmap or by using generators.\ntorch.manual_seed()\nNote\nFinally, our randomness differs from JAX because we aren\u2019t using a stateless PRNG, in part because PyTorch\ndoesn\u2019t have full support for a stateless PRNG. Instead, we\u2019ve introduced a flag system to allow for the\nmost common forms of randomness that we see. If your use case does not fit these forms of randomness, please\nfile an issue.",
    "url": "https://pytorch.org/docs/stable/func.ux_limitations.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "77102d3354278204a028726e375e33f4",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/export/programming_model.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "56c92329acaf36c44a4ad86f6cb7190b",
    "source": "pytorch_docs",
    "title": "torch.export Programming Model \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.export Programming Model#\n\nCreated On: Dec 18, 2024 | Last Updated On: Jul 16, 2025\nThis document aims to explain the behaviors and capabilities oftorch.export.export(). It is intended to help build your intuition\nfor howtorch.export.export()handles code.\ntorch.export.export()\ntorch.export.export()\n\n## Basics of Tracing#\n\ntorch.export.export()captures a graph representing your model by\ntracing its execution on \u201cexample\u201d inputs and recording the PyTorch operations\nand conditions observed along the traced path. This graph can then be run\non different inputs as long as they satisfy the same conditions.\ntorch.export.export()\nThe basic output oftorch.export.export()is a single graph of PyTorch\noperations, with associated metadata. The exact format of this output is\ncovered in theexport IR spec.\ntorch.export.export()\n\n## Strict vs. Non-Strict Tracing#\n\ntorch.export.export()provides two modes of tracing.\ntorch.export.export()\nInnon-strict mode, we trace through the program using the normal Python\ninterpreter. Your code executes exactly as it would in eager mode; the only\ndifference is that all Tensors are replaced byfake Tensors,which have shapes and other forms of metadata but no data, wrapped inProxy objectsthat record all\noperations on them into a graph. We also captureconditions on Tensor shapesthat guard the correctness of the generated code.\nInstrict mode, we first trace through the program usingTorchDynamo, a Python bytecode\nanalysis engine. TorchDynamo does not actually execute your Python code.\nInstead, it symbolically analyzes it and builds a graph based on the results.\nOn the one hand, this analysis allowstorch.export.export()to provide\nadditional guarantees on Python-level safety (beyond capturing conditions on\nTensor shapes, as in non-strict mode). On the other hand, not all Python\nfeatures are supported by this analysis.\ntorch.export.export()\nAlthough currently the default mode of tracing is strict,we strongly\nrecommend using non-strict, which will soon become the default.\nFor most models, conditions on Tensor shapes are enough for soundness, and\nthe additional guarantees on Python-level safety have no impact; at the same\ntime, the possibility of hitting unsupported Python features in TorchDynamo\npresents an unnecessary risk.\nIn the rest of this document we assume we are tracing innon-strict mode;\nin particular, we assume thatall Python features are supported.\n\n## Values: Static vs. Dynamic#\n\nA key concept in understanding the behavior oftorch.export.export()is\nthe difference betweenstaticanddynamicvalues.\ntorch.export.export()\n\n## Static Values#\n\nAstaticvalue is a value that isfixed at export time and cannot change\nbetween executions of the exported program. When the value is encountered\nduring tracing, we treat it as a constant and hard-code it into the graph.\nWhen an operation is performed (e.g.x+y) and all inputs are static,\nthe output of the operation is directly hard-coded into the graph and the\noperation does not show up (i.e. it gets \u201cconstant-folded\u201d).\nx+y\nWhen a value has been hard-coded into the graph, we say that the graph has\nbeenspecializedto that value. For example:\n\n```python\nimport torch\n\nclass MyMod(torch.nn.Module):\n    def forward(self, x, y):\n        z = y + 7\n        return x + z\n\nm = torch.export.export(MyMod(), (torch.randn(1), 3))\nprint(m.graph_module.code)\n\n\"\"\"\ndef forward(self, arg0_1, arg1_1):\n    add = torch.ops.aten.add.Tensor(arg0_1, 10);  arg0_1 = None\n    return (add,)\n\n\"\"\"\n\n```\n\nHere, we provide3as the traced value fory; it is treated as a static\nvalue and added to7, burning in the static value10in the graph.\n3\ny\n7\n10\n\n## Dynamic Values#\n\nAdynamicvalue is one thatcan change from run to run. It behaves just\nlike a \u201cnormal\u201d function argument: you can pass different inputs and expect\nyour function to do the right thing.\n\n## Which values are static vs. dynamic?#\n\nWhether a value is static or dynamic depends on its type:\nFor Tensor:\nTensordatais treated as dynamic.\nTensorshapescan be treated by the system as static or dynamic.\nBy default, shapes of all input Tensors are considered static.\nThe user can override this behavior for any input Tensor by specifying\nadynamic shapefor it.\nTensors that are part of module state, i.e., parameters and buffers,\nalways have static shapes.\nOther forms of Tensormetadata(e.g.device,dtype) are static.\ndevice\ndtype\nPythonprimitives(int,float,bool,str,None) are static.\nint\nfloat\nbool\nstr\nNone\nThere are dynamic variants for some primitive types (SymInt,SymFloat,SymBool). Typically users do not have to deal with them.\nSymInt\nSymFloat\nSymBool\nUsers can specify integer inputs as dynamic by specifying\nadynamic shapefor it.\nFor Pythonstandard containers(list,tuple,dict,namedtuple):\nlist\ntuple\ndict\nnamedtuple\nThe structure (i.e., length forlistandtuplevalues, and key\nsequence fordictandnamedtuplevalues) is static.\nlist\ntuple\ndict\nnamedtuple\nThe contained elements have these rules applied to them recursively\n(basically thePyTreescheme)\nwith leaves that are either Tensor or primitive types.\nOtherclasses(including data classes) can be registered with PyTree\n(see below), and follow the same rules as the standard containers.\n\n## Input types#\n\nInputs will be treated as either static or dynamic, based on their type\n(as explained above).\nA static input will get hard-coded into the graph, and passing a different\nvalue at run time will result in an error. Recall that these are mostly\nvalues of primitive types.\nA dynamic input behaves like a \u201cnormal\u201d function input. Recall that these\nare mostly values of Tensor types.\nBy default, the types of inputs you can use for your program are:\nTensor\nPython primitives (int,float,bool,str,None)\nint\nfloat\nbool\nstr\nNone\nPython standard containers (list,tuple,dict,namedtuple)\nlist\ntuple\ndict\nnamedtuple\n\n## Custom Input Types (PyTree)#\n\nIn addition, you can also define your own (custom) class and use it as an\ninput type, but you will need to register such a class as a PyTree.\nHere\u2019s an example of using an utility to register a dataclass that is used as\nan input type.\n\n```python\n@dataclass\nclass Input:\n    f: torch.Tensor\n    p: torch.Tensor\n\nimport torch.utils._pytree as pytree\npytree.register_dataclass(Input)\n\nclass M(torch.nn.Module):\n    def forward(self, x: Input):\n        return x.f + 1\n\ntorch.export.export(M(), (Input(f=torch.ones(10, 4), p=torch.zeros(10, 4)),))\n\n```\n\n\n## Optional input types#\n\nFor optional inputs to the program that are not passed in,torch.export.export()will specialize to their default values. As a\nresult, the exported program will require users to explicitly pass in all\narguments, and will lose the defaulting behavior. For example:\ntorch.export.export()\n\n```python\nclass M(torch.nn.Module):\n    def forward(self, x, y=None):\n        if y is not None:\n            return y * x\n        return x + x\n\n# Optional input is passed in\nep = torch.export.export(M(), (torch.randn(3, 3), torch.randn(3, 3)))\nprint(ep)\n\"\"\"\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 3]\", y: \"f32[3, 3]\"):\n            # File: /data/users/angelayi/pytorch/moo.py:15 in forward, code: return y * x\n            mul: \"f32[3, 3]\" = torch.ops.aten.mul.Tensor(y, x);  y = x = None\n            return (mul,)\n\"\"\"\n\n# Optional input is not passed in\nep = torch.export.export(M(), (torch.randn(3, 3),))\nprint(ep)\n\"\"\"\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 3]\", y):\n            # File: /data/users/angelayi/pytorch/moo.py:16 in forward, code: return x + x\n            add: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\"\"\"\n\n```\n\n\n## Control Flow: Static vs. Dynamic#\n\nControl flow is supported bytorch.export.export(). The behavior of\ncontrol flow depends on whether the value you are branching on is static or\ndynamic.\ntorch.export.export()\n\n## Static Control Flow#\n\nPython control flow over static values is supported transparently. (Recall\nthat static values include static shapes, so control flow over static shapes\nis also covered by this case.)\nAs mentioned above, we \u201cburn in\u201d static values, so the exported graph will\nnever see any control flow over static values.\nIn the case of anifstatement, we will continue tracing the branch taken\nat export time. In the case of afororwhilestatement, we will continue\ntracing by unrolling the loop.\nif\nfor\nwhile\n\n## Dynamic Control Flow: Shape-Dependent vs. Data-Dependent#\n\nWhen the value involved in a control flow is dynamic, it could depend on\ndynamic shapes or dynamic data. Given that the compiler traces with\ninformation on shapes rather than data, the implications on the programming\nmodel are different in these cases.\nWhen the value involved in a control flow is adynamic shape,\nin most caseswe will also know the concrete value of the dynamic shape\nduring tracing: see the following section for more details on how the\ncompiler tracks this information.\nIn these cases we say that the control flow is shape-dependent.We use the\nconcrete value of the dynamic shape to evaluate the conditionto eitherTrueorFalseand continue tracing (as discussed above), additionally\nemitting a guard corresponding to the condition just evaluated.\nTrue\nFalse\nOtherwise the control flow is considered data-dependent. We cannot evaluate\nthe condition to eitherTrueorFalse, so cannot continue tracing and have to\nraise an error at export time. See next section.\nTrue\nFalse\nData-dependent control flow over dynamic values is supported, but you must\nuse one of PyTorch\u2019s explicit operatorsto continue tracing. Using Python\ncontrol flow statements over dynamic values is not permitted, because the\ncompiler cannot evaluate the conditions necessary to continue tracing and\nthus an error must be raised at export time.\nWe provideoperators to express general conditionals and loops over dynamic\nvalues, e.g.,torch.cond,torch.map. Note that you only need to use these\nif you truly wantdata-dependent control flow.\ntorch.cond\ntorch.map\nHere\u2019s an example of anifstatement on a data-dependent condition,x.sum()>0, wherexis an input Tensor, rewritten usingtorch.cond.\nInstead of having to decide which branch to trace, now both branches are\ntraced.\nif\nx.sum()>0\nx\ntorch.cond\n\n```python\nclass M_old(torch.nn.Module):\n    def forward(self, x):\n        if x.sum() > 0:\n            return x.sin()\n        else:\n            return x.cos()\n\nclass M_new(torch.nn.Module):\n    def forward(self, x):\n        return torch.cond(\n            pred=x.sum() > 0,\n            true_fn=lambda x: x.sin(),\n            false_fn=lambda x: x.cos(),\n            operands=(x,),\n        )\n\n```\n\nA special case of data-dependent control flow is where it involves adata-dependent dynamic shape:\ntypically, the shape of some intermediate Tensor that depends on input data\nrather than on input shapes (thus not shape-dependent). Instead of using a\ncontrol flow operator, in this case you can provide an assertion that decides\nwhether the condition isTrueorFalse. Given such an assertion, we can\ncontinue tracing, emitting a guard as above.\nTrue\nFalse\nWe provideoperators to express assertions on dynamic shapes, e.g.,torch._check. Note that you only need to use this when there is control\nflow on data-dependent dynamic shapes.\ntorch._check\nHere\u2019s an example of anifstatement on a condition involving a\ndata-dependent dynamic shape,nz.shape[0]>0, wherenzis the result of\ncallingtorch.nonzero(), an operator whose output shape depends on input\ndata. Instead of rewriting it, you can add an assertion usingtorch._checkto effectively decide which branch to trace.\nif\nnz.shape[0]>0\nnz\ntorch.nonzero()\ntorch._check\n\n```python\nclass M_old(torch.nn.Module):\n    def forward(self, x):\n        nz = x.nonzero()\n        if nz.shape[0] > 0:\n            return x.sin()\n        else:\n            return x.cos()\n\nclass M_new(torch.nn.Module):\n    def forward(self, x):\n        nz = x.nonzero()\n        torch._check(nz.shape[0] > 0)\n        if nz.shape[0] > 0:\n            return x.sin()\n        else:\n            return x.cos()\n\n```\n\n\n## Basics of Symbolic Shapes#\n\nDuring tracing, dynamic Tensor shapes and conditions over them are encoded as\n\u201csymbolic expressions.\u201d (In contrast, static Tensor shapes and conditions\nover them are simplyintandboolvalues.)\nint\nbool\nAsymbolis like a variable; it describes a dynamic Tensor shape.\nAs tracing proceeds, shapes of intermediate Tensors may be described by more\ngeneral expressions, typically involving integer arithmetic operators. This\nis becausefor most PyTorch operators, shapes of output Tensors can be\ndescribed as functions of shapes of input Tensors. For example, the shape of\nthe output oftorch.cat()is the sum of the shapes of its inputs.\ntorch.cat()\nMoreover, as we encounter control flow in the program, we create boolean\nexpressions, typically involving relational operators, describing conditions\nalong the traced path. Theseexpressions are evaluated to decide which path\nto trace through the program, and recorded in ashape environmentto guard the correctness of the traced path and to evaluate subsequently\ncreated expressions.\nWe briefly introduce these subsystems next.\n\n## Fake Implementations of PyTorch Operators#\n\nRecall that during tracing, we are executing the program withfake Tensors,\nwhich have no data. In general we cannot call the actual implementations of\nPyTorch operators with fake Tensors. Thus each operator needs to have an\nadditional fake (a.k.a. \u201cmeta\u201d) implementation, which inputs and outputs fake\nTensors, that matches the behavior of the actual implementation in terms of\nshapes and other forms of metadata carried by fake Tensors.\nFor example, note how the fake implementation oftorch.index_select()computes the shape of the output using the shape of the input (while ignoring\ninput data and returning empty output data).\ntorch.index_select()\n\n```python\ndef meta_index_select(self, dim, index):\n    result_size = list(self.size())\n    if self.dim() > 0:\n        result_size[dim] = index.numel()\n    return self.new_empty(result_size)\n\n```\n\nShapes are propagated using fake implementations of PyTorch operators.\nA key concept to understand the propagation of dynamic shapes in particular\nis the difference betweenbackedandunbackeddynamic shapes: we know the\nconcrete values of the former but not the latter.\nPropagation of shapes, including tracking backed and unbacked dynamic shapes,\nproceeds as follows:\nThe shapes of Tensors representing inputs can be static or dynamic. When\ndynamic, they are described by symbols; moreover,such symbols are backed\nsince we also know their concrete values given the \u201creal\u201d example inputs\nprovided by the user at export time.\nThe output shape of an operator is computed by its fake implementation, and\nis either static or dynamic. When dynamic, in general it is described by a\nsymbolic expression. Moreover:\nIf the output shape depends only on input shapes, it is either static or\nbacked dynamic whenever the input shapes are all static or backed dynamic.\nOn the other hand,if the output shape depends on input data, it is\nnecessarily dynamic, and moreover,because we cannot know its concrete\nvalue it is unbacked.\n\n## Control Flow: Guards and Assertions#\n\nWhen a condition on shapes is encountered, it either involves only static\nshapes, in which case it is abool, or it involves dynamic shapes, in which\ncase it is a symbolic boolean expression. For the latter:\nbool\nWhen the condition involves only backed dynamic shapes, we can use the\nconcrete values of those dynamic shapes to evaluate the condition toTrueorFalse. We can then add a guard to the shape environment that states\nthat the corresponding symbolic boolean expression isTrueorFalse,\nand continue tracing.\nTrue\nFalse\nTrue\nFalse\nOtherwise the condition involves unbacked dynamic shapes. In general we\ncannot evaluate such a condition without additional information; thus we\ncannot continue tracing, and we must raise an error at export time. The\nuser is expected to use an explicit PyTorch operator for tracing to\ncontinue. This information is added as a guard in the shape environment,\nand can also possibly help evaluate other subsequently encountered\nconditions toTrueorFalse.\nTrue\nFalse\nOnce the model is exported,any guards on backed dynamic shapes can be\nunderstood as conditions on input dynamic shapes. These are verified against\na dynamic shape specification that must have been provided to export,\ndescribing conditions on dynamic shapes that not only example inputs but also\nall future inputs are expected to satisfy for the generated code to be\ncorrect. More precisely, the dynamic shape specification must logically imply\nthe generated guards, otherwise an error is raised at export time (along with\nsuggested fixes to the dynamic shape specification). On the other hand, when\nthere are no generated guards on backed dynamic shapes (in particular, when\nall shapes are static) no dynamic shape specification needs to be provided to\nexport. In general, the dynamic shape specification is converted to runtime\nassertions on the inputs of the generated code.\nFinally,any guards on unbacked dynamic shapes are converted to \u201cinline\u201d\nruntime assertions. These are added in the generated code at the locations\nwhere those unbacked dynamic shapes were created: typically, right after\ndata-dependent operator calls.\n\n## Allowed PyTorch operators#\n\nAll PyTorch operators are permitted.\n\n## Custom operators#\n\nIn addition, you can define and usecustom operators.\nDefining a custom operator includes defining a fake implementation for it,\njust like any other PyTorch operator (see previous section).\nHere\u2019s an example of a customsinoperator that wraps NumPy, and its\nregistered (trivial) fake implementation.\nsin\n\n```python\n@torch.library.custom_op(\"mylib::sin\", mutates_args=())\ndef sin(x: Tensor) -> Tensor:\n    x_np = x.numpy()\n    y_np = np.sin(x_np)\n    return torch.from_numpy(y_np)\n\n@torch.library.register_fake(\"mylib::sin\")\ndef _(x: Tensor) -> Tensor:\n    return torch.empty_like(x)\n\n```\n\nSometimes your custom operator\u2019s fake implementation will involve\ndata-dependent shapes. Here\u2019s how a fake implementation for a customnonzeromight look like.\nnonzero\n\n```python\n...\n\n@torch.library.register_fake(\"mylib::custom_nonzero\")\ndef _(x):\n    nnz = torch.library.get_ctx().new_dynamic_size()\n    shape = [nnz, x.dim()]\n    return x.new_empty(shape, dtype=torch.int64)\n\n```\n\n\n## Module State: Reads vs. Updates#\n\nModule states include parameters, buffers, and regular attributes.\nA regular attribute can be of any type.\nOn the other hand, parameters and buffers are always Tensors.\nModule states can be dynamic or static, based on their types as outlined\nabove. For example,self.trainingis abool, which means it is static; on\nthe other hand, any parameter or buffer is dynamic.\nself.training\nbool\nTheshapesof any Tensors contained in module states cannot be dynamic, i.e.,\nthose shapes are fixed at export time, and cannot change between executions\nof the exported program.\n\n## Access rules#\n\nAll module states must be initialized. Accessing a module state that is\nnot already initialized causes an error to be raised at export time.\nReading module states is always permitted.\nUpdating module states is possible, but must follow the rules below:\nA static regular attribute(e.g., of primitive type)can be updated.\nReads and updates can be freely interleaved, and as expected, any reads\nwill always see the values of the latest updates. Because these attributes\nare static, we will also burn the values in, so the generated code will not\nhave any instructions to actually \u201cget\u201d or \u201cset\u201d such attributes.\nA dynamic regular attribute(e.g., of Tensor type)cannot be updated.\nTo do so, it must be registered as a buffer during module initialization.\nA buffer can be updated, where the updating can be in-place (e.g.,self.buffer[:]=...) or not (e.g.,self.buffer=...).\nself.buffer[:]=...\nself.buffer=...\nA parameter cannot be updated. Typically parameters are updated only\nduring training, not during inference. We recommend exporting withtorch.no_grad()to avoid parameter updates at export time.\ntorch.no_grad()\n\n## Effects of functionalization#\n\nAny dynamic module state that is read and/or updated is \u201clifted\u201d\n(respectively) as an input and/or output of the generated code.\nThe exported program stores, along with the generated code, the initial\nvalues of parameters and buffers and the constant values of other Tensor\nattributes.",
    "url": "https://pytorch.org/docs/stable/export/programming_model.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "474904cd7007febe586d6915f05ad3b5",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/mobile_optimizer.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5a58440918ea7e6962166b82f2a19efd",
    "source": "pytorch_docs",
    "title": "Out Notes \u2014 PyTorch 2.9 documentation",
    "text": "\n## Out Notes#\n\nCreated On: Apr 24, 2025 | Last Updated On: Apr 24, 2025\nWhen a user passes one or more tensors to out= the contract is as follows:\nif an out tensor has no elements it will be resized to the shape, stride, and memory format of the output of the computation.\nif an out tensor has a different shape than the result of the computation an error is thrown OR the out tensor is resized to the same shape, stride, and memory format of the output computation, just like a tensor with no elements. (This resizing behavior is deprecated and PyTorch is updating its operators to consistently throw an error.)\npassing out= tensors with the correct shape is numerically equivalent to performing the operation and \u201csafe copying\u201d its results to the (possibly resized) out tensor. In this case strides and memory format are preserved.\npassing out= tensors with grad needed is not supported.\nif multiple tensors are passed to out= then the above behavior applies to each independently.\nA \u201csafe copy\u201d is different from PyTorch\u2019s regular copy. For operations that do not participate in type promotion the device and dtype of the source and destination tensors must match. For operations that do participate in type promotion the copy can be to a different dtype, but the destination of the copy cannot be a lower \u201ctype kind\u201d than the source. PyTorch has four type kinds: boolean, integer, float, and complex, in that order. So, for example, an operation like add (which participates in type promotion) will throw a runtime error if given float inputs but an integer out= tensor.\nNote that while the numerics of out= for correctly shaped tensors are that the operation is performed and then its results are \u201csafe copied,\u201d behind the scenes operations may reuse the storage of out= tensors and fuse the copy for efficiency. Many operations, like add, perform these optimizations. Also, while PyTorch\u2019s \u201cout= contract\u201d is specified above, many operations in PyTorch do not correctly implement the contract and need to be updated.",
    "url": "https://pytorch.org/docs/stable/notes/out.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "61cd9cf2c821c927c161dd38db25b096",
    "source": "pytorch_docs",
    "title": "torch.dynamic-shape \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.dynamic-shape#\n\n\n## cond_branch_class_method#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass MySubModule(torch.nn.Module):\n    def foo(self, x):\n        return x.cos()\n\n    def forward(self, x):\n        return self.foo(x)\n\nclass CondBranchClassMethod(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n\n    This example demonstrates using class method in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.subm = MySubModule()\n\n    def bar(self, x):\n        return x.sin()\n\n    def forward(self, x):\n        return cond(x.shape[0] <= 2, self.subm.forward, self.bar, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchClassMethod()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 sin: \"f32[3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nested_function#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNestedFunction(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n      - both branches must take the same args, which must also match the branch args passed to cond.\n      - both branches must return a single tensor\n      - returned tensor must have the same tensor metadata, e.g. shape and dtype\n      - branch function can be free function, nested function, lambda, class methods\n      - branch function can not have closure variables\n      - no inplace mutations on inputs or global variables\n\n    This example demonstrates using nested function in cond().\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        def true_fn(x):\n            def inner_true_fn(y):\n                return x + y\n\n            return inner_true_fn(x)\n\n        def false_fn(x):\n            def inner_false_fn(y):\n                return x - y\n\n            return inner_false_fn(x)\n\n        return cond(x.shape[0] < 10, true_fn, false_fn, [x])\n\nexample_args = (torch.randn(3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNestedFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3]\"):\n                 add: \"f32[3]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_branch_nonlocal_variables#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondBranchNonlocalVariables(torch.nn.Module):\n    \"\"\"\n    The branch functions (`true_fn` and `false_fn`) passed to cond() must follow these rules:\n    - both branches must take the same args, which must also match the branch args passed to cond.\n    - both branches must return a single tensor\n    - returned tensor must have the same tensor metadata, e.g. shape and dtype\n    - branch function can be free function, nested function, lambda, class methods\n    - branch function can not have closure variables\n    - no inplace mutations on inputs or global variables\n\n    This example demonstrates how to rewrite code to avoid capturing closure variables in branch functions.\n\n    The code below will not work because capturing closure variables is not supported.\n    ```\n    my_tensor_var = x + 100\n    my_primitive_var = 3.14\n\n    def true_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y + my_tensor_var + my_primitive_var\n\n    def false_fn(y):\n        nonlocal my_tensor_var, my_primitive_var\n        return y - my_tensor_var - my_primitive_var\n\n    return cond(x.shape[0] > 5, true_fn, false_fn, [x])\n    ```\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        my_tensor_var = x + 100\n        my_primitive_var = 3.14\n\n        def true_fn(x, y, z):\n            return x + y + z\n\n        def false_fn(x, y, z):\n            return x - y - z\n\n        return cond(\n            x.shape[0] > 5,\n            true_fn,\n            false_fn,\n            [x, my_tensor_var, torch.tensor(my_primitive_var)],\n        )\n\nexample_args = (torch.randn(6),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondBranchNonlocalVariables()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, c_lifted_tensor_0: \"f32[]\", x: \"f32[6]\"):\n                 add: \"f32[6]\" = torch.ops.aten.add.Tensor(x, 100)\n\n                 lift_fresh_copy: \"f32[]\" = torch.ops.aten.lift_fresh_copy.default(c_lifted_tensor_0);  c_lifted_tensor_0 = None\n            detach_: \"f32[]\" = torch.ops.aten.detach_.default(lift_fresh_copy);  lift_fresh_copy = None\n\n                 add_1: \"f32[6]\" = torch.ops.aten.add.Tensor(x, add);  x = add = None\n            add_2: \"f32[6]\" = torch.ops.aten.add.Tensor(add_1, detach_);  add_1 = detach_ = None\n            return (add_2,)\n\nGraph signature:\n    # inputs\n    c_lifted_tensor_0: CONSTANT_TENSOR target='lifted_tensor_0'\n    x: USER_INPUT\n\n    # outputs\n    add_2: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## cond_operands#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.export import Dim\n\nx = torch.randn(3, 2)\ny = torch.randn(2)\ndim0_x = Dim(\"dim0_x\")\n\nclass CondOperands(torch.nn.Module):\n    \"\"\"\n    The operands passed to cond() must be:\n    - a list of tensors\n    - match arguments of `true_fn` and `false_fn`\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x, y):\n        def true_fn(x, y):\n            return x + y\n\n        def false_fn(x, y):\n            return x - y\n\n        return torch.cond(x.shape[0] > 2, true_fn, false_fn, [x, y])\n\nexample_args = (x, y)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nextra_inputs = (torch.randn(2, 2), torch.randn(2))\ndynamic_shapes = {\"x\": {0: dim0_x}, \"y\": None}\nmodel = CondOperands()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n             #\n            sym_size_int_1: \"Sym(s77)\" = torch.ops.aten.sym_size.int(x, 0)\n\n                 gt: \"Sym(s77 > 2)\" = sym_size_int_1 > 2;  sym_size_int_1 = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, (x, y));  gt = true_graph_0 = false_graph_0 = x = y = None\n            getitem: \"f32[s77, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         add: \"f32[s77, 2]\" = torch.ops.aten.add.Tensor(x, y);  x = y = None\n                return (add,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[s77, 2]\", y: \"f32[2]\"):\n                         sub: \"f32[s77, 2]\" = torch.ops.aten.sub.Tensor(x, y);  x = y = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {s77: VR[0, int_oo]}\n\n```\n\n\n## cond_predicate#\n\nNote\nTags:torch.dynamic-shape,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondPredicate(torch.nn.Module):\n    \"\"\"\n    The conditional statement (aka predicate) passed to cond() must be one of the following:\n      - torch.Tensor with a single element\n      - boolean expression\n\n    NOTE: If the `pred` is test on a dim with batch size < 2, it will be specialized.\n    \"\"\"\n\n    def forward(self, x):\n        pred = x.dim() > 2 and x.shape[2] > 10\n\n        return cond(pred, lambda x: x.cos(), lambda y: y.sin(), [x])\n\nexample_args = (torch.randn(6, 4, 3),)\ntags = {\n    \"torch.cond\",\n    \"torch.dynamic-shape\",\n}\nmodel = CondPredicate()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[6, 4, 3]\"):\n                 sin: \"f32[6, 4, 3]\" = torch.ops.aten.sin.default(x);  x = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_constructor#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeConstructor(torch.nn.Module):\n    \"\"\"\n    Tensor constructors should be captured with dynamic shape inputs rather\n    than being baked in with static shape.\n    \"\"\"\n\n    def forward(self, x):\n        return torch.zeros(x.shape[0] * 2)\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\"}\nmodel = DynamicShapeConstructor()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 zeros: \"f32[6]\" = torch.ops.aten.zeros.default([6], device = device(type='cpu'), pin_memory = False)\n            return (zeros,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    zeros: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_if_guard#\n\nNote\nTags:torch.dynamic-shape,python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeIfGuard(torch.nn.Module):\n    \"\"\"\n    `if` statement with backed dynamic shape predicate will be specialized into\n    one particular branch and generate a guard. However, export will fail if the\n    the dimension is marked as dynamic shape from higher level API.\n    \"\"\"\n\n    def forward(self, x):\n        if x.shape[0] == 3:\n            return x.cos()\n\n        return x.sin()\n\nexample_args = (torch.randn(3, 2, 2),)\ntags = {\"torch.dynamic-shape\", \"python.control-flow\"}\nmodel = DynamicShapeIfGuard()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2, 2]\"):\n                 cos: \"f32[3, 2, 2]\" = torch.ops.aten.cos.default(x);  x = None\n            return (cos,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    cos: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_map#\n\nNote\nTags:torch.dynamic-shape,torch.map\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import map\n\nclass DynamicShapeMap(torch.nn.Module):\n    \"\"\"\n    functorch map() maps a function over the first tensor dimension.\n    \"\"\"\n\n    def forward(self, xs, y):\n        def body(x, y):\n            return x + y\n\n        return map(body, xs, y)\n\nexample_args = (torch.randn(3, 2), torch.randn(2))\ntags = {\"torch.dynamic-shape\", \"torch.map\"}\nmodel = DynamicShapeMap()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, xs: \"f32[3, 2]\", y: \"f32[2]\"):\n                 body_graph_0 = self.body_graph_0\n            map_impl = torch.ops.higher_order.map_impl(body_graph_0, [xs], [y]);  body_graph_0 = xs = y = None\n            getitem: \"f32[3, 2]\" = map_impl[0];  map_impl = None\n            return (getitem,)\n\n        class body_graph_0(torch.nn.Module):\n            def forward(self, xs: \"f32[2]\", y: \"f32[2]\"):\n                         add: \"f32[2]\" = torch.ops.aten.add.Tensor(xs, y);  xs = y = None\n                return (add,)\n\nGraph signature:\n    # inputs\n    xs: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_round#\n\nNote\nTags:python.builtin,torch.dynamic-shape\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch._export.db.case import SupportLevel\nfrom torch.export import Dim\n\nclass DynamicShapeRound(torch.nn.Module):\n    \"\"\"\n    Calling round on dynamic shapes is not supported.\n    \"\"\"\n\n    def forward(self, x):\n        return x[: round(x.shape[0] / 2)]\n\nx = torch.randn(3, 2)\ndim0_x = Dim(\"dim0_x\")\nexample_args = (x,)\ntags = {\"torch.dynamic-shape\", \"python.builtin\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\ndynamic_shapes = {\"x\": {0: dim0_x}}\nmodel = DynamicShapeRound()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nUnsupported: Constraints violated (dim0_x)! For more information, run with TORCH_LOGS=\"+dynamic\".\n\n```\n\n\n## dynamic_shape_slicing#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeSlicing(torch.nn.Module):\n    \"\"\"\n    Slices with dynamic shape arguments should be captured into the graph\n    rather than being baked in.\n    \"\"\"\n\n    def forward(self, x):\n        return x[: x.shape[0] - 2, x.shape[1] - 1 :: 2]\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\"}\nmodel = DynamicShapeSlicing()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 slice_1: \"f32[1, 2]\" = torch.ops.aten.slice.Tensor(x, 0, 0, 1);  x = None\n            slice_2: \"f32[1, 1]\" = torch.ops.aten.slice.Tensor(slice_1, 1, 1, 9223372036854775807, 2);  slice_1 = None\n            return (slice_2,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    slice_2: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## dynamic_shape_view#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeView(torch.nn.Module):\n    \"\"\"\n    Dynamic shapes should be propagated to view arguments instead of being\n    baked into the exported graph.\n    \"\"\"\n\n    def forward(self, x):\n        new_x_shape = x.size()[:-1] + (2, 5)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1)\n\nexample_args = (torch.randn(10, 10),)\ntags = {\"torch.dynamic-shape\"}\nmodel = DynamicShapeView()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[10, 10]\"):\n                 view: \"f32[10, 2, 5]\" = torch.ops.aten.view.default(x, [10, 2, 5]);  x = None\n\n                 permute: \"f32[10, 5, 2]\" = torch.ops.aten.permute.default(view, [0, 2, 1]);  view = None\n            return (permute,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    permute: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_contains#\n\nNote\nTags:python.data-structure,torch.dynamic-shape,python.assert\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass ListContains(torch.nn.Module):\n    \"\"\"\n    List containment relation can be checked on a dynamic shape or constants.\n    \"\"\"\n\n    def forward(self, x):\n        assert x.size(-1) in [6, 2]\n        assert x.size(0) not in [4, 5, 6]\n        assert \"monkey\" not in [\"cow\", \"pig\"]\n        return x + x\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.dynamic-shape\", \"python.data-structure\", \"python.assert\"}\nmodel = ListContains()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## scalar_output#\n\nNote\nTags:torch.dynamic-shape\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom torch.export import Dim\n\nx = torch.randn(3, 2)\ndim1_x = Dim(\"dim1_x\")\n\nclass ScalarOutput(torch.nn.Module):\n    \"\"\"\n    Returning scalar values from the graph is supported, in addition to Tensor\n    outputs. Symbolic shapes are captured and rank is specialized.\n    \"\"\"\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(self, x):\n        return x.shape[1] + 1\n\nexample_args = (x,)\ntags = {\"torch.dynamic-shape\"}\ndynamic_shapes = {\"x\": {1: dim1_x}}\nmodel = ScalarOutput()\n\n\ntorch.export.export(model, example_args, dynamic_shapes=dynamic_shapes)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, s27]\"):\n             #\n            sym_size_int_1: \"Sym(s27)\" = torch.ops.aten.sym_size.int(x, 1);  x = None\n\n                 add: \"Sym(s27 + 1)\" = sym_size_int_1 + 1;  sym_size_int_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {s27: VR[0, int_oo]}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/torch.dynamic-shape.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "265f219349c3e12fb2ae97bccd85fbcd",
    "source": "pytorch_docs",
    "title": "Distributed Optimizers \u2014 PyTorch 2.9 documentation",
    "text": "\n## Distributed Optimizers#\n\nCreated On: Mar 01, 2021 | Last Updated On: Jun 16, 2025\nWarning\nDistributed optimizer is not currently supported when using CUDA tensors\ntorch.distributed.optimexposes DistributedOptimizer, which takes a list\nof remote parameters (RRef) and runs the\noptimizer locally on the workers where the parameters live.  The distributed\noptimizer can use any of the local optimizerBase classto\napply the gradients on each worker.\ntorch.distributed.optim\nRRef\nDistributedOptimizer takes remote references to parameters scattered\nacross workers and applies the given optimizer locally for each parameter.\nThis class usesget_gradients()in order\nto retrieve the gradients for specific parameters.\nget_gradients()\nConcurrent calls tostep(),\neither from the same or different clients, will\nbe serialized on each worker \u2013 as each worker\u2019s optimizer can only work\non one set of gradients at a time. However, there is no guarantee that\nthe full forward-backward-optimizer sequence will execute for one client\nat a time. This means that the gradients being applied may not correspond\nto the latest forward pass executed on a given worker. Also, there is no\nguaranteed ordering across workers.\nstep()\nDistributedOptimizercreates the local optimizer with TorchScript enabled\nby default, so that optimizer updates are not blocked by the Python Global\nInterpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed\nModel Parallel). This feature is currently enabled for most optimizers. You\ncan also followthe recipein PyTorch tutorials to enable TorchScript support\nfor your own custom optimizers.\noptimizer_class(optim.Optimizer) \u2013 the class of optimizer to\ninstantiate on each worker.\nparams_rref(list[RRef]) \u2013 list of RRefs to local or remote parameters\nto optimize.\nargs\u2013 arguments to pass to the optimizer constructor on each worker.\nkwargs\u2013 arguments to pass to the optimizer constructor on each worker.\n\n```python\n>>> import torch.distributed.autograd as dist_autograd\n>>> import torch.distributed.rpc as rpc\n>>> from torch import optim\n>>> from torch.distributed.optim import DistributedOptimizer\n>>>\n>>> with dist_autograd.context() as context_id:\n>>>   # Forward pass.\n>>>   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>>   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>>   loss = rref1.to_here() + rref2.to_here()\n>>>\n>>>   # Backward pass.\n>>>   dist_autograd.backward(context_id, [loss.sum()])\n>>>\n>>>   # Optimizer.\n>>>   dist_optim = DistributedOptimizer(\n>>>      optim.SGD,\n>>>      [rref1, rref2],\n>>>      lr=0.05,\n>>>   )\n>>>   dist_optim.step(context_id)\n\n```\n\nPerforms a single optimization step.\nThis will calltorch.optim.Optimizer.step()on each worker\ncontaining parameters to be optimized, and will block until all workers\nreturn. The providedcontext_idwill be used to retrieve the\ncorrespondingcontextthat\ncontains the gradients that should be applied to the parameters.\ntorch.optim.Optimizer.step()\ncontext_id\ncontext\ncontext_id\u2013 the autograd context id for which we should run the\noptimizer step.\nWraps an arbitrarytorch.optim.Optimizerand runspost-local SGD,\nThis optimizer runs local optimizer at every step.\nAfter the warm-up stage, it averages parameters periodically after the local optimizer is applied.\ntorch.optim.Optimizer\noptim(Optimizer) \u2013 The local optimizer.\naverager(ModelAverager) \u2013 A model averager instance to run post-localSGD algorithm.\nExample:\n\n```python\n>>> import torch\n>>> import torch.distributed as dist\n>>> import torch.distributed.algorithms.model_averaging.averagers as averagers\n>>> import torch.nn as nn\n>>> from torch.distributed.optim import PostLocalSGDOptimizer\n>>> from torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook import (\n>>>   PostLocalSGDState,\n>>>   post_localSGD_hook,\n>>> )\n>>>\n>>> model = nn.parallel.DistributedDataParallel(\n>>>    module, device_ids=[rank], output_device=rank\n>>> )\n>>>\n>>> # Register a post-localSGD communication hook.\n>>> state = PostLocalSGDState(process_group=None, subgroup=None, start_localSGD_iter=100)\n>>> model.register_comm_hook(state, post_localSGD_hook)\n>>>\n>>> # Create a post-localSGD optimizer that wraps a local optimizer.\n>>> # Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as\n>>> # ``start_localSGD_iter`` used in ``PostLocalSGDState``.\n>>> local_optim = torch.optim.SGD(params=model.parameters(), lr=0.01)\n>>> opt = PostLocalSGDOptimizer(\n>>>     optim=local_optim,\n>>>     averager=averagers.PeriodicModelAverager(period=4, warmup_steps=100)\n>>> )\n>>>\n>>> # In the first 100 steps, DDP runs global gradient averaging at every step.\n>>> # After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),\n>>> # and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.\n>>> for step in range(0, 200):\n>>>    opt.zero_grad()\n>>>    loss = loss_fn(output, labels)\n>>>    loss.backward()\n>>>    opt.step()\n\n```\n\nThis is the same astorch.optim.Optimizerload_state_dict(),\nbut also restores model averager\u2019s step value to the one\nsaved in the providedstate_dict.\ntorch.optim.Optimizer\nload_state_dict()\nstate_dict\nIf there is no\"step\"entry instate_dict,\nit will raise a warning and initialize the model averager\u2019s step to 0.\n\"step\"\nstate_dict\nThis is the same astorch.optim.Optimizerstate_dict(),\nbut adds an extra entry to record model averager\u2019s step to the checkpoint\nto ensure reload does not cause unnecessary warm up again.\ntorch.optim.Optimizer\nstate_dict()\nPerforms a single optimization step (parameter update).\nWrap an arbitraryoptim.Optimizerand shards its states across ranks in the group.\noptim.Optimizer\nThe sharing is done as described byZeRO.\nThe local optimizer instance in each rank is only\nresponsible for updating approximately1/world_sizeparameters and\nhence only needs to keep1/world_sizeoptimizer states. After\nparameters are updated locally, each rank will broadcast its parameters to\nall other peers to keep all model replicas in the same state.ZeroRedundancyOptimizercan be used in conjunction withtorch.nn.parallel.DistributedDataParallelto reduce per-rank peak\nmemory consumption.\n1/world_size\n1/world_size\nZeroRedundancyOptimizer\ntorch.nn.parallel.DistributedDataParallel\nZeroRedundancyOptimizeruses a sorted-greedy algorithm to pack a number\nof parameters at each rank. Each parameter belongs to a single rank and is\nnot divided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order.\nZeroRedundancyOptimizer\nparams(Iterable) \u2013 anIterableoftorch.Tensors\nordicts giving all parameters, which will be sharded\nacross ranks.\nIterable\nIterable\ntorch.Tensor\ndict\noptimizer_class(torch.nn.Optimizer) \u2013 the class of the local\noptimizer.\ntorch.nn.Optimizer\nprocess_group(ProcessGroup, optional) \u2013torch.distributedProcessGroup(default:dist.group.WORLDinitialized bytorch.distributed.init_process_group()).\nProcessGroup\ntorch.distributed\nProcessGroup\ndist.group.WORLD\ntorch.distributed.init_process_group()\nparameters_as_bucket_view(bool,optional) \u2013 ifTrue, parameters are\npacked into buckets to speed up communication, andparam.datafields point to bucket views at different offsets; ifFalse,\neach individual parameter is communicated separately, and eachparams.datastays intact (default:False).\nTrue\nparam.data\nFalse\nparams.data\nFalse\noverlap_with_ddp(bool,optional) \u2013 ifTrue,step()is\noverlapped withDistributedDataParallel\u2018s gradient\nsynchronization; this requires (1) either a functional optimizer\nfor theoptimizer_classargument or one with a functional\nequivalent and (2) registering a DDP communication hook\nconstructed from one of the functions inddp_zero_hook.py;\nparameters are packed into buckets matching those inDistributedDataParallel, meaning that theparameters_as_bucket_viewargument is ignored.\nIfFalse,step()runs disjointly after the backward pass\n(per normal).\n(default:False)\nTrue\nstep()\nDistributedDataParallel\noptimizer_class\nddp_zero_hook.py\nDistributedDataParallel\nparameters_as_bucket_view\nFalse\nstep()\nFalse\n**defaults\u2013 any trailing arguments, which are forwarded to the local\noptimizer.\nExample:\n\n```python\n>>> import torch.nn as nn\n>>> from torch.distributed.optim import ZeroRedundancyOptimizer\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n>>> ddp = DDP(model, device_ids=[rank])\n>>> opt = ZeroRedundancyOptimizer(\n>>>     ddp.parameters(),\n>>>     optimizer_class=torch.optim.Adam,\n>>>     lr=0.01\n>>> )\n>>> ddp(inputs).sum().backward()\n>>> opt.step()\n\n```\n\nWarning\nCurrently,ZeroRedundancyOptimizerrequires that all of the\npassed-in parameters are the same dense type.\nZeroRedundancyOptimizer\nWarning\nIf you passoverlap_with_ddp=True, be wary of the following: Given\nthe way that overlappingDistributedDataParallelwithZeroRedundancyOptimizeris currently implemented, the first\ntwo or three training iterations do not perform parameter updates in\nthe optimizer step, depending on ifstatic_graph=Falseorstatic_graph=True, respectively. This is because it needs\ninformation about the gradient bucketing strategy used byDistributedDataParallel, which is not finalized until the\nsecond forward pass ifstatic_graph=Falseor until the third\nforward pass ifstatic_graph=True. To adjust for this, one option\nis to prepend dummy inputs.\noverlap_with_ddp=True\nDistributedDataParallel\nZeroRedundancyOptimizer\nstatic_graph=False\nstatic_graph=True\nDistributedDataParallel\nstatic_graph=False\nstatic_graph=True\nWarning\nZeroRedundancyOptimizer is experimental and subject to change.\nAdd a parameter group to theOptimizer\u2018sparam_groups.\nOptimizer\nparam_groups\nThis can be useful when fine tuning a pre-trained network, as frozen\nlayers can be made trainable and added to theOptimizeras\ntraining progresses.\nOptimizer\nparam_group(dict) \u2013 specifies the parameters to be optimized and\ngroup-specific optimization options.\nWarning\nThis method handles updating the shards on all partitions\nbut needs to be called on all ranks. Calling this on a subset of\nthe ranks will cause the training to hang because communication\nprimitives are called depending on the managed parameters and\nexpect all the ranks to participate on the same set of parameters.\nConsolidate a list ofstate_dicts (one per rank) on the target rank.\nstate_dict\nto(int) \u2013 the rank that receives the optimizer states (default: 0).\nRuntimeError\u2013 ifoverlap_with_ddp=Trueand this method is\n    called before thisZeroRedundancyOptimizerinstance\n    has been fully initialized, which happens onceDistributedDataParallelgradient buckets have been\n    rebuilt.\noverlap_with_ddp=True\nZeroRedundancyOptimizer\nDistributedDataParallel\nWarning\nThis needs to be called on all ranks.\nReturn default device.\nReturn the ZeRO join hook.\nIt enables training on uneven inputs by\nshadowing the collective communications in the optimizer step.\nGradients must be properly set before this hook is called.\nkwargs(dict) \u2013 adictcontaining any keyword arguments\nto modify the behavior of the join hook at run time; allJoinableinstances sharing the same join context\nmanager are forwarded the same value forkwargs.\ndict\nJoinable\nkwargs\nThis hook does not support any keyword arguments; i.e.kwargsis\nunused.\nkwargs\nReturn process group.\nLoad the state pertaining to the given rank from the inputstate_dict, updating the local optimizer as needed.\nstate_dict\nstate_dict(dict) \u2013 optimizer state; should be an object returned\nfrom a call tostate_dict().\nstate_dict()\nRuntimeError\u2013 ifoverlap_with_ddp=Trueand this method is\n    called before thisZeroRedundancyOptimizerinstance\n    has been fully initialized, which happens onceDistributedDataParallelgradient buckets have been\n    rebuilt.\noverlap_with_ddp=True\nZeroRedundancyOptimizer\nDistributedDataParallel\nReturn the last global optimizer state known to this rank.\nRuntimeError\u2013 ifoverlap_with_ddp=Trueand this method is\n    called before thisZeroRedundancyOptimizerinstance\n    has been fully initialized, which happens onceDistributedDataParallelgradient buckets have been\n    rebuilt; or if this method is called without a preceding call\n    toconsolidate_state_dict().\noverlap_with_ddp=True\nZeroRedundancyOptimizer\nDistributedDataParallel\nconsolidate_state_dict()\ndict[str,Any]\nPerform a single optimizer step and syncs parameters across all ranks.\nclosure(Callable) \u2013 a closure that re-evaluates the model and\nreturns the loss; optional for most optimizers.\nOptional loss depending on the underlying local optimizer.\nOptional[float]\nNote\nAny extra parameters are passed to the base optimizer as-is.",
    "url": "https://pytorch.org/docs/stable/distributed.optim.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8b8336c8b74050449ae45d62c8b51978",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/customization.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a65420255090caa73364fa1622b935f9",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/amp.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "91ac5dd9187ce04274c9fd9a724dccf3",
    "source": "pytorch_docs",
    "title": "torch.utils.checkpoint \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.checkpoint#\n\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\nNote\nCheckpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward propagation.  This can cause persistent\nstates like the RNG state to be more advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supplypreserve_rng_state=Falsetocheckpointorcheckpoint_sequentialto omit stashing and\nrestoring the RNG state during each checkpoint.\npreserve_rng_state=False\ncheckpoint\ncheckpoint_sequential\nThe stashing logic saves and restores the RNG state for CPU and another\ndevice type (infer the device type from Tensor arguments excluding CPU\ntensors by_infer_device_type) to therun_fn. If there are multiple\ndevice, device state will only be saved for devices of a single device type,\nand the remaining devices will be ignored. Consequently, if any checkpointed\nfunctions involve randomness, this may result in incorrect gradients. (Note\nthat if CUDA devices are among the devices detected, it will be prioritized;\notherwise, the first device encountered will be selected.) If there are no\nCPU-tensors, the default device type state (default value iscuda, and it\ncould be set to other device byDefaultDeviceType) will be saved and restored.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within therun_fnitself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) withinrun_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed.\n_infer_device_type\nrun_fn\ncuda\nDefaultDeviceType\nrun_fn\nrun_fn\nCheckpoint a model or part of the model.\nActivation checkpointing is a technique that trades compute for memory.\nInstead of keeping tensors needed for backward alive until they are used in\ngradient computation during backward, forward computation in checkpointed\nregions omits saving tensors for backward and recomputes them during the\nbackward pass. Activation checkpointing can be applied to any part of a\nmodel.\nThere are currently two checkpointing implementations available, determined\nby theuse_reentrantparameter. It is recommended that you useuse_reentrant=False. Please refer the note below for a discussion of\ntheir differences.\nuse_reentrant\nuse_reentrant=False\nWarning\nIf thefunctioninvocation during the backward pass differs\nfrom the forward pass, e.g., due to a global variable, the checkpointed\nversion may not be equivalent, potentially causing an\nerror being raised or leading to silently incorrect gradients.\nfunction\nWarning\nTheuse_reentrantparameter should be passed explicitly. In version\n2.9 we will raise an exception ifuse_reentrantis not passed.\nIf you are using theuse_reentrant=Truevariant, please refer to the\nnote below for important considerations and potential limitations.\nuse_reentrant\nuse_reentrant\nuse_reentrant=True\nNote\nThe reentrant variant of checkpoint (use_reentrant=True) and\nthe non-reentrant variant of checkpoint (use_reentrant=False)\ndiffer in the following ways:\nuse_reentrant=True\nuse_reentrant=False\nNon-reentrant checkpoint stops recomputation as soon as all needed\nintermediate activations have been recomputed. This feature is enabled\nby default, but can be disabled withset_checkpoint_early_stop().\nReentrant checkpoint always recomputesfunctionin its\nentirety during the backward pass.\nset_checkpoint_early_stop()\nfunction\nThe reentrant variant does not record the autograd graph during the\nforward pass, as it runs with the forward pass undertorch.no_grad(). The non-reentrant version does record the\nautograd graph, allowing one to perform backward on the graph within\ncheckpointed regions.\ntorch.no_grad()\nThe reentrant checkpoint only supports thetorch.autograd.backward()API for the backward pass without itsinputsargument, while the non-reentrant version supports all ways\nof performing the backward pass.\ntorch.autograd.backward()\nAt least one input and output must haverequires_grad=Truefor the\nreentrant variant. If this condition is unmet, the checkpointed part\nof the model will not have gradients. The non-reentrant version does\nnot have this requirement.\nrequires_grad=True\nThe reentrant version does not consider tensors in nested structures\n(e.g., custom objects, lists, dicts, etc) as participating in\nautograd, while the non-reentrant version does.\nThe reentrant checkpoint does not support checkpointed regions with\ndetached tensors from the computational graph, whereas the\nnon-reentrant version does. For the reentrant variant, if the\ncheckpointed segment contains tensors detached usingdetach()or\nwithtorch.no_grad(), the backward pass will raise an error.\nThis is becausecheckpointmakes all the outputs require gradients\nand this causes issues when a tensor is defined to have no gradient in\nthe model. To avoid this, detach the tensors outside of thecheckpointfunction.\ndetach()\ntorch.no_grad()\ncheckpoint\ncheckpoint\nfunction\u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes(activation,hidden),functionshould correctly use the\nfirst input asactivationand the second input ashidden\n(activation,hidden)\nfunction\nactivation\nhidden\nargs\u2013 tuple containing inputs to thefunction\nfunction\npreserve_rng_state(bool,optional) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Note that under torch.compile,\nthis flag doesn\u2019t take effect and we always preserve RNG state.\nDefault:True\nTrue\nuse_reentrant(bool) \u2013 specify whether to use the activation checkpoint variant that\nrequires reentrant autograd. This parameter should be passed\nexplicitly. In version 2.9 we will raise an exception ifuse_reentrantis not passed. Ifuse_reentrant=False,checkpointwill use an implementation that does not require\nreentrant autograd. This allowscheckpointto support additional\nfunctionality, such as working as expected withtorch.autograd.gradand support for keyword arguments input into\nthe checkpointed function.\nuse_reentrant\nuse_reentrant=False\ncheckpoint\ncheckpoint\ntorch.autograd.grad\ncontext_fn(Callable,optional) \u2013 A callable returning a tuple of two\ncontext managers. The function and its recomputation will be run\nunder the first and second context managers respectively.\nThis argument is only supported ifuse_reentrant=False.\nuse_reentrant=False\ndeterminism_check(str,optional) \u2013 A string specifying the determinism\ncheck to perform. By default it is set to\"default\"which\ncompares the shapes, dtypes, and devices of the recomputed tensors\nagainst those the saved tensors. To turn off this check, specify\"none\". Currently these are the only two supported values.\nPlease open an issue if you would like to see more determinism\nchecks. This argument is only supported ifuse_reentrant=False,\nifuse_reentrant=True, the determinism check is always disabled.\n\"default\"\n\"none\"\nuse_reentrant=False\nuse_reentrant=True\ndebug(bool,optional) \u2013 IfTrue, error messages will also include\na trace of the operators ran during the original forward computation\nas well as the recomputation. This argument is only supported ifuse_reentrant=False.\nTrue\nuse_reentrant=False\nearly_stop(bool,optional) \u2013 IfTrue, non-reentrant checkpoint stops\nrecomputation as soon as it has computed all needed Tensors. This\nargument is ignored ifuse_reentrant=True. Can be overridden\nglobally usingset_checkpoint_early_stop()context manager.\nDefault:True.\nTrue\nuse_reentrant=True\nset_checkpoint_early_stop()\nTrue\nOutput of runningfunctionon*args\nfunction\n*args\nCheckpoint a sequential model to save memory.\nSequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will not store\nthe intermediate activations. The inputs of each checkpointed segment will\nbe saved for re-running the segment in the backward pass.\nWarning\nTheuse_reentrantparameter should be passed explicitly. In version\n2.9 we will raise an exception ifuse_reentrantis not passed.\nIf you are using theuse_reentrant=True`variant,pleasesee:func:`~torch.utils.checkpoint.checkpoint`fortheimportantconsiderationsandlimitationsofthisvariant.Itisrecommendedthatyouuse``use_reentrant=False.\nuse_reentrant\nuse_reentrant\nuse_reentrant=True`variant,pleasesee:func:`~torch.utils.checkpoint.checkpoint`fortheimportantconsiderationsandlimitationsofthisvariant.Itisrecommendedthatyouuse``use_reentrant=False\nfunctions\u2013 Atorch.nn.Sequentialor the list of modules or\nfunctions (comprising the model) to run sequentially.\ntorch.nn.Sequential\nsegments\u2013 Number of chunks to create in the model\ninput\u2013 A Tensor that is input tofunctions\nfunctions\npreserve_rng_state(bool,optional) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint.\nDefault:True\nTrue\nuse_reentrant(bool) \u2013 specify whether to use the activation checkpoint variant that\nrequires reentrant autograd. This parameter should be passed\nexplicitly. In version 2.5 we will raise an exception ifuse_reentrantis not passed. Ifuse_reentrant=False,checkpointwill use an implementation that does not require\nreentrant autograd. This allowscheckpointto support additional\nfunctionality, such as working as expected withtorch.autograd.gradand support for keyword arguments input into\nthe checkpointed function.\nuse_reentrant\nuse_reentrant=False\ncheckpoint\ncheckpoint\ntorch.autograd.grad\nOutput of runningfunctionssequentially on*inputs\nfunctions\n*inputs\nExample\n\n```python\n>>> model = nn.Sequential(...)\n>>> input_var = checkpoint_sequential(model, chunks, input_var)\n\n```\n\nContext manager that sets whether checkpoint should print additional debug\ninformation when running. See thedebugflag forcheckpoint()for more information. Note that\nwhen set, this context manager overrides the value ofdebugpassed to\ncheckpoint. To defer to the local setting, passNoneto this context.\ndebug\ncheckpoint()\ndebug\nNone\nenabled(bool) \u2013 Whether checkpoint should print debug information.\nDefault is \u2018None\u2019.\nEnum for specifying the policy for checkpointing during backpropagation.\nThe following policies are supported:\n{MUST,PREFER}_SAVE: The operation\u2019s output will be saved during the forward\npass and will not be recomputed during the backward pass\n{MUST,PREFER}_SAVE\n{MUST,PREFER}_RECOMPUTE: The operation\u2019s output will not be saved during the\nforward pass and will be recomputed during the backward pass\n{MUST,PREFER}_RECOMPUTE\nUseMUST_*overPREFER_*to indicate that the policy should not be overridden\nby other subsystems liketorch.compile.\nMUST_*\nPREFER_*\nNote\nA policy function that always returnsPREFER_RECOMPUTEis\nequivalent to vanilla checkpointing.\nPREFER_RECOMPUTE\nA policy function that returnsPREFER_SAVEevery op is\nNOT equivalent to not using checkpointing. Using such a policy would\nsave additional tensors not limited to ones that are actually needed for\ngradient computation.\nPREFER_SAVE\nContext passed to policy function during selective checkpointing.\nThis class is used to pass relevant metadata to the policy function during\nselective checkpointing. The metadata includes whether the current invocation\nof the policy function is during recomputation or not.\nExample\n\n```python\n>>>\n>>> def policy_fn(ctx, op, *args, **kwargs):\n>>>    print(ctx.is_recompute)\n>>>\n>>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n>>>\n>>> out = torch.utils.checkpoint.checkpoint(\n>>>     fn, x, y,\n>>>     use_reentrant=False,\n>>>     context_fn=context_fn,\n>>> )\n\n```\n\nHelper to avoid recomputing certain ops during activation checkpointing.\nUse this withtorch.utils.checkpoint.checkpointto control which\noperations are recomputed during the backward pass.\npolicy_fn_or_list(CallableorList) \u2013If a policy function is provided, it should accept aSelectiveCheckpointContext, theOpOverload, args and\nkwargs to the op, and return aCheckpointPolicyenum value\nindicating whether the execution of the op should be recomputed or not.If a list of operations is provided, it is equivalent to a policy\nreturningCheckpointPolicy.MUST_SAVEfor the specified\noperations andCheckpointPolicy.PREFER_RECOMPUTEfor all other\noperations.\nIf a policy function is provided, it should accept aSelectiveCheckpointContext, theOpOverload, args and\nkwargs to the op, and return aCheckpointPolicyenum value\nindicating whether the execution of the op should be recomputed or not.\nSelectiveCheckpointContext\nOpOverload\nCheckpointPolicy\nIf a list of operations is provided, it is equivalent to a policy\nreturningCheckpointPolicy.MUST_SAVEfor the specified\noperations andCheckpointPolicy.PREFER_RECOMPUTEfor all other\noperations.\nallow_cache_entry_mutation(bool,optional) \u2013 By default, an error is\nraised if any tensors cached by selective activation checkpoint are\nmutated in order to ensure correctness. If set toTrue, this check\nis disabled.\nA tuple of two context managers.\nExample\n\n```python\n>>> import functools\n>>>\n>>> x = torch.rand(10, 10, requires_grad=True)\n>>> y = torch.rand(10, 10, requires_grad=True)\n>>>\n>>> ops_to_save = [\n>>>    torch.ops.aten.mm.default,\n>>> ]\n>>>\n>>> def policy_fn(ctx, op, *args, **kwargs):\n>>>    if op in ops_to_save:\n>>>        return CheckpointPolicy.MUST_SAVE\n>>>    else:\n>>>        return CheckpointPolicy.PREFER_RECOMPUTE\n>>>\n>>> context_fn = functools.partial(create_selective_checkpoint_contexts, policy_fn)\n>>>\n>>> # or equivalently\n>>> context_fn = functools.partial(create_selective_checkpoint_contexts, ops_to_save)\n>>>\n>>> def fn(x, y):\n>>>     return torch.sigmoid(torch.matmul(torch.matmul(x, y), y)) * y\n>>>\n>>> out = torch.utils.checkpoint.checkpoint(\n>>>     fn, x, y,\n>>>     use_reentrant=False,\n>>>     context_fn=context_fn,\n>>> )\n\n```\n",
    "url": "https://pytorch.org/docs/stable/checkpoint.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "939bdbcb9d4be5320cfb410b6c838989",
    "source": "pytorch_docs",
    "title": "Metrics \u2014 PyTorch 2.9 documentation",
    "text": "\n## Metrics#\n\nCreated On: May 04, 2021 | Last Updated On: May 04, 2021\nMetrics API.\nOverview:\nThe metrics API in torchelastic is used to publish telemetry metrics.\nIt is designed to be used by torchelastic\u2019s internal modules to\npublish metrics for the end user with the goal of increasing visibility\nand helping with debugging. However you may use the same API in your\njobs to publish metrics to the same metricssink.\nsink\nAmetriccan be thought of as timeseries data\nand is uniquely identified by the string-valued tuple(metric_group,metric_name).\nmetric\n(metric_group,metric_name)\ntorchelastic makes no assumptions about what ametric_groupis\nand what relationship it has withmetric_name. It is totally up\nto the user to use these two fields to uniquely identify a metric.\nmetric_group\nmetric_name\nNote\nThe metric grouptorchelasticis reserved by torchelastic for\nplatform level metrics that it produces.\nFor instance torchelastic may output the latency (in milliseconds)\nof a re-rendezvous operation from the agent as(torchelastic,agent.rendezvous.duration.ms)\ntorchelastic\n(torchelastic,agent.rendezvous.duration.ms)\nA sensible way to use metric groups is to map them to a stage or module\nin your job. You may also encode certain high level properties\nthe job such as the region or stage (dev vs prod).\nPublish Metrics:\nUsing torchelastic\u2019s metrics API is similar to using python\u2019s logging\nframework. You first have to configure a metrics handler before\ntrying to add metric data.\nThe example below measures the latency for thecalculate()function.\ncalculate()\n\n```python\nimport time\nimport torch.distributed.elastic.metrics as metrics\n\n# makes all metrics other than the one from \"my_module\" to go /dev/null\nmetrics.configure(metrics.NullMetricsHandler())\nmetrics.configure(metrics.ConsoleMetricsHandler(), \"my_module\")\n\n\ndef my_method():\n    start = time.time()\n    calculate()\n    end = time.time()\n    metrics.put_metric(\"calculate_latency\", int(end - start), \"my_module\")\n\n```\n\nYou may also use the torch.distributed.elastic.metrics.prof` decorator\nto conveniently and succinctly profile functions\n\n```python\n# -- in module examples.foobar --\n\nimport torch.distributed.elastic.metrics as metrics\n\nmetrics.configure(metrics.ConsoleMetricsHandler(), \"foobar\")\nmetrics.configure(metrics.ConsoleMetricsHandler(), \"Bar\")\n\n\n@metrics.prof\ndef foo():\n    pass\n\n\nclass Bar:\n    @metrics.prof\n    def baz():\n        pass\n\n```\n\n@metrics.profwill publish the following metrics\n@metrics.prof\n\n```python\n<leaf_module or classname>.success - 1 if the function finished successfully\n<leaf_module or classname>.failure - 1 if the function threw an exception\n<leaf_module or classname>.duration.ms - function duration in milliseconds\n\n```\n\nConfiguring Metrics Handler:\ntorch.distributed.elastic.metrics.MetricHandleris responsible for emitting\nthe added metric values to a particular destination. Metric groups can be\nconfigured with different metric handlers.\nBy default torchelastic emits all metrics to/dev/null.\nBy adding the following configuration metrics,torchelasticandmy_appmetric groups will be printed out to\nconsole.\n/dev/null\ntorchelastic\nmy_app\n\n```python\nimport torch.distributed.elastic.metrics as metrics\n\nmetrics.configure(metrics.ConsoleMetricHandler(), group=\"torchelastic\")\nmetrics.configure(metrics.ConsoleMetricHandler(), group=\"my_app\")\n\n```\n\nWriting a Custom Metric Handler:\nIf you want your metrics to be emitted to a custom location, implement\nthetorch.distributed.elastic.metrics.MetricHandlerinterface\nand configure your job to use your custom metric handler.\nBelow is a toy example that prints the metrics tostdout\nstdout\n\n```python\nimport torch.distributed.elastic.metrics as metrics\n\n\nclass StdoutMetricHandler(metrics.MetricHandler):\n    def emit(self, metric_data):\n        ts = metric_data.timestamp\n        group = metric_data.group_name\n        name = metric_data.name\n        value = metric_data.value\n        print(f\"[{ts}][{group}]: {name}={value}\")\n\n\nmetrics.configure(StdoutMetricHandler(), group=\"my_app\")\n\n```\n\nNow all metrics in the groupmy_appwill be printed to stdout as:\nmy_app\n\n```python\n[1574213883.4182858][my_app]: my_metric=<value>\n[1574213940.5237644][my_app]: my_metric=<value>\n\n```\n\n\n## Metric Handlers#\n\nBelow are the metric handlers that come included with torchelastic.\n\n## Methods#\n\n@profile decorator publishes duration.ms, count, success, failure metrics for the function that it decorates.\nThe metric name defaults to the qualified name (class_name.def_name) of the function.\nIf the function does not belong to a class, it uses the leaf module name instead.\nclass_name.def_name\nUsage\n\n```python\n@metrics.prof\ndef x():\n    pass\n\n\n@metrics.prof(group=\"agent\")\ndef y():\n    pass\n\n```\n\nPublish a metric data point.\nUsage\n\n```python\nput_metric(\"metric_name\", 1)\nput_metric(\"metric_name\", 1, \"metric_group_name\")\n\n```\n",
    "url": "https://pytorch.org/docs/stable/elastic/metrics.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "874df2e01d7e68317f047f0efac0b9fc",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/examples.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "959dc4e403e6bb39656d3bdaab3c679a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributed.tensor.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "279a09b3d2ec33236a0f3d1cbdd16300",
    "source": "pytorch_docs",
    "title": "Getting Started \u2014 PyTorch 2.9 documentation",
    "text": "\n## Getting Started#\n\nCreated On: Jun 16, 2025 | Last Updated On: Jun 16, 2025\nBefore you read this section, make sure to read thetorch.compiler\nlet\u2019s start by looking at a simpletorch.compileexample that demonstrates\nhow to usetorch.compilefor inference. This example demonstrates thetorch.cos()andtorch.sin()features which are examples of pointwise\noperators as they operate element by element on a vector. This example might\nnot show significant performance gains but should help you form an intuitive\nunderstanding of how you can usetorch.compilein your own programs.\ntorch.compile\ntorch.compile\ntorch.cos()\ntorch.sin()\ntorch.compile\nNote\nTo run this script, you need to have at least one GPU on your machine.\nIf you do not have a GPU, you can remove the.to(device=\"cuda:0\")code\nin the snippet below and it will run on CPU. You can also set device toxpu:0to run on Intel\u00ae GPUs.\n.to(device=\"cuda:0\")\nxpu:0\n\n```python\nimport torch\ndef fn(x):\n   a = torch.cos(x)\n   b = torch.sin(a)\n   return b\nnew_fn = torch.compile(fn, backend=\"inductor\")\ninput_tensor = torch.randn(10000).to(device=\"cuda:0\")\na = new_fn(input_tensor)\n\n```\n\nA more famous pointwise operator you might want to use would\nbe something liketorch.relu(). Pointwise ops in eager mode are\nsuboptimal because each one would need to read a tensor from the\nmemory, make some changes, and then write back those changes. The single\nmost important optimization that inductor performs is fusion. In the\nexample above we can turn 2 reads (x,a) and\n2 writes (a,b) into 1 read (x) and 1 write (b), which\nis crucial especially for newer GPUs where the bottleneck is memory\nbandwidth (how quickly you can send data to a GPU) rather than compute\n(how quickly your GPU can crunch floating point operations).\ntorch.relu()\nx\na\na\nb\nx\nb\nAnother major optimization that inductor provides is automatic\nsupport for CUDA graphs.\nCUDA graphs help eliminate the overhead from launching individual\nkernels from a Python program which is especially relevant for newer GPUs.\nTorchDynamo supports many different backends, but TorchInductor specifically works\nby generatingTritonkernels. Let\u2019s save\nour example above into a file calledexample.py. We can inspect the code\ngenerated Triton kernels by runningTORCH_COMPILE_DEBUG=1pythonexample.py.\nAs the script executes, you should seeDEBUGmessages printed to the\nterminal. Closer to the end of the log, you should see a path to a folder\nthat containstorchinductor_<your_username>. In that folder, you can find\ntheoutput_code.pyfile that contains the generated kernel code similar to\nthe following:\nexample.py\nTORCH_COMPILE_DEBUG=1pythonexample.py\nDEBUG\ntorchinductor_<your_username>\noutput_code.py\n\n```python\n@pointwise(size_hints=[16384], filename=__file__, triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': 0, 'constants': {}, 'mutated_arg_names': [], 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n@triton.jit\ndef triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n   xnumel = 10000\n   xoffset = tl.program_id(0) * XBLOCK\n   xindex = xoffset + tl.arange(0, XBLOCK)[:]\n   xmask = xindex < xnumel\n   x0 = xindex\n   tmp0 = tl.load(in_ptr0 + (x0), xmask, other=0.0)\n   tmp1 = tl.cos(tmp0)\n   tmp2 = tl.sin(tmp1)\n   tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp2, xmask)\n\n```\n\nNote\nThe above code snippet is an example. Depending on your hardware,\nyou might see different code generated.\nAnd you can verify that fusing thecosandsindid actually occur\nbecause thecosandsinoperations occur within a single Triton kernel\nand the temporary variables are held in registers with very fast access.\ncos\nsin\ncos\nsin\nRead more on Triton\u2019s performancehere. Because the code is written\nin Python, it\u2019s fairly easy to understand even if you have not written all that\nmany CUDA kernels.\nNext, let\u2019s try a real model like resnet50 from the PyTorch\nhub.\n\n```python\nimport torch\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(1,3,64,64))\n\n```\n\nAnd that is not the only available backend, you can run in a REPLtorch.compiler.list_backends()to see all the available backends. Try out thecudagraphsnext as inspiration.\ntorch.compiler.list_backends()\ncudagraphs\n\n## Using a pretrained model#\n\nPyTorch users frequently leverage pretrained models fromtransformersorTIMMand one of\nthe design goals is TorchDynamo and TorchInductor is to work out of the box with\nany model that people would like to author.\nLet\u2019s download a pretrained model directly from the HuggingFace hub and optimize\nit:\n\n```python\nimport torch\nfrom transformers import BertTokenizer, BertModel\n# Copy pasted from here https://huggingface.co/bert-base-uncased\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\").to(device=\"cuda:0\")\nmodel = torch.compile(model, backend=\"inductor\") # This is the only line of code that we changed\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt').to(device=\"cuda:0\")\noutput = model(**encoded_input)\n\n```\n\nIf you remove theto(device=\"cuda:0\")from the model andencoded_input, then Triton will generate C++ kernels that will be\noptimized for running on your CPU. You can inspect both Triton or C++\nkernels for BERT. They are more complex than the trigonometry\nexample we tried above but you can similarly skim through it and see if you\nunderstand how PyTorch works.\nto(device=\"cuda:0\")\nencoded_input\nSimilarly, let\u2019s try out a TIMM example:\n\n```python\nimport timm\nimport torch\nmodel = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(64,3,7,7))\n\n```\n\n\n## Next Steps#\n\nIn this section, we have reviewed a few inference examples and developed a\nbasic understanding of how torch.compile works. Here is what you check out next:\ntorch.compile tutorial on training\ntorch.compiler API reference\nTorchDynamo APIs for fine-grained tracing",
    "url": "https://pytorch.org/docs/stable/torch.compiler_get_started.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5afa6a98afaa7970e4a807192501e11c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.graph_breaks_index.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "08d438643e31b96bc57886d647b17f0f",
    "source": "pytorch_docs",
    "title": "Torch Distributed Elastic \u2014 PyTorch 2.9 documentation",
    "text": "\n## Torch Distributed Elastic#\n\nCreated On: Jun 16, 2025 | Last Updated On: Jul 25, 2025\nMakes distributed PyTorch fault-tolerant and elastic.\n\n## Get Started#\n\nUsage\n\n## Documentation#\n\nAPI\nAdvanced\nPlugins",
    "url": "https://pytorch.org/docs/stable/distributed.elastic.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6849cb16ce5bca4fbfaf4541cd4c57ff",
    "source": "pytorch_docs",
    "title": "Reference API \u2014 PyTorch 2.9 documentation",
    "text": "\n## Reference API#\n\nCreated On: Apr 16, 2025 | Last Updated On: Nov 10, 2025\nPython API",
    "url": "https://pytorch.org/docs/stable/pytorch-api.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "253bca2ddc2bf8f2345e65236b36e612",
    "source": "pytorch_docs",
    "title": "torch.masked \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.masked#\n\nCreated On: Aug 15, 2022 | Last Updated On: Jun 17, 2025\n\n## Introduction#\n\n\n## Motivation#\n\nWarning\nThe PyTorch API of masked tensors is in the prototype stage and may or may not change in the future.\nMaskedTensor serves as an extension totorch.Tensorthat provides the user with the ability to:\ntorch.Tensor\nuse any masked semantics (e.g. variable length tensors, nan* operators, etc.)\ndifferentiate between 0 and NaN gradients\nvarious sparse applications (see tutorial below)\n\u201cSpecified\u201d and \u201cunspecified\u201d have a long history in PyTorch without formal semantics and certainly without\nconsistency; indeed, MaskedTensor was born out of a build up of issues that the vanillatorch.Tensorclass could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for\nsaid \u201cspecified\u201d and \u201cunspecified\u201d values in PyTorch where they are a first class citizen instead of an afterthought.\nIn turn, this should further unlocksparsity\u2019spotential,\nenable safer and more consistent operators, and provide a smoother and more intuitive experience\nfor users and developers alike.\ntorch.Tensor\n\n## What is a MaskedTensor?#\n\nA MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us\nwhich entries from the input should be included or ignored.\nBy way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray)\nand take the max:\nOn top is the vanilla tensor example while the bottom is MaskedTensor where all the 0\u2019s are masked out.\nThis clearly yields a different result depending on whether we have the mask, but this flexible structure\nallows the user to systematically ignore any elements they\u2019d like during computation.\nThere are already a number of existing tutorials that we\u2019ve written to help users onboard, such as:\nOverview \u2013 the place to start for new users, discusses how to use MaskedTensors and why they\u2019re useful\nSparsity \u2013 MaskedTensor supports sparse COO and CSR data and mask Tensors\nAdagrad sparse semantics \u2013 a practical example of how MaskedTensor can simplify sparse semantics and implementations\nAdvanced semantics \u2013 discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations), differences with NumPy\u2019s MaskedArray, and reduction semantics\n\n## Supported Operators#\n\n\n## Unary Operators#\n\nUnary operators are operators that only contain only a single input.\nApplying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index,\nwe apply the operator, otherwise we\u2019ll continue to mask out the data.\nThe available unary operators are:\nabs\n\nabs\nComputes the absolute value of each element ininput.\ninput\nabsolute\n\nabsolute\nAlias fortorch.abs()\ntorch.abs()\nacos\n\nacos\nComputes the inverse cosine of each element ininput.\ninput\narccos\n\narccos\nAlias fortorch.acos().\ntorch.acos()\nacosh\n\nacosh\nReturns a new tensor with the inverse hyperbolic cosine of the elements ofinput.\ninput\narccosh\n\narccosh\nAlias fortorch.acosh().\ntorch.acosh()\nangle\n\nangle\nComputes the element-wise angle (in radians) of the giveninputtensor.\ninput\nasin\n\nasin\nReturns a new tensor with the arcsine of the elements ofinput.\ninput\narcsin\n\narcsin\nAlias fortorch.asin().\ntorch.asin()\nasinh\n\nasinh\nReturns a new tensor with the inverse hyperbolic sine of the elements ofinput.\ninput\narcsinh\n\narcsinh\nAlias fortorch.asinh().\ntorch.asinh()\natan\n\natan\nReturns a new tensor with the arctangent of the elements ofinput.\ninput\narctan\n\narctan\nAlias fortorch.atan().\ntorch.atan()\natanh\n\natanh\nReturns a new tensor with the inverse hyperbolic tangent of the elements ofinput.\ninput\narctanh\n\narctanh\nAlias fortorch.atanh().\ntorch.atanh()\nbitwise_not\n\nbitwise_not\nComputes the bitwise NOT of the given input tensor.\nceil\n\nceil\nReturns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.\ninput\nclamp\n\nclamp\nClamps all elements ininputinto the range[min,max].\ninput\nmin\nmax\nclip\n\nclip\nAlias fortorch.clamp().\ntorch.clamp()\nconj_physical\n\nconj_physical\nComputes the element-wise conjugate of the giveninputtensor.\ninput\ncos\n\ncos\nReturns a new tensor with the cosine  of the elements ofinput.\ninput\ncosh\n\ncosh\nReturns a new tensor with the hyperbolic cosine  of the elements ofinput.\ninput\ndeg2rad\n\ndeg2rad\nReturns a new tensor with each of the elements ofinputconverted from angles in degrees to radians.\ninput\ndigamma\n\ndigamma\nAlias fortorch.special.digamma().\ntorch.special.digamma()\nerf\n\nerf\nAlias fortorch.special.erf().\ntorch.special.erf()\nerfc\n\nerfc\nAlias fortorch.special.erfc().\ntorch.special.erfc()\nerfinv\n\nerfinv\nAlias fortorch.special.erfinv().\ntorch.special.erfinv()\nexp\n\nexp\nReturns a new tensor with the exponential of the elements of the input tensorinput.\ninput\nexp2\n\nexp2\nAlias fortorch.special.exp2().\ntorch.special.exp2()\nexpm1\n\nexpm1\nAlias fortorch.special.expm1().\ntorch.special.expm1()\nfix\n\nfix\nAlias fortorch.trunc()\ntorch.trunc()\nfloor\n\nfloor\nReturns a new tensor with the floor of the elements ofinput, the largest integer less than or equal to each element.\ninput\nfrac\n\nfrac\nComputes the fractional portion of each element ininput.\ninput\nlgamma\n\nlgamma\nComputes the natural logarithm of the absolute value of the gamma function oninput.\ninput\nlog\n\nlog\nReturns a new tensor with the natural logarithm of the elements ofinput.\ninput\nlog10\n\nlog10\nReturns a new tensor with the logarithm to the base 10 of the elements ofinput.\ninput\nlog1p\n\nlog1p\nReturns a new tensor with the natural logarithm of (1 +input).\ninput\nlog2\n\nlog2\nReturns a new tensor with the logarithm to the base 2 of the elements ofinput.\ninput\nlogit\n\nlogit\nAlias fortorch.special.logit().\ntorch.special.logit()\ni0\n\ni0\nAlias fortorch.special.i0().\ntorch.special.i0()\nisnan\n\nisnan\nReturns a new tensor with boolean elements representing if each element ofinputis NaN or not.\ninput\nnan_to_num\n\nnan_to_num\nReplacesNaN, positive infinity, and negative infinity values ininputwith the values specified bynan,posinf, andneginf, respectively.\nNaN\ninput\nnan\nposinf\nneginf\nneg\n\nneg\nReturns a new tensor with the negative of the elements ofinput.\ninput\nnegative\n\nnegative\nAlias fortorch.neg()\ntorch.neg()\npositive\n\npositive\nReturnsinput.\ninput\npow\n\npow\nTakes the power of each element ininputwithexponentand returns a tensor with the result.\ninput\nexponent\nrad2deg\n\nrad2deg\nReturns a new tensor with each of the elements ofinputconverted from angles in radians to degrees.\ninput\nreciprocal\n\nreciprocal\nReturns a new tensor with the reciprocal of the elements ofinput\ninput\nround\n\nround\nRounds elements ofinputto the nearest integer.\ninput\nrsqrt\n\nrsqrt\nReturns a new tensor with the reciprocal of the square-root of each of the elements ofinput.\ninput\nsigmoid\n\nsigmoid\nAlias fortorch.special.expit().\ntorch.special.expit()\nsign\n\nsign\nReturns a new tensor with the signs of the elements ofinput.\ninput\nsgn\n\nsgn\nThis function is an extension of torch.sign() to complex tensors.\nsignbit\n\nsignbit\nTests if each element ofinputhas its sign bit set or not.\ninput\nsin\n\nsin\nReturns a new tensor with the sine of the elements ofinput.\ninput\nsinc\n\nsinc\nAlias fortorch.special.sinc().\ntorch.special.sinc()\nsinh\n\nsinh\nReturns a new tensor with the hyperbolic sine of the elements ofinput.\ninput\nsqrt\n\nsqrt\nReturns a new tensor with the square-root of the elements ofinput.\ninput\nsquare\n\nsquare\nReturns a new tensor with the square of the elements ofinput.\ninput\ntan\n\ntan\nReturns a new tensor with the tangent of the elements ofinput.\ninput\ntanh\n\ntanh\nReturns a new tensor with the hyperbolic tangent of the elements ofinput.\ninput\ntrunc\n\ntrunc\nReturns a new tensor with the truncated integer values of the elements ofinput.\ninput\nThe available inplace unary operators are all of the aboveexcept:\nangle\n\nangle\nComputes the element-wise angle (in radians) of the giveninputtensor.\ninput\npositive\n\npositive\nReturnsinput.\ninput\nsignbit\n\nsignbit\nTests if each element ofinputhas its sign bit set or not.\ninput\nisnan\n\nisnan\nReturns a new tensor with boolean elements representing if each element ofinputis NaN or not.\ninput\n\n## Binary Operators#\n\nAs you may have seen in the tutorial,MaskedTensoralso has binary operations implemented with the caveat\nthat the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you\nneed support for a particular operator or have proposed semantics for how they should behave instead, please open\nan issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users\nknow exactly what is going on and are being intentional about their decisions with masked semantics.\nMaskedTensor\nThe available binary operators are:\nadd\n\nadd\nAddsother, scaled byalpha, toinput.\nother\nalpha\ninput\natan2\n\natan2\nElement-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.\narctan2\n\narctan2\nAlias fortorch.atan2().\ntorch.atan2()\nbitwise_and\n\nbitwise_and\nComputes the bitwise AND ofinputandother.\ninput\nother\nbitwise_or\n\nbitwise_or\nComputes the bitwise OR ofinputandother.\ninput\nother\nbitwise_xor\n\nbitwise_xor\nComputes the bitwise XOR ofinputandother.\ninput\nother\nbitwise_left_shift\n\nbitwise_left_shift\nComputes the left arithmetic shift ofinputbyotherbits.\ninput\nother\nbitwise_right_shift\n\nbitwise_right_shift\nComputes the right arithmetic shift ofinputbyotherbits.\ninput\nother\ndiv\n\ndiv\nDivides each element of the inputinputby the corresponding element ofother.\ninput\nother\ndivide\n\ndivide\nAlias fortorch.div().\ntorch.div()\nfloor_divide\n\nfloor_divide\n\nfmod\n\nfmod\nApplies C++'sstd::fmodentrywise.\nlogaddexp\n\nlogaddexp\nLogarithm of the sum of exponentiations of the inputs.\nlogaddexp2\n\nlogaddexp2\nLogarithm of the sum of exponentiations of the inputs in base-2.\nmul\n\nmul\nMultipliesinputbyother.\ninput\nother\nmultiply\n\nmultiply\nAlias fortorch.mul().\ntorch.mul()\nnextafter\n\nnextafter\nReturn the next floating-point value afterinputtowardsother, elementwise.\ninput\nother\nremainder\n\nremainder\nComputesPython's modulus operationentrywise.\nsub\n\nsub\nSubtractsother, scaled byalpha, frominput.\nother\nalpha\ninput\nsubtract\n\nsubtract\nAlias fortorch.sub().\ntorch.sub()\ntrue_divide\n\ntrue_divide\nAlias fortorch.div()withrounding_mode=None.\ntorch.div()\nrounding_mode=None\neq\n\neq\nComputes element-wise equality\nne\n\nne\nComputesinput\u2260other\\text{input} \\neq \\text{other}input\ue020=otherelement-wise.\nle\n\nle\nComputesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.\nge\n\nge\nComputesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.\ngreater\n\ngreater\nAlias fortorch.gt().\ntorch.gt()\ngreater_equal\n\ngreater_equal\nAlias fortorch.ge().\ntorch.ge()\ngt\n\ngt\nComputesinput>other\\text{input} > \\text{other}input>otherelement-wise.\nless_equal\n\nless_equal\nAlias fortorch.le().\ntorch.le()\nlt\n\nlt\nComputesinput<other\\text{input} < \\text{other}input<otherelement-wise.\nless\n\nless\nAlias fortorch.lt().\ntorch.lt()\nmaximum\n\nmaximum\nComputes the element-wise maximum ofinputandother.\ninput\nother\nminimum\n\nminimum\nComputes the element-wise minimum ofinputandother.\ninput\nother\nfmax\n\nfmax\nComputes the element-wise maximum ofinputandother.\ninput\nother\nfmin\n\nfmin\nComputes the element-wise minimum ofinputandother.\ninput\nother\nnot_equal\n\nnot_equal\nAlias fortorch.ne().\ntorch.ne()\nThe available inplace binary operators are all of the aboveexcept:\nlogaddexp\n\nlogaddexp\nLogarithm of the sum of exponentiations of the inputs.\nlogaddexp2\n\nlogaddexp2\nLogarithm of the sum of exponentiations of the inputs in base-2.\nequal\n\nequal\nTrueif two tensors have the same size and elements,Falseotherwise.\nTrue\nFalse\nfmin\n\nfmin\nComputes the element-wise minimum ofinputandother.\ninput\nother\nminimum\n\nminimum\nComputes the element-wise minimum ofinputandother.\ninput\nother\nfmax\n\nfmax\nComputes the element-wise maximum ofinputandother.\ninput\nother\n\n## Reductions#\n\nThe following reductions are available (with autograd support). For more information, theOverviewtutorial\ndetails some examples of reductions, while theAdvanced semanticstutorial\nhas some further in-depth discussions about how we decided on certain reduction semantics.\nsum\n\nsum\nReturns the sum of all elements in theinputtensor.\ninput\nmean\n\nmean\n\namin\n\namin\nReturns the minimum value of each slice of theinputtensor in the given dimension(s)dim.\ninput\ndim\namax\n\namax\nReturns the maximum value of each slice of theinputtensor in the given dimension(s)dim.\ninput\ndim\nargmin\n\nargmin\nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension\nargmax\n\nargmax\nReturns the indices of the maximum value of all elements in theinputtensor.\ninput\nprod\n\nprod\nReturns the product of all elements in theinputtensor.\ninput\nall\n\nall\nTests if all elements ininputevaluate toTrue.\ninput\nnorm\n\nnorm\nReturns the matrix norm or vector norm of a given tensor.\nvar\n\nvar\nCalculates the variance over the dimensions specified bydim.\ndim\nstd\n\nstd\nCalculates the standard deviation over the dimensions specified bydim.\ndim\n\n## View and select functions#\n\nWe\u2019ve included a number of view and select functions as well; intuitively, these operators will apply to\nboth the data and the mask and then wrap the result in aMaskedTensor. For a quick example,\nconsiderselect():\nMaskedTensor\nselect()\n\n```python\n    >>> data = torch.arange(12, dtype=torch.float).reshape(3, 4)\n    >>> data\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11.]])\n    >>> mask = torch.tensor([[True, False, False, True], [False, True, False, False], [True, True, True, True]])\n    >>> mt = masked_tensor(data, mask)\n    >>> data.select(0, 1)\n    tensor([4., 5., 6., 7.])\n    >>> mask.select(0, 1)\n    tensor([False,  True, False, False])\n    >>> mt.select(0, 1)\n    MaskedTensor(\n      [      --,   5.0000,       --,       --]\n    )\n\n```\n\nThe following ops are currently supported:\natleast_1d\n\natleast_1d\nReturns a 1-dimensional view of each input tensor with zero dimensions.\nbroadcast_tensors\n\nbroadcast_tensors\nBroadcasts the given tensors according toBroadcasting semantics.\nbroadcast_to\n\nbroadcast_to\nBroadcastsinputto the shapeshape.\ninput\nshape\ncat\n\ncat\nConcatenates the given sequence of tensors intensorsin the given dimension.\ntensors\nchunk\n\nchunk\nAttempts to split a tensor into the specified number of chunks.\ncolumn_stack\n\ncolumn_stack\nCreates a new tensor by horizontally stacking the tensors intensors.\ntensors\ndsplit\n\ndsplit\nSplitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.\ninput\nindices_or_sections\nflatten\n\nflatten\nFlattensinputby reshaping it into a one-dimensional tensor.\ninput\nhsplit\n\nhsplit\nSplitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.\ninput\nindices_or_sections\nhstack\n\nhstack\nStack tensors in sequence horizontally (column wise).\nkron\n\nkron\nComputes the Kronecker product, denoted by\u2297\\otimes\u2297, ofinputandother.\ninput\nother\nmeshgrid\n\nmeshgrid\nCreates grids of coordinates specified by the 1D inputs inattr:tensors.\nnarrow\n\nnarrow\nReturns a new tensor that is a narrowed version ofinputtensor.\ninput\nnn.functional.unfold\nnn.functional.unfold\nExtract sliding local blocks from a batched input tensor.\nravel\n\nravel\nReturn a contiguous flattened tensor.\nselect\n\nselect\nSlices theinputtensor along the selected dimension at the given index.\ninput\nsplit\n\nsplit\nSplits the tensor into chunks.\nstack\n\nstack\nConcatenates a sequence of tensors along a new dimension.\nt\n\nt\nExpectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.\ninput\ntranspose\n\ntranspose\nReturns a tensor that is a transposed version ofinput.\ninput\nvsplit\n\nvsplit\nSplitsinput, a tensor with two or more dimensions, into multiple tensors vertically according toindices_or_sections.\ninput\nindices_or_sections\nvstack\n\nvstack\nStack tensors in sequence vertically (row wise).\nTensor.expand\nTensor.expand\nReturns a new view of theselftensor with singleton dimensions expanded to a larger size.\nself\nTensor.expand_as\nTensor.expand_as\nExpand this tensor to the same size asother.\nother\nTensor.reshape\nTensor.reshape\nReturns a tensor with the same data and number of elements asselfbut with the specified shape.\nself\nTensor.reshape_as\nTensor.reshape_as\nReturns this tensor as the same shape asother.\nother\nTensor.unfold\nTensor.unfold\nReturns a view of the original tensor which contains all slices of sizesizefromselftensor in the dimensiondimension.\nsize\nself\ndimension\nTensor.view\nTensor.view\nReturns a new tensor with the same data as theselftensor but of a differentshape.\nself\nshape",
    "url": "https://pytorch.org/docs/stable/masked.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "51c50cfdd0a191abb56ae62e230be686",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/torch.cond.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "609e87f72d6951540b982b5e7fc69309",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/python.object-model.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "92ce9f9e96ee662fa1948d8e9e62376d",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b1fb07e6d39556b9cb0d8d38d921571a",
    "source": "pytorch_docs",
    "title": "CUDA Environment Variables \u2014 PyTorch 2.9 documentation",
    "text": "\n## CUDA Environment Variables#\n\nCreated On: Feb 15, 2024 | Last Updated On: Feb 15, 2024\nFor more information on CUDA runtime environment variables, seeCUDA Environment Variables.\nPyTorch Environment Variables\nVariable\nDescription\nPYTORCH_NO_CUDA_MEMORY_CACHING\nPYTORCH_NO_CUDA_MEMORY_CACHING\nIf set to1, disables caching of memory allocations in CUDA. This can be useful for debugging.\n1\nPYTORCH_CUDA_ALLOC_CONF\nPYTORCH_CUDA_ALLOC_CONF\nFor a more in depth explanation of this environment variable, seeMemory management.\nPYTORCH_NVML_BASED_CUDA_CHECK\nPYTORCH_NVML_BASED_CUDA_CHECK\nIf set to1, before importing PyTorch modules that check if CUDA is available, PyTorch will use NVML to check if the CUDA driver is functional instead of using the CUDA runtime. This can be helpful if forked processes fail with a CUDA initialization error.\n1\nTORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\nTORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\nThe cache limit for the cuDNN v8 API. This is used to limit the memory used by the cuDNN v8 API. The default value is 10000, which roughly corresponds to 2GiB assuming 200KiB per ExecutionPlan. Set to0for no limit or a negative value for no caching.\n0\nTORCH_CUDNN_V8_API_DISABLED\nTORCH_CUDNN_V8_API_DISABLED\nIf set to1, disables the cuDNN v8 API. And will fall back to the cuDNN v7 API.\n1\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE\nTORCH_ALLOW_TF32_CUBLAS_OVERRIDE\nIf set to1, forces TF32 enablement, overridesset_float32_matmul_precisionsetting.\n1\nset_float32_matmul_precision\nTORCH_NCCL_USE_COMM_NONBLOCKING\nTORCH_NCCL_USE_COMM_NONBLOCKING\nIf set to1, enables non-blocking error handling in NCCL.\n1\nTORCH_NCCL_AVOID_RECORD_STREAMS\nTORCH_NCCL_AVOID_RECORD_STREAMS\nIf set to0, enables fallback to record streams-based synchronization behavior in NCCL.\n0\nTORCH_CUDNN_V8_API_DEBUG\nTORCH_CUDNN_V8_API_DEBUG\nIf set to1, sanity check whether cuDNN V8 is being used.\n1\nCUDA Runtime and Libraries Environment Variables\nVariable\nDescription\nCUDA_VISIBLE_DEVICES\nCUDA_VISIBLE_DEVICES\nComma-separated list of GPU device IDs that should be made available to CUDA runtime. If set to-1, no GPUs are made available.\n-1\nCUDA_LAUNCH_BLOCKING\nCUDA_LAUNCH_BLOCKING\nIf set to1, makes CUDA calls synchronous. This can be useful for debugging.\n1\nCUBLAS_WORKSPACE_CONFIG\nCUBLAS_WORKSPACE_CONFIG\nThis environment variable is used to set the workspace configuration for cuBLAS per allocation. The format is:[SIZE]:[COUNT].\nAs an example, the default workspace size per allocation isCUBLAS_WORKSPACE_CONFIG=:4096:2:16:8which specifies a total size of2*4096+8*16KiB.\nTo force cuBLAS to avoid using workspaces, setCUBLAS_WORKSPACE_CONFIG=:0:0.\n:[SIZE]:[COUNT]\nCUBLAS_WORKSPACE_CONFIG=:4096:2:16:8\n2*4096+8*16KiB\nCUBLAS_WORKSPACE_CONFIG=:0:0\nCUDNN_CONV_WSCAP_DBG\nCUDNN_CONV_WSCAP_DBG\nSimilar toCUBLAS_WORKSPACE_CONFIG, this environment variable is used to set the workspace configuration for cuDNN per allocation.\nCUBLAS_WORKSPACE_CONFIG\nCUBLASLT_WORKSPACE_SIZE\nCUBLASLT_WORKSPACE_SIZE\nSimilar toCUBLAS_WORKSPACE_CONFIG, this environment variable is used to set the workspace size for cuBLASLT.\nCUBLAS_WORKSPACE_CONFIG\nCUDNN_ERRATA_JSON_FILE\nCUDNN_ERRATA_JSON_FILE\nCan be set to a file path for an errata filter that can be passed to cuDNN to avoid specific engine configs, used primarily for debugging or to hardcode autotuning.\nNVIDIA_TF32_OVERRIDE\nNVIDIA_TF32_OVERRIDE\nIf set to0, disables TF32 globally across all kernels, overriding all PyTorch settings.\n0",
    "url": "https://pytorch.org/docs/stable/cuda_environment_variables.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "545348562900199d219986dc8c3bc35e",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributed.fsdp.fully_shard.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "cc8b5c983436749780ec4ed2efdc81ed",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_fine_grain_apis.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "24a9ef5ffcf101ff83e9bbbeb6b9eb3e",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/python.closure.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "2dd340842e0dafc7b8929040f5b44127",
    "source": "pytorch_docs",
    "title": "torch.compile Programming Model \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.compile Programming Model#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 29, 2025\nThetorch.compileprogramming model:\ntorch.compile\nClarifies some internal behaviors oftorch.compileso that one can better predict compiler behavior on user code and\ntorch.compile\nProvides ways for one to take more fine-grained control overtorch.compile.\ntorch.compile\nBy understanding thetorch.compileprogramming model, one can systematically unblock themselves when encountering issues withtorch.compile.\ntorch.compile\ntorch.compile\nfullgraph=True\ntorch._dynamo.nonstrict_trace\nfullgraph=False\ncompile(model)\nmodel.compile()\nerror_on_graph_break\nerror_on_graph_break(False)\nerror_on_graph_break(True)\nerror_on_graph_break\nfullgraph\nfullgraph=True/False\nerror_on_graph_break",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "15bdcc7f6c55c47249b094daa9bb15ee",
    "source": "pytorch_docs",
    "title": "torchrun (Elastic Launch) \u2014 PyTorch 2.9 documentation",
    "text": "\n## torchrun (Elastic Launch)#\n\nCreated On: May 04, 2021 | Last Updated On: Aug 26, 2021\nModuletorch.distributed.run.\ntorch.distributed.run\ntorch.distributed.runis a module that spawns up multiple distributed\ntraining processes on each of the training nodes.\ntorch.distributed.run\ntorchrunis a pythonconsole scriptto the main moduletorch.distributed.rundeclared in theentry_pointsconfiguration insetup.py.\nIt is equivalent to invokingpython-mtorch.distributed.run.\ntorchrun\nentry_points\npython-mtorch.distributed.run\ntorchruncan be used for single-node distributed training, in which one or\nmore processes per node will be spawned. It can be used for either\nCPU training or GPU training. If it is used for GPU training,\neach distributed process will be operating on a single GPU. This can achieve\nwell-improved single-node training performance.torchruncan also be used in\nmulti-node distributed training, by spawning up multiple processes on each node\nfor well-improved multi-node distributed training performance as well.\nThis will especially be beneficial for systems with multiple Infiniband\ninterfaces that have direct-GPU support, since all of them can be utilized for\naggregated communication bandwidth.\ntorchrun\ntorchrun\nIn both cases of single-node distributed training or multi-node distributed\ntraining,torchrunwill launch the given number of processes per node\n(--nproc-per-node). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (nproc_per_node),\nand each process will be operating on a single GPU fromGPU 0 to\nGPU (nproc_per_node - 1).\ntorchrun\n--nproc-per-node\nnproc_per_node\nChanged in version 2.0.0:torchrunwill pass the--local-rank=<rank>argument to your script.\nFrom PyTorch 2.0.0 onwards, the dashed--local-rankis preferred over the\npreviously used underscored--local_rank.\ntorchrun\n--local-rank=<rank>\n--local-rank\n--local_rank\nFor backward compatibility, it may be necessary for users to handle both\ncases in their argument parsing code. This means including both\"--local-rank\"and\"--local_rank\"in the argument parser. If only\"--local_rank\"is\nprovided,torchrunwill trigger an error: \u201cerror: unrecognized arguments:\n\u2013local-rank=<rank>\u201d. For training code that only supports PyTorch 2.0.0+,\nincluding\"--local-rank\"should be sufficient.\n\"--local-rank\"\n\"--local_rank\"\n\"--local_rank\"\ntorchrun\n\"--local-rank\"\n\n```python\n>>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local-rank\", \"--local_rank\", type=int)\n>>> args = parser.parse_args()\n\n```\n\n\n## Usage#\n\n\n## Single-node multi-worker#\n\n\n```python\ntorchrun\n    --standalone\n    --nnodes=1\n    --nproc-per-node=$NUM_TRAINERS\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\nNote\n--nproc-per-nodemay be\"gpu\"(spawn one process per GPU),\"cpu\"(spawn one process per CPU),\"auto\"(equivalent to\"gpu\"if CUDA is available,\nelse equivalent to\"cpu\"),\nor an integer specifying the number of processes.\nSeetorch.distributed.run.determine_local_world_sizefor more details.\n--nproc-per-node\n\"gpu\"\n\"cpu\"\n\"auto\"\n\"gpu\"\n\"cpu\"\n\n## Stacked single-node multi-worker#\n\nTo run multiple instances (separate jobs) of single-node, multi-worker on the\nsame host, we need to make sure that each instance (job) is\nsetup on different ports to avoid port conflicts (or worse, two jobs being merged\nas a single job). To do this you have to run with--rdzv-backend=c10dand specify a different port by setting--rdzv-endpoint=localhost:$PORT_k.\nFor--nodes=1, its often convenient to lettorchrunpick a free random\nport automatically instead of manually assigning different ports for each run.\n--rdzv-backend=c10d\n--rdzv-endpoint=localhost:$PORT_k\n--nodes=1\ntorchrun\n\n```python\ntorchrun\n    --rdzv-backend=c10d\n    --rdzv-endpoint=localhost:0\n    --nnodes=1\n    --nproc-per-node=$NUM_TRAINERS\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\n\n## Fault tolerant (fixed sized number of workers, no elasticity, tolerates 3 failures)#\n\n\n```python\ntorchrun\n    --nnodes=$NUM_NODES\n    --nproc-per-node=$NUM_TRAINERS\n    --max-restarts=3\n    --rdzv-id=$JOB_ID\n    --rdzv-backend=c10d\n    --rdzv-endpoint=$HOST_NODE_ADDR\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\nHOST_NODE_ADDR, in form <host>[:<port>] (e.g. node1.example.com:29400), specifies the node and\nthe port on which the C10d rendezvous backend should be instantiated and hosted. It can be any\nnode in your training cluster, but ideally you should pick a node that has a high bandwidth.\nHOST_NODE_ADDR\nNote\nIf no port number is specifiedHOST_NODE_ADDRdefaults to 29400.\nHOST_NODE_ADDR\n\n## Elastic (min=1,max=4, tolerates up to 3 membership changes or failures)#\n\nmin=1\nmax=4\n\n```python\ntorchrun\n    --nnodes=1:4\n    --nproc-per-node=$NUM_TRAINERS\n    --max-restarts=3\n    --rdzv-id=$JOB_ID\n    --rdzv-backend=c10d\n    --rdzv-endpoint=$HOST_NODE_ADDR\n    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n\n```\n\nHOST_NODE_ADDR, in form <host>[:<port>] (e.g. node1.example.com:29400), specifies the node and\nthe port on which the C10d rendezvous backend should be instantiated and hosted. It can be any\nnode in your training cluster, but ideally you should pick a node that has a high bandwidth.\nHOST_NODE_ADDR\nNote\nIf no port number is specifiedHOST_NODE_ADDRdefaults to 29400.\nHOST_NODE_ADDR\n\n## Note on rendezvous backend#\n\nFor multi-node training you need to specify:\n--rdzv-id: A unique job id (shared by all nodes participating in the job)\n--rdzv-id\n--rdzv-backend: An implementation oftorch.distributed.elastic.rendezvous.RendezvousHandler\n--rdzv-backend\ntorch.distributed.elastic.rendezvous.RendezvousHandler\n--rdzv-endpoint: The endpoint where the rendezvous backend is running; usually in formhost:port.\n--rdzv-endpoint\nhost:port\nCurrentlyc10d(recommended),etcd-v2, andetcd(legacy)  rendezvous backends are\nsupported out of the box. To useetcd-v2oretcd, setup an etcd server with thev2api\nenabled (e.g.--enable-v2).\nc10d\netcd-v2\netcd\netcd-v2\netcd\nv2\n--enable-v2\nWarning\netcd-v2andetcdrendezvous use etcd API v2. You MUST enable the v2 API on the etcd\nserver. Our tests use etcd v3.4.3.\netcd-v2\netcd\nWarning\nFor etcd-based rendezvous we recommend usingetcd-v2overetcdwhich is functionally\nequivalent, but uses a revised implementation.etcdis in maintenance mode and will be\nremoved in a future version.\netcd-v2\netcd\netcd\n\n## Definitions#\n\nNode- A physical instance or a container; maps to the unit that the job manager works with.\nNode\nWorker- A worker in the context of distributed training.\nWorker\nWorkerGroup- The set of workers that execute the same function (e.g. trainers).\nWorkerGroup\nLocalWorkerGroup- A subset of the workers in the worker group running on the same node.\nLocalWorkerGroup\nRANK- The rank of the worker within a worker group.\nRANK\nWORLD_SIZE- The total number of workers in a worker group.\nWORLD_SIZE\nLOCAL_RANK- The rank of the worker within a local worker group.\nLOCAL_RANK\nLOCAL_WORLD_SIZE- The size of the local worker group.\nLOCAL_WORLD_SIZE\nrdzv_id- A user-defined id that uniquely identifies the worker group for a job. This id is\nused by each node to join as a member of a particular worker group.\nrdzv_id\nrdzv_backend- The backend of the rendezvous (e.g.c10d). This is typically a strongly\nconsistent key-value store.\nrdzv_backend\nc10d\nrdzv_endpoint- The rendezvous backend endpoint; usually in form<host>:<port>.\nrdzv_endpoint\n<host>:<port>\nANoderunsLOCAL_WORLD_SIZEworkers which comprise aLocalWorkerGroup. The union of\nallLocalWorkerGroupsin the nodes in the job comprise theWorkerGroup.\nNode\nLOCAL_WORLD_SIZE\nLocalWorkerGroup\nLocalWorkerGroups\nWorkerGroup\n\n## Environment Variables#\n\nThe following environment variables are made available to you in your script:\nLOCAL_RANK-  The local rank.\nLOCAL_RANK\nRANK-  The global rank.\nRANK\nGROUP_RANK- The rank of the worker group. A number between 0 andmax_nnodes. When\nrunning a single worker group per node, this is the rank of the node.\nGROUP_RANK\nmax_nnodes\nROLE_RANK-  The rank of the worker across all the workers that have the same role. The role\nof the worker is specified in theWorkerSpec.\nROLE_RANK\nWorkerSpec\nLOCAL_WORLD_SIZE- The local world size (e.g. number of workers running locally); equals to--nproc-per-nodespecified ontorchrun.\nLOCAL_WORLD_SIZE\n--nproc-per-node\ntorchrun\nWORLD_SIZE- The world size (total number of workers in the job).\nWORLD_SIZE\nROLE_WORLD_SIZE- The total number of workers that was launched with the same role specified\ninWorkerSpec.\nROLE_WORLD_SIZE\nWorkerSpec\nMASTER_ADDR- The FQDN of the host that is running worker with rank 0; used to initialize\nthe Torch Distributed backend.\nMASTER_ADDR\nMASTER_PORT- The port on theMASTER_ADDRthat can be used to host the C10d TCP store.\nMASTER_PORT\nMASTER_ADDR\nTORCHELASTIC_RESTART_COUNT- The number of worker group restarts so far.\nTORCHELASTIC_RESTART_COUNT\nTORCHELASTIC_MAX_RESTARTS- The configured maximum number of restarts.\nTORCHELASTIC_MAX_RESTARTS\nTORCHELASTIC_RUN_ID- Equal to the rendezvousrun_id(e.g. unique job id).\nTORCHELASTIC_RUN_ID\nrun_id\nPYTHON_EXEC- System executable override. If provided, the python user script will\nuse the value ofPYTHON_EXECas executable. Thesys.executableis used by default.\nPYTHON_EXEC\nPYTHON_EXEC\n\n## Deployment#\n\n(Not needed for the C10d backend) Start the rendezvous backend server and get the endpoint (to be\npassed as--rdzv-endpointtotorchrun)\n--rdzv-endpoint\ntorchrun\nSingle-node multi-worker: Starttorchrunon the host to start the agent process which\ncreates and monitors a local worker group.\ntorchrun\nMulti-node multi-worker: Starttorchrunwith the same arguments on all the nodes\nparticipating in training.\ntorchrun\nWhen using a job/cluster manager, the entry point command to the multi-node job should betorchrun.\ntorchrun\n\n## Failure Modes#\n\nWorker failure: For a training job withnworkers, ifk<=nworkers fail all workers\nare stopped and restarted up tomax_restarts.\nn\nk<=n\nmax_restarts\nAgent failure: An agent failure results in a local worker group failure. It is up to the job\nmanager to fail the entire job (gang semantics) or attempt to replace the node. Both behaviors\nare supported by the agent.\nNode failure: Same as agent failure.\n\n## Membership Changes#\n\nNode departure (scale-down): The agent is notified of the departure, all existing workers are\nstopped, a newWorkerGroupis formed, and all workers are started with a newRANKandWORLD_SIZE.\nWorkerGroup\nRANK\nWORLD_SIZE\nNode arrival (scale-up): The new node is admitted to the job, all existing workers are stopped,\na newWorkerGroupis formed, and all workers are started with a newRANKandWORLD_SIZE.\nWorkerGroup\nRANK\nWORLD_SIZE\n\n## Important Notices#\n\nThis utility and multi-process distributed (single-node or\nmulti-node) GPU training currently only achieves the best performance using\nthe NCCL distributed backend. Thus NCCL backend is the recommended backend to\nuse for GPU training.\nThe environment variables necessary to initialize a Torch process group are provided to you by\nthis module, no need for you to passRANKmanually.  To initialize a process group in your\ntraining script, simply run:\nRANK\n\n```python\n>>> import torch.distributed as dist\n>>> dist.init_process_group(backend=\"gloo|nccl\")\n\n```\n\nIn your training program, you can either use regular distributed functions\nor usetorch.nn.parallel.DistributedDataParallel()module. If your\ntraining program uses GPUs for training and you would like to usetorch.nn.parallel.DistributedDataParallel()module,\nhere is how to configure it.\ntorch.nn.parallel.DistributedDataParallel()\ntorch.nn.parallel.DistributedDataParallel()\n\n```python\nlocal_rank = int(os.environ[\"LOCAL_RANK\"])\nmodel = torch.nn.parallel.DistributedDataParallel(\n    model, device_ids=[local_rank], output_device=local_rank\n)\n\n```\n\nPlease ensure thatdevice_idsargument is set to be the only GPU device id\nthat your code will be operating on. This is generally the local rank of the\nprocess. In other words, thedevice_idsneeds to be[int(os.environ(\"LOCAL_RANK\"))],\nandoutput_deviceneeds to beint(os.environ(\"LOCAL_RANK\"))in order to use this\nutility\ndevice_ids\ndevice_ids\n[int(os.environ(\"LOCAL_RANK\"))]\noutput_device\nint(os.environ(\"LOCAL_RANK\"))\nOn failures or membership changes ALL surviving workers are killed immediately. Make sure to\ncheckpoint your progress. The frequency of checkpoints should depend on your job\u2019s tolerance\nfor lost work.\nThis module only supports homogeneousLOCAL_WORLD_SIZE. That is, it is assumed that all\nnodes run the same number of local workers (per role).\nLOCAL_WORLD_SIZE\nRANKis NOT stable. Between restarts, the local workers on a node can be assigned a\ndifferent range of ranks than before. NEVER hard code any assumptions about the stable-ness of\nranks or some correlation betweenRANKandLOCAL_RANK.\nRANK\nRANK\nLOCAL_RANK\nWhen using elasticity (min_size!=max_size) DO NOT hard code assumptions aboutWORLD_SIZEas the world size can change as nodes are allowed to leave and join.\nmin_size!=max_size\nWORLD_SIZE\nIt is recommended for your script to have the following structure:\n\n```python\ndef main():\n    load_checkpoint(checkpoint_path)\n    initialize()\n    train()\n\n\ndef train():\n    for batch in iter(dataset):\n        train_step(batch)\n\n        if should_checkpoint:\n            save_checkpoint(checkpoint_path)\n\n```\n\n(Recommended) On worker errors, this tool will summarize the details of the error\n(e.g. time, rank, host, pid, traceback, etc). On each node, the first error (by timestamp)\nis heuristically reported as the \u201cRoot Cause\u201d error. To get tracebacks as part of this\nerror summary print out, you must decorate your main entrypoint function in your\ntraining script as shown in the example below. If not decorated, then the summary\nwill not include the traceback of the exception and will only contain the exitcode.\nFor details on torchelastic error handling see:https://pytorch.org/docs/stable/elastic/errors.html\n\n```python\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\n\n@record\ndef main():\n    # do train\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n",
    "url": "https://pytorch.org/docs/stable/elastic/run.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0def6a32b906807d10ede23dacb93860",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_get_started.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "8e2d90bc1f748ad77c2102df67771a25",
    "source": "pytorch_docs",
    "title": "Dynamo Core Concepts \u2014 PyTorch 2.9 documentation",
    "text": "\n## Dynamo Core Concepts#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nSummary:\nDynamo,torch.compile\u2019s frontend, performstracingto capture the semantics of a Python function\n(and its nested function calls) into a linear sequence of operations (the \u201c(FX) graph\u201d),\nresidual bytecode, and \u201cguards\u201d (a list of conditions under which the graph and bytecode are valid).\ntorch.compile\nUnsupported Python features lead tograph breaks, where Dynamo compiles a partial graph acquired from tracing,\nthen runs the unsupported code, then resumes tracing.\nGraph breaks may lead to slowness in torch.compile and prevent backend optimization opportunities.\nIf you\u2019re not seeing the performance you expect, then check for graph breaks.\n\n## Dynamo Tracing#\n\ntorch.compile\u2019s frontend (Dynamo) is a custom Python bytecode interpreter designed to allow graph compilation\nin PyTorch programs while retaining the full flexibility of Python. Given a function to be compiled, Dynamo\ninterprets Python bytecode to extract sequences of PyTorch operations into 1 or more FX graphs that may be further optimized by a backend.\ntorch.compile\n\nFor example, for the functionfin the above diagram, Dynamo produces:\nf\na singleFX graphthat takes in the original input plus some additional inputs required by the function.\nPython bytecodethat can be used as a drop-in replacement forf. In our example, the bytecode retrieves\nthe additional inputs and passes it to the graph and also contains unoptimizable Python side effects (the list append)\nf\nguardsthat specify the conditions under which the graph and bytecode are valid. Unless otherwise specified,\nthe graph produced by Dynamo specializes on the shapes of input Tensors.\n\n## Graph Breaks#\n\nDynamo traces your code and attempts to capture your PyTorch code into a single computation graph of PyTorch\noperators (FX graph). However, this is not always possible. When encountering code that can\u2019t be traced, a \u201cgraph break\u201d occurs.\nIn the defaulttorch.compilesettings, a graph break involves compiling the FX graph that has been determined so far,\nrunning the unsupported code in regular Python, then resuming tracing after the unsupported code with a new FX graph.\ntorch.compile\nGraph breaks are a feature that allows Dynamo to run over arbitrary Python code and carve out functional subgraphs that can each be individually optimized.\nHowever, it is possible for graph breaks to lead to unexpected slowness intorch.compile.\nIf you\u2019re not getting the speedups you expect, we recommend checking for graph breaks and removing them.\ntorch.compile\nGraph breaks may occur on things like:\nData-dependent if-statements\nMany Python built-in functions\nC functions\nBelow is an example of a graph break due to calling an unsupported operationtorch.save:\ntorch.save\n\n```python\n@torch.compile\ndef f(x):\n   y = x ** 2  / 2\n   torch.save(y, \"foo.pt\")  # torch.save is an unsupported operation\n   z = y ** 3 / 6\n   return z\n\nx = torch.randn(3)\nprint(f(x))\n\n```\n\n\n```python\ntensor([6.3085e-03, 8.2592e-01, 5.1903e-08])\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_265/215272159.py:4\nGraph Break Reason: Attempted to call function marked as skipped\n  Explanation: Dynamo developers have intentionally marked that the function `save` in file `/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/serialization.py` should not be traced.\n  Hint: Avoid calling the function `save`.\n  Hint: Apply `@torch._dynamo.dont_skip_tracing` to the function `save` to force tracing into the function. More graph breaks may occur as a result of attempting to trace into the function.\n  Hint: Please file an issue to PyTorch.\n\n  Developer debug context: module: torch.serialization, qualname: save, skip reason: <missing reason>\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0007.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_265/215272159.py\", line 9, in <module>\n    print(f(x))\n  File \"/tmp/ipykernel_265/215272159.py\", line 4, in f\n    torch.save(y, \"foo.pt\")  # torch.save is an unsupported operation\n\n```\n\nThe semantics oftorch.compile(f)(x)are roughly this:\ntorch.compile(f)(x)\n\n```python\ndef compiled_f_semantics(x):\n   y = torch.compile(g, fullgraph=True)(x)\n   torch.save(y, \"foo.pt\")\n   z = torch.compile(h, fullgraph=True)(x)\n   return z\n\ndef g(x):\n    return x ** 2  / 2\n\ndef h(x):\n    return y ** 3 / 6\n\n```\n\n\n## Guards#\n\ntorch.compilemakes some assumptions about runtime values as we trace through code. During tracing, we generate \u201cguards\u201d,\nwhich are runtime checks for these assumptions. Guards are run in future calls to the compiled function to determine if we\ncan reuse previously compiled code. Examples of runtime checks are constant values, types, and object IDs.\ntorch.compile\nBelow is an example of generated guards. TheTENSOR_MATCHguard checks for the input\u2019s type, device, dtype, shape, etc.\nTENSOR_MATCH\n\n```python\n@torch.compile\ndef fn(x):\n    return x + 1\n\nprint(fn(torch.ones(3, 3)))\n\n```\n\n\n```python\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n```\n\n\n```python\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3, 3], stride=[3, 1])  # return x + 1  # mp/ipykernel_265/1068332425.py:3 in fn\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x + 1  # mp/ipykernel_265/1068332425.py:3 in fn\n\nGuard eval latency = 155.78 us\n\n```\n\n\n## Recompilations#\n\nIf the guards fail for every instance of previously compiled code, thentorch.compilemust \u201crecompile\u201d the function,\nrequiring the original code to be traced again. In the example below, recompilation is necessary because the guard checking the tensor argument\u2019s shape failed.\ntorch.compile\n\n```python\n@torch.compile\ndef fn(x):\n    return x + 1\n\nprint(fn(torch.ones(3, 3)))\nprint(fn(torch.ones(4, 4)))\n\n```\n\n\n```python\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n```\n\n\n```python\nRecompiling function fn in /tmp/ipykernel_265/420870727.py:1\n    triggered by the following guard failure(s):\n    - 3/0: tensor 'x' size mismatch at index 0. expected 3, actual 4\n\n```\n\n\n## Dynamic Shapes#\n\ntorch.compileinitially assumes tensor shapes are static/constant and guards based on these assumptions. By using \u201cdynamic shapes,\u201d\nwe can gettorch.compileto produce compiled code that can accept tensor inputs with different shapes - we avoid recompiling every time shapes differ.\nBy default, automatic dynamic shapes are enabled intorch.compile(dynamic=None)- if compilation fails due to shape mismatch,\nrecompilation is attempted with dynamic shapes. Dynamic shapes can also be fully enabled (dynamic=True) or disabled (dynamic=False).\ntorch.compile\ntorch.compile\ntorch.compile(dynamic=None)\ndynamic=True\ndynamic=False\nBelow, we enable dynamic shapes and note that we no longer need to recompile.\n\n```python\n@torch.compile(dynamic=True)\ndef fn(x):\n    return x + 1\n\nprint(fn(torch.ones(3, 3)))\nprint(fn(torch.ones(4, 4)))\n\n```\n\n\n```python\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n```\n\n\n```python\ncreate_env\ncreate_symbol s77 = 3 for L['x'].size()[0] [2, int_oo] return x + 1  # mp/ipykernel_265/1458103805.py:3 in fn (_dynamo/variables/builder.py:3508 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s77\" or to suppress this message run with TORCHDYNAMO_EXTENDED_ADVICE=\"0\"\ncreate_symbol s77 duck sized L['x'].size()[1]\neval False == False [statically known]\neval False == False [statically known]\nproduce_guards\ntrack_symint L['x'].size()[0] s77 None\ntrack_symint L['x'].size()[1] s77 None\ntrack_symint L['x'].stride()[0] s77 None\ntrack_symint L['x'].stride()[1] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].stride()[1] == 1\nSkipping guard L['x'].storage_offset() == 0\n\n```\n\nFor more information on dynamic shapes, seeThe dynamic shapes manual.",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.dynamo_core_concepts.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "49cbd6df93329d7b3dd6dcbe8a39f29c",
    "source": "pytorch_docs",
    "title": "Working with fullgraph=False \u2014 PyTorch 2.9 documentation",
    "text": "\n## Working withfullgraph=False#\n\nfullgraph=False\nCreated On: Jul 28, 2025 | Last Updated On: Sep 03, 2025\nWhilefullgraph=Falseis the defaulttorch.compilesetting, the semantics of resuming compilation upon encountering a graph break are more complicated.\nYou can find details on thefullgraph=Falsesemantics in the subsections.\nfullgraph=False\ntorch.compile\nfullgraph=False\nThe strategy for usingtorch.compile(fullgraph=False)is as follows:\ntorch.compile(fullgraph=False)\nDetermine the ideal location to placetorch.compile. Normally, it is the highest-level function that doesn\u2019t result in excessive graph breaks.\nFunctions that do a lot of preprocessing or I/O operations are examples of functions that result in many graph breaks and do not significantly benefit fromtorch.compile.\na. You can isolate issues by first compiling individual functions/modules before compiling entire models.\ntorch.compile\ntorch.compile\nApplytorch.compiler.disableto functions in the compiled region that result in a lot of graph breaks\nand do not benefit from compilation. In this case, one graph break is better than potentially tens or hundreds.\ntorch.compiler.disable\nUseTORCH_LOGS=\"graph_breaks\"or tlparse to investigate remaining graph breaks.Work around these graph breaks using the same approaches as working around graph breaks under\nthefullgraph=Trueprogramming model. Not all graph breaks need to be removed - some may\nimpact performance more than others. The general rule is to focus on graph breaks that are happening during model computation.\na. We recommend usingtorch.compile(backend='eager')when debugging graph breaks, for faster debugging iteration times\nTORCH_LOGS=\"graph_breaks\"\nfullgraph=True\ntorch.compile(backend='eager')\ncompile(model)\nmodel.compile()\nerror_on_graph_break\nerror_on_graph_break(False)\nerror_on_graph_break(True)\nerror_on_graph_break\nfullgraph\nfullgraph=True/False\nerror_on_graph_break",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.fullgraph_false.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3be9ee5a5c39523452045c723581faaa",
    "source": "pytorch_docs",
    "title": "Dynamo Deep-Dive \u2014 PyTorch 2.9 documentation",
    "text": "\n## Dynamo Deep-Dive#\n\nCreated On: Apr 02, 2024 | Last Updated On: Aug 12, 2025\nTorchDynamo (or simply Dynamo) is the tracer withintorch.compile,\nand it is, more often than not, the one to blame for those insane\nbacktraces. However, we cannot blindly blame Dynamo for these errors. In\norder to provide the user with the flexibility it does, Dynamo is given\nthe arduous task of understanding any Python program. In particular,\nDynamo has to implement a good part of the Python programming language\ninternally!\ntorch.compile\nIn this post, we will go over the internal design of Dynamo from the\nground up. We will discuss the functionality it provides, and how it is\nimplemented. By the end of this post, you will have a better\nunderstanding of what went wrong when youtorch.compileda PyTorch\nprogram and the compilation errored out, or succeeded but the speed-up\nwas not what you expected.\ntorch.compiled\n\n## A Gentle Introduction to Dynamo#\n\nBefore getting our hands dirty with all the implementation details,\nlet\u2019s start by discussing what it is that Dynamo does.\nDynamo is a tracer. This means, given and function and inputs to it, it\nexecutes the function and records a linear sequence of instructions\n(without control flow) into a graph. For example, consider the following\nprogram:\n\n```python\nimport torch\n\n@torch.compile\ndef mse(x, y):\n    z = (x - y) ** 2\n    return z.sum()\n\nx = torch.randn(200)\ny = torch.randn(200)\nmse(x, y)\n\n```\n\nIf we save this program into the fileexample.pyand we run\nexample.py\n\n```python\nTORCH_LOGS=graph_code python example.py\n\n```\n\nwe see the output that Dynamo traced\n\n```python\ndef forward(l_x_: torch.Tensor, l_y_: torch.Tensor):\n    # File: example.py:5, code: z = (x - y) ** 2\n    sub = l_x_ - l_y_\n    z = sub ** 2\n    # File: example.py:6, code: return z.sum()\n    sum_1 = z.sum()\n    return (sum_1,)\n\n```\n\nWe call this agraph (or trace) of the function for the given\ninputs. This is represented via anFX\ngraph. We will simply think\nof an FX graph as a container that stores a list of function calls.\nThe first thing we should notice is that the graph is a linear sequence\nof PyTorch operations.1Dynamo records all the PyTorch operations\nand stores them sequentially. For example, it splitz=(x-y)**2into its two constituting operations,sub=l_x_-l_y_andz=sub**2.\nz=(x-y)**2\nsub=l_x_-l_y_\nz=sub**2\nWhen we say that the trace is linear, we mean that there is no branching\nor any control flow. To see this, consider\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, n):\n    y = x ** 2\n    if n >= 0:\n        return (n + 1) * y\n    else:\n        return y / n\n\nx = torch.randn(200)\nfn(x, 2)\n\n```\n\nwhich, when executed withTORCH_LOGS=graph_code, returns\nTORCH_LOGS=graph_code\n\n```python\ndef forward(l_x_: torch.Tensor):\n    # File: example.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n    # File: example.py:7, code: return (n + 1) * y\n    mul = 3 * y\n    return (mul,)\n\n```\n\nWe see that Dynamo completely removed theifstatement from the\ntrace and just recorded the operations that were executed with the\ninputs.\nif\nAs such, it should be clear thatthe trace of a function depends on\nthe inputs. In particular, this means that the trace is not generated\nwhen we write@torch.compile, but when we execute the functionfn(x,2)with the actual arguments.\n@torch.compile\nfn(x,2)\nThe other interesting thing to note here is that Dynamo removed the\nsecond argument to the function. Instead, it treated it as a constant\nand recorded the result of the operationn+1in the graph. This is\nanother feature of Dynamo: Dynamo will treat as constant any non-tensor\nvalue\u2026 other than ints. Let\u2019s see now how are ints special.\nn+1\nThe last defining property of Dynamo is that it knows how to handle\ndynamic shapes. Symbolic shapes refer to Dynamo\u2019s ability of tracing\nshapes, and more generally, integers, rather than leaving them as\nconstants. This allows for avoiding recompilations and deploying generic\nmodels that work for any size in production. The main examples of places\nwhere dynamic shapes appear are the batch size, where we might train a\nmodel with a fixed batch size but then perform inference for an\narbitrary batch size, or the variable sequence length that one\nencounters when processing text or audio.\nWe can see this by executing a few more times the example above\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, n):\n    y = x ** 2\n    if n >= 0:\n        return (n + 1) * y\n    else:\n        return y / n\n\nx = torch.randn(200)\nfn(x, 2)\nfn(x, 3)\nfn(x, -2)\n\n```\n\nIn this case,TORCH_LOGS=graph_codegenerates two more graphs\nTORCH_LOGS=graph_code\n\n```python\n# Graph for n==2 omitted\n\ndef forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):\n    # File: a.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n\n    # File: a.py:7, code: return (n + 1) * y\n    add = l_n_ + 1\n    mul = add * y\n    return (mul,)\n\n```\n\n\n```python\ndef forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):\n    # File: a.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n\n    # File: a.py:9, code: return y / n\n    truediv = y / l_n_\n    return (truediv,)\n\n```\n\nDynamo detected that one integer changed its value after the first call\nand started tracing it. We see that these graphs are generic, and trace\nthe variablensymbolically via an object of typeSymInt.\nn\nSymInt\nIf after these calls we callfn(x,4), Dynamo would not recompile,\nbut rather reuse the graph that was already traced.\nfn(x,4)\nTo summarize: 1. Dynamo is a Python tracer 2. Given some inputs, it\nreturns an FX graph with the PyTorch functions that were executed 3. It\ncan also trace integers if it detects that they changed between calls 4.\nIt specializes any other value that is not a tensor or a scalar\nOf course, Dynamo does many more things, like figuring out when it needs\nto retrace, rewriting the bytecode of the function, implementing graph\nbreaks\u2026 To keep the introduction short, we will incrementally discuss\nall these in the sequel.\n\n## PEP 523: Adding a frame evaluation API to CPython#\n\nImagine now that we are given the task to implement Dynamo. Where would\nwe even start? Rather conveniently for us,PEP\n523was released with Python 3.6.\nThis PEPwas\ndesignedto\nallow third parties to create JIT compilers for Python. Let\u2019s see how.\nA note on CPython: CPython is internally implemented as astack\nmachine. A Python\nprogram is compiled intobytecodesthat then are\nexecuted by this interpreter. To learn more about these bytecodes, see\nthedis modulefrom the\nstandard library. See alsothe developer\ndocsfor an\nintroduction to CPython\u2019s interpreter. We will assume that the reader is\nfamiliar with the notion of a stack machine.\nPEP 523 exposes an API where a user can add a custom per-function\ninterpreter. Then, CPython will use this interpreter rather than its own\nto execute the function. In order to be able to execute the function, on\nentry, CPython provides the custom interpreter with things like - The\nbytecode of the function - The value of the arguments of the function\n(i.e., the local variables) and their names - The value of the global\nvariables and their names - The builtin functions likeabsorprint\nabs\nprint\nYou can see all the fieldshere.2\nIn summary, CPython provides the user\u2019s interpreter with all the\ninformation necessary to execute the function.3\nWith this API, we can implement a tracer by implementing an interpreter\nthat runs the code and records in a graph all the PyTorch operations\nthat occur during this execution. This is exactly what Dynamo does.\nDynamo uses this CPython API to parse all these objects and packs them\nintoa Python\nstructure.\nAfter it has done so\u2026 it goes back from C to python. Other than for this\npiece of code that communicates with CPython, Dynamo is fully\nimplemented in Python.\nIt should be clear that it is the decorator@torch.compile\u2019s job\nto install the necessary scaffolding that will pass the bytecode, the\nargs, global variables and so on to Dynamo when the function is called.\nAgain,@torch.compiledoes not actually compile anything.\n@torch.compile\n@torch.compile\n\n## Implementing CPython in Python#\n\nSo, we are back in the Python world. We have the bytecode of a function,\nand all the context necessary to execute it. In particular, we have\nlanded at_convert_frame_assert.\nThis is the function that the decoratortorch.compilereturns! We\nget to this function from_dynamo.optimize.\nThe decoratortorch.compileis just a nice API around_dynamo.optimize.\ntorch.compile\ntorch.compile\n_dynamo.optimize\nBefore getting into implementing a Python interpreter, we want to define\nanIR.\nIn particular, we want to wrap all the local and global variables in our\nown internal classes. This allows us to better track these objects and\ngroup together objects that can be treated in the same way to the eyes\nof Dynamo.\nThe parent class of the internal class structure isVariableTrackerand represents the different objects that Dynamo understands. For\nexample,ListVariable, represents alistobject, and keeps\ninternally alist of VariableTrackers.\nAnother example ofVariableTrackerisConstantVariable.\nConstantVariable wraps all theobjects considered constant by\nDynamo.\nWe also have special subclasses for objects that require special\nattention, likeTensorVariable.\nAll these internal classes are defined in thetorch/_dynamo/variablesfolder.\nVariableTracker\nListVariable\nlist\nVariableTracker\nPython objects are wrapped into their correspondingVariableTrackerclass inVariableBuilder._wrap.\nThis function is just a very long chain ofelifs that tries to\nrecursively pattern-match the Python inputs into the appropriate type ofVariableTracker.\nVariableTracker\nelif\nVariableTracker\nDebugging tip. When we get unexpected results from dynamo, it is\nsometimes caused by the builder. If the logic of the builder is wrong,\nsometimes Dynamo may wrap a variable in the incorrectVariableTrackertype, and this may cause issues later on. It is\nrather useful to have a look at theVariableTrackertypes that\nappear in the errors, and theVariableTrackermethod that throws the\nexception when you encounter a Dynamo error. In particular, sometimes we\nfind that an object is tracked as aUserDefinedObjectVariable(this\nis Dynamo\u2019s catch-all class), when it should have been tracked as\nsomething more specific. In these cases, theVariableBuilderlogic is often to blame.\nVariableTracker\nVariableTracker\nVariableTracker\nUserDefinedObjectVariable\nVariableBuilder\nDebugging tip. When running a program withTORCH_LOGS=dynamo,\none of the artifacts that are printed out is lines of the form\nTORCH_LOGS=dynamo\n\n```python\nTRACE LOAD_GLOBAL y [TorchInGraphFunctionVariable(<built-in method any>), TensorVariable()]\n\n```\n\nThis is the bytecode for the original program and the state of the stack\nat that point. This is very useful to find where an object was not\ntraced into the rightVariableTracker.\nVariableTracker\nOk, so we have an IR for our tracer, now wejustneed to reimplement\nCPython\u2019s stack machine. This is implemented byInstructorTranslatorBaseinsymbolic_convert.py.\nInstructionTranslatorBasehas about 200 methods, implementing almost\nall of Python bytecodes. As an example, we can see the implementation ofBUILD_LIST\nInstructionTranslatorBase\nBUILD_LIST\n\n```python\ndef BUILD_LIST(self, inst):\n    items = self.popn(inst.argval)\n    self.push(ListVariable(items, mutation_type=ValueMutationNew()))\n\n```\n\nThis is the bytecode generated by constructions likel=[2,3,4].\nIn this case, since there are three elements, the generated bytecode isBUILD_LIST3. This means that we pop the top3elements of the\nstack and push a new list object to the top of the stack formed by these\nthree elements.\nl=[2,3,4]\nBUILD_LIST3\n3\n\n## Generating the Output Graph#\n\nWith a way to symbolically execute Python code, we are set to extract\nthe PyTorch operations that happen during the symbolic execution of a\nprogram given some inputs. This is implemented in Dynamo via theOutputGraphobject. TheOutputGraphobject isbound to an\n`InstructionTranslator objectand it tracks all the data necessary to create the FX graph which will\nbe returned by Dynamo.\nOutputGraph\nAll the inputs and intermediary elements of the FX graph arefx.Nodes. In Dynamo,fx.Nodes are wrapped infx.Proxys.fx.Proxys are used to build the FX graph.\nIn particular, they record every PyTorch operation performed on them\ninto the graph. You can create a new operation to be added to\nthe graph by callingcreate_proxy.\nThen, we can add it to the graph through the functionwrap_fx_proxy.\nfx.Node\nfx.Node\nfx.Proxy\nfx.Proxy\nA graph stores operations on tensors\u2026 and operations on symbolic\nintegers. We will discuss symbolic integers later on, but first we will\ndiscuss how Dynamo addresses a rather important correctness issue.\n\n## Making Dynamo Sound: Guards#\n\nAt this point, we have a way to trace programs completely disregarding control flow.\nAnd for that, we have reimplemented all of CPython\u2026 If this sounds like a bit of an\noverkill, that is because it is.torch.jit.tracealready implements this without all this machinery, so what gives?\nThe issue withtorch.jit.trace, as it is warned in its docs, is that\nit just works if the traced program is not data dependent. In other\nwords, it will just work if the program itself is linear. This means\nwriting our program without using if-elses, for-while loops, exceptions.\nEven more, none of the libraries that we use can use any control flow!\nAll in all, not using control flow in a language as dynamic as Python\nis, in fact, a huge constraint.\ntorch.jit.trace\nJAX solves this problem by always retracing and caching the graph after\nretracing. Dynamo, on the other hand, uses guards to avoid retracing the\nwhole program every time.\nAguardis an assumption (a boolean expression on an input) made in\norder to specialize a frame for one set of example inputs. Reusing the\ngraph is only valid if these assumptions hold on the new inputs.\nFor example, any constant input to a function, like a string, installs a\nguard stating that that input should be of typestrand equal to the\nstring we passed. Running\nstr\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a * len(b)\n\nfn(torch.arange(10), \"Hello\")\n\n```\n\nwithTORCH_LOGS=guardsprints (among other guards)\nTORCH_LOGS=guards\n\n```python\n___check_type_id(L['b'], 94334122025024)\nL['b'] == 'Hello'\n\n```\n\nThis reads as \u201cthe local variablebshould have a specific type\n(strin this case, represented by the constant9433...) and\nits value should be'Hello'\u201d. If we then execute the function\nagain passing a different argument\nb\nstr\n9433...\n'Hello'\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a * len(b)\n\nfn(torch.arange(10), \"Hello\")\nfn(torch.arange(10), \"Hi\")\n\n```\n\nwe can see the guard that failed by runningTORCH_LOGS=recompiles\nTORCH_LOGS=recompiles\n\n```python\nRecompiling function fn in script.py:3\ntriggered by the following guard failure(s):\n     - L['b'] == 'Hello'\n\n```\n\nGuards are accumulated whilethe inputs to the function are wrapped in\nthe\nbuilderandduring the execution of the\nprogram.\nWe will show many more examples of guards in the next section, but first\nlet us discuss sources.\nAsourcetracks how to reconstruct a variable from the original\nlocal or global variables present when entering the current frame. In\nparticular, it tracks the original local and global objects and any of\nthe objects they contain. In\n\n```python\ndef foo(x: Tensor, y: List[Tensor]):\n    a = x * y[0]\n    return a * x\n\n```\n\nxandyhaveLocalSourceas their source, andy[0]hasGetItemSource,\nwhich stores aLocalSourceinside. On the other hand,awill not\nhave a source as it is an intermediate variable that only exists within\nthe fx graph.\nx\ny\ny[0]\nLocalSource\na\nAll these are defined intorch/_dynamo/source.py.\nWe can see the guard generated byGetItemSourcein the following\nexample:\nGetItemSource\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, l):\n    return x * len(l[0])\n\nfn(torch.randn(8), [\"Hi\", \"Hello\"])\n\n```\n\ngenerates the following guards\n\n```python\n___check_type_id(L['l'], 94439025877664)\nlen(L['l']) == 2\n___check_type_id(L['l'][0], 94439025840192)\nL['l'][0] == 'Hi'\n___check_type_id(L['l'][1], 94439025840192)\nL['l'][1] == 'Hello'\n\n```\n\nHere, we see the code generated byGetItemSource([0]and[1]) wrapping aLocalSource(L['l']).\nGetItemSource\n[0]\n[1]\nLocalSource\nL['l']\nAt this point, with sources and guards, we are able to implement a\ncaching system to avoid recompilation without having to retrace every\ntime. We will discuss a bit more in detail this caching system in the\nsequel.\nThe attentive reader will have noticed that this does not explain yet\nwhy we need to have such fine control over the Python interpreter as to\nhaving to reimplement it. The examples of guards that we have shown\ndepend on the input objects, so we could still compute these before\nexecuting the function. In other words, we could implement this guard\nsystem on top oftorch.jit.traceand get the same functionality with\nmuch less effort\u2026 Enter symbolic shapes.\ntorch.jit.trace\n\n## Symbolic Shapes#\n\nAnother point we discussed in the introduction is that Dynamo knows how\nto trace integers. In order to implement this, we use a symbolic classtorch.SymIntthat acts like anintbut it records all the operations performed on\nit in the output FX graph.4We already saw this class in the introduction\nwhen introducing symbolic integer tracing.\nint\nLet us now discuss the three properties that define symbolic shape\ntracing in Dynamo, and how to implement them.\n\n## Static by default#\n\nDynamo assumes that every integer, let that be an input or the shape of\na tensor, is static by default. In other words, no integers will be\ntraced on the first execution of a function. Then, only if it detects\nthat an integer or a shape changed value during the execution, it will\ntrace it and generate a graph generic on that variable.\nWe already saw this behavior in the introduction using integers. Let us\nnow look at an example using shapes of tensors.\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a.shape[0] * a * b\n\nfn(torch.randn(4, 3), torch.randn(4, 3))\nfn(torch.randn(8, 3), torch.randn(8, 3))\n\n```\n\nRunning this program withTORCH_LOGS=graph_codewe see that these\ntwo calls are traced as\nTORCH_LOGS=graph_code\n\n```python\ndef forward(self, l_a_: torch.Tensor, l_b_: torch.Tensor):\n    mul = 4 * l_a_\n    mul_1 = mul * l_b_\n    return (mul_1,)\n\ndef forward(self, s0: torch.SymInt, l_a_: torch.Tensor, l_b_: torch.Tensor):\n    size = l_a_.size()\n    getitem = size[0]\n    mul = getitem * l_a_\n    mul_1 = mul * l_b_\n    return (mul_1,)\n\n```\n\nIn the first graph the shape is traced as a constant, but once it\nchanges, it traces it symbolically using aSymInts. In general, a\nsimpler way to see the shapes of the intermediary values is by running\nthe program withTORCH_LOGS=graph_sizes\nSymInt\nTORCH_LOGS=graph_sizes\n\n```python\nTRACED GRAPH TENSOR SIZES\n===== __compiled_fn_1 =====\nl_a_: (s0, 3)\nl_a_ (concrete): (8, 3)\nl_b_: (s0, 3)\nl_b_ (concrete): (8, 3)\nmul: (s0, 3)\nmul (concrete): (8, 3)\nmul_1: (s0, 3)\nmul_1 (concrete): (8, 3)\n\n```\n\nwhere we can see that the first dimension of the two tensor args is\ndynamic, given that it is represented by thes0variable.\ns0\nWe can find how Dynamo implements this by runningTORCH_LOGS=guards\nTORCH_LOGS=guards\n\n```python\n# Guards first call\ncheck_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])\ncheck_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])\n\n# Guards second call\ncheck_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])\ncheck_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])\n\nL['b'].size()[0] == L['a'].size()[0]\n2 <= L['a'].size()[0]\n\n```\n\nWe see that on the first call, the guards check that the tensors have\nsome fixed sizes and strides. These guards fail in the second execution,\nso it retraces. Since it was anintguard that failed, in this\nsecond iteration it traces thisintsymbolically and it installs\nmore general guards on this more generic kernel.\nint\nint\nCompilation performance tip. If you know that a dimension will vary\nin size, you can mark it as dynamic by callingtorch._dynamo.mark_dynamicbefore callingtorch.compile. This will avoid the first compilation\nwith a static shape. There are other useful utility functions likemaybe_mark_dynamicormark_static. You can also have all\nintegers and shapes traced by callingtorch.compile(dynamic=True).\nThis is mostly useful for debugging purposes.\ntorch.compile\nmaybe_mark_dynamic\nmark_static\ntorch.compile(dynamic=True)\n\n## 0, 1 are always specialized#\n\nRegardless of whether we mark a dimension as dynamic, if we pass an input\nwhere that dimension is 0 or 1, Dynamo will trace it as non-dynamic and it\nwill generate a specific graph for it. This is the reason why in the example\nabove we find guards of the form2<=L['a'].size()[0].\n2<=L['a'].size()[0]\nThere are several reasons for this choice. There are two particularly\nimportant - A tensor is empty if and only if any of its dimensions is\nzero - A tensor can only be contiguous if one of the strides is one\nThis policy decision does NOT apply to plain Python ints; if we think a Python\nint should be compiled dynamically, we won\u2019t specialize them by default;\ninstead, whether or not it gets specialized depends on its usage.\n\n## Duck shaping#\n\nDynamo performs what we call \u201cduck shaping\u201d. If two dynamic integers\nhave the same value at trace time, we will assume that they are equal\nand guard on it. Effectively, this means that rather than having two\nsymbolss0,s1in the example above, we just unified them tos0and had the guardL['b'].size()[0]==L['a'].size()[0]. This\nenables performing fusions within the compiler while being able to\ngenerate kernels that are generic enough.\ns0\ns1\ns0\nL['b'].size()[0]==L['a'].size()[0]\n\n## Guards on symbolic ints#\n\nWe now understand how symbolic shapes are implemented at a high level\nand the properties they have. Now, why is that symbolic shapes forced us\nthrough the tricky route of getting control of the CPython interpreter?\nConsider the following example:\n\n```python\nimport torch\n\n@torch.compile(dynamic=True)\ndef fn(a):\n    if a.shape[0] * 2 < 16:\n        return a\n    else:\n        return a + 1\n\nfn(torch.randn(8))\n\n```\n\nThis code has a guard of the form2*L['a'].size()[0]>=16. This is\na non-trivial guard in terms of the inputs of the function, but it is\nregistered in the middle of the execution of the program. Even more so,\nwe cannot know this guard is needed until we see theifstatement\nconditional on aSymNodeVariableargument. Such conditions are\ninvisible totorch.jit.traceand require deep analysis of the python\ncode.\n2*L['a'].size()[0]>=16\nif\nSymNodeVariable\ntorch.jit.trace\nDebugging tipRunning this code withTORCH_LOGS=dynamotells us\nwhere this guard was added\nTORCH_LOGS=dynamo\n\n```python\neval 2*s0 >= 16 [guard added] at script.py:5 in fn (_dynamo/variables/tensor.py:812 in evaluate_expr)\n\n```\n\nPlacing a breakpoint there and looking at the backtrace is rather useful\nto understand where a guard came from.\n\n## Making Dynamo Complete: Graph Breaks#\n\nWith all the tools we have discussed, we have a tracer that can trace\nPyTorch operations on tensors and integers and has a caching system that\nknows when it can reuse a previously traced graph and when it needs to\nretrace. All this executing arbitrary Python code!\nThere is just one small issue with this. The statement \u201cexecuting\narbitrary Python code\u201d is perhaps a bit too general. Dynamo implements a\ngood part of Python, but does it implement the more complex parts, like\ncoroutines or async? Does it implement the whole Python standard\nlibrary? NumPy also has a Python API. Doestorch.compilealso\nunderstand NumPy? and Django?5\ntorch.compile\nPython\u2019s ecosystem is massive, and a good part of it is written in other\nmore performant languages like C++ or Rust, and it just exposes Python\nbindings. There is no hope in Dynamo tracing through Python objects that\nare implemented in C++. What can a tracer do when it finds an operation\nthat it does not understand?\nThe usual way machine learning tracers handle this issue is by informing\nthe user that the operation they choked on and giving up tracing\naltogether. This would pose a real usability issue in the case of\nPyTorch, where its users are used to the flexibility it gives them. As a\nreal-world example thedoctr_det_predictormodel uses NumPy and thecv2library topostprocess the model\u2019s\nresult.\ndoctr_det_predictor\ncv2\nHere is another place where having access to CPython is interesting.\nRather than erroring out, Dynamo can let CPython run that problematic\ncode! To do this, Dynamo generates at trace time one graph with all the\noperations before the problematic code, and one with all the operations\nafter.6Then, at runtime, it will delegate to CPython to execute the\nfirst graph, then the problematic code, and then the second graph. This\nprocess of stopping the tracing and generating multiple graphs is called\nagraph break.\nA small confession: I lied all throughout the introduction and the first\nsections. Dynamo does not generate one graph, butmultiple graphs!\nFor all practical purposes, starting retracing after a second graph can\nbe thought of as starting tracing a new function. The new graph after\nthe graph break will have its own guards, its new set of local\nvariables, and so on.\nTo discuss how to implement graph breaks, we need to first revisit how\nDynamo interacts with CPython. Using PEP 523, CPython allows a user to\nuse their own frame evaluation mechanism. What we had not discussed is\nthat CPython also exposes its own frame evaluation for others to use.\nDynamo leverages this to let the fast CPython interpreter run the\ncompiled code. For a function without graph breaks, the whole tracing /\nexecution process of a program that calls the function 2 times with the\nsame arguments looks like this:\nIn the first call to the function\nDynamo traces the function into an FX graph\nThe FX graph is compiled by the compiler (Inductor) into\nefficient low-level code\u2026 but that\u2019s a story for another day\nIt rewrites the bytecode of the function so that it simply calls\nthe compiled function\nIt gives CPython this new bytecode and asks it to run ithere\nIn the second call to the function\nIt checks the guards from the first call against the new argumentshere.\nSince they are the same arguments as before, they pass\nIt asks CPython to run the bytecode associated to those guardshere\nThis process on its own looks overly complicated. Why generate new\nbytecode and ask CPython to run it rather than simply creating a C++\nbinding to the compiled function and executing it? Well, this pattern\nallows us to implement graph breaks! The bytecode generated by a graph\nbreak has the following structure:\nBytecode that executes the first graph\nBytecode that leaves the stack as it would be if CPython would have\nexecuted the first graph. It also replays any modifications to local\nor global variables that would be visible at this point\nThe bytecode that made Dynamo graph break\nBytecode that executes the second graph\nLet us see this in a simple example\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a):\n    b = a + 2\n    print(\"Hi\")\n    return b + a\n\nfn(torch.randn(4))\n\n```\n\nRunning this withTORCH_LOGS=bytecodeshows us the initial bytecode\nand the modified bytecode\nTORCH_LOGS=bytecode\n\n```python\nMODIFIED BYTECODE fn script.py line 3\n 0 LOAD_GLOBAL              1 (__compiled_fn_0)\n 2 LOAD_FAST                0 (a)\n 4 CALL_FUNCTION            1\n 6 STORE_FAST               3 (graph_out_0)\n 8 LOAD_GLOBAL              0 (print)\n10 LOAD_CONST               2 ('Hi')\n12 LOAD_FAST                3 (graph_out_0)\n14 LOAD_CONST               3 (0)\n16 BINARY_SUBSCR\n18 STORE_FAST               1 (b)\n\n20 CALL_FUNCTION            1\n22 LOAD_GLOBAL              2 (__resume_at_14_1)\n24 ROT_TWO\n26 LOAD_FAST                0 (a)\n28 LOAD_FAST                1 (b)\n30 CALL_FUNCTION            3\n32 RETURN_VALUE\n\nMODIFIED BYTECODE resume_in_fn script.py line 6\n 0 LOAD_GLOBAL              1 (__compiled_fn_2)\n 2 LOAD_FAST                2 (b)\n 4 LOAD_FAST                1 (a)\n 6 CALL_FUNCTION            2\n 8 UNPACK_SEQUENCE          1\n10 RETURN_VALUE\n\n```\n\nWe can see that the modified bytecode is split into two functions,fn, the original function, and a function calledresume_in_fn.\nThis second function is a function created by Dynamo to implement the\nexecution of the program starting at the graph break. This is often\ncalled acontinuation\nfunction. This\ncontinuation function simply calls the second compiled function with the\nright arguments. The code for the initial function is rewritten\nimplementing the strategy that we described before\nfn\nresume_in_fn\nL0-4. Call the compiled function (a+2).\na+2\nL6. Store its result in a local variable calledgraph_out_0.graph_out_0is a tuple\ngraph_out_0\ngraph_out_0\nL8-18. Leave the stack as it would be at the point of the graph break\nL20. Execute the code that caused the graph break\nL22-32. Call the compiled continuation function (a+b)\na+b\nThe code generation of the stack in Dynamo is delegated toVariableTrackersubclasses. EveryVariableTrackerobject in\nDynamo has areconstructmethod that generates the necessary bytecode to create the python object\nit represents on the stack.\nVariableTracker\nVariableTracker\nDebugging tip. Graph breaks hamper performance, and as such, it is\nbest to avoid them. Running a program withTORCH_LOGS=graph_breaksis a great way to find how many graph breaks did our program hit. The\ninformation it returns is in terms ofVariableTrackerobjects, so\nthe debugging tips above are sometimes also helpful to figure out what\ncaused that graph break.\nTORCH_LOGS=graph_breaks\nVariableTracker\n\n## Conclusion#\n\nDynamo is a complex piece of software. Once you sign up to implement a\nCPython interpreter you know you are in for a ride. That being said, we\nhope that this post helps demystify it a bit.\nDynamo is (mostly) implemented in Python. We left plenty of links to the\npieces of the code that we discussed. We hope that reading those pieces\nof code and grepping for the places that call them, or putting\nbreakpoints on them and looking at the call stack helps understanding\nthe rest of the code base.\nOf course, the best way to learn how a piece of software works is by\nextending it. In this case, the best way is to have a look at theopen\ndynamo issues on\ngithub.\nMany of them require very minor changes in the code, once you find where\nyou need to make those changes.\n\n## Footnotes#\n\nBelow are additional details and references for concepts mentioned in this document.\nIn the literature, this is called a Directed Acyclical Graph (DAG).\nAll this binding code lives intorch/csrc/dynamo/eval_frame.c.\ntorch/csrc/dynamo/eval_frame.c\nIn CPython lingo, the set of all these objects are calleda\nframe.\nThere are alsoSymBoolandSymFloatclasses. The latter one\nis not used all that much at the time of this writing.\nSymBool\nSymFloat\nInterestingly enough, it does understand NumPy code! Have a look atthis blogpostandthe docs.\nNow, this is just possible because we reimplemented NumPy using\nPyTorch. Good luck implementing Django in PyTorch though\u2026\nAssuming there is just one piece of problematic code. If there are\nmore, Dynamo can split the code into as many graphs as it needs.",
    "url": "https://pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "6c7a97cab84b3764862b205ae02807d9",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/notes/autograd.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "020079abaca4d74e17cc709fb5d3287b",
    "source": "pytorch_docs",
    "title": "FullyShardedDataParallel \u2014 PyTorch 2.9 documentation",
    "text": "\n## FullyShardedDataParallel#\n\nCreated On: Feb 02, 2022 | Last Updated On: Jun 11, 2025\nA wrapper for sharding module parameters across data parallel workers.\nThis is inspired byXu et al.as\nwell as the ZeRO Stage 3 fromDeepSpeed.\nFullyShardedDataParallel is commonly shortened to FSDP.\nExample:\n\n```python\n>>> import torch\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> torch.cuda.set_device(device_id)\n>>> sharded_module = FSDP(my_module)\n>>> optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)\n>>> x = sharded_module(x, y=3, z=torch.Tensor([1]))\n>>> loss = x.sum()\n>>> loss.backward()\n>>> optim.step()\n\n```\n\nUsing FSDP involves wrapping your module and then initializing your\noptimizer after. This is required since FSDP changes the parameter\nvariables.\nWhen setting up FSDP, you need to consider the destination CUDA\ndevice. If the device has an ID (dev_id), you have three options:\ndev_id\nPlace the module on that device\nSet the device usingtorch.cuda.set_device(dev_id)\ntorch.cuda.set_device(dev_id)\nPassdev_idinto thedevice_idconstructor argument.\ndev_id\ndevice_id\nThis ensures that the FSDP instance\u2019s compute device is the\ndestination device. For option 1 and 3, the FSDP initialization\nalways occurs on GPU. For option 2, the FSDP initialization\nhappens on module\u2019s current device, which may be a CPU.\nIf you\u2019re using thesync_module_states=Trueflag, you need to\nensure that the module is on a GPU or use thedevice_idargument to specify a CUDA device that FSDP will move the module\nto in the FSDP constructor. This is necessary becausesync_module_states=Truerequires GPU communication.\nsync_module_states=True\ndevice_id\nsync_module_states=True\nFSDP also takes care of moving input tensors to the forward method\nto the GPU compute device, so you don\u2019t need to manually move them\nfrom CPU.\nForuse_orig_params=True,ShardingStrategy.SHARD_GRAD_OPexposes the unsharded\nparameters, not the sharded parameters after forward, unlikeShardingStrategy.FULL_SHARD. If you want\nto inspect the gradients, you can use thesummon_full_paramsmethod withwith_grads=True.\nuse_orig_params=True\nShardingStrategy.SHARD_GRAD_OP\nShardingStrategy.FULL_SHARD\nsummon_full_params\nwith_grads=True\nWithlimit_all_gathers=True, you may see a gap in the FSDP\npre-forward where the CPU thread is not issuing any kernels. This is\nintentional and shows the rate limiter in effect. Synchronizing the CPU\nthread in that way prevents over-allocating memory for subsequent\nall-gathers, and it should not actually delay GPU kernel execution.\nlimit_all_gathers=True\nFSDP replaces managed modules\u2019 parameters withtorch.Tensorviews during forward and backward computation for autograd-related\nreasons. If your module\u2019s forward relies on saved references to\nthe parameters instead of reacquiring the references each\niteration, then it will not see FSDP\u2019s newly created views,\nand autograd will not work correctly.\ntorch.Tensor\nFinally, when usingsharding_strategy=ShardingStrategy.HYBRID_SHARDwith the sharding process group being intra-node and the\nreplication process group being inter-node, settingNCCL_CROSS_NIC=1can help improve the all-reduce times over\nthe replication process group for some cluster setups.\nsharding_strategy=ShardingStrategy.HYBRID_SHARD\nNCCL_CROSS_NIC=1\nLimitations\nThere are several limitations to be aware of when using FSDP:\nFSDP currently does not support gradient accumulation outsideno_sync()when using CPU offloading. This is because FSDP\nuses the newly-reduced gradient instead of accumulating with any\nexisting gradient, which can lead to incorrect results.\nno_sync()\nFSDP does not support running the forward pass of a submodule\nthat is contained in an FSDP instance. This is because the\nsubmodule\u2019s parameters will be sharded, but the submodule itself\nis not an FSDP instance, so its forward pass will not all-gather\nthe full parameters appropriately.\nFSDP does not work with double backwards due to the way it\nregisters backward hooks.\nFSDP has some constraints when freezing parameters.\nForuse_orig_params=False, each FSDP instance must manage\nparameters that are all frozen or all non-frozen. Foruse_orig_params=True, FSDP supports mixing frozen and\nnon-frozen parameters, but it\u2019s recommended to avoid doing so to\nprevent higher than expected gradient memory usage.\nuse_orig_params=False\nuse_orig_params=True\nAs of PyTorch 1.12, FSDP offers limited support for shared\nparameters. If enhanced shared parameter support is needed for\nyour use case, please post inthis issue.\nYou should avoid modifying the parameters between forward and\nbackward without using thesummon_full_paramscontext, as\nthe modifications may not persist.\nsummon_full_params\nmodule(nn.Module) \u2013 This is the module to be wrapped with FSDP.\nprocess_group(Optional[Union[ProcessGroup,Tuple[ProcessGroup,ProcessGroup]]]) \u2013 This is the process group over which the model is sharded and thus\nthe one used for FSDP\u2019s all-gather and reduce-scatter collective\ncommunications. IfNone, then FSDP uses the default process\ngroup. For hybrid sharding strategies such asShardingStrategy.HYBRID_SHARD, users can pass in a tuple of\nprocess groups, representing the groups over which to shard and\nreplicate, respectively. IfNone, then FSDP constructs process\ngroups for the user to shard intra-node and replicate inter-node.\n(Default:None)\nNone\nShardingStrategy.HYBRID_SHARD\nNone\nNone\nsharding_strategy(Optional[ShardingStrategy]) \u2013 This configures the sharding strategy, which may trade off memory\nsaving and communication overhead. SeeShardingStrategyfor details. (Default:FULL_SHARD)\nShardingStrategy\nFULL_SHARD\ncpu_offload(Optional[CPUOffload]) \u2013 This configures CPU offloading. If this is set toNone, then\nno CPU offloading happens. SeeCPUOffloadfor details.\n(Default:None)\nNone\nCPUOffload\nNone\nauto_wrap_policy(Optional[Union[Callable[[nn.Module,bool,int],bool],ModuleWrapPolicy,CustomPolicy]]) \u2013This specifies a policy to apply FSDP to submodules ofmodule,\nwhich is needed for communication and computation overlap and thus\naffects performance. IfNone, then FSDP only applies tomodule, and users should manually apply FSDP to parent modules\nthemselves (proceeding bottom-up). For convenience, this acceptsModuleWrapPolicydirectly, which allows users to specify the\nmodule classes to wrap (e.g. the transformer block). Otherwise,\nthis should be a callable that takes in three argumentsmodule:nn.Module,recurse:bool, andnonwrapped_numel:intand should return aboolspecifying\nwhether the passed-inmoduleshould have FSDP applied ifrecurse=Falseor if the traversal should continue into the\nmodule\u2019s subtree ifrecurse=True. Users may add additional\narguments to the callable. Thesize_based_auto_wrap_policyintorch.distributed.fsdp.wrap.pygives an example callable that\napplies FSDP to a module if the parameters in its subtree exceed\n100M numel. We recommend printing the model after applying FSDP\nand adjusting as needed.Example:>>>defcustom_auto_wrap_policy(>>>module:nn.Module,>>>recurse:bool,>>>nonwrapped_numel:int,>>># Additional custom arguments>>>min_num_params:int=int(1e8),>>>)->bool:>>>returnnonwrapped_numel>=min_num_params>>># Configure a custom `min_num_params`>>>my_auto_wrap_policy=functools.partial(custom_auto_wrap_policy,min_num_params=int(1e5))\nThis specifies a policy to apply FSDP to submodules ofmodule,\nwhich is needed for communication and computation overlap and thus\naffects performance. IfNone, then FSDP only applies tomodule, and users should manually apply FSDP to parent modules\nthemselves (proceeding bottom-up). For convenience, this acceptsModuleWrapPolicydirectly, which allows users to specify the\nmodule classes to wrap (e.g. the transformer block). Otherwise,\nthis should be a callable that takes in three argumentsmodule:nn.Module,recurse:bool, andnonwrapped_numel:intand should return aboolspecifying\nwhether the passed-inmoduleshould have FSDP applied ifrecurse=Falseor if the traversal should continue into the\nmodule\u2019s subtree ifrecurse=True. Users may add additional\narguments to the callable. Thesize_based_auto_wrap_policyintorch.distributed.fsdp.wrap.pygives an example callable that\napplies FSDP to a module if the parameters in its subtree exceed\n100M numel. We recommend printing the model after applying FSDP\nand adjusting as needed.\nmodule\nNone\nmodule\nModuleWrapPolicy\nmodule:nn.Module\nrecurse:bool\nnonwrapped_numel:int\nbool\nmodule\nrecurse=False\nrecurse=True\nsize_based_auto_wrap_policy\ntorch.distributed.fsdp.wrap.py\nExample:\n\n```python\n>>> def custom_auto_wrap_policy(\n>>>     module: nn.Module,\n>>>     recurse: bool,\n>>>     nonwrapped_numel: int,\n>>>     # Additional custom arguments\n>>>     min_num_params: int = int(1e8),\n>>> ) -> bool:\n>>>     return nonwrapped_numel >= min_num_params\n>>> # Configure a custom `min_num_params`\n>>> my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=int(1e5))\n\n```\n\nbackward_prefetch(Optional[BackwardPrefetch]) \u2013 This configures explicit backward prefetching of all-gathers. IfNone, then FSDP does not backward prefetch, and there is no\ncommunication and computation overlap in the backward pass. SeeBackwardPrefetchfor details. (Default:BACKWARD_PRE)\nNone\nBackwardPrefetch\nBACKWARD_PRE\nmixed_precision(Optional[MixedPrecision]) \u2013 This configures native mixed precision for FSDP. If this is set toNone, then no mixed precision is used. Otherwise, parameter,\nbuffer, and gradient reduction dtypes can be set. SeeMixedPrecisionfor details. (Default:None)\nNone\nMixedPrecision\nNone\nignored_modules(Optional[Iterable[torch.nn.Module]]) \u2013 Modules whose\nown parameters and child modules\u2019 parameters and buffers are\nignored by this instance. None of the modules directly inignored_modulesshould beFullyShardedDataParallelinstances, and any child modules that are already-constructedFullyShardedDataParallelinstances will not be ignored if\nthey are nested under this instance. This argument may be used to\navoid sharding specific parameters at module granularity when using anauto_wrap_policyor if parameters\u2019 sharding is not managed by\nFSDP. (Default:None)\nignored_modules\nFullyShardedDataParallel\nFullyShardedDataParallel\nauto_wrap_policy\nNone\nparam_init_fn(Optional[Callable[[nn.Module],None]]) \u2013ACallable[torch.nn.Module]->Nonethat\nspecifies how modules that are currently on the meta device should\nbe initialized onto an actual device. As of v1.12, FSDP detects\nmodules with parameters or buffers on meta device viais_metaand either appliesparam_init_fnif specified or callsnn.Module.reset_parameters()otherwise. For both cases, the\nimplementation shouldonlyinitialize the parameters/buffers of\nthe module, not those of its submodules. This is to avoid\nre-initialization. In addition, FSDP also supports deferred\ninitialization via torchdistX\u2019s (pytorch/torchdistX)deferred_init()API, where the deferred modules are initialized\nby callingparam_init_fnif specified or torchdistX\u2019s defaultmaterialize_module()otherwise. Ifparam_init_fnis\nspecified, then it is applied to all meta-device modules, meaning\nthat it should probably case on the module type. FSDP calls the\ninitialization function before parameter flattening and sharding.Example:>>>module=MyModule(device=\"meta\")>>>defmy_init_fn(module:nn.Module):>>># E.g. initialize depending on the module type>>>...>>>fsdp_model=FSDP(module,param_init_fn=my_init_fn,auto_wrap_policy=size_based_auto_wrap_policy)>>>print(next(fsdp_model.parameters()).device)# current CUDA device>>># With torchdistX>>>module=deferred_init.deferred_init(MyModule,device=\"cuda\")>>># Will initialize via deferred_init.materialize_module().>>>fsdp_model=FSDP(module,auto_wrap_policy=size_based_auto_wrap_policy)\nACallable[torch.nn.Module]->Nonethat\nspecifies how modules that are currently on the meta device should\nbe initialized onto an actual device. As of v1.12, FSDP detects\nmodules with parameters or buffers on meta device viais_metaand either appliesparam_init_fnif specified or callsnn.Module.reset_parameters()otherwise. For both cases, the\nimplementation shouldonlyinitialize the parameters/buffers of\nthe module, not those of its submodules. This is to avoid\nre-initialization. In addition, FSDP also supports deferred\ninitialization via torchdistX\u2019s (pytorch/torchdistX)deferred_init()API, where the deferred modules are initialized\nby callingparam_init_fnif specified or torchdistX\u2019s defaultmaterialize_module()otherwise. Ifparam_init_fnis\nspecified, then it is applied to all meta-device modules, meaning\nthat it should probably case on the module type. FSDP calls the\ninitialization function before parameter flattening and sharding.\nCallable[torch.nn.Module]->None\nis_meta\nparam_init_fn\nnn.Module.reset_parameters()\ndeferred_init()\nparam_init_fn\nmaterialize_module()\nparam_init_fn\nExample:\n\n```python\n>>> module = MyModule(device=\"meta\")\n>>> def my_init_fn(module: nn.Module):\n>>>     # E.g. initialize depending on the module type\n>>>     ...\n>>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)\n>>> print(next(fsdp_model.parameters()).device) # current CUDA device\n>>> # With torchdistX\n>>> module = deferred_init.deferred_init(MyModule, device=\"cuda\")\n>>> # Will initialize via deferred_init.materialize_module().\n>>> fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)\n\n```\n\ndevice_id(Optional[Union[int,torch.device]]) \u2013 Anintortorch.devicegiving the CUDA device on which FSDP\ninitialization takes place, including the module initialization\nif needed and the parameter sharding. This should be specified to\nimprove initialization speed ifmoduleis on CPU. If the\ndefault CUDA device was set (e.g. viatorch.cuda.set_device),\nthen the user may passtorch.cuda.current_deviceto this.\n(Default:None)\nint\ntorch.device\nmodule\ntorch.cuda.set_device\ntorch.cuda.current_device\nNone\nsync_module_states(bool) \u2013 IfTrue, then each FSDP module will\nbroadcast module parameters and buffers from rank 0 to ensure that\nthey are replicated across ranks (adding communication overhead to\nthis constructor). This can help loadstate_dictcheckpoints\nviaload_state_dictin a memory efficient way. SeeFullStateDictConfigfor an example of this. (Default:False)\nTrue\nstate_dict\nload_state_dict\nFullStateDictConfig\nFalse\nforward_prefetch(bool) \u2013 IfTrue, then FSDPexplicitlyprefetches\nthe next forward-pass all-gather before the current forward\ncomputation. This is only useful for CPU-bound workloads, in which\ncase issuing the next all-gather earlier may improve overlap. This\nshould only be used for static-graph models since the prefetching\nfollows the first iteration\u2019s execution order. (Default:False)\nTrue\nFalse\nlimit_all_gathers(bool) \u2013 IfTrue, then FSDP explicitly\nsynchronizes the CPU thread to ensure GPU memory usage from onlytwoconsecutive FSDP instances (the current instance running\ncomputation and the next instance whose all-gather is prefetched).\nIfFalse, then FSDP allows the CPU thread to issue all-gathers\nwithout any extra synchronization. (Default:True) We often\nrefer to this feature as the \u201crate limiter\u201d. This flag should only\nbe set toFalsefor specific CPU-bound workloads with low\nmemory pressure in which case the CPU thread can aggressively issue\nall kernels without concern for the GPU memory usage.\nTrue\nFalse\nTrue\nFalse\nuse_orig_params(bool) \u2013 Setting this toTruehas FSDP usemodule\u2018s original parameters. FSDP exposes those original\nparameters to the user viann.Module.named_parameters()instead of FSDP\u2019s internalFlatParameters. This means\nthat the optimizer step runs on the original parameters, enabling\nper-original-parameter hyperparameters. FSDP preserves the original\nparameter variables and manipulates their data between unsharded\nand sharded forms, where they are always views into the underlying\nunsharded or shardedFlatParameter, respectively. With the\ncurrent algorithm, the sharded form is always 1D, losing the\noriginal tensor structure. An original parameter may have all,\nsome, or none of its data present for a given rank. In the none\ncase, its data will be like a size-0 empty tensor. Users should not\nauthor programs relying on what data is present for a given\noriginal parameter in its sharded form.Trueis required to\nusetorch.compile(). Setting this toFalseexposes FSDP\u2019s\ninternalFlatParameters to the user viann.Module.named_parameters(). (Default:False)\nTrue\nmodule\nnn.Module.named_parameters()\nFlatParameter\nFlatParameter\nTrue\ntorch.compile()\nFalse\nFlatParameter\nnn.Module.named_parameters()\nFalse\nignored_states(Optional[Iterable[torch.nn.Parameter]],Optional[Iterable[torch.nn.Module]]) \u2013 Ignored parameters or modules that will not be managed by this FSDP\ninstance, meaning that the parameters are not sharded and their\ngradients are not reduced across ranks. This argument unifies with\nthe existingignored_modulesargument, and we may deprecateignored_modulessoon. For backward compatibility, we keep bothignored_statesandignored_modules`, but FSDP only allows one\nof them to be specified as notNone.\nignored_modules\nignored_modules\nignored_states\nNone\ndevice_mesh(Optional[DeviceMesh]) \u2013 DeviceMesh can be used as an alternative to\nprocess_group. When device_mesh is passed, FSDP will use the underlying process\ngroups for all-gather and reduce-scatter collective communications. Therefore,\nthese two args need to be mutually exclusive. For hybrid sharding strategies such asShardingStrategy.HYBRID_SHARD, users can pass in a 2D DeviceMesh instead\nof a tuple of process groups. For 2D FSDP + TP, users are required to pass in\ndevice_mesh instead of process_group. For more DeviceMesh info, please visit:https://pytorch.org/tutorials/recipes/distributed_device_mesh.html\nShardingStrategy.HYBRID_SHARD\nApplyfnrecursively to every submodule (as returned by.children()) as well as self.\nfn\n.children()\nTypical use includes initializing the parameters of a model (see alsotorch.nn.init).\nCompared totorch.nn.Module.apply, this version additionally gathers\nthe full parameters before applyingfn. It should not be called from\nwithin anothersummon_full_paramscontext.\ntorch.nn.Module.apply\nfn\nsummon_full_params\nfn(Module-> None) \u2013 function to be applied to each submodule\nModule\nself\nModule\n\nCheck if this instance is a root FSDP module.\nbool\nClip the gradient norm of all parameters.\nThe norm is computed over all parameters\u2019 gradients as viewed as a single vector, and the\ngradients are modified in-place.\nmax_norm(floatorint) \u2013 max norm of the gradients\nnorm_type(floatorint) \u2013 type of the used p-norm. Can be'inf'for infinity norm.\n'inf'\nTotal norm of the parameters (viewed as a single vector).\nTensor\nIf every FSDP instance usesNO_SHARD, meaning that no\ngradients are sharded across ranks, then you may directly usetorch.nn.utils.clip_grad_norm_().\nNO_SHARD\ntorch.nn.utils.clip_grad_norm_()\nIf at least some FSDP instance uses a sharded strategy (i.e.\none other thanNO_SHARD), then you should use this method\ninstead oftorch.nn.utils.clip_grad_norm_()since this method\nhandles the fact that gradients are sharded across ranks.\nNO_SHARD\ntorch.nn.utils.clip_grad_norm_()\nThe total norm returned will have the \u201clargest\u201d dtype across\nall parameters/gradients as defined by PyTorch\u2019s type promotion\nsemantics. For example, ifallparameters/gradients use a low\nprecision dtype, then the returned norm\u2019s dtype will be that low\nprecision dtype, but if there exists at least one parameter/\ngradient using FP32, then the returned norm\u2019s dtype will be FP32.\nWarning\nThis needs to be called on all ranks since it uses\ncollective communications.\nFlatten a sharded optimizer state-dict.\nThe API is similar toshard_full_optim_state_dict(). The only\ndifference is that the inputsharded_optim_state_dictshould be\nreturned fromsharded_optim_state_dict(). Therefore, there will\nbe all-gather calls on each rank to gatherShardedTensors.\nshard_full_optim_state_dict()\nsharded_optim_state_dict\nsharded_optim_state_dict()\nShardedTensor\nsharded_optim_state_dict(Dict[str,Any]) \u2013 Optimizer state dict\ncorresponding to the unflattened parameters and holding the\nsharded optimizer state.\nmodel(torch.nn.Module) \u2013 Refer toshard_full_optim_state_dict().\nshard_full_optim_state_dict()\noptim(torch.optim.Optimizer) \u2013 Optimizer formodel\u2018s\nparameters.\nmodel\nRefer toshard_full_optim_state_dict().\nshard_full_optim_state_dict()\ndict[str,Any]\nRun the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.\nAny\nReturn all nested FSDP instances.\nThis possibly includesmoduleitself and only includes FSDP root modules ifroot_only=True.\nmodule\nroot_only=True\nmodule(torch.nn.Module) \u2013 Root module, which may or may not be anFSDPmodule.\nFSDP\nroot_only(bool) \u2013 Whether to return only FSDP root modules.\n(Default:False)\nFalse\nFSDP modules that are nested in\nthe inputmodule.\nmodule\nList[FullyShardedDataParallel]\nReturn the full optimizer state-dict.\nConsolidates the full optimizer state on rank 0 and returns it\nas adictfollowing the convention oftorch.optim.Optimizer.state_dict(), i.e. with keys\"state\"and\"param_groups\". The flattened parameters inFSDPmodules\ncontained inmodelare mapped back to their unflattened parameters.\ndict\ntorch.optim.Optimizer.state_dict()\n\"state\"\n\"param_groups\"\nFSDP\nmodel\nThis needs to be called on all ranks since it uses\ncollective communications. However, ifrank0_only=True, then\nthe state dict is only populated on rank 0, and all other ranks\nreturn an emptydict.\nrank0_only=True\ndict\nUnliketorch.optim.Optimizer.state_dict(), this method\nuses full parameter names as keys instead of parameter IDs.\ntorch.optim.Optimizer.state_dict()\nLike intorch.optim.Optimizer.state_dict(), the tensors\ncontained in the optimizer state dict are not cloned, so there may\nbe aliasing surprises. For best practices, consider saving the\nreturned optimizer state dict immediately, e.g. usingtorch.save().\ntorch.optim.Optimizer.state_dict()\ntorch.save()\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\nwere passed into the optimizeroptim.\nFullyShardedDataParallel\noptim\noptim(torch.optim.Optimizer) \u2013 Optimizer formodel\u2018s\nparameters.\nmodel\noptim_input(Optional[Union[List[Dict[str,Any]],Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizeroptimrepresenting either alistof parameter groups or an iterable of parameters;\nifNone, then this method assumes the input wasmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default:None)\noptim\nlist\nNone\nmodel.parameters()\nNone\nrank0_only(bool) \u2013 IfTrue, saves the populateddictonly on rank 0; ifFalse, saves it on all ranks. (Default:True)\nTrue\ndict\nFalse\nTrue\ngroup(dist.ProcessGroup) \u2013 Model\u2019s process group orNoneif using\nthe default process group. (Default:None)\nNone\nNone\nAdictcontaining the optimizer state formodel\u2018s original unflattened parameters and including keys\n\u201cstate\u201d and \u201cparam_groups\u201d following the convention oftorch.optim.Optimizer.state_dict(). Ifrank0_only=True,\nthen nonzero ranks return an emptydict.\ndict\nmodel\ntorch.optim.Optimizer.state_dict()\nrank0_only=True\ndict\nDict[str, Any]\nGet the state_dict_type and the corresponding configurations for the FSDP modules rooted atmodule.\nmodule\nThe target module does not have to be an FSDP module.\nAStateDictSettingscontaining the state_dict_type and\nstate_dict / optim_state_dict configs that are currently set.\nStateDictSettings\nAssertionError` if the StateDictSettings for differen\u2013\nFSDP submodules differ.\u2013\nStateDictSettings\nReturn the wrapped module.\nReturn an iterator over module buffers, yielding both the name of the buffer and the buffer itself.\nIntercepts buffer names and removes all occurrences of the FSDP-specific flattened buffer prefix\nwhen inside thesummon_full_params()context manager.\nsummon_full_params()\nIterator[tuple[str,torch.Tensor]]\nReturn an iterator over module parameters, yielding both the name of the parameter and the parameter itself.\nIntercepts parameter names and removes all occurrences of the FSDP-specific flattened parameter prefix\nwhen inside thesummon_full_params()context manager.\nsummon_full_params()\nIterator[tuple[str,torch.nn.parameter.Parameter]]\nDisable gradient synchronizations across FSDP instances.\nWithin this context, gradients will be accumulated in module\nvariables, which will later be synchronized in the first\nforward-backward pass after exiting the context. This should only be\nused on the root FSDP instance and will recursively apply to all\nchildren FSDP instances.\nNote\nThis likely results in higher memory usage because FSDP will\naccumulate the full model gradients (instead of gradient shards)\nuntil the eventual sync.\nNote\nWhen used with CPU offloading, the gradients will not be\noffloaded to CPU when inside the context manager. Instead, they\nwill only be offloaded right after the eventual sync.\nGenerator\nTransform the state-dict of an optimizer corresponding to a sharded model.\nThe given state-dict can be transformed to one of three types:\n1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.\nFor full optimizer state_dict, all states are unflattened and not sharded.\nRank0 only and CPU only can be specified viastate_dict_type()to\navoid OOM.\nstate_dict_type()\nFor sharded optimizer state_dict, all states are unflattened but sharded.\nCPU only can be specified viastate_dict_type()to further save\nmemory.\nstate_dict_type()\nFor local state_dict, no transformation will be performed. But a state\nwill be converted from nn.Tensor to ShardedTensor to represent its sharding\nnature (this is not supported yet).\nExample:\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     model, optim, optim_state_dict\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n\n```\n\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\nwere passed into the optimizeroptim.\nFullyShardedDataParallel\noptim\noptim(torch.optim.Optimizer) \u2013 Optimizer formodel\u2018s\nparameters.\nmodel\noptim_state_dict(Dict[str,Any]) \u2013 the target optimizer state_dict to\ntransform. If the value is None, optim.state_dict() will be used. (\nDefault:None)\nNone\ngroup(dist.ProcessGroup) \u2013 Model\u2019s process group across which parameters\nare sharded orNoneif using the default process group. (\nDefault:None)\nNone\nNone\nAdictcontaining the optimizer state formodel. The sharding of the optimizer state is based onstate_dict_type.\ndict\nmodel\nstate_dict_type\nDict[str, Any]\nConvert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.\nGiven aoptim_state_dictthat is transformed throughoptim_state_dict(), it gets converted to the flattened optimizer\nstate_dict that can be loaded tooptimwhich is the optimizer formodel.modelmust be sharded by FullyShardedDataParallel.\noptim_state_dict\noptim_state_dict()\noptim\nmodel\nmodel\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> original_osd = optim.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(\n>>>     model,\n>>>     optim,\n>>>     optim_state_dict=original_osd\n>>> )\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     model, optim, optim_state_dict\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n\n```\n\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\nwere passed into the optimizeroptim.\nFullyShardedDataParallel\noptim\noptim(torch.optim.Optimizer) \u2013 Optimizer formodel\u2018s\nparameters.\nmodel\noptim_state_dict(Dict[str,Any]) \u2013 The optimizer states to be loaded.\nis_named_optimizer(bool) \u2013 Is this optimizer a NamedOptimizer or\nKeyedOptimizer. Only set to True ifoptimis TorchRec\u2019s\nKeyedOptimizer or torch.distributed\u2019s NamedOptimizer.\noptim\nload_directly(bool) \u2013 If this is set to True, this API will also\ncall optim.load_state_dict(result) before returning the result.\nOtherwise, users are responsible to calloptim.load_state_dict()(Default:False)\noptim.load_state_dict()\nFalse\ngroup(dist.ProcessGroup) \u2013 Model\u2019s process group across which parameters\nare sharded orNoneif using the default process group. (\nDefault:None)\nNone\nNone\ndict[str,Any]\nRegister a communication hook.\nThis is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates\ngradients across multiple workers.\nThis hook can be used to implement several algorithms likeGossipGradand gradient compression\nwhich involve different communication strategies for\nparameter syncs while training withFullyShardedDataParallel.\nFullyShardedDataParallel\nWarning\nFSDP communication hook should be registered before running an initial forward pass\nand only once.\nstate(object) \u2013Passed to the hook to maintain any state information during the training process.\nExamples include error feedback in gradient compression,\npeers to communicate with next inGossipGrad, etc.\nIt is locally stored by each worker\nand shared by all the gradient tensors on the worker.\nPassed to the hook to maintain any state information during the training process.\nExamples include error feedback in gradient compression,\npeers to communicate with next inGossipGrad, etc.\nIt is locally stored by each worker\nand shared by all the gradient tensors on the worker.\nhook(Callable) \u2013 Callable, which has one of the following signatures:\n1)hook:Callable[torch.Tensor]->None:\nThis function takes in a Python tensor, which represents\nthe full, flattened, unsharded gradient with respect to all variables\ncorresponding to the model this FSDP unit is wrapping\n(that are not wrapped by other FSDP sub-units).\nIt then performs all necessary processing and returnsNone;\n2)hook:Callable[torch.Tensor,torch.Tensor]->None:\nThis function takes in two Python tensors, the first one represents\nthe full, flattened, unsharded gradient with respect to all variables\ncorresponding to the model this FSDP unit is wrapping\n(that are not wrapped by other FSDP sub-units). The latter\nrepresents a pre-sized tensor to store a chunk of a sharded gradient after\nreduction.\nIn both cases, callable performs all necessary processing and returnsNone.\nCallables with signature 1 are expected to handle gradient communication for aNO_SHARDcase.\nCallables with signature 2 are expected to handle gradient communication for sharded cases.\nhook:Callable[torch.Tensor]->None\nNone\nhook:Callable[torch.Tensor,torch.Tensor]->None\nNone\nRe-keys the optimizer state dictoptim_state_dictto use the key typeoptim_state_key_type.\noptim_state_dict\noptim_state_key_type\nThis can be used to achieve compatibility between optimizer state dicts from models with FSDP\ninstances and ones without.\nTo re-key an FSDP full optimizer state dict (i.e. fromfull_optim_state_dict()) to use parameter IDs and be loadable to\na non-wrapped model:\nfull_optim_state_dict()\n\n```python\n>>> wrapped_model, wrapped_optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)\n>>> nonwrapped_optim.load_state_dict(rekeyed_osd)\n\n```\n\nTo re-key a normal optimizer state dict from a non-wrapped model to be\nloadable to a wrapped model:\n\n```python\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> osd = nonwrapped_optim.state_dict()\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)\n>>> wrapped_model, wrapped_optim = ...\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)\n>>> wrapped_optim.load_state_dict(sharded_osd)\n\n```\n\nThe optimizer state dict re-keyed using the\nparameter keys specified byoptim_state_key_type.\noptim_state_key_type\nDict[str, Any]\nScatter the full optimizer state dict from rank 0 to all other ranks.\nReturns the sharded optimizer state dict on each rank.\nThe return value is the same asshard_full_optim_state_dict(), and on rank\n0, the first argument should be the return value offull_optim_state_dict().\nshard_full_optim_state_dict()\nfull_optim_state_dict()\nExample:\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim, new_group = ...\n>>> sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)\n>>> new_optim.load_state_dict(sharded_osd)\n\n```\n\nNote\nBothshard_full_optim_state_dict()andscatter_full_optim_state_dict()may be used to get the\nsharded optimizer state dict to load. Assuming that the full\noptimizer state dict resides in CPU memory, the former requires\neach rank to have the full dict in CPU memory, where each rank\nindividually shards the dict without any communication, while the\nlatter requires only rank 0 to have the full dict in CPU memory,\nwhere rank 0 moves each shard to GPU memory (for NCCL) and\ncommunicates it to ranks appropriately. Hence, the former has\nhigher aggregate CPU memory cost, while the latter has higher\ncommunication cost.\nshard_full_optim_state_dict()\nscatter_full_optim_state_dict()\nfull_optim_state_dict(Optional[Dict[str,Any]]) \u2013 Optimizer state\ndict corresponding to the unflattened parameters and holding\nthe full non-sharded optimizer state if on rank 0; the argument\nis ignored on nonzero ranks.\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\ncorrespond to the optimizer state infull_optim_state_dict.\nFullyShardedDataParallel\nfull_optim_state_dict\noptim_input(Optional[Union[List[Dict[str,Any]],Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer representing either alistof parameter groups or an iterable of parameters;\nifNone, then this method assumes the input wasmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default:None)\nlist\nNone\nmodel.parameters()\nNone\noptim(Optional[torch.optim.Optimizer]) \u2013 Optimizer that will load\nthe state dict returned by this method. This is the preferred\nargument to use overoptim_input. (Default:None)\noptim_input\nNone\ngroup(dist.ProcessGroup) \u2013 Model\u2019s process group orNoneif\nusing the default process group. (Default:None)\nNone\nNone\nThe full optimizer state dict now remapped to\nflattened parameters instead of unflattened parameters and\nrestricted to only include this rank\u2019s part of the optimizer state.\nDict[str, Any]\nSet thestate_dict_typeof all the descendant FSDP modules of the target module.\nstate_dict_type\nAlso takes (optional) configuration for the model\u2019s and optimizer\u2019s state dict.\nThe target module does not have to be a FSDP module. If the target\nmodule is a FSDP module, itsstate_dict_typewill also be changed.\nstate_dict_type\nNote\nThis API should be called for only the top-level (root)\nmodule.\nNote\nThis API enables users to transparently use the conventionalstate_dictAPI to take model checkpoints in cases where the\nroot FSDP module is wrapped by anothernn.Module. For example,\nthe following will ensurestate_dictis called on all non-FSDP\ninstances, while dispatching intosharded_state_dictimplementation\nfor FSDP:\nstate_dict\nnn.Module\nstate_dict\nExample:\n\n```python\n>>> model = DDP(FSDP(...))\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>>     state_dict_config = ShardedStateDictConfig(offload_to_cpu=True),\n>>>     optim_state_dict_config = OptimStateDictConfig(offload_to_cpu=True),\n>>> )\n>>> param_state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n\n```\n\nmodule(torch.nn.Module) \u2013 Root module.\nstate_dict_type(StateDictType) \u2013 the desiredstate_dict_typeto set.\nstate_dict_type\nstate_dict_config(Optional[StateDictConfig]) \u2013 the configuration for the\ntargetstate_dict_type.\nstate_dict_type\noptim_state_dict_config(Optional[OptimStateDictConfig]) \u2013 the configuration\nfor the optimizer state dict.\nA StateDictSettings that include the previous state_dict type and\nconfiguration for the module.\nStateDictSettings\nShard a full optimizer state-dict.\nRemaps the state infull_optim_state_dictto flattened parameters instead of unflattened\nparameters and restricts to only this rank\u2019s part of the optimizer state.\nThe first argument should be the return value offull_optim_state_dict().\nfull_optim_state_dict\nfull_optim_state_dict()\nExample:\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)\n>>> torch.save(full_osd, PATH)\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim = ...\n>>> full_osd = torch.load(PATH)\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)\n>>> new_optim.load_state_dict(sharded_osd)\n\n```\n\nNote\nBothshard_full_optim_state_dict()andscatter_full_optim_state_dict()may be used to get the\nsharded optimizer state dict to load. Assuming that the full\noptimizer state dict resides in CPU memory, the former requires\neach rank to have the full dict in CPU memory, where each rank\nindividually shards the dict without any communication, while the\nlatter requires only rank 0 to have the full dict in CPU memory,\nwhere rank 0 moves each shard to GPU memory (for NCCL) and\ncommunicates it to ranks appropriately. Hence, the former has\nhigher aggregate CPU memory cost, while the latter has higher\ncommunication cost.\nshard_full_optim_state_dict()\nscatter_full_optim_state_dict()\nfull_optim_state_dict(Dict[str,Any]) \u2013 Optimizer state dict\ncorresponding to the unflattened parameters and holding the\nfull non-sharded optimizer state.\nmodel(torch.nn.Module) \u2013 Root module (which may or may not be aFullyShardedDataParallelinstance) whose parameters\ncorrespond to the optimizer state infull_optim_state_dict.\nFullyShardedDataParallel\nfull_optim_state_dict\noptim_input(Optional[Union[List[Dict[str,Any]],Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer representing either alistof parameter groups or an iterable of parameters;\nifNone, then this method assumes the input wasmodel.parameters(). This argument is deprecated, and there\nis no need to pass it in anymore. (Default:None)\nlist\nNone\nmodel.parameters()\nNone\noptim(Optional[torch.optim.Optimizer]) \u2013 Optimizer that will load\nthe state dict returned by this method. This is the preferred\nargument to use overoptim_input. (Default:None)\noptim_input\nNone\nThe full optimizer state dict now remapped to\nflattened parameters instead of unflattened parameters and\nrestricted to only include this rank\u2019s part of the optimizer state.\nDict[str, Any]\nReturn the optimizer state-dict in its sharded form.\nThe API is similar tofull_optim_state_dict()but this API chunks\nall non-zero-dimension states toShardedTensorto save memory.\nThis API should only be used when the modelstate_dictis derived\nwith the context managerwithstate_dict_type(SHARDED_STATE_DICT):.\nfull_optim_state_dict()\nShardedTensor\nstate_dict\nwithstate_dict_type(SHARDED_STATE_DICT):\nFor the detailed usage, refer tofull_optim_state_dict().\nfull_optim_state_dict()\nWarning\nThe returned state dict containsShardedTensorand\ncannot be directly used by the regularoptim.load_state_dict.\nShardedTensor\noptim.load_state_dict\ndict[str,Any]\nSet thestate_dict_typeof all the descendant FSDP modules of the target module.\nstate_dict_type\nThis context manager has the same functions asset_state_dict_type(). Read the document ofset_state_dict_type()for the detail.\nset_state_dict_type()\nset_state_dict_type()\nExample:\n\n```python\n>>> model = DDP(FSDP(...))\n>>> with FSDP.state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>> ):\n>>>     checkpoint = model.state_dict()\n\n```\n\nmodule(torch.nn.Module) \u2013 Root module.\nstate_dict_type(StateDictType) \u2013 the desiredstate_dict_typeto set.\nstate_dict_type\nstate_dict_config(Optional[StateDictConfig]) \u2013 the modelstate_dictconfiguration for the targetstate_dict_type.\nstate_dict\nstate_dict_type\noptim_state_dict_config(Optional[OptimStateDictConfig]) \u2013 the optimizerstate_dictconfiguration for the targetstate_dict_type.\nstate_dict\nstate_dict_type\nGenerator\nExpose full params for FSDP instances with this context manager.\nCan be usefulafterforward/backward for a model to get\nthe params for additional processing or checking. It can take a non-FSDP\nmodule and will summon full params for all contained FSDP modules as\nwell as their children, depending on therecurseargument.\nrecurse\nNote\nThis can be used on inner FSDPs.\nNote\nThis cannotbe used within a forward or backward pass. Nor\ncan forward and backward be started from within this context.\nNote\nParameters will revert to their local shards after the context\nmanager exits, storage behavior is the same as forward.\nNote\nThe full parameters can be modified, but only the portion\ncorresponding to the local param shard will persist after the\ncontext manager exits (unlesswriteback=False, in which case\nchanges will be discarded). In the case where FSDP does not shard\nthe parameters, currently only whenworld_size==1, orNO_SHARDconfig, the modification is persisted regardless ofwriteback.\nwriteback=False\nworld_size==1\nNO_SHARD\nwriteback\nNote\nThis method works on modules which are not FSDP themselves but\nmay contain multiple independent FSDP units. In that case, the given\narguments will apply to all contained FSDP units.\nWarning\nNote thatrank0_only=Truein conjunction withwriteback=Trueis not currently supported and will raise an\nerror. This is because model parameter shapes would be different\nacross ranks within the context, and writing to them can lead to\ninconsistency across ranks when the context is exited.\nrank0_only=True\nwriteback=True\nWarning\nNote thatoffload_to_cpuandrank0_only=Falsewill\nresult in full parameters being redundantly copied to CPU memory for\nGPUs that reside on the same machine, which may incur the risk of\nCPU OOM. It is recommended to useoffload_to_cpuwithrank0_only=True.\noffload_to_cpu\nrank0_only=False\noffload_to_cpu\nrank0_only=True\nrecurse(bool,Optional) \u2013 recursively summon all params for nested\nFSDP instances (default: True).\nwriteback(bool,Optional) \u2013 ifFalse, modifications to params are\ndiscarded after the context manager exits;\ndisabling this can be slightly more efficient (default: True)\nFalse\nrank0_only(bool,Optional) \u2013 ifTrue, full parameters are\nmaterialized on only global rank 0. This means that within the\ncontext, only rank 0 will have full parameters and the other\nranks will have sharded parameters. Note that settingrank0_only=Truewithwriteback=Trueis not supported,\nas model parameter shapes will be different across ranks\nwithin the context, and writing to them can lead to\ninconsistency across ranks when the context is exited.\nTrue\nrank0_only=True\nwriteback=True\noffload_to_cpu(bool,Optional) \u2013 IfTrue, full parameters are\noffloaded to CPU. Note that this offloading currently only\noccurs if the parameter is sharded (which is only not the case\nfor world_size = 1 orNO_SHARDconfig). It is recommended\nto useoffload_to_cpuwithrank0_only=Trueto avoid\nredundant copies of model parameters being offloaded to the same CPU memory.\nTrue\nNO_SHARD\noffload_to_cpu\nrank0_only=True\nwith_grads(bool,Optional) \u2013 IfTrue, gradients are also\nunsharded with the parameters. Currently, this is only\nsupported when passinguse_orig_params=Trueto the FSDP\nconstructor andoffload_to_cpu=Falseto this method.\n(Default:False)\nTrue\nuse_orig_params=True\noffload_to_cpu=False\nFalse\nGenerator\nThis configures explicit backward prefetching, which improves throughput by\nenabling communication and computation overlap in the backward pass at the\ncost of slightly increased memory usage.\nBACKWARD_PRE: This enables the most overlap but increases memory\nusage the most. This prefetches the next set of parametersbeforethe\ncurrent set of parameters\u2019 gradient computation. This overlaps thenext\nall-gatherand thecurrent gradient computation, and at the peak, it\nholds the current set of parameters, next set of parameters, and current\nset of gradients in memory.\nBACKWARD_PRE\nBACKWARD_POST: This enables less overlap but requires less memory\nusage. This prefetches the next set of parametersafterthe current\nset of parameters\u2019 gradient computation. This overlaps thecurrent\nreduce-scatterand thenext gradient computation, and it frees the\ncurrent set of parameters before allocating memory for the next set of\nparameters, only holding the next set of parameters and current set of\ngradients in memory at the peak.\nBACKWARD_POST\nFSDP\u2019sbackward_prefetchargument acceptsNone, which disables\nthe backward prefetching altogether. This has no overlap and does not\nincrease memory usage. In general, we do not recommend this setting since\nit may degrade throughput significantly.\nbackward_prefetch\nNone\nFor more technical context: For a single process group using NCCL backend,\nany collectives, even if issued from different streams, contend for the\nsame per-device NCCL stream, which implies that the relative order in which\nthe collectives are issued matters for overlapping. The two backward\nprefetching values correspond to different issue orders.\nThis specifies the sharding strategy to be used for distributed training byFullyShardedDataParallel.\nFullyShardedDataParallel\nFULL_SHARD: Parameters, gradients, and optimizer states are sharded.\nFor the parameters, this strategy unshards (via all-gather) before the\nforward, reshards after the forward, unshards before the backward\ncomputation, and reshards after the backward computation. For gradients,\nit synchronizes and shards them (via reduce-scatter) after the backward\ncomputation. The sharded optimizer states are updated locally per rank.\nFULL_SHARD\nSHARD_GRAD_OP: Gradients and optimizer states are sharded during\ncomputation, and additionally, parameters are sharded outside\ncomputation. For the parameters, this strategy unshards before the\nforward, does not reshard them after the forward, and only reshards them\nafter the backward computation. The sharded optimizer states are updated\nlocally per rank. Insideno_sync(), the parameters are not resharded\nafter the backward computation.\nSHARD_GRAD_OP\nno_sync()\nNO_SHARD: Parameters, gradients, and optimizer states are not sharded\nbut instead replicated across ranks similar to PyTorch\u2019sDistributedDataParallelAPI. For gradients, this strategy\nsynchronizes them (via all-reduce) after the backward computation. The\nunsharded optimizer states are updated locally per rank.\nNO_SHARD\nDistributedDataParallel\nHYBRID_SHARD: ApplyFULL_SHARDwithin a node, and replicate parameters across\nnodes. This results in reduced communication volume as expensive all-gathers and\nreduce-scatters are only done within a node, which can be more performant for medium\n-sized models.\nHYBRID_SHARD\nFULL_SHARD\n_HYBRID_SHARD_ZERO2: ApplySHARD_GRAD_OPwithin a node, and replicate parameters across\nnodes. This is likeHYBRID_SHARD, except this may provide even higher throughput\nsince the unsharded parameters are not freed after the forward pass, saving the\nall-gathers in the pre-backward.\n_HYBRID_SHARD_ZERO2\nSHARD_GRAD_OP\nHYBRID_SHARD\nThis configures FSDP-native mixed precision training.\nparam_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for model\nparameters during forward and backward and thus the dtype for\nforward and backward computation. Outside forward and backward, theshardedparameters are kept in full precision (e.g. for the\noptimizer step), and for model checkpointing, the parameters are\nalways saved in full precision. (Default:None)\nNone\nreduce_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\ngradient reduction (i.e. reduce-scatter or all-reduce). If this isNonebutparam_dtypeis notNone, then this takes on\ntheparam_dtypevalue, still running gradient reduction in low\nprecision. This is permitted to differ fromparam_dtype, e.g.\nto force gradient reduction to run in full precision. (Default:None)\nNone\nparam_dtype\nNone\nparam_dtype\nparam_dtype\nNone\nbuffer_dtype(Optional[torch.dtype]) \u2013 This specifies the dtype for\nbuffers. FSDP does not shard buffers. Rather, FSDP casts them tobuffer_dtypein the first forward pass and keeps them in that\ndtype thereafter. For model checkpointing, the buffers are saved\nin full precision except forLOCAL_STATE_DICT. (Default:None)\nbuffer_dtype\nLOCAL_STATE_DICT\nNone\nkeep_low_precision_grads(bool) \u2013 IfFalse, then FSDP upcasts\ngradients to full precision after the backward pass in preparation\nfor the optimizer step. IfTrue, then FSDP keeps the gradients\nin the dtype used for gradient reduction, which can save memory if\nusing a custom optimizer that supports running in low precision.\n(Default:False)\nFalse\nTrue\nFalse\ncast_forward_inputs(bool) \u2013 IfTrue, then this FSDP module casts\nits forward args and kwargs toparam_dtype. This is to ensure\nthat parameter and input dtypes match for forward computation, as\nrequired by many ops. This may need to be set toTruewhen only\napplying mixed precision to some but not all FSDP modules, in which\ncase a mixed-precision FSDP submodule needs to recast its inputs.\n(Default:False)\nTrue\nparam_dtype\nTrue\nFalse\ncast_root_forward_inputs(bool) \u2013 IfTrue, then the root FSDP module\ncasts its forward args and kwargs toparam_dtype, overriding\nthe value ofcast_forward_inputs. For non-root FSDP modules,\nthis does not do anything. (Default:True)\nTrue\nparam_dtype\ncast_forward_inputs\nTrue\n_module_classes_to_ignore(collections.abc.Sequence[type[torch.nn.modules.module.Module]]) \u2013 (Sequence[Type[nn.Module]]): This specifies\nmodule classes to ignore for mixed precision when using anauto_wrap_policy: Modules of these classes will have FSDP\napplied to them separately with mixed precision disabled (meaning\nthat the final FSDP construction would deviate from the specified\npolicy). Ifauto_wrap_policyis not specified, then this does\nnot do anything. This API is experimental and subject to change.\n(Default:(_BatchNorm,))\nauto_wrap_policy\nauto_wrap_policy\n(_BatchNorm,)\nNote\nThis API is experimental and subject to change.\nNote\nOnly floating point tensors are cast to their specified dtypes.\nNote\nInsummon_full_params, parameters are forced to full\nprecision, but buffers are not.\nsummon_full_params\nNote\nLayer norm and batch norm accumulate infloat32even when\ntheir inputs are in a low precision likefloat16orbfloat16.\nDisabling FSDP\u2019s mixed precision for those norm modules only means that\nthe affine parameters are kept infloat32. However, this incurs\nseparate all-gathers and reduce-scatters for those norm modules, which\nmay be inefficient, so if the workload permits, the user should prefer\nto still apply mixed precision to those modules.\nfloat32\nfloat16\nbfloat16\nfloat32\nNote\nBy default, if the user passes a model with any_BatchNormmodules and specifies anauto_wrap_policy, then the batch norm\nmodules will have FSDP applied to them separately with mixed precision\ndisabled. See the_module_classes_to_ignoreargument.\n_BatchNorm\nauto_wrap_policy\n_module_classes_to_ignore\nNote\nMixedPrecisionhascast_root_forward_inputs=Trueandcast_forward_inputs=Falseby default. For the root FSDP instance,\nitscast_root_forward_inputstakes precedence over itscast_forward_inputs. For non-root FSDP instances, theircast_root_forward_inputsvalues are ignored. The default setting is\nsufficient for the typical case where each FSDP instance has the sameMixedPrecisionconfiguration and only needs to cast inputs to theparam_dtypeat the beginning of the model\u2019s forward pass.\nMixedPrecision\ncast_root_forward_inputs=True\ncast_forward_inputs=False\ncast_root_forward_inputs\ncast_forward_inputs\ncast_root_forward_inputs\nMixedPrecision\nparam_dtype\nNote\nFor nested FSDP instances with differentMixedPrecisionconfigurations, we recommend setting individualcast_forward_inputsvalues to configure casting inputs or not before each instance\u2019s\nforward. In such a case, since the casts happen before each FSDP\ninstance\u2019s forward, a parent FSDP instance should have its non-FSDP\nsubmodules run before its FSDP submodules to avoid the activation dtype\nbeing changed due to a differentMixedPrecisionconfiguration.\nMixedPrecision\ncast_forward_inputs\nMixedPrecision\nExample:\n\n```python\n>>> model = nn.Sequential(nn.Linear(3, 3), nn.Linear(3, 3))\n>>> model[1] = FSDP(\n>>>     model[1],\n>>>     mixed_precision=MixedPrecision(param_dtype=torch.float16, cast_forward_inputs=True),\n>>> )\n>>> model = FSDP(\n>>>     model,\n>>>     mixed_precision=MixedPrecision(param_dtype=torch.bfloat16, cast_forward_inputs=True),\n>>> )\n\n```\n\nThe above shows a working example. On the other hand, ifmodel[1]were replaced withmodel[0], meaning that the submodule using\ndifferentMixedPrecisionran its forward first, thenmodel[1]would incorrectly seefloat16activations instead ofbfloat16ones.\nmodel[1]\nmodel[0]\nMixedPrecision\nmodel[1]\nfloat16\nbfloat16\nThis configures CPU offloading.\noffload_params(bool) \u2013 This specifies whether to offload parameters to\nCPU when not involved in computation. IfTrue, then this\noffloads gradients to CPU as well, meaning that the optimizer step\nruns on CPU.\nTrue\nStateDictConfigis the base class for allstate_dictconfiguration\nclasses. Users should instantiate a child class (e.g.FullStateDictConfig) in order to configure settings for the\ncorrespondingstate_dicttype supported by FSDP.\nStateDictConfig\nstate_dict\nFullStateDictConfig\nstate_dict\noffload_to_cpu(bool) \u2013 IfTrue, then FSDP offloads the state dict\nvalues to CPU, and ifFalse, then FSDP keeps them on GPU.\n(Default:False)\nTrue\nFalse\nFalse\nFullStateDictConfigis a config class meant to be used withStateDictType.FULL_STATE_DICT. We recommend enabling bothoffload_to_cpu=Trueandrank0_only=Truewhen saving full state\ndicts to save GPU memory and CPU memory, respectively. This config class\nis meant to be used via thestate_dict_type()context manager as\nfollows:\nFullStateDictConfig\nStateDictType.FULL_STATE_DICT\noffload_to_cpu=True\nrank0_only=True\nstate_dict_type()\n\n```python\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> fsdp = FSDP(model, auto_wrap_policy=...)\n>>> cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n>>> with FSDP.state_dict_type(fsdp, StateDictType.FULL_STATE_DICT, cfg):\n>>>     state = fsdp.state_dict()\n>>> # `state` will be empty on non rank 0 and contain CPU tensors on rank 0.\n>>> # To reload checkpoint for inference, finetuning, transfer learning, etc:\n>>> model = model_fn()  # Initialize model in preparation for wrapping with FSDP\n>>> if dist.get_rank() == 0:\n>>> # Load checkpoint only on rank 0 to avoid memory redundancy\n>>>     state_dict = torch.load(\"my_checkpoint.pt\")\n>>>     model.load_state_dict(state_dict)\n>>> # All ranks initialize FSDP module as usual. `sync_module_states` argument\n>>> # communicates loaded checkpoint states from rank 0 to rest of the world.\n>>> fsdp = FSDP(\n...     model,\n...     device_id=torch.cuda.current_device(),\n...     auto_wrap_policy=...,\n...     sync_module_states=True,\n... )\n>>> # After this point, all ranks have FSDP model with loaded checkpoint.\n\n```\n\nrank0_only(bool) \u2013 IfTrue, then only rank 0 saves the full state\ndict, and nonzero ranks save an empty dict. IfFalse, then all\nranks save the full state dict. (Default:False)\nTrue\nFalse\nFalse\nShardedStateDictConfigis a config class meant to be used withStateDictType.SHARDED_STATE_DICT.\nShardedStateDictConfig\nStateDictType.SHARDED_STATE_DICT\n_use_dtensor(bool) \u2013 IfTrue, then FSDP saves the state dict values\nasDTensor, and ifFalse, then FSDP saves them asShardedTensor. (Default:False)\nTrue\nDTensor\nFalse\nShardedTensor\nFalse\nWarning\n_use_dtensoris a private field ofShardedStateDictConfigand it is used by FSDP to determine the type of state dict values. Users should not\nmanually modify_use_dtensor.\n_use_dtensor\nShardedStateDictConfig\n_use_dtensor\nOptimStateDictConfigis the base class for alloptim_state_dictconfiguration classes.  Users should instantiate a child class (e.g.FullOptimStateDictConfig) in order to configure settings for the\ncorrespondingoptim_state_dicttype supported by FSDP.\nOptimStateDictConfig\noptim_state_dict\nFullOptimStateDictConfig\noptim_state_dict\noffload_to_cpu(bool) \u2013 IfTrue, then FSDP offloads the state dict\u2019s\ntensor values to CPU, and ifFalse, then FSDP keeps them on the\noriginal device (which is GPU unless parameter CPU offloading is\nenabled). (Default:True)\nTrue\nFalse\nTrue\nrank0_only(bool) \u2013 IfTrue, then only rank 0 saves the full state\ndict, and nonzero ranks save an empty dict. IfFalse, then all\nranks save the full state dict. (Default:False)\nTrue\nFalse\nFalse\nShardedOptimStateDictConfigis a config class meant to be used withStateDictType.SHARDED_STATE_DICT.\nShardedOptimStateDictConfig\nStateDictType.SHARDED_STATE_DICT\n_use_dtensor(bool) \u2013 IfTrue, then FSDP saves the state dict values\nasDTensor, and ifFalse, then FSDP saves them asShardedTensor. (Default:False)\nTrue\nDTensor\nFalse\nShardedTensor\nFalse\nWarning\n_use_dtensoris a private field ofShardedOptimStateDictConfigand it is used by FSDP to determine the type of state dict values. Users should not\nmanually modify_use_dtensor.\n_use_dtensor\nShardedOptimStateDictConfig\n_use_dtensor",
    "url": "https://pytorch.org/docs/stable/fsdp.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "71bee94fec55405152e38dc43ef129a4",
    "source": "pytorch_docs",
    "title": "Events \u2014 PyTorch 2.9 documentation",
    "text": "\n## Events#\n\nCreated On: May 04, 2021 | Last Updated On: Jun 10, 2024\nModule contains events processing mechanisms that are integrated with the standard python logging.\nExample of usage:\n\n```python\nfrom torch.distributed.elastic import events\n\nevent = events.Event(\n    name=\"test_event\", source=events.EventSource.WORKER, metadata={...}\n)\nevents.get_logging_handler(destination=\"console\").info(event)\n\n```\n\n\n## API Methods#\n\nInitialize rendezvous event object and record its operations.\nrun_id(str) \u2013 The run id of the rendezvous.\nmessage(str) \u2013 The message describing the event.\nnode_state(NodeState) \u2013 The state of the node (INIT, RUNNING, SUCCEEDED, FAILED).\nname(str) \u2013 Event name. (E.g. Current action being performed).\nhostname(str) \u2013 Hostname of the node.\npid(Optional[int]) \u2013 The process id of the node.\nmaster_endpoint(str) \u2013 The master endpoint for the rendezvous store, if known.\nlocal_id(Optional[int]) \u2013 The local_id of the node, if defined in dynamic_rendezvous.py\nrank(Optional[int]) \u2013 The rank of the node, if known.\nNone\nNone\nExample\n\n```python\n>>> # See DynamicRendezvousHandler class\n>>> def _record(\n...     self,\n...     message: str,\n...     node_state: NodeState = NodeState.RUNNING,\n...     rank: Optional[int] = None,\n... ) -> None:\n...     construct_and_record_rdzv_event(\n...         name=f\"{self.__class__.__name__}.{get_method_name()}\",\n...         run_id=self._settings.run_id,\n...         message=message,\n...         node_state=node_state,\n...         hostname=self._this_node.addr,\n...         pid=self._this_node.pid,\n...         local_id=self._this_node.local_id,\n...         rank=rank,\n...     )\n\n```\n\nHandler\n\n## Event Objects#\n\nThe class represents the generic event that occurs during the torchelastic job execution.\nThe event can be any kind of meaningful action.\nname(str) \u2013 event name.\nsource(EventSource) \u2013 the event producer, e.g. agent or worker\ntimestamp(int) \u2013 timestamp in milliseconds when event occurred.\nmetadata(dict[str,Union[str,int,float,bool,NoneType]]) \u2013 additional data that is associated with the event.\nKnown identifiers of the event producers.\nalias ofOptional[Union[str,int,float,bool]]\nOptional\nUnion\nstr\nint\nfloat\nbool",
    "url": "https://pytorch.org/docs/stable/elastic/events.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a93dc6a433c1c6e5b7e95569ac56d41c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributed.algorithms.join.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5e20dd8c02cb74171e7b1ee7276ae5de",
    "source": "pytorch_docs",
    "title": "CUDA semantics \u2014 PyTorch 2.9 documentation",
    "text": "\n## CUDA semantics#\n\nCreated On: Jan 16, 2017 | Last Updated On: Sep 04, 2025\ntorch.cudais used to set up and run CUDA operations. It keeps track of\nthe currently selected GPU, and all CUDA tensors you allocate will by default be\ncreated on that device. The selected device can be changed with atorch.cuda.devicecontext manager.\ntorch.cuda\ntorch.cuda.device\nHowever, once a tensor is allocated, you can do operations on it irrespective\nof the selected device, and the results will be always placed on the same\ndevice as the tensor.\nCross-GPU operations are not allowed by default, with the exception ofcopy_()and other methods with copy-like functionality\nsuch asto()andcuda().\nUnless you enable peer-to-peer memory access, any attempts to launch ops on\ntensors spread across different devices will raise an error.\ncopy_()\nto()\ncuda()\nBelow you can find a small example showcasing this:\n\n```python\ncuda = torch.device('cuda')     # Default CUDA device\ncuda0 = torch.device('cuda:0')\ncuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)\n\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n\nwith torch.cuda.device(1):\n    # allocates a tensor on GPU 1\n    a = torch.tensor([1., 2.], device=cuda)\n\n    # transfers a tensor from CPU to GPU 1\n    b = torch.tensor([1., 2.]).cuda()\n    # a.device and b.device are device(type='cuda', index=1)\n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n    b2 = torch.tensor([1., 2.]).to(device=cuda)\n    # b.device and b2.device are device(type='cuda', index=1)\n\n    c = a + b\n    # c.device is device(type='cuda', index=1)\n\n    z = x + y\n    # z.device is device(type='cuda', index=0)\n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n    d = torch.randn(2, device=cuda2)\n    e = torch.randn(2).to(cuda2)\n    f = torch.randn(2).cuda(cuda2)\n    # d.device, e.device, and f.device are all device(type='cuda', index=2)\n\n```\n\n\n## TensorFloat-32 (TF32) on Ampere (and later) devices#\n\nAfter Pytorch 2.9, we provide a new sets of APIs to control the TF32 behavior in a more fine-grained way, and\nsuggest to use the new APIs for better control.\nWe can set float32 precision per backend and per operators. We can also override the global setting for a specific operator.\n\n```python\ntorch.backends.fp32_precision = \"ieee\"\ntorch.backends.cuda.matmul.fp32_precision = \"ieee\"\ntorch.backends.cudnn.fp32_precision = \"ieee\"\ntorch.backends.cudnn.conv.fp32_precision = \"tf32\"\ntorch.backends.cudnn.rnn.fp32_precision = \"tf32\"\n\n```\n\nThe fp32_precision can be set toieeeortf32forcuda/cudnn.ieeefp32_precision indicate that we will useFP32as internal computation precision.tf32fp32_precision indicate that we will allow to useTF32as internal computation precision.\nWe can override a generic setting for a specific operator if the fp32_precision is set toieee.\n\n```python\ntorch.backends.cudnn.fp32_precision = \"tf32\"\ntorch.backends.cudnn.conv.fp32_precision = \"ieee\"\ntorch.backends.cudnn.rnn.fp32_precision = \"ieee\"\n\n```\n\nWe can also override a generic setting for a specific backend if the fp32_precision is set toieee.\n\n```python\ntorch.backends.fp32_precision = \"tf32\"\ntorch.backends.cudnn.fp32_precision = \"ieee\"\ntorch.backends.cudnn.conv.fp32_precision = \"ieee\"\ntorch.backends.cudnn.rnn.fp32_precision = \"ieee\"\n\n```\n\nFor above 2 cases, bothtorch.backends.cudnn.conv.fp32_precisionandtorch.backends.cudnn.rnn.fp32_precisionis overridden toieee.\nWe suggest to use the new settings for better control. And we do not support to use mix of old and new settings.\nWarning\nOld settings withallow_tf32as follows is going to be deprecated. We suggest to use the above new settings for\nbetter control. And we do not support to use mix of old and new settings.\nStarting in PyTorch 1.7, there is a new flag calledallow_tf32. This flag\ndefaults to True in PyTorch 1.7 to PyTorch 1.11, and False in PyTorch 1.12 and later.\nThis flag controls whether PyTorch is allowed to use the TensorFloat32 (TF32) tensor cores,\navailable on NVIDIA GPUs since Ampere, internally to compute matmul (matrix multiplies\nand batched matrix multiplies) and convolutions.\nTF32 tensor cores are designed to achieve better performance on matmul and convolutions ontorch.float32tensors by rounding input data to have 10 bits of mantissa, and accumulating\nresults with FP32 precision, maintaining FP32 dynamic range.\nmatmuls and convolutions are controlled separately, and their corresponding flags can be accessed at:\n\n```python\n# The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n# in PyTorch 1.12 and later.\ntorch.backends.cuda.matmul.allow_tf32 = True\n\n# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\ntorch.backends.cudnn.allow_tf32 = True\n\n```\n\nThe precision of matmuls can also be set more broadly (limited not just to CUDA) viaset_float32_matmul_precision().\nNote that besides matmuls and convolutions themselves, functions and nn modules that internally uses\nmatmuls or convolutions are also affected. These includenn.Linear,nn.Conv*, cdist, tensordot,\naffine grid and grid sample, adaptive log softmax, GRU and LSTM.\nset_float32_matmul_precision()\nTo get an idea of the precision and speed, see the example code and benchmark data (on A100) below:\n\n```python\na_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\nb_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')\nab_full = a_full @ b_full\nmean = ab_full.abs().mean()  # 80.7277\n\na = a_full.float()\nb = b_full.float()\n\n# Do matmul at TF32 mode.\ntorch.backends.cuda.matmul.allow_tf32 = True\nab_tf32 = a @ b  # takes 0.016s on GA100\nerror = (ab_tf32 - ab_full).abs().max()  # 0.1747\nrelative_error = error / mean  # 0.0022\n\n# Do matmul with TF32 disabled.\ntorch.backends.cuda.matmul.allow_tf32 = False\nab_fp32 = a @ b  # takes 0.11s on GA100\nerror = (ab_fp32 - ab_full).abs().max()  # 0.0031\nrelative_error = error / mean  # 0.000039\n\n```\n\nFrom the above example, we can see that with TF32 enabled, the speed is ~7x faster on A100, and that\nrelative error compared to double precision is approximately 2 orders of magnitude larger. Note that\nthe exact ratio of TF32 to single precision speed depends on the hardware generation, as properties\nsuch as the ratio of memory bandwidth to compute as well as the ratio of TF32 to FP32 matmul throughput\nmay vary from generation to generation or model to model.\nIf full FP32 precision is needed, users can disable TF32 by:\n\n```python\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.allow_tf32 = False\n\n```\n\nTo toggle the TF32 flags off in C++, you can do\n\n```python\nat::globalContext().setAllowTF32CuBLAS(false);\nat::globalContext().setAllowTF32CuDNN(false);\n\n```\n\nFor more information about TF32, see:\nTensorFloat-32\nCUDA 11\nAmpere architecture\n\n## Reduced Precision Reduction in FP16 GEMMs#\n\n(Distinct from full FP16 accumulation that is intended for hardware that has higher throughput\nwith FP16 accumulation than FP32 accumulation, seeFull FP16 accumulation)\nfp16 GEMMs are potentially done with some intermediate reduced precision reductions (e.g., in fp16 rather than fp32). These selective reductions in precision can allow for higher performance on certain workloads (particularly those with a largekdimension) and GPU architectures at the cost of numerical precision and potential for overflow.\nSome example benchmark data on V100:\n\n```python\n[--------------------------- bench_gemm_transformer --------------------------]\n      [  m ,  k  ,  n  ]    |  allow_fp16_reduc=True  |  allow_fp16_reduc=False\n1 threads: --------------------------------------------------------------------\n      [4096, 4048, 4096]    |           1634.6        |           1639.8\n      [4096, 4056, 4096]    |           1670.8        |           1661.9\n      [4096, 4080, 4096]    |           1664.2        |           1658.3\n      [4096, 4096, 4096]    |           1639.4        |           1651.0\n      [4096, 4104, 4096]    |           1677.4        |           1674.9\n      [4096, 4128, 4096]    |           1655.7        |           1646.0\n      [4096, 4144, 4096]    |           1796.8        |           2519.6\n      [4096, 5096, 4096]    |           2094.6        |           3190.0\n      [4096, 5104, 4096]    |           2144.0        |           2663.5\n      [4096, 5112, 4096]    |           2149.1        |           2766.9\n      [4096, 5120, 4096]    |           2142.8        |           2631.0\n      [4096, 9728, 4096]    |           3875.1        |           5779.8\n      [4096, 16384, 4096]   |           6182.9        |           9656.5\n(times in microseconds).\n\n```\n\nIf full precision reductions are needed, users can disable reduced precision reductions in fp16 GEMMs with:\n\n```python\ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\n```\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\n```python\nat::globalContext().setAllowFP16ReductionCuBLAS(false);\n\n```\n\n\n## Reduced Precision Reduction in BF16 GEMMs#\n\nA similar flag (as above) exists for BFloat16 GEMMs.\nNote that this switch is set toTrueby default for BF16, if you observe\nnumerical instability in your workload, you may wish to set it toFalse.\nIf reduced precision reductions are not desired, users can disable reduced\nprecision reductions in bf16 GEMMs with:\n\n```python\ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\n```\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\n```python\nat::globalContext().setAllowBF16ReductionCuBLAS(true);\n\n```\n\n\n## Full FP16 Accmumulation in FP16 GEMMs#\n\nCertain GPUs have increased performance when doing _all_ FP16 GEMM accumulation\nin FP16, at the cost of numerical precision and greater likelihood of overflow.\nNote that this setting only has an effect on GPUs of compute capability 7.0 (Volta)\nor newer.\nThis behavior can be enabled via:\n\n```python\ntorch.backends.cuda.matmul.allow_fp16_accumulation = True\n\n```\n\nTo toggle the reduced precision reduction flags in C++, one can do\n\n```python\nat::globalContext().setAllowFP16AccumulationCuBLAS(true);\n\n```\n\n\n## Asynchronous execution#\n\nBy default, GPU operations are asynchronous.  When you call a function that\nuses the GPU, the operations areenqueuedto the particular device, but not\nnecessarily executed until later.  This allows us to execute more computations\nin parallel, including operations on CPU or other GPUs.\nIn general, the effect of asynchronous computation is invisible to the caller,\nbecause (1) each device executes operations in the order they are queued, and\n(2) PyTorch automatically performs necessary synchronization when copying data\nbetween CPU and GPU or between two GPUs.  Hence, computation will proceed as if\nevery operation was executed synchronously.\nYou can force synchronous computation by setting environment variableCUDA_LAUNCH_BLOCKING=1.  This can be handy when an error occurs on the GPU.\n(With asynchronous execution, such an error isn\u2019t reported until after the\noperation is actually executed, so the stack trace does not show where it was\nrequested.)\nCUDA_LAUNCH_BLOCKING=1\nA consequence of the asynchronous computation is that time measurements without\nsynchronizations are not accurate. To get precise measurements, one should either\ncalltorch.cuda.synchronize()before measuring, or usetorch.cuda.Eventto record times as following:\ntorch.cuda.synchronize()\ntorch.cuda.Event\n\n```python\nstart_event = torch.cuda.Event(enable_timing=True)\nend_event = torch.cuda.Event(enable_timing=True)\nstart_event.record()\n\n# Run some things here\n\nend_event.record()\ntorch.cuda.synchronize()  # Wait for the events to be recorded!\nelapsed_time_ms = start_event.elapsed_time(end_event)\n\n```\n\nAs an exception, several functions such asto()andcopy_()admit an explicitnon_blockingargument,\nwhich lets the caller bypass synchronization when it is unnecessary.\nAnother exception is CUDA streams, explained below.\nto()\ncopy_()\nnon_blocking\n\n## CUDA streams#\n\nACUDA streamis a linear sequence of execution that belongs to a specific\ndevice.  You normally do not need to create one explicitly: by default, each\ndevice uses its own \u201cdefault\u201d stream.\nOperations inside each stream are serialized in the order they are created,\nbut operations from different streams can execute concurrently in any\nrelative order, unless explicit synchronization functions (such assynchronize()orwait_stream()) are\nused.  For example, the following code is incorrect:\nsynchronize()\nwait_stream()\n\n```python\ncuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\nwith torch.cuda.stream(s):\n    # sum() may start execution before normal_() finishes!\n    B = torch.sum(A)\n\n```\n\nWhen the \u201ccurrent stream\u201d is the default stream, PyTorch automatically performs\nnecessary synchronization when data is moved around, as explained above.\nHowever, when using non-default streams, it is the user\u2019s responsibility to\nensure proper synchronization.  The fixed version of this example is:\n\n```python\ncuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)\ns.wait_stream(torch.cuda.default_stream(cuda))  # NEW!\nwith torch.cuda.stream(s):\n    B = torch.sum(A)\nA.record_stream(s)  # NEW!\n\n```\n\nThere are two new additions.  Thetorch.cuda.Stream.wait_stream()call\nensures that thenormal_()execution has finished before we start runningsum(A)on a side stream.  Thetorch.Tensor.record_stream()(see for\nmore details) ensures that we do not deallocate A beforesum(A)has\ncompleted.  You can also manually wait on the stream at some later point in\ntime withtorch.cuda.default_stream(cuda).wait_stream(s)(note that it\nis pointless to wait immediately, since that will prevent the stream execution\nfrom running in parallel with other work on the default stream.)  See the\ndocumentation fortorch.Tensor.record_stream()on more details on when\nto use one or another.\ntorch.cuda.Stream.wait_stream()\nnormal_()\nsum(A)\ntorch.Tensor.record_stream()\nsum(A)\ntorch.cuda.default_stream(cuda).wait_stream(s)\ntorch.Tensor.record_stream()\nNote that this synchronization is necessary even when there is no\nread dependency, e.g., as seen in this example:\n\n```python\ncuda = torch.device('cuda')\ns = torch.cuda.Stream()  # Create a new stream.\nA = torch.empty((100, 100), device=cuda)\ns.wait_stream(torch.cuda.default_stream(cuda))  # STILL REQUIRED!\nwith torch.cuda.stream(s):\n    A.normal_(0.0, 1.0)\n    A.record_stream(s)\n\n```\n\nDespite the computation onsnot reading the contents ofAand no\nother uses ofA, it is still necessary to synchronize, becauseAmay correspond to memory reallocated by the CUDA caching allocator, with\npending operations from the old (deallocated) memory.\ns\nA\nA\nA\n\n## Stream semantics of backward passes#\n\nEach backward CUDA op runs on the same stream that was used for its corresponding forward op.\nIf your forward pass runs independent ops in parallel on different streams,\nthis helps the backward pass exploit that same parallelism.\nThe stream semantics of a backward call with respect to surrounding ops are the same\nas for any other call. The backward pass inserts internal syncs to ensure this even when\nbackward ops run on multiple streams as described in the previous paragraph.\nMore concretely, when callingautograd.backward,autograd.grad, ortensor.backward,\nand optionally supplying CUDA tensor(s) as the  initial gradient(s) (e.g.,autograd.backward(...,grad_tensors=initial_grads),autograd.grad(...,grad_outputs=initial_grads), ortensor.backward(...,gradient=initial_grad)),\nthe acts of\nautograd.backward\nautograd.grad\ntensor.backward\nautograd.backward(...,grad_tensors=initial_grads)\nautograd.grad(...,grad_outputs=initial_grads)\ntensor.backward(...,gradient=initial_grad)\noptionally populating initial gradient(s),\ninvoking the backward pass, and\nusing the gradients\nhave the same stream-semantics relationship as any group of ops:\n\n```python\ns = torch.cuda.Stream()\n\n# Safe, grads are used in the same stream context as backward()\nwith torch.cuda.stream(s):\n    loss.backward()\n    use grads\n\n# Unsafe\nwith torch.cuda.stream(s):\n    loss.backward()\nuse grads\n\n# Safe, with synchronization\nwith torch.cuda.stream(s):\n    loss.backward()\ntorch.cuda.current_stream().wait_stream(s)\nuse grads\n\n# Safe, populating initial grad and invoking backward are in the same stream context\nwith torch.cuda.stream(s):\n    loss.backward(gradient=torch.ones_like(loss))\n\n# Unsafe, populating initial_grad and invoking backward are in different stream contexts,\n# without synchronization\ninitial_grad = torch.ones_like(loss)\nwith torch.cuda.stream(s):\n    loss.backward(gradient=initial_grad)\n\n# Safe, with synchronization\ninitial_grad = torch.ones_like(loss)\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    initial_grad.record_stream(s)\n    loss.backward(gradient=initial_grad)\n\n```\n\nIn prior versions of PyTorch (1.9 and earlier), the autograd engine always synced\nthe default stream with all backward ops, so the following pattern:\n\n```python\nwith torch.cuda.stream(s):\n    loss.backward()\nuse grads\n\n```\n\nwas safe as long asusegradshappened on the default stream.\nIn present PyTorch, that pattern is no longer safe. Ifbackward()andusegradsare in different stream contexts, you must sync the streams:\nusegrads\nbackward()\nusegrads\n\n```python\nwith torch.cuda.stream(s):\n    loss.backward()\ntorch.cuda.current_stream().wait_stream(s)\nuse grads\n\n```\n\neven ifusegradsis on the default stream.\nusegrads\n\n## Memory management#\n\nPyTorch uses a caching memory allocator to speed up memory allocations. This\nallows fast memory deallocation without device synchronizations. However, the\nunused memory managed by the allocator will still show as if used innvidia-smi. You can usememory_allocated()andmax_memory_allocated()to monitor memory occupied by\ntensors, and usememory_reserved()andmax_memory_reserved()to monitor the total amount of memory\nmanaged by the caching allocator. Callingempty_cache()releases allunusedcached memory from PyTorch so that those can be used\nby other GPU applications. However, the occupied GPU memory by tensors will not\nbe freed so it can not increase the amount of GPU memory available for PyTorch.\nnvidia-smi\nmemory_allocated()\nmax_memory_allocated()\nmemory_reserved()\nmax_memory_reserved()\nempty_cache()\nTo better understand how CUDA memory is being used over time,Understanding CUDA Memory Usagedescribes tools for capturing and visualizing traces of memory use.\nFor more advanced users, we offer more comprehensive memory benchmarking viamemory_stats(). We also offer the capability to capture a\ncomplete snapshot of the memory allocator state viamemory_snapshot(), which can help you understand the\nunderlying allocation patterns produced by your code.\nmemory_stats()\nmemory_snapshot()\n\n## Optimizing memory usage  withPYTORCH_CUDA_ALLOC_CONF#\n\nPYTORCH_CUDA_ALLOC_CONF\nUse of a caching allocator can interfere with memory checking tools such ascuda-memcheck.  To debug memory errors usingcuda-memcheck, setPYTORCH_NO_CUDA_MEMORY_CACHING=1in your environment to disable caching.\ncuda-memcheck\ncuda-memcheck\nPYTORCH_NO_CUDA_MEMORY_CACHING=1\nThe behavior of the caching allocator can be controlled via the environment variablePYTORCH_CUDA_ALLOC_CONF.\nThe format isPYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...Available options:\nPYTORCH_CUDA_ALLOC_CONF\nPYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>...\nbackendallows selecting the underlying allocator implementation.\nCurrently, valid options arenative, which uses PyTorch\u2019s native\nimplementation, andcudaMallocAsync, which usesCUDA\u2019s built-in asynchronous allocator.cudaMallocAsyncrequires CUDA 11.4 or newer. The default isnative.backendapplies to all devices used by the process, and can\u2019t be\nspecified on a per-device basis.\nbackend\nnative\ncudaMallocAsync\ncudaMallocAsync\nnative\nbackend\nmax_split_size_mbprevents the native allocator\nfrom splitting blocks larger than this size (in MB). This can reduce\nfragmentation and may allow some borderline workloads to complete without\nrunning out of memory. Performance cost can range from \u2018zero\u2019 to \u2018substantial\u2019\ndepending on allocation patterns.  Default value is unlimited, i.e. all blocks\ncan be split. Thememory_stats()andmemory_summary()methods are useful for tuning.  This\noption should be used as a last resort for a workload that is aborting\ndue to \u2018out of memory\u2019 and showing a large amount of inactive split blocks.max_split_size_mbis only meaningful withbackend:native.\nWithbackend:cudaMallocAsync,max_split_size_mbis ignored.\nmax_split_size_mb\nmemory_stats()\nmemory_summary()\nmax_split_size_mb\nbackend:native\nbackend:cudaMallocAsync\nmax_split_size_mb\nroundup_power2_divisionshelps with rounding the requested allocation\nsize to nearest power-2 division and making better use of the blocks. In\nthe native CUDACachingAllocator, the sizes are rounded up in multiple\nof blocks size of 512, so this works fine for smaller sizes. However, this\ncan be inefficient for large near-by allocations as each will go to different\nsize of blocks and reuse of those blocks are minimized. This might create\nlots of unused blocks and will waste GPU memory capacity. This option enables\nthe rounding of allocation size to nearest power-2 division. For example, if\nwe need to round-up size of 1200 and if number of divisions is 4,\nthe size 1200 lies between 1024 and 2048 and if we do 4 divisions between\nthem, the values are 1024, 1280, 1536, and 1792. So, allocation size of 1200\nwill be rounded to 1280 as the nearest ceiling of power-2 division.\nSpecify a single value to apply for all allocation sizes or specify an\narray of key value pairs to set power-2 division individually for each\npower of two interval. For example to set 1 division for all allocations\nunder 256MB, 2 division for allocations between 256MB and 512MB, 4 divisions\nfor allocations between 512MB and 1GB and 8 divisions for any larger allocations,\nset the knob value to: [256:1,512:2,1024:4,>:8].roundup_power2_divisionsis only meaningful withbackend:native.\nWithbackend:cudaMallocAsync,roundup_power2_divisionsis ignored.\nroundup_power2_divisions\nroundup_power2_divisions\nbackend:native\nbackend:cudaMallocAsync\nroundup_power2_divisions\nmax_non_split_rounding_mb\na 1024MB cached block can be reused for a 512MB allocation request. In the default\ncase, we only allow up to 20MB of rounding of non-split blocks, so a 512MB block\ncan only be served with between 512-532 MB size block. If we set the value of this\noption to 1024, it will allow 512-1536 MB size blocks to be used for a 512MB block\nwhich increases reuse of larger blocks. This will also help in reducing the stalls\nin avoiding expensive cudaMalloc calls.\ngarbage_collection_thresholdhelps actively reclaiming unused GPU memory to\navoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks),\nwhich can be unfavorable to latency-critical GPU applications (e.g., servers).\nUpon setting this threshold (e.g., 0.8), the allocator will start reclaiming\nGPU memory blocks if the GPU memory capacity usage exceeds the threshold (i.e.,\n80% of the total memory allocated to the GPU application). The algorithm prefers\nto free old & unused blocks first to avoid freeing blocks that are actively being\nreused. The threshold value should be between greater than 0.0 and less than 1.0.\nThe default value is set at 1.0.\ngarbage_collection_threshold\ngarbage_collection_thresholdis only meaningful withbackend:native.\nWithbackend:cudaMallocAsync,garbage_collection_thresholdis ignored.\ngarbage_collection_threshold\nbackend:native\nbackend:cudaMallocAsync\ngarbage_collection_threshold\nexpandable_segments(experimental, default:False) If set toTrue, this setting instructs\nthe allocator to create CUDA allocations that can later be expanded to better handle cases\nwhere a job changing allocation sizes frequently, such as having a changing batch size.\nNormally for large (>2MB) allocations, the allocator calls cudaMalloc to get allocations\nthat are the same size as what the user requests. In the future, parts of these\nallocations can be reused for other requests if they are free. This works well\nwhen the program makes many requests of exactly the same size or of sizes that\neven multiples of that size. Many deep learning models follow this behavior.\nHowever, one common exception is when the batch size changes slightly from one\niteration to the next, e.g. in batched inference. When the program runs\ninitially with batch sizeN, it will make allocations appropriate for that size.\nIf in the future, it runs at sizeN - 1, the existing allocations will still be\nbig enough. However, if it runs at sizeN + 1, then it will have to make new\nallocations that are slightly larger. Not all the tensors are the same size.\nSome might be(N + 1)*Aand others(N + 1)*A*BwhereAandBare some non-batch\ndimensions in the model. Because the allocator reuses existing allocations when\nthey are big enough, some number of(N + 1)*Aallocations will actually fit in\nthe already existingN*B*Asegments, though not perfectly. As the model runs it\nwill partially fill up all of these segments leaving unusable free slices of\nmemory at the end of these segments. The allocator at some point will need tocudaMalloca new(N + 1)*A*Bsegment. If there is not enough memory, there is\nnow no way to recover the slices of memory that are free at the end of existing\nsegments. With models 50+ layers deep, this pattern might repeat 50+ times\ncreating many slivers.\nexpandable_segments\nexpandable_segmentsallows the allocator to create a segment initially and then\nexpand its size later when more memory is needed. Instead of making one segment\nper allocation, it tries to make one segment (per stream) that grows as\nnecessary. Now when theN + 1case runs, the allocations will tile nicely into\nthe one large segment until it fills up. Then more memory is requested and\nappended to the end of the segment. This process does not create as many slivers\nof unusable memory, so it is more likely to succeed at finding this memory.\npinned_use_cuda_host_registeroption is a boolean flag that determines whether to\nuse the CUDA API\u2019s cudaHostRegister function for allocating pinned memory instead\nof the default cudaHostAlloc. When set to True, the memory is allocated using regular\nmalloc and then pages are mapped to the memory before calling cudaHostRegister.\nThis pre-mapping of pages helps reduce the lock time during the execution\nof cudaHostRegister.\npinned_num_register_threadsoption is only valid when pinned_use_cuda_host_register\nis set to True. By default, one thread is used to map the pages. This option allows\nusing more threads to parallelize the page mapping operations to reduce the overall\nallocation time of pinned memory. A good value for this option is 8 based on\nbenchmarking results.\npinned_use_background_threadsoption is a boolean flag to enable background thread\nfor processing events. This avoids any slow path associated with querying/processing of\nevents in the fast allocation path. This feature is disabled by default.\ngraph_capture_record_stream_reuse(experimental, default:False)\nIf set toTrue, the CUDA caching allocator will attempt to reclaim device memory during\nCUDA Graph capture by using the graph topology (instead of CUDA events) to determine\nwhen a freed block is safe to reuse. This can reduce peak memory during long captures that free\nand reallocate buffers across multiple streams, especially when the capture DAG frequently\nreaches joined frontiers. Note: Enabling this option can significantly increase the time spent\ncapturing the graph.\ngraph_capture_record_stream_reuse\nNote\nSome stats reported by theCUDA memory management APIare specific tobackend:native, and are not meaningful withbackend:cudaMallocAsync.\nSee each function\u2019s docstring for details.\nbackend:native\nbackend:cudaMallocAsync\n\n## Using custom memory allocators for CUDA#\n\nIt is possible to define allocators as simple functions in C/C++ and compile\nthem as a shared library, the code below shows a basic allocator that just\ntraces all the memory operations.\n\n```python\n#include <sys/types.h>\n#include <cuda_runtime_api.h>\n#include <iostream>\n// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC\nextern \"C\" {\nvoid* my_malloc(ssize_t size, int device, cudaStream_t stream) {\n   void *ptr;\n   cudaMalloc(&ptr, size);\n   std::cout<<\"alloc \"<<ptr<<size<<std::endl;\n   return ptr;\n}\n\nvoid my_free(void* ptr, ssize_t size, int device, cudaStream_t stream) {\n   std::cout<<\"free \"<<ptr<< \" \"<<stream<<std::endl;\n   cudaFree(ptr);\n}\n}\n\n```\n\nThis can be used in python through thetorch.cuda.memory.CUDAPluggableAllocator.\nThe user is responsible for supplying the path to the.sofile and the name\nof the alloc/free functions that match the signatures specified above.\ntorch.cuda.memory.CUDAPluggableAllocator\n\n```python\nimport torch\n\n# Load the allocator\nnew_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n    'alloc.so', 'my_malloc', 'my_free')\n# Swap the current allocator\ntorch.cuda.memory.change_current_allocator(new_alloc)\n# This will allocate memory in the device using the new allocator\nb = torch.zeros(10, device='cuda')\n\n```\n\n\n```python\nimport torch\n\n# Do an initial memory allocator\nb = torch.zeros(10, device='cuda')\n# Load the allocator\nnew_alloc = torch.cuda.memory.CUDAPluggableAllocator(\n    'alloc.so', 'my_malloc', 'my_free')\n# This will error since the current allocator was already instantiated\ntorch.cuda.memory.change_current_allocator(new_alloc)\n\n```\n\n\n## Mixing different CUDA system allocators in the same program#\n\nDepending on your use case,change_current_allocator()may not be what you\nwant to use, since it swaps the CUDA allocator for the entire program (similar toPYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync). For instance, if the swapped allocator doesn\u2019t\nhave caching mechanism, you will lose all the benefits of PyTorch\u2019s CUDACachingAllocator. Instead,\nyou can selectively mark a region of PyTorch code to use a custom allocator usingtorch.cuda.MemPool. This will let you use multiple CUDA system allocators in the same\nPyTorch program, along with most of the benefits of the CUDACachingAllocator (e.g. caching).\nUsingtorch.cuda.MemPool, you can utilize custom allocators that enable several features,\nsuch as:\nchange_current_allocator()\nPYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync\ntorch.cuda.MemPool\ntorch.cuda.MemPool\nAllocating output buffers for an all-reduce usingncclMemAllocallocator can enable NVLink\nSwitch Reductions (NVLS). This can reduce contention between overlapping compute and communication\nkernels on GPU resources (SMs, and Copy Engines), especially on tensor-parallel workloads.\nncclMemAlloc\nFor Grace CPU based systems, allocating host outputs buffers for an all-gather usingcuMemCreateand specifyingCU_MEM_LOCATION_TYPE_HOST_NUMAcan enable Extended GPU Memory (EGM) based memory transfers\nfrom source GPUs to the destination CPU. This accelerates the all-gather since the transfer\nhappens over NVLinks, which otherwise would have happened over bandwidth-limited, Network Interface\nCard (NIC) links. Such an accelerated all-gather can in turn speed up model checkpointing.\ncuMemCreate\nCU_MEM_LOCATION_TYPE_HOST_NUMA\nIf you are crafting a model and don\u2019t want to think about the optimal memory placements of a memory\nintensive module at first (e.g. an embedding table), or perhaps you have a module which is not\nperformance sensitive and doesn\u2019t fit in the GPU, then you could just allocate that module withcudaMallocManagedwith preferred CPU location and get your model working first.\ncudaMallocManaged\nNote\nWhilecudaMallocManagedoffers convenient automatic memory management using CUDA Unified Virtual Memory (UVM),\nit is not recommended for DL workloads. For DL workloads that fit in GPU memory, explicit placement consistently\noutperforms UVM, since there are no page faults and access patterns remain predictable. When GPU memory gets\nsaturated, UVM has to perform costly double transfers, evicting pages to CPU before bringing in new ones.\ncudaMallocManaged\nThe code below showsncclMemAllocwrapped in atorch.cuda.memory.CUDAPluggableAllocator.\nncclMemAlloc\ntorch.cuda.memory.CUDAPluggableAllocator\n\n```python\nimport os\n\nimport torch\nimport torch.distributed as dist\nfrom torch.cuda.memory import CUDAPluggableAllocator\nfrom torch.distributed.distributed_c10d import _get_default_group\nfrom torch.utils import cpp_extension\n\n\n# create allocator\nnccl_allocator_source = \"\"\"\n#include <nccl.h>\n#include <iostream>\nextern \"C\" {\n\nvoid* nccl_alloc_plug(size_t size, int device, void* stream) {\n  std::cout << \"Using ncclMemAlloc\" << std::endl;\n  void* ptr;\n  ncclResult_t err = ncclMemAlloc(&ptr, size);\n  return ptr;\n\n}\n\nvoid nccl_free_plug(void* ptr, size_t size, int device, void* stream) {\n  std::cout << \"Using ncclMemFree\" << std::endl;\n  ncclResult_t err = ncclMemFree(ptr);\n}\n\n}\n\"\"\"\nnccl_allocator_libname = \"nccl_allocator\"\nnccl_allocator = torch.utils.cpp_extension.load_inline(\n    name=nccl_allocator_libname,\n    cpp_sources=nccl_allocator_source,\n    with_cuda=True,\n    extra_ldflags=[\"-lnccl\"],\n    verbose=True,\n    is_python_module=False,\n    build_directory=\"./\",\n)\n\nallocator = CUDAPluggableAllocator(\n    f\"./{nccl_allocator_libname}.so\", \"nccl_alloc_plug\", \"nccl_free_plug\"\n).allocator()\n\n# setup distributed\nrank = int(os.getenv(\"RANK\"))\nlocal_rank = int(os.getenv(\"LOCAL_RANK\"))\nworld_size = int(os.getenv(\"WORLD_SIZE\"))\ntorch.cuda.set_device(local_rank)\ndist.init_process_group(backend=\"nccl\")\ndevice = torch.device(f\"cuda:{local_rank}\")\ndefault_pg = _get_default_group()\nbackend = default_pg._get_backend(device)\n\n# Note: for convenience, ProcessGroupNCCL backend provides\n# the ncclMemAlloc allocator as backend.mem_allocator\nallocator = backend.mem_allocator\n\n```\n\nYou can now define a new memory pool by passing this allocator totorch.cuda.MemPool:\ntorch.cuda.MemPool\n\n```python\npool = torch.cuda.MemPool(allocator)\n\n```\n\nThe pool can then be used with thetorch.cuda.use_mem_poolcontext manager to\nallocate tensors into that pool:\ntorch.cuda.use_mem_pool\n\n```python\nwith torch.cuda.use_mem_pool(pool):\n    # tensor gets allocated with ncclMemAlloc passed in the pool\n    tensor = torch.arange(1024 * 1024 * 2, device=device)\n    print(f\"tensor ptr on rank {rank} is {hex(tensor.data_ptr())}\")\n\n# register user buffers using ncclCommRegister (called under the hood)\nbackend.register_mem_pool(pool)\n\n# Collective uses Zero Copy NVLS\ndist.all_reduce(tensor[0:4])\ntorch.cuda.synchronize()\nprint(tensor[0:4])\n\n```\n\nNote the usage ofregister_mem_poolin the above example. This is an extra step for\nNVLS reductions, where the user buffers need to be registered with NCCL. A user can\nde-register the buffers with a similarderegister_mem_poolcall.\nregister_mem_pool\nderegister_mem_pool\nTo reclaim memory, users will first need to ensure nothing is using the pool. When none\nof the tensors are holding a reference to the pool,empty_cache()will\nbe called internally on deletion of the pool, hence returning all the memory to the system.\nempty_cache()\n\n```python\ndel tensor, del pool\n\n```\n\nUsers can optionally specify ause_on_oombool (which is False by default) during MemPool\ncreation. If true, then the CUDACachingAllocator will be able to use memory in this pool as\na last resort instead of OOMing.\nuse_on_oom\n\n```python\npool = torch.cuda.MemPool(allocator, use_on_oom=True)\nwith torch.cuda.use_mem_pool(pool):\n    a = torch.randn(40 * 1024 * 1024, dtype=torch.uint8, device=\"cuda\")\ndel a\n\n# at the memory limit, this will succeed by using pool's memory in order to avoid the oom\nb = torch.randn(40 * 1024 * 1024, dtype=torch.uint8, device=\"cuda\")\n\n```\n\nThe followingtorch.cuda.MemPool.use_count()andtorch.cuda.MemPool.snapshot()APIs can be used for debugging purposes:\ntorch.cuda.MemPool.use_count()\ntorch.cuda.MemPool.snapshot()\n\n```python\npool = torch.cuda.MemPool(allocator)\n\n# pool's use count should be 1 at this point as MemPool object\n# holds a reference\nassert pool.use_count() == 1\n\nnelem_1mb = 1024 * 1024 // 4\n\nwith torch.cuda.use_mem_pool(pool):\n    out_0 = torch.randn(nelem_1mb, device=\"cuda\")\n\n    # pool's use count should be 2 at this point as use_mem_pool\n    # holds a reference\n    assert pool.use_count() == 2\n\n# pool's use count should be back to 1 at this point as use_mem_pool\n# released its reference\nassert pool.use_count() == 1\n\nwith torch.cuda.use_mem_pool(pool):\n    # pool should have 1 segment since we made a small allocation (1 MB)\n    # above and so the CUDACachingAllocator packed it into a 2 MB buffer\n    assert len(pool.snapshot()) == 1\n\n    out_1 = torch.randn(nelem_1mb, device=\"cuda\")\n\n    # pool should still have 1 segment since we made another small allocation\n    # (1 MB) that got packed into the existing 2 MB buffer\n    assert len(pool.snapshot()) == 1\n\n    out_2 = torch.randn(nelem_1mb, device=\"cuda\")\n\n    # pool now should have 2 segments since the CUDACachingAllocator had\n    # to make a new 2 MB buffer to accommodate out_2\n    assert len(pool.snapshot()) == 2\n\n```\n\nNote\ntorch.cuda.MemPoolholds a reference to the pool. When you use thetorch.cuda.use_mem_poolcontext manager, it will also acquire another reference\nto the pool. On exit of the context manager, it will release its reference. After that,\nideally it should only be tensors holding references to the pool. Once the tensors release\ntheir references, the use count of the pool will be 1, reflecting that only thetorch.cuda.MemPoolobject is holding a reference. Only at that point, can the memory\nheld by the pool be returned to the system when the pool\u2019s destructor is called usingdel.\ntorch.cuda.MemPool\ntorch.cuda.use_mem_pool\ntorch.cuda.MemPool\ndel\ntorch.cuda.MemPooldoesn\u2019t currently supportexpandable_segmentsmode of\nCUDACachingAllocator.\ntorch.cuda.MemPool\nexpandable_segments\nNCCL has specific requirementsfor a buffer to be compatible with NVLS reductions.\nThese requirements can be broken in a dynamic workload, for instance, the buffer being\nsent to NCCL by the CUDACachingAllocator might be split and hence, not correctly aligned.\nIn those cases, NCCL can use a fallback algorithm instead of NVLS.\nAllocators likencclMemAlloccan use more memory than requested, due to alignment\nrequirements (CU_MULTICAST_GRANULARITY_RECOMMENDED,CU_MULTICAST_GRANULARITY_MINIMUM),\nand can cause your workload to run out of memory.\nncclMemAlloc\nCU_MULTICAST_GRANULARITY_RECOMMENDED\nCU_MULTICAST_GRANULARITY_MINIMUM\n\n## Tuning NVLink Performance with Custom Memory Allocator on H100/H200 GPUs#\n\nIn rare cases, performance of NVLink on H100/H200 GPUs can be influenced by the physical memory\nlayout of data, creating an opportunity for developers to tune their applications for optimal\nthroughput.\nAn example of how physical memory layout of data affects performance is when communication\nkernels issue unbalanced NVLink read/write operations. In the following figure, we can see\nthat each warp accesses memory addresses with a consistent strided pattern in each single wave.\nWe can have a more balanced load by tuning the stride size in the workload or we can implement\na custom CUDA allocator.\n\n```python\n_______________________________  _______________________________      _______________________________\n| Warp 0 Reading | No-reading |  | Warp 1 Reading | No-reading |  ...  Warp N Reading | No-reading |\n_______________________________  _______________________________      _______________________________\n<----------------------------->\n        Stride size\n\n```\n\nSuch an allocator can maintain contiguous virtual memory addresses for the kernel while strategically\narranging the mapping to physical memory addresses (e.g., through shuffling). This technique allows\ndevelopers to explore different physical access patterns to find the most efficient one, unlocking\nhigher performance without modifying the kernel\u2019s logic. A practical implementation of such an allocator\ncan be achieved using PyTorch\u2019s custom allocator support as mentioned before, where the malloc and free\nfunctions are:\n\n```python\n// assuming a system with 8 GPUs\nstruct CustomAllocInfo {\n  void** devPtr;  // This will be the usable virtual memory address\n  CUdeviceptr dptr;\n  size_t totalSize;  // Total size of the allocated memory\n  size_t padded_size;\n  int device_id;\n  std::vector<CUmemGenericAllocationHandle> handles;  // Handles to physical memory allocations\n};\n\n// loop over pages\ncudaError_t customCudaMalloc(CustomAllocInfo* info) {\n    if (!info) return cudaErrorInvalidValue;\n\n    CUdeviceptr dptr;\n\n    // Handles to redundant physical memory allocations which help truncate stride pattern in physical memory\n    std::vector<CUmemGenericAllocationHandle> handles_redundant;\n\n    size_t granularity = 0;\n    CUmemAllocationProp prop = {};\n\n    int currentDev = info->device_id;\n    size_t totalSize = info->totalSize;\n\n    prop.type = CU_MEM_ALLOCATION_TYPE_PINNED;\n    prop.location.type = CU_MEM_LOCATION_TYPE_DEVICE;\n    prop.location.id = currentDev;\n    cuMemGetAllocationGranularity(&granularity, &prop, CU_MEM_ALLOC_GRANULARITY_MINIMUM);\n    size_t padded_size = ROUND_UP(totalSize, granularity);\n\n    info->padded_size = padded_size;\n\n    // loop over pages\n    size_t iter_granularity = granularity * 64; // 64 * granularity with shift_size = 2 works\n    uint32_t iteration_count = (totalSize + iter_granularity - 1) / iter_granularity;\n\n    cuMemAddressReserve(&dptr, padded_size, 0ULL, 0ULL, 0ULL);\n\n    const int shift_size = 2;\n    for (size_t i = 0; i < iteration_count; i+=shift_size) {\n\n        CUmemGenericAllocationHandle allocHandle[shift_size];\n        for (int shift = 0; (shift < shift_size)&&(i+shift < iteration_count); shift++){\n            CHECK_CUDA(cuMemCreate(&allocHandle[shift], iter_granularity, &prop, 0));\n            info->handles.push_back(allocHandle[shift]);\n        }\n\n        for (int shift = 0; (shift < shift_size)&&(i+shift < iteration_count); shift++){\n\n            // mapping makes the shift (shift -> (shift+1)%shift_size  )\n            CHECK_CUDA(cuMemMap(dptr + (i+shift) * iter_granularity, iter_granularity, 0, allocHandle[(shift+1)%shift_size], 0));\n\n            setupMultiGPUAccess(dptr + (i+shift) * iter_granularity, iter_granularity, {0, 1, 2, 3, 4, 5, 6, 7}); // Enable access for all 8 GPUs\n        }\n\n        // std::cout << \"Here we allocate one redundant page (2MB)...\" << std::endl;\n        // this is an extra optimization on top of the swizzling. It helps \"break\"\n        // the physical access pattern even more. It can be left out if workload is already\n        // performing at SOL with just swizzling.\n        CUmemGenericAllocationHandle allocHandle_redundant;\n        CHECK_CUDA(cuMemCreate(&allocHandle_redundant, granularity, &prop, 0));\n        handles_redundant.push_back(allocHandle_redundant);\n    }\n\n    *info->devPtr = (void*)dptr;\n    info->dptr = dptr;\n\n    // Release each redundant allocation\n    for (auto handle : handles_redundant) {\n        // std::cout << \"Here we release one redundant page (2MB)...\" << std::endl;\n        CHECK_CUDA(cuMemRelease(handle));\n    }\n\n    return cudaSuccess;\n}\n\nvoid customCudaFree(CustomAllocInfo* info) {\n    if (!info) return;\n\n    // CHECK_CUDA(cudaSetDevice(info->device_id));\n\n    CHECK_CUDA(cuMemUnmap(info->dptr, info->padded_size));\n\n    // Unmap and release each allocation\n    for (auto handle : info->handles) {\n        CHECK_CUDA(cuMemRelease(handle));\n    }\n\n    // Unreserve the virtual address space\n    // CHECK_CUDA(cuMemAddressFree((CUdeviceptr)*info->devPtr, info->padded_size));\n    CHECK_CUDA(cuMemAddressFree(info->dptr, info->padded_size));\n}\n\n```\n\n\n## cuBLAS workspaces#\n\nFor each combination of cuBLAS handle and CUDA stream, a cuBLAS workspace will be allocated\nif that handle and stream combination executes a cuBLAS kernel that requires a workspace.\nIn order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unlesstorch._C._cuda_clearCublasWorkspaces()is called. The workspace size per allocation can be\nspecified via the environment variableCUBLAS_WORKSPACE_CONFIGwith the format:[SIZE]:[COUNT].\nAs an example, the default workspace size per allocation isCUBLAS_WORKSPACE_CONFIG=:4096:2:16:8which specifies a total size of2*4096+8*16KiB. To force cuBLAS to avoid using workspaces,\nsetCUBLAS_WORKSPACE_CONFIG=:0:0.\ntorch._C._cuda_clearCublasWorkspaces()\nCUBLAS_WORKSPACE_CONFIG\n:[SIZE]:[COUNT]\nCUBLAS_WORKSPACE_CONFIG=:4096:2:16:8\n2*4096+8*16KiB\nCUBLAS_WORKSPACE_CONFIG=:0:0\n\n## cuFFT plan cache#\n\nFor each CUDA device, an LRU cache of cuFFT plans is used to speed up repeatedly\nrunning FFT methods (e.g.,torch.fft.fft()) on CUDA tensors of same geometry\nwith same configuration. Because some cuFFT plans may allocate GPU memory,\nthese caches have a maximum capacity.\ntorch.fft.fft()\nYou may control and query the properties of the cache of current device with\nthe following APIs:\ntorch.backends.cuda.cufft_plan_cache.max_sizegives the capacity of the\ncache (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions).\nSetting this value directly modifies the capacity.\ntorch.backends.cuda.cufft_plan_cache.max_size\ntorch.backends.cuda.cufft_plan_cache.sizegives the number of plans\ncurrently residing in the cache.\ntorch.backends.cuda.cufft_plan_cache.size\ntorch.backends.cuda.cufft_plan_cache.clear()clears the cache.\ntorch.backends.cuda.cufft_plan_cache.clear()\nTo control and query plan caches of a non-default device, you can index thetorch.backends.cuda.cufft_plan_cacheobject with either atorch.deviceobject or a device index, and access one of the above attributes. E.g., to set\nthe capacity of the cache for device1, one can writetorch.backends.cuda.cufft_plan_cache[1].max_size=10.\ntorch.backends.cuda.cufft_plan_cache\ntorch.device\n1\ntorch.backends.cuda.cufft_plan_cache[1].max_size=10\n\n## Just-in-Time Compilation#\n\nPyTorch just-in-time compiles some operations, like torch.special.zeta, when\nperformed on CUDA tensors. This compilation can be time consuming\n(up to a few seconds depending on your hardware and software)\nand may occur multiple times for a single operator since many PyTorch operators actually\nselect from a variety of kernels, each of which must be compiled once, depending on their input.\nThis compilation occurs once per process, or just once if a kernel cache is used.\nBy default, PyTorch creates a kernel cache in $XDG_CACHE_HOME/torch/kernels if\nXDG_CACHE_HOME is defined and $HOME/.cache/torch/kernels if it\u2019s not (except on Windows,\nwhere the kernel cache is not yet supported). The caching behavior can be directly\ncontrolled with two environment variables. If USE_PYTORCH_KERNEL_CACHE is set to 0 then no\ncache will be used, and if PYTORCH_KERNEL_CACHE_PATH is set then that path will be used\nas a kernel cache instead of the default location.\n\n## Best practices#\n\n\n## Device-agnostic code#\n\nDue to the structure of PyTorch, you may need to explicitly write\ndevice-agnostic (CPU or GPU) code; an example may be creating a new tensor as\nthe initial hidden state of a recurrent neural network.\nThe first step is to determine whether the GPU should be used or not. A common\npattern is to use Python\u2019sargparsemodule to read in user arguments, and\nhave a flag that can be used to disable CUDA, in combination withis_available(). In the following,args.deviceresults in atorch.deviceobject that can be used to move tensors to CPU or CUDA.\nargparse\nis_available()\nargs.device\ntorch.device\n\n```python\nimport argparse\nimport torch\n\nparser = argparse.ArgumentParser(description='PyTorch Example')\nparser.add_argument('--disable-cuda', action='store_true',\n                    help='Disable CUDA')\nargs = parser.parse_args()\nargs.device = None\nif not args.disable_cuda and torch.cuda.is_available():\n    args.device = torch.device('cuda')\nelse:\n    args.device = torch.device('cpu')\n\n```\n\nNote\nWhen assessing the availability of CUDA in a given environment (is_available()), PyTorch\u2019s default\nbehavior is to call the CUDA Runtime API methodcudaGetDeviceCount. Because this call in turn initializes the\nCUDA Driver API (viacuInit) if it is not already initialized, subsequent forks of a process that has runis_available()will fail with a CUDA initialization error.\nis_available()\nis_available()\nOne can setPYTORCH_NVML_BASED_CUDA_CHECK=1in your environment before importing PyTorch modules that executeis_available()(or before executing it directly) in order to directis_available()to attempt an NVML-based assessment (nvmlDeviceGetCount_v2). If the\nNVML-based assessment is successful (i.e. NVML discovery/initialization does not fail),is_available()calls will not poison subsequent forks.\nPYTORCH_NVML_BASED_CUDA_CHECK=1\nis_available()\nis_available()\nis_available()\nIf NVML discovery/initialization fails,is_available()will fallback to the standard CUDA Runtime\nAPI assessment and the aforementioned fork constraint will apply.\nis_available()\nNote that the above NVML-based CUDA availability assessment provides a weaker guarantee than the default CUDA\nRuntime API approach (which requires CUDA initialization to succeed). In some circumstances, the NVML-based check\nmay succeed while later CUDA initialization fails.\nNow that we haveargs.device, we can use it to create a Tensor on the\ndesired device.\nargs.device\n\n```python\nx = torch.empty((8, 42), device=args.device)\nnet = Network().to(device=args.device)\n\n```\n\nThis can be used in a number of cases to produce device agnostic code. Below\nis an example when using a dataloader:\n\n```python\ncuda0 = torch.device('cuda:0')  # CUDA GPU 0\nfor i, x in enumerate(train_loader):\n    x = x.to(cuda0)\n\n```\n\nWhen working with multiple GPUs on a system, you can use theCUDA_VISIBLE_DEVICESenvironment flag to manage which GPUs are available to\nPyTorch. As mentioned above, to manually control which GPU a tensor is created\non, the best practice is to use atorch.cuda.devicecontext manager.\nCUDA_VISIBLE_DEVICES\ntorch.cuda.device\n\n```python\nprint(\"Outside device is 0\")  # On device 0 (default in most scenarios)\nwith torch.cuda.device(1):\n    print(\"Inside device is 1\")  # On device 1\nprint(\"Outside device is still 0\")  # On device 0\n\n```\n\nIf you have a tensor and would like to create a new tensor of the same type on\nthe same device, then you can use atorch.Tensor.new_*method\n(seetorch.Tensor).\nWhilst the previously mentionedtorch.*factory functions\n(Creation Ops) depend on the current GPU context and\nthe attributes arguments you pass in,torch.Tensor.new_*methods preserve\nthe device and other attributes of the tensor.\ntorch.Tensor.new_*\ntorch.Tensor\ntorch.*\ntorch.Tensor.new_*\nThis is the recommended practice when creating modules in which new\ntensors need to be created internally during the forward pass.\n\n```python\ncuda = torch.device('cuda')\nx_cpu = torch.empty(2)\nx_gpu = torch.empty(2, device=cuda)\nx_cpu_long = torch.empty(2, dtype=torch.int64)\n\ny_cpu = x_cpu.new_full([3, 2], fill_value=0.3)\nprint(y_cpu)\n\n    tensor([[ 0.3000,  0.3000],\n            [ 0.3000,  0.3000],\n            [ 0.3000,  0.3000]])\n\ny_gpu = x_gpu.new_full([3, 2], fill_value=-5)\nprint(y_gpu)\n\n    tensor([[-5.0000, -5.0000],\n            [-5.0000, -5.0000],\n            [-5.0000, -5.0000]], device='cuda:0')\n\ny_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])\nprint(y_cpu_long)\n\n    tensor([[ 1,  2,  3]])\n\n```\n\nIf you want to create a tensor of the same type and size of another tensor, and\nfill it with either ones or zeros,ones_like()orzeros_like()are provided as convenient helper functions (which\nalso preservetorch.deviceandtorch.dtypeof a Tensor).\nones_like()\nzeros_like()\ntorch.device\ntorch.dtype\n\n```python\nx_cpu = torch.empty(2, 3)\nx_gpu = torch.empty(2, 3)\n\ny_cpu = torch.ones_like(x_cpu)\ny_gpu = torch.zeros_like(x_gpu)\n\n```\n\n\n## Use pinned memory buffers#\n\nWarning\nThis is an advanced tip. If you overuse pinned memory, it can cause serious\nproblems when running low on RAM, and you should be aware that pinning is\noften an expensive operation.\nHost to GPU copies are much faster when they originate from pinned (page-locked)\nmemory. CPU tensors and storages expose apin_memory()method, that returns a copy of the object, with data put in a pinned region.\npin_memory()\nAlso, once you pin a tensor or storage, you can use asynchronous GPU copies.\nJust pass an additionalnon_blocking=Trueargument to ato()or acuda()call. This can be used\nto overlap data transfers with computation.\nnon_blocking=True\nto()\ncuda()\nYou can make theDataLoaderreturn batches placed in\npinned memory by passingpin_memory=Trueto its constructor.\nDataLoader\npin_memory=True\n\n## Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel#\n\nMost use cases involving batched inputs and multiple GPUs should default to\nusingDistributedDataParallelto utilize more\nthan one GPU.\nDistributedDataParallel\nThere are significant caveats to using CUDA models withmultiprocessing; unless care is taken to meet the data handling\nrequirements exactly, it is likely that your program will have incorrect or\nundefined behavior.\nmultiprocessing\nIt is recommended to useDistributedDataParallel,\ninstead ofDataParallelto do multi-GPU training, even if\nthere is only a single node.\nDistributedDataParallel\nDataParallel\nThe difference betweenDistributedDataParallelandDataParallelis:DistributedDataParalleluses multiprocessing where a process is created for each GPU, whileDataParalleluses multithreading. By using multiprocessing,\neach GPU has its dedicated process, this avoids the performance overhead caused\nby GIL of Python interpreter.\nDistributedDataParallel\nDataParallel\nDistributedDataParallel\nDataParallel\nIf you useDistributedDataParallel, you could usetorch.distributed.launchutility to launch your program, seeLaunch utility.\nDistributedDataParallel\n\n## CUDA Graphs#\n\nA CUDA graph is a record of the work (mostly kernels and their arguments) that a\nCUDA stream and its dependent streams perform.\nFor general principles and details on the underlying CUDA API, seeGetting Started with CUDA Graphsand theGraphs sectionof the CUDA C Programming Guide.\nPyTorch supports the construction of CUDA graphs usingstream capture, which puts a\nCUDA stream incapture mode. CUDA work issued to a capturing stream doesn\u2019t actually\nrun on the GPU. Instead, the work is recorded in a graph.\nAfter capture, the graph can belaunchedto run the GPU work as many times as needed.\nEach replay runs the same kernels with the same arguments. For pointer arguments this\nmeans the same memory addresses are used.\nBy filling input memory with new data (e.g., from a new batch) before each replay,\nyou can rerun the same work on new data.\n\n## Why CUDA Graphs?#\n\nReplaying a graph sacrifices the dynamic flexibility of typical eager execution in exchange forgreatly reduced CPU overhead. A graph\u2019s arguments and kernels are fixed, so a graph replay\nskips all layers of argument setup and kernel dispatch, including Python, C++, and CUDA driver\noverheads. Under the hood, a replay submits the entire graph\u2019s work to the GPU with\na single call tocudaGraphLaunch.  Kernels in a replay also execute slightly faster\non the GPU, but eliding CPU overhead is the main benefit.\nYou should try CUDA graphs if all or part of your network is graph-safe (usually this means\nstatic shapes and static control flow, but see the otherconstraints)\nand you suspect its runtime is at least somewhat CPU-limited.\n\n## PyTorch API#\n\nWarning\nThis API is in beta and may change in future releases.\nPyTorch exposes graphs via a rawtorch.cuda.CUDAGraphclass\nand two convenience wrappers,torch.cuda.graphandtorch.cuda.make_graphed_callables.\ntorch.cuda.CUDAGraph\ntorch.cuda.graph\ntorch.cuda.make_graphed_callables\ntorch.cuda.graphis a simple, versatile context manager that\ncaptures CUDA work in its context.\nBefore capture, warm up the workload to be captured by running\na few eager iterations. Warmup must occur on a side stream.\nBecause the graph reads from and writes to the same memory addresses in every\nreplay, you must maintain long-lived references to tensors that hold\ninput and output data during capture.\nTo run the graph on new input data, copy new data to the capture\u2019s input tensor(s),\nreplay the graph, then read the new output from the capture\u2019s output tensor(s).\nExample:\ntorch.cuda.graph\n\n```python\ng = torch.cuda.CUDAGraph()\n\n# Placeholder input used for capture\nstatic_input = torch.empty((5,), device=\"cuda\")\n\n# Warmup before capture\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for _ in range(3):\n        static_output = static_input * 2\ntorch.cuda.current_stream().wait_stream(s)\n\n# Captures the graph\n# To allow capture, automatically sets a side stream as the current stream in the context\nwith torch.cuda.graph(g):\n    static_output = static_input * 2\n\n# Fills the graph's input memory with new data to compute on\nstatic_input.copy_(torch.full((5,), 3, device=\"cuda\"))\ng.replay()\n# static_output holds the results\nprint(static_output)  # full of 3 * 2 = 6\n\n# Fills the graph's input memory with more data to compute on\nstatic_input.copy_(torch.full((5,), 4, device=\"cuda\"))\ng.replay()\nprint(static_output)  # full of 4 * 2 = 8\n\n```\n\nSeeWhole-network capture,Usage with torch.cuda.amp, andUsage with multiple streamsfor realistic and advanced patterns.\nmake_graphed_callablesis more sophisticated.make_graphed_callablesaccepts Python functions andtorch.nn.Modules. For each passed function or Module,\nit creates separate graphs of the forward-pass and backward-pass work. SeePartial-network capture.\nmake_graphed_callables\nmake_graphed_callables\ntorch.nn.Module\nA set of ops iscapturableif it doesn\u2019t violate any of the following constraints.\nConstraints apply to all work in atorch.cuda.graphcontext and all work in the forward and backward passes\nof any callable you pass totorch.cuda.make_graphed_callables().\ntorch.cuda.graph\ntorch.cuda.make_graphed_callables()\nViolating any of these will likely cause a runtime error:\nCapture must occur on a non-default stream. (This is only a concern if you use the rawCUDAGraph.capture_beginandCUDAGraph.capture_endcalls.graphandmake_graphed_callables()set a side stream for you.)\nCUDAGraph.capture_begin\nCUDAGraph.capture_end\ngraph\nmake_graphed_callables()\nOps that synchronize the CPU with the GPU (e.g.,.item()calls) are prohibited.\n.item()\nCUDA RNG operations are permitted, and when using multipletorch.Generatorinstances within a graph,\nthey must be registered usingCUDAGraph.register_generator_statebefore graph capture.\nAvoid usingGenerator.get_stateandGenerator.set_stateduring capture;\ninstead, utilizeGenerator.graphsafe_set_stateandGenerator.graphsafe_get_statefor managing generator states safely within the graph context. This ensures proper RNG operation and generator management within CUDA graphs.\ntorch.Generator\nCUDAGraph.register_generator_state\nGenerator.get_state\nGenerator.set_state\nGenerator.graphsafe_set_state\nGenerator.graphsafe_get_state\nViolating any of these will likely cause silent numerical errors or undefined behavior:\nWithin a process, only one capture may be underway at a time.\nNo non-captured CUDA work may run in this process (on any thread) while capture is underway.\nCPU work is not captured. If the captured ops include CPU work, that work will be elided during replay.\nEvery replay reads from and writes to the same (virtual) memory addresses.\nDynamic control flow (based on CPU or GPU data) is prohibited.\nDynamic shapes are prohibited. The graph assumes every tensor in the captured op sequence\nhas the same size and layout in every replay.\nUsing multiple streams in a capture is allowed, but there arerestrictions.\nOnce captured, the graph may be replayed on any stream.\n\n## Whole-network capture#\n\nIf your entire network is capturable, you can capture and replay an entire iteration:\n\n```python\nN, D_in, H, D_out = 640, 4096, 2048, 1024\nmodel = torch.nn.Sequential(torch.nn.Linear(D_in, H),\n                            torch.nn.Dropout(p=0.2),\n                            torch.nn.Linear(H, D_out),\n                            torch.nn.Dropout(p=0.1)).cuda()\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n# Placeholders used for capture\nstatic_input = torch.randn(N, D_in, device='cuda')\nstatic_target = torch.randn(N, D_out, device='cuda')\n\n# warmup\n# Uses static_input and static_target here for convenience,\n# but in a real setting, because the warmup includes optimizer.step()\n# you must use a few batches of real data.\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for i in range(3):\n        optimizer.zero_grad(set_to_none=True)\n        y_pred = model(static_input)\n        loss = loss_fn(y_pred, static_target)\n        loss.backward()\n        optimizer.step()\ntorch.cuda.current_stream().wait_stream(s)\n\n# capture\ng = torch.cuda.CUDAGraph()\n# Sets grads to None before capture, so backward() will create\n# .grad attributes with allocations from the graph's private pool\noptimizer.zero_grad(set_to_none=True)\nwith torch.cuda.graph(g):\n    static_y_pred = model(static_input)\n    static_loss = loss_fn(static_y_pred, static_target)\n    static_loss.backward()\n    optimizer.step()\n\nreal_inputs = [torch.rand_like(static_input) for _ in range(10)]\nreal_targets = [torch.rand_like(static_target) for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    # Fills the graph's input memory with new data to compute on\n    static_input.copy_(data)\n    static_target.copy_(target)\n    # replay() includes forward, backward, and step.\n    # You don't even need to call optimizer.zero_grad() between iterations\n    # because the captured backward refills static .grad tensors in place.\n    g.replay()\n    # Params have been updated. static_y_pred, static_loss, and .grad\n    # attributes hold values from computing on this iteration's data.\n\n```\n\n\n## Partial-network capture#\n\nIf some of your network is unsafe to capture (e.g., due to dynamic control flow,\ndynamic shapes, CPU syncs, or essential CPU-side logic), you can run the unsafe\npart(s) eagerly and usetorch.cuda.make_graphed_callables()to graph only\nthe capture-safe part(s).\ntorch.cuda.make_graphed_callables()\nBy default, callables returned bymake_graphed_callables()are autograd-aware, and can be used in the training loop as direct replacements\nfor the functions ornn.Modules you passed.\nmake_graphed_callables()\nnn.Module\nmake_graphed_callables()internally createsCUDAGraphobjects, runs warmup iterations, and maintains\nstatic inputs and outputs as needed.  Therefore (unlike withtorch.cuda.graph) you don\u2019t need to handle those manually.\nmake_graphed_callables()\nCUDAGraph\ntorch.cuda.graph\nIn the following example, data-dependent dynamic control flow means the\nnetwork isn\u2019t capturable end-to-end, butmake_graphed_callables()lets us capture and run graph-safe sections as graphs regardless:\nmake_graphed_callables()\n\n```python\nN, D_in, H, D_out = 640, 4096, 2048, 1024\n\nmodule1 = torch.nn.Linear(D_in, H).cuda()\nmodule2 = torch.nn.Linear(H, D_out).cuda()\nmodule3 = torch.nn.Linear(H, D_out).cuda()\n\nloss_fn = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(chain(module1.parameters(),\n                                  module2.parameters(),\n                                  module3.parameters()),\n                            lr=0.1)\n\n# Sample inputs used for capture\n# requires_grad state of sample inputs must match\n# requires_grad state of real inputs each callable will see.\nx = torch.randn(N, D_in, device='cuda')\nh = torch.randn(N, H, device='cuda', requires_grad=True)\n\nmodule1 = torch.cuda.make_graphed_callables(module1, (x,))\nmodule2 = torch.cuda.make_graphed_callables(module2, (h,))\nmodule3 = torch.cuda.make_graphed_callables(module3, (h,))\n\nreal_inputs = [torch.rand_like(x) for _ in range(10)]\nreal_targets = [torch.randn(N, D_out, device=\"cuda\") for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    optimizer.zero_grad(set_to_none=True)\n\n    tmp = module1(data)  # forward ops run as a graph\n\n    if tmp.sum().item() > 0:\n        tmp = module2(tmp)  # forward ops run as a graph\n    else:\n        tmp = module3(tmp)  # forward ops run as a graph\n\n    loss = loss_fn(tmp, target)\n    # module2's or module3's (whichever was chosen) backward ops,\n    # as well as module1's backward ops, run as graphs\n    loss.backward()\n    optimizer.step()\n\n```\n\n\n## Usage with torch.cuda.amp#\n\nFor typical optimizers,GradScaler.stepsyncs\nthe CPU with the GPU, which is prohibited during capture. To avoid errors, either usepartial-network capture, or (if forward, loss,\nand backward are capture-safe) capture forward, loss, and backward but not the\noptimizer step:\nGradScaler.step\n\n```python\n# warmup\n# In a real setting, use a few batches of real data.\ns = torch.cuda.Stream()\ns.wait_stream(torch.cuda.current_stream())\nwith torch.cuda.stream(s):\n    for i in range(3):\n        optimizer.zero_grad(set_to_none=True)\n        with torch.cuda.amp.autocast():\n            y_pred = model(static_input)\n            loss = loss_fn(y_pred, static_target)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\ntorch.cuda.current_stream().wait_stream(s)\n\n# capture\ng = torch.cuda.CUDAGraph()\noptimizer.zero_grad(set_to_none=True)\nwith torch.cuda.graph(g):\n    with torch.cuda.amp.autocast():\n        static_y_pred = model(static_input)\n        static_loss = loss_fn(static_y_pred, static_target)\n    scaler.scale(static_loss).backward()\n    # don't capture scaler.step(optimizer) or scaler.update()\n\nreal_inputs = [torch.rand_like(static_input) for _ in range(10)]\nreal_targets = [torch.rand_like(static_target) for _ in range(10)]\n\nfor data, target in zip(real_inputs, real_targets):\n    static_input.copy_(data)\n    static_target.copy_(target)\n    g.replay()\n    # Runs scaler.step and scaler.update eagerly\n    scaler.step(optimizer)\n    scaler.update()\n\n```\n\n\n## Usage with multiple streams#\n\nCapture mode automatically propagates to any streams that sync with a capturing stream.\nWithin capture, you may expose parallelism by issuing calls to different streams,\nbut the overall stream dependency DAG must branch out from the\ninitial capturing stream after capture begins and rejoin the initial stream\nbefore capture ends:\n\n```python\nwith torch.cuda.graph(g):\n    # at context manager entrance, torch.cuda.current_stream()\n    # is the initial capturing stream\n\n    # INCORRECT (does not branch out from or rejoin initial stream)\n    with torch.cuda.stream(s):\n        cuda_work()\n\n    # CORRECT:\n    # branches out from initial stream\n    s.wait_stream(torch.cuda.current_stream())\n    with torch.cuda.stream(s):\n        cuda_work()\n    # rejoins initial stream before capture ends\n    torch.cuda.current_stream().wait_stream(s)\n\n```\n\nNote\nTo avoid confusion for power users looking at replays in nsight systems or nvprof:\nUnlike eager execution, the graph interprets a nontrivial stream DAG in capture\nas a hint, not a command. During replay, the graph may reorganize independent ops\nonto different streams or enqueue them in a different order (while respecting your\noriginal DAG\u2019s overall dependencies).\n\n## Usage with DistributedDataParallel#\n\nNCCL versions earlier than 2.9.6 don\u2019t allow collectives to be captured.\nYou must usepartial-network capture,\nwhich defers allreduces to happen outside graphed sections of backward.\nCallmake_graphed_callables()on graphable network sectionsbeforewrapping the network with DDP.\nmake_graphed_callables()\nNCCL versions 2.9.6 or later allow collectives in the graph.\nApproaches that capture anentire backward passare a viable option, but need three setup steps.\nDisable DDP\u2019s internal async error handling:\n\n```python\nos.environ[\"NCCL_ASYNC_ERROR_HANDLING\"] = \"0\"\ntorch.distributed.init_process_group(...)\n\n```\n\nBefore full-backward capture, DDP must be constructed in a side-stream context:\n\n```python\nwith torch.cuda.stream(s):\n    model = DistributedDataParallel(model)\n\n```\n\nYour warmup must run at least 11 DDP-enabled eager iterations before capture.\n\n## Graph memory management#\n\nA captured graph acts on the same virtual addresses every time it replays.\nIf PyTorch frees the memory, a later replay can hit an illegal memory access.\nIf PyTorch reassigns the memory to new tensors, the replay can corrupt the values\nseen by those tensors.  Therefore, the virtual addresses used by the graph must be\nreserved for the graph across replays. The PyTorch caching allocator achieves this\nby detecting when capture is underway and satisfying the capture\u2019s allocations\nfrom a graph-private memory pool. The private pool stays alive until itsCUDAGraphobject and all tensors created during capture\ngo out of scope.\nCUDAGraph\nPrivate pools are maintained automatically. By default, the allocator creates a\nseparate private pool for each capture. If you capture multiple graphs,\nthis conservative approach ensures graph replays never corrupt each other\u2019s values,\nbut sometimes needlessly wastes memory.\nTo economize the memory stashed in private pools,torch.cuda.graphandtorch.cuda.make_graphed_callables()optionally allow different\ncaptures to share the same private pool.\nIt\u2019s safe for a set of graphs to share a private pool if you know they\u2019ll always\nbe replayed in the same order they were captured,\nand never be replayed concurrently.\ntorch.cuda.graph\ntorch.cuda.make_graphed_callables()\ntorch.cuda.graph\u2019spoolargument is a hint to use a particular private pool,\nand can be used to share memory across graphs as shown:\ntorch.cuda.graph\npool\n\n```python\ng1 = torch.cuda.CUDAGraph()\ng2 = torch.cuda.CUDAGraph()\n\n# (create static inputs for g1 and g2, run warmups of their workloads...)\n\n# Captures g1\nwith torch.cuda.graph(g1):\n    static_out_1 = g1_workload(static_in_1)\n\n# Captures g2, hinting that g2 may share a memory pool with g1\nwith torch.cuda.graph(g2, pool=g1.pool()):\n    static_out_2 = g2_workload(static_in_2)\n\nstatic_in_1.copy_(real_data_1)\nstatic_in_2.copy_(real_data_2)\ng1.replay()\ng2.replay()\n\n```\n\nWithtorch.cuda.make_graphed_callables(), if you want to graph several\ncallables and you know they\u2019ll always run in the same order (and never concurrently)\npass them as a tuple in the same order they\u2019ll run in the live workload, andmake_graphed_callables()will capture their graphs using a shared\nprivate pool.\ntorch.cuda.make_graphed_callables()\nmake_graphed_callables()\nIf, in the live workload, your callables will run in an order that occasionally changes,\nor if they\u2019ll run concurrently, passing them as a tuple to a single invocation ofmake_graphed_callables()is not allowed. Instead, you must callmake_graphed_callables()separately for each one.\nmake_graphed_callables()\nmake_graphed_callables()",
    "url": "https://pytorch.org/docs/stable/notes/cuda.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "a8c35fa645f187f111321e0471958b79",
    "source": "pytorch_docs",
    "title": "Distributed Checkpoint - torch.distributed.checkpoint \u2014 PyTorch 2.9 documentation",
    "text": "\n## Distributed Checkpoint - torch.distributed.checkpoint#\n\nCreated On: Nov 16, 2022 | Last Updated On: Sep 04, 2025\nDistributed Checkpoint (DCP) support loading and saving models from multiple ranks in parallel.\nIt handles load-time resharding which enables saving in one cluster topology and loading into another.\nDCP is different thantorch.saveandtorch.loadin a few significant ways:\ntorch.save\ntorch.load\nIt produces multiple files per checkpoint, with at least one per rank.\nIt operates in place, meaning that the model should allocate its data first and DCP uses that storage instead.\nThe entrypoints to load and save a checkpoint are the following:\n\n## Additional resources:#\n\nGetting Started with Distributed Checkpoint (DCP)\nAsynchronous Saving with Distributed Checkpoint (DCP)\nTorchTitan Checkpointing Docs\nTorchTitan DCP Implementation\nEnum for async checkpointer type.\nThis class contains futures for staging and upload completion.\nIt is returned by async_save().\nstaging_completion is a future that indicates when local copy\nof state_dict is complete.\nupload_completion is a future that indicates when a checkpoint\ncompleted saving.\nSave a distributed model in SPMD style.\nThis function is different fromtorch.save()as it handlesShardedTensor, andDTensorby having each rank only save their local shards.\ntorch.save()\nShardedTensor\nDTensor\nFor eachStatefulobject (having both astate_dictand aload_state_dict),\nsave will callstate_dictbefore serialization.\nStateful\nstate_dict\nload_state_dict\nstate_dict\nWarning\nThere is no guarantees of Backwards Compatibility across PyTorch versions\nfor saved state_dicts.\nWarning\nIf using theprocess_groupargument, make sure that only its ranks\ncallsave_state_dictand that all data in state_dict belong to it.\nNote\nWhen saving checkpoint for FSDP\u2019sShardingStrategy.HYBRID_SHARD, only one of\nthe shard_group should be callingsave_state_dictand the corresponding process\ngroup needs to be passed in.\nNote\nstate_dict in the local process.\nstate_dict(Dict[str,Any]) \u2013 The state_dict to save.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default:None)\nNone\nstorage_writer(Optional[StorageWriter]) \u2013 Instance of StorageWriter used to perform writes. If this is not\nspecified, DCP will automatically infer the writer based on the\ncheckpoint_id. If checkpoint_id is also None, an exception will\nbe raised. (Default:None)\nNone\nplanner(Optional[SavePlanner]) \u2013 Instance of SavePlanner. If this is not specified, the default\nplanner will be used. (Default:None)\nNone\nprocess_group(Optional[ProcessGroup]) \u2013 ProcessGroup to be used for cross-rank synchronization.\n(Default:None)\nNone\nno_dist(bool) \u2013 IfTrue, this function will assume the intent is to load\na checkpoint on a single rank/process.\n(Default:False)\nTrue\nFalse\nuse_collectives(bool) \u2013 IfFalse, this function will assume the intent is to save\na checkpoint without using cross-rank synchronization.\n(Default:True)\nThis configuration is experimental and should be used with caution.\nIt will change the format of the saved checkpoint and may not be backward compatible.\nFalse\nTrue\nMetadata object for the saved checkpoint.\nMetadata\nExample\n\n```python\n>>> my_model = MyModule()\n\n```\n\n\n```python\n>>> state_dict = {\"model\": my_model}\n\n```\n\n\n```python\n>>> fs_storage_writer = torch.distributed.checkpoint.FileSystemWriter(\n...     \"/checkpoint/1\"\n... )\n>>> torch.distributed.checkpoint.save(\n>>>     state_dict=state_dict,\n>>>     storage_writer=fs_storage_writer,\n>>> )\n\n```\n\nNote\nsave_state_dict uses collectives to coordinate writes across ranks.\nFor NCCL-based process groups, internal tensor representations of\nobjects must be moved to the GPU device before communication takes place.\nIn this case, the device used is given bytorch.cuda.current_device()and it is the user\u2019s responsibility to ensure that this is set so that\neach rank has an individual GPU, viatorch.cuda.set_device().\ntorch.cuda.current_device()\ntorch.cuda.set_device()\nAsynchronous version ofsave. This code first de-stages the state_dict on to the\nstaging storage (defaults to CPU memory), and then calls thesavein a separate thread.\nsave\nWarning\nThis feature is experimental and subject to change.\nMUST CALL CLOSE AFTER LAST CHECKPOINT IS SAVED\nstate_dict(Dict[str,Any]) \u2013 The state_dict to save.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default:None)\nNone\nstorage_writer(Optional[StorageWriter]) \u2013 Instance of StorageWriter used to perform \u2018stage\u2019 and  \u2018save\u2019. If\nthis is not specified, DCP will automatically infer the writer based on the\ncheckpoint_id. If checkpoint_id is also None, an exception will\nbe raised. (Default:None)\nNone\nplanner(Optional[SavePlanner]) \u2013 Instance of SavePlanner. If this is not specified, the default\nplanner will be used. (Default:None)\nNone\nprocess_group(Optional[ProcessGroup]) \u2013 ProcessGroup to be used for cross-rank synchronization.\n(Default:None)\nNone\nasync_checkpointer_type(AsyncCheckpointerType) \u2013 whether to do checkpoint in separate thread or process\n(Default:AsyncCheckpointerType.THREAD)\nAsyncCheckpointerType.THREAD\nasync_stager(AsyncStager) \u2013 provides staging implementation. If storage_writer implements AsyncStager\nand async_stager is provided, async_stager will be used for staging\nno_dist(bool) \u2013 IfTrue, this function will assume the intent is to save\na checkpoint on a single rank/process.\n(Default:False)\nTrue\nFalse\nuse_collectives(bool) \u2013 If False, Save the checkpoint without rank coordination. (Default:True)\nThis configuration is experimental and should be used with caution.\nIt will change the format of the saved checkpoint and may not be backward compatible.\nTrue\nA future holding the resultant Metadata object fromsave.\nFuture\nExample\n\n```python\n>>> my_model = MyModule()\n\n```\n\n\n```python\n>>> state_dict = {\"model\": my_model}\n\n```\n\n\n```python\n>>> fs_storage_writer = torch.distributed.checkpoint.FileSystemWriter(\n...     \"/checkpoint/1\"\n... )\n>>> checkpoint_future = torch.distributed.checkpoint.async_save(\n>>>     state_dict=state_dict,\n>>>     storage_writer=fs_storage_writer,\n>>> )\n>>>\n>>> # ... do some work ...\n>>>\n>>> checkpoint_future.result()\n\n```\n\nThis method is deprecated. Please switch to \u2018save\u2019.\nMetadata\nLoad a checkpoint into a distributed state dict in SPMD style.\nEach rank must have the same keys in theirstate_dictprovided to this\nAPI. Mismatched keys may result in hangs or errors. If unsure, you can use\ntheutils._assert_same_keysAPI to check (but may incur communication\ncosts).\nstate_dict\nutils._assert_same_keys\nEach rank will try to read the least amount of data necessary\nto fulfill the requestedstate_dict. When loadingShardedTensororDTensorinstances, each rank only reads data for their local shards.\nShardedTensor\nDTensor\nFor eachStatefulobject (having both astate_dictand aload_state_dict),\nload will first callstate_dictbefore attempting deserialization, followed byload_state_dictonce the deserialization is complete.\nFor each non-Statefulobject, load will deserialize the object, and then replace\nit in thestate_dictwith the deserialized object.\nStateful\nstate_dict\nload_state_dict\nstate_dict\nload_state_dict\nStateful\nstate_dict\nWarning\nAll tensors instate_dictmust be allocated on their\ndestination deviceprior tocalling this function.\nstate_dict\nAll non-tensor data is loaded usingtorch.load()and modified in place\non state_dict.\nWarning\nUsers must callload_state_dicton the root module to ensure load\npos-processing and non-tensor data properly propagates.\nstate_dict(Dict[str,Any]) \u2013 The state_dict to load the checkpoint into.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default:None)\nNone\nstorage_reader(Optional[StorageReader]) \u2013 Instance of StorageWriter used to perform reads. If this is not\nspecified, DCP will automatically infer the reader based on the\ncheckpoint_id. If checkpoint_id is also None, an exception will\nbe raised. (Default:None)\nNone\nplanner(Optional[LoadPlanner]) \u2013 Instance of LoadPlanner. If this is not specified, the default\nplanner will be used. (Default:None)\nNone\nprocess_group(Optional[ProcessGroup]) \u2013 ProcessGroup to be used for cross-rank synchronization.\n(Default:None)\nNone\nno_dist(bool) \u2013 IfTrue, this function will assume the intent is to load\na checkpoint without using cross-rank synchronization. (Default:False)\nTrue\nFalse\nNone.\nNone\n\n```python\n>>> my_model = MyModule()\n>>> optimizer = Adagrad(my_model.parameters())\n>>> model_state_dict = my_model.state_dict()\n>>> fs_storage_reader = torch.distributed.checkpoint.FileSystemReader(\n...     \"/checkpoint/1\"\n... )\n\n```\n\n\n```python\n>>> torch.distributed.checkpoint.load_state_dict(\n>>>     state_dict=model_state_dict,\n>>>     storage_reader=fs_storage_reader,\n>>> )\n\n```\n\n\n```python\n>>> # module.load_state_dict() function might have customized steps\n>>> # to flush the state_dict, must call it to\n>>> # ensure correct behavior.\n>>> my_model.load_state_dict(model_state_dict)\n\n```\n\nNote\nload_state_dict uses collectives to coordinate reads across ranks.\nFor NCCL-based process groups, internal tensor representations of\nobjects must be moved to the GPU device before communication takes place.\nIn this case, the device used is given bytorch.cuda.current_device()and it is the user\u2019s responsibility to ensure that this is set so that each\nrank has an individual GPU, viatorch.cuda.set_device().\ntorch.cuda.current_device()\ntorch.cuda.set_device()\nThis method is deprecated. Please switch to \u2018load\u2019.\nThe following module is also useful for additional customization of the staging mechanisms used for asynchronous checkpointing (torch.distributed.checkpoint.async_save):\ntorch.distributed.checkpoint.async_save\nThis protocol is meant to provide customization and extensibility for dcp.async_save, allowing users\nto customize how data is staged previous to executing the usual dcp.save path in parallel.\nThe expected order of operations (concretely defined intorch.distributed.state_dict_saver.async_save)\nis the following:\nThis call gives the AsyncStager the opportunity to \u2018stage\u2019\nthe state_dict. The expectation and purpose of staging in this context is to create a \u201ctraining-safe\u201d\nrepresentation of the state dict, meaning that any updates to module data after staging is complete\nshould not be reflected in the state dict returned from this method. For example, in the default\ncase a copy of the entire state dict is created on CPU RAM and returned here, allowing users\nto continue training without risking changes to data which is being serialized.\nfor serializing the state_dict and writing it to storage.\nthe serialization thread starts and before returning from dcp.async_save. If this is set to False,\nthe assumption is the user has defined a custom synchronization point for the the purpose of further\noptimizing save latency in the training loop (for example, by overlapping staging with the\nforward/backward pass), and it is the respondsibility of the user to callAsyncStager.synchronize_stagingat the appropriate time.\nClean up all resources used by the stager.\nWhether to synchronize after executing the stage.\nReturns a \u201cstaged\u201d copy ofstate_dict. The expectation of the staged copy is that it is\ninoculated from any updates incurred after the stage call is complete.\nUnion[Future[dict[str,Union[~StatefulT,Any]]],dict[str,Union[~StatefulT,Any]]]\nIn the casestageis async in some way, this method should be called to ensure staging\nis complete and it is safe to begin modifying the originalstate_dict\nDefaultStager provides a full-featured staging implementation that combines\nmultiple optimization techniques for efficient checkpoint preparation.\nThe staging process works as follows:\n1. State dictionary is submitted for staging (sync or async)\n2. Tensors are copied from GPU to optimized CPU storage\n3. CUDA operations are synchronized if non-blocking copies are used\n4. Staged state dictionary is returned or made available via Future\n# Synchronous staging\nstager = DefaultStager(StagingOptions(use_async_staging=False))\nstaged_dict = stager.stage(state_dict)\nstager.close()\n# Asynchronous staging\nstager = DefaultStager(StagingOptions(use_async_staging=True))\nfuture = stager.stage(state_dict)\n# \u2026 do other work \u2026\nstaged_dict = future.result()\nstager.close()\n# Context manager pattern (recommended)\nstager = DefaultStager(config)\nwith stager:\nresult = stager.stage(state_dict)\nAsync staging provides best performance when model computation\ncan overlap with staging operations\nPinned memory improves CPU-GPU transfer speeds but uses more memory\nShared memory allows efficient IPC to checkpoint process\nNon-blocking copies reduce GPU idle time during memory transfers\nDefaultStager is not thread-safe. Each thread should use its own\ninstance, or external synchronization should be provided.\nClean up all resources used by the DefaultStager. Shuts down the ThreadPoolExecutor\nused for async staging operations and cleans up the underlying StateDictStager\u2019s\ncached storages. Should be called when the stager is no longer needed to prevent\nresource leaks, especially in long-running applications. After calling close(),\nthe stager should not be used for further staging operations.\nstager = DefaultStager(StagingOptions(use_async_staging=True))\nfuture = stager.stage(state_dict)\nresult = future.result()\nstager.close()  # Clean up all resources\nThis function is responsible for staging staging the state_dict.\nSee class docstring for more details on staging.\nIf use_async_staging is True, it will return a Future object that will be\nfulfilled when staging is complete.\nIf use_async_staging is False, it will return the fully staged state_dict.\nstate_dict(STATE_DICT_TYPE) \u2013 The state_dict to be staged.\nUnion[dict[str,Union[~StatefulT,Any]],Future[dict[str,Union[~StatefulT,Any]]]]\nWhen use_async_staging is True, this method will wait until staging is complete.\nIf use_async_staging is False, this method is a no-op.\nConfiguration options for checkpoint staging behavior.\nuse_pinned_memory(bool) \u2013 Enable pinned memory allocation for faster\nCPU-GPU transfers. Requires CUDA to be available. Default: True\nuse_shared_memory(bool) \u2013 Enable shared memory for multi-process\nscenarios. Useful when multiple processes need access to the\nsame staged data. Default: True\nuse_async_staging(bool) \u2013 Enable asynchronous staging using a\nbackground thread pool. Allows overlapping computation with\nstaging operations. Requires CUDA. Default: True\nuse_non_blocking_copy(bool) \u2013 Use non-blocking device memory\ncopies with stream synchronization. Improves performance by\nallowing CPU work to continue during GPU transfers. Default: True\nNote\nCUDA-dependent features will raise exception if CUDA is not available.\nAn implementation of AsyncStager which stages the state_dict on CPU RAM and blocks until the copy is complete.\nThis implementation also provides an option to optimize stage latency using pinned memory.\nN.B. synchronize_staging is a no-op in this case.\nReturns a copy ofstate_dicton the CPU.\ndict[str,Union[~StatefulT,Any]]\nNo-op function, since staging is blocking.\nIn addition to the above entrypoints,Statefulobjects, as described below, provide additional customization during saving/loading\nStateful\nStateful protocol for objects that can be checkpointed and restored.\nRestore the object\u2019s state from the provided state_dict.\nstate_dict(dict[str,Any]) \u2013 The state dict to restore from\nObjects should return their state_dict representation as a dictionary.\nThe output of this function will be checkpointed, and later restored inload_state_dict().\nWarning\nBecause of the inplace nature of restoring a checkpoint, this function\nis also called duringtorch.distributed.checkpoint.load.\nThe objects state dict\nDict\nThisexampleshows how to use Pytorch Distributed Checkpoint to save a FSDP model.\nThe following types define the IO interface used during checkpoint:\nInterface used byload_state_dictto read from storage.\nload_state_dict\nOne StorageReader instance acts as both the coordinator and the follower\nin a distributed checkpoint. As part of initialization, each instance\nis told its role.\nA subclass should expected the following sequence of calls byload_state_dict:\nload_state_dict\n(all ranks) set checkpoint_id if users pass a valid checkpoint_id.\n(all ranks) read_metadata()\n(all ranks) set_up_storage_reader()\n(all ranks) prepare_local_plan()\n(coordinator) prepare_global_plan()\n(all ranks) read_data()\nPerform centralized planning of storage loading.\nThis method is only called on the coordinator instance.\nWhile this method can produce a completely different plan, the preferred\nway is to store storage specific data in LoadPlan::storage_data.\nplans(list[torch.distributed.checkpoint.planner.LoadPlan]) \u2013 A list ofLoadPlaninstances, one for each rank.\nLoadPlan\nA list of transformedLoadPlanafter storage global planning\nLoadPlan\nlist[torch.distributed.checkpoint.planner.LoadPlan]\nPerform storage-specific local planning.\nWhile this method can produce a completely different plan, the recommended\nway is to store storage specific data in LoadPlan::storage_data.\nplan(LoadPlan) \u2013 The local plan from theLoadPlanin use.\nLoadPlan\nA transformedLoadPlanafter storage local planning\nLoadPlan\nLoadPlan\nRead all items fromplanusingplannerto resolve the data.\nplan\nplanner\nA subclass should callLoadPlanner::load_bytesto deserialize a BytesIO\nobject into the right place.\nLoadPlanner::load_bytes\nA subclass should callLoadPlanner::resolve_tensorto get access to the\ntensors that in should load data into.\nLoadPlanner::resolve_tensor\nIt\u2019s the StorageLayer responsibility to properly schedule any cross device copies\nrequired.\nplan(LoadPlan) \u2013 The local plan to execute on\nplanner(LoadPlanner) \u2013 The planner object to use to resolve items.\nA future that completes once all reads are finished.\nFuture[None]\nRead the checkpoint metadata.\nThe metadata object associated with the checkpoint being loaded.\nMetadata\nCalls to indicates a brand new checkpoint read is going to happen.\nA checkpoint_id may be present if users set the checkpoint_id for\nthis checkpoint read. The meaning of the checkpiont_id is\nstorage-dependent. It can be a path to a folder/file or a key for\na key-value storage.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is more like a key-value store.\n(Default:None)\nNone\nInitialize this instance.\nmetadata(Metadata) \u2013 The metadata schema to use.\nis_coordinator(bool) \u2013 Whether this instance is responsible for coordinating\nthe checkpoint.\nCheck if the given checkpoint_id is supported by the storage. This allow\nus to enable automatic storage selection.\nbool\nInterface used bysave_state_dictto write to storage.\nsave_state_dict\nOne StorageWriter instance acts as both the coordinator and the follower\nin a distributed checkpoint. As part of initialization, each instance\nis told its role.\nA subclass should expect the following sequence of calls.\n(all ranks) set checkpoint_id if users pass a valid checkpoint_id.\n(all ranks) set_up_storage_writer()\n(all ranks) prepare_local_plan()\n(coordinator) prepare_global_plan()\n(all ranks) write_data()\n(coordinator) finish()\nWrite the metadata and marks the current checkpoint as successful.\nThe actual format/schema used for serializingmetadatais an\nimplementation detail. The only requirement is that it\u2019s recoverable\nin to the same object graph.\nmetadata(Metadata) \u2013 metadata for the new checkpoint\nresults(list[list[torch.distributed.checkpoint.storage.WriteResult]]) \u2013 A list of WriteResults from all ranks.\nNone\nNone\nPerform centralized planning of storage.\nThis method is only called on the coordinator instance.\nWhile this method can produce a completely different plan, the preferred\nway is to store storage specific data in SavePlan::storage_data.\nplans(list[torch.distributed.checkpoint.planner.SavePlan]) \u2013 A list ofSavePlaninstances, one for each rank.\nSavePlan\nA list of transformedSavePlanafter storage global planning\nSavePlan\nlist[torch.distributed.checkpoint.planner.SavePlan]\nPerform storage-specific local planning.\nWhile this method can produce a completely different plan, the recommended\nway is to store storage specific data in SavePlan::storage_data.\nplan(SavePlan) \u2013 The local plan from theSavePlannerin use.\nSavePlanner\nA transformedSavePlanafter storage local planning\nSavePlan\nSavePlan\nCalls to indicates a brand new checkpoint write is going to happen.\nA checkpoint_id may be present if users set the checkpoint_id for\nthis checkpoint write. The meaning of the checkpiont_id is\nstorage-dependent. It can be a path to a folder/file or a key for\na key-value storage.\ncheckpoint_id(Union[str,os.PathLike,None]) \u2013 The ID of this checkpoint instance. The meaning of the checkpoint_id\ndepends on the storage. It can be a path to a folder or to a file.\nIt can also be a key if the storage is a key-value store.\n(Default:None)\nNone\nInitialize this instance.\nis_coordinator(bool) \u2013 Whether this instance is responsible for coordinating\nthe checkpoint.\nReturn the storage-specific metadata. This is used to store additional information\nin a checkpoint that can be useful for providing request-level observability. StorageMeta\nis passed to theSavePlannerduring save calls. Returns None by default.\nSavePlanner\nTODO: provide an example\nOptional[StorageMeta]\nCheck if the given checkpoint_id is supported by the storage. This allow\nus to enable automatic storage selection.\nbool\nWrite all items fromplanusingplannerto resolve the data.\nplan\nplanner\nA subclass should callSavePlanner::resolve_dataon each item\nfrom the plan to get access to the underlying object to write.\nSavePlanner::resolve_data\nSubclasses should lazily callresolve_dataas it can allocate memory.\nIn case of tensors, make following assumptions:\nThey might be on any device, including not matching the one onWriteItem::tensor_data\nWriteItem::tensor_data\nThey might be views or not contiguous. Only the projection needs to be saved.\nplan(SavePlan) \u2013 The save plan to execute.\nplanner(SavePlanner) \u2013 Planner object to be used to resolve items to data.\nA future that completes to a list of WriteResult\nFuture[list[torch.distributed.checkpoint.storage.WriteResult]]\nThe following types define the planner interface used during checkpoint:\nAbstract class defining the protocol used by load_state_dict to plan the load process.\nLoadPlanner are stateful objects that can be used to customize the whole load process.\nLoadPlanner acts as an access proxy to the state_dict, so any transformation done to it\nwill be visible to the whole process.\nA planner subclass can expect the following sequence of calls during load_state_dict:\nSignals the start of loading a checkpoint.\nProcess the state_dict and produces aLoadPlanthat will be sent for global planning.\nTakes the LoadPlan from all ranks and make any global decision.\nThis is called once per non-tensor value in state_dict.\nThey are called in pair for each Tensor value in state_dict.\nUsers are recommended to extend DefaultLoadPlanner instead of this interface directly as\nmost changes can be expressed by changes in a single method.\nThere are two usual patterns of extension:\nRewriting state_dict. This is the simplest way to extend the load process as it\ndoesn\u2019t requite understanding the intrincacies of how LoadPlan works. We need\nto keep a reference to the original state_dict as load happens in place so\nwe need to be able to perform it in place\n\n```python\n>>> class RenamePlanner(DefaultLoadPlanner):\n>>>     def set_up_planner(\n>>>         self,\n>>>         state_dict: STATE_DICT_TYPE,\n>>>         metadata: Metadata,\n>>>         is_coordinator: bool,\n>>>     ) -> None:\n>>>         self.original_state_dict = state_dict\n>>>         state_dict = {\"foo_\" + k: v for k, v in state_dict.items()}\n>>>\n>>>         if self.flatten_sharded_tensors:\n>>>             state_dict = _flatten_sharded_tensors(state_dict)\n>>>\n>>>         if self.flatten_state_dict:\n>>>             state_dict, self.mappings = flatten_state_dict(state_dict)\n>>>\n>>>         self.state_dict = state_dict\n>>>         self.metadata = metadata\n>>>         self.is_coordinator = is_coordinator\n>>>\n>>>     def load_bytes(self, read_item, value):\n>>> # Remove the \"foo_\" prefix\n>>>         self.original_state_dict[read_item.dest_index.fqn[4:]] = torch.load(value, weights_only=False)\n\n```\n\nModifying resolve_tensor and commit_tensor to handle load time transformation.\n\n```python\n>>> class MetaModelMaterialize(DefaultSavePlanner):\n>>>     def resolve_tensor(self, read_item):\n>>>         tensor = super().resolve_tensor(read_item)\n>>>         return torch.empty_like(tensor, device=\"cpu\")\n>>>\n>>>     def commit_tensor(self, read_item, tensor):\n>>>         self.state_dict[read_item.dest_index.fqn] = tensor\n\n```\n\nCall once the StorageReader finished loading data intotensor.\ntensor\nThe provided tensor is the same one returned by the call toresolve_tensor.\nThis method is only needed if this LoadPlanner needs to post processtensorprior to\ncopying it back to the one in the state_dict.\nresolve_tensor\ntensor\nThe contents of tensor will follow its device synchronization model.\nCompute the global load plan and return plans for each rank.\n. N.B. This is called on the coordinator rank only\nlist[torch.distributed.checkpoint.planner.LoadPlan]\nCreate a LoadPlan based on state_dict and metadata provided by set_up_planner.\n. N.B. This is called on every rank.\nLoadPlan\nAccept the plan from coordinator and return final LoadPlan.\nLoadPlan\nLoad the item described byread_item``and``value.\nread_item``and``value\nThis method is expected to modify in-place the underlying state_dict.\nThe contents ofvalueare defined by the SavePlanner used to produce\nthe checkpoint being loaded.\nvalue\nReturn the BytesIO to be used by the StorageReader to loadread_item.\nThe BytesIO should alias with one on the underlying state_dict as StorageReader will replace its contents.\nBytesIO\nReturn the tensor described byread_itemto be used by the StorageReader to loadread_item.\nread_item\nThe tensor should alias with one on the underlying state_dict as StorageReader will replace its contents.\nIf, for any reason, that\u2019s not possible, the planner can use thecommit_tensormethod to copy the data\nback to the one in state_dict.\ncommit_tensor\nTensor\nInitialize this instance to load data intostate_dict.\nstate_dict\n. N.B. This is called on every rank.\nAbstract class defining the protocol used by save_state_dict to plan the save process.\nSavePlanners are stateful objects that can be used to customize the whole save process.\nSavePlanner acts as an access proxy to the state_dict, so any transformation done to it\nwill be visible to the whole process.\nA planner subclass can expect the following sequence of calls during save_state_dict:\nSignals the start of a checkpoint save.\nProcess the state_dict and produces aSavePlanthat will be sent for global planning.\nTakes the SavePlan from all ranks and make any global decision.\nThis gives each rank a chance to adjust to global planning decisions.\nLookups a value on thestate_dictfor the storage layer to write.\nUsers are recommended to extend DefaultSavePlanner instead of this interface directly as\nmost changes can be expressed by changes in a single method.\nThere are 3 usual patterns of extension:\nRewriting state_dict. This is the simplest way to extend the save process as it\ndoesn\u2019t requite understanding the intrincacies of how SavePlan works:\n\n```python\n>>> class RenamePlanner(DefaultSavePlanner):\n>>>     def set_up_planner(\n>>>         self,\n>>>         state_dict: STATE_DICT_TYPE,\n>>>         storage_meta: Optional[StorageMeta],\n>>>         is_coordinator: bool,\n>>>     ) -> None:\n>>> # prefix all keys with `foo_``\n>>>         super().set_up_planner({\"foo_\" + k: v for k, v in state_dict.items()}, storage_meta, is_coordinator)\n\n```\n\nModifying local plan and lookup in tandem. This is useful when fine control of how data is persisted\n\n```python\n>>> class FP16Planner(DefaultSavePlanner):\n>>>     def create_local_plan(self):\n>>>         plan = super().create_local_plan()\n>>>         for p in plan:\n>>>             if p.tensor_data is not None:\n>>>                 p.tensor_data.properties.dtype = torch.float16\n>>>         return plan\n>>>\n>>>     def resolve_data(self, write_item):\n>>>         item = super().resolve_data(write_item)\n>>>         return item if write_item.type == WriteItemType.BYTE_IO else item.to(torch.float16)\n\n```\n\nUsing the global planning step to make central decisions that can\u2019t be made individually by each rank\n\n```python\n>>> from itertools import zip_longest\n>>> from dataclasses import replace\n>>> class DDPLoadBalancingPlanner(DefaultSavePlanner):\n>>> # This uses the default local plan behavior of having all non-sharded writes in rank 0\n>>> # This sample doesn't handle ShardedTensors\n>>>     def create_global_plan(self, all_plans):\n>>>         iters = [iter(all_plans[0].items)] * len(all_plans)\n>>>         items_per_rank = [\n>>>             [item for item in items if item is not None]\n>>>             for items in zip(*zip_longest(*iters), strict=True)\n>>>         ]\n>>>         all_plans = [\n>>>             replace(plan, items=items)\n>>>             for plan, items in zip(all_plans, items_per_rank, strict=True)\n>>>         ]\n>>>         return super().create_global_plan(all_plans)\n\n```\n\nFinally, some planners need to save additional metadata in the checkpoint, this is\naccomplished by having each rank contribute their data items in the local plan and\nthe global planner aggregate them:\n\n```python\n>>> class SaveExtraDataPlanner(DefaultSavePlanner):\n>>>     def create_local_plan(self) -> SavePlan:\n>>>         plan = super().create_local_plan()\n>>>         return replace(plan, planner_data=\"per-rank-data\")\n>>>\n>>>     def create_global_plan(self, all_plans: List[SavePlan]) -> Tuple[List[SavePlan], Metadata]:\n>>>         global_plan, metadata = super().create_global_plan(all_plans)\n>>>         merged_data = [p.planner_data for p in global_plan]\n>>>         metadata = replace(metadata, planner_data=merged_data)\n>>>         return global_plan, metadata\n\n```\n\nCompute the global checkpoint plan and return the local plan of each rank.\nThis is called on the coordinator rank only.\ntuple[list[torch.distributed.checkpoint.planner.SavePlan], torch.distributed.checkpoint.metadata.Metadata]\nCompute the save plan for the current rank.\nThis will be aggregated and passed to create_global_plan.\nPlanner specific data can be passed through SavePlan::planner_data.\nThis is called on all ranks.\nSavePlan\nMerge the plan created bycreate_local_planand the result ofcreate_global_plan.\nThis is called on all ranks.\nSavePlan\nTransform and preparewrite_itemfromstate_dictfor storage, ensuring idempotency and thread-safety.\nwrite_item\nstate_dict\nLookup the object associated withwrite_iteminstate_dictand apply any\ntransformation (such as serialization) prior to the storage layer consuming it.\nwrite_item\nstate_dict\nCalled on each rank multiple times, at least once per WriteItem in the final SavePlan.\nThis method should be idempotent and thread-save. StorageWriter implementations\nare free to call it as frequently as they need.\nAny transformation that allocates memory should be lazily done when his method\nis called in order to reduce peak memory required by checkpointing.\nWhen returning tensors, they can be on any device or format, they can be views too.\nIt\u2019s the storage layer responsibility to figure out how to save them.\nUnion[Tensor,BytesIO]\nInitialize this planner to savestate_dict.\nstate_dict\nImplementations should save those values as they won\u2019t be provided lated in the save process.\nThis is called on all ranks.\nDataclass which holds information about what needs to be written to storage.\nCalculates the storage size of the underlying tensor, or None if this is not a tensor write.\nOptional[int] storage size, in bytes of underlying tensor if any.\nOptional[int]\nWe provide a filesystem based storage layer:\nreturn the checkpoint_id that will be used to load the checkpoint.\nBasic implementation of StorageWriter using file IO.\nThis implementation makes the following assumptions and simplifications:\nThe checkpoint path is an empty or non-existing directory.\nFile creation is atomic\nThe checkpoint consist of one file per write request plus\na global.metadatafile with the serialized metadata if rank coordination is enabled.\na rank local__{rank}.metadatafile with the serialized metadata if rank coordination is NOT enabled.\nOverride of AsyncStager.stage\ndict[str,Union[~StatefulT,Any]]\nWe also provide other storage layers, including ones to interact with HuggingFace safetensors:\n.. autoclass:: torch.distributed.checkpoint.HuggingFaceStorageReader\n:members:\n.. autoclass:: torch.distributed.checkpoint.HuggingFaceStorageWriter\n:members:\n.. autoclass:: torch.distributed.checkpoint.QuantizedHuggingFaceStorageReader\n:members:\nWe provide default implementations ofLoadPlannerandSavePlannerthat\ncan handle all of torch.distributed constructs such as FSDP, DDP, ShardedTensor and DistributedTensor.\nLoadPlanner\nSavePlanner\nExtension from the planner interface to make it easy to extend the default planner.\nAny\nExtension from the planner interface to make it easy to extend the default planner.\nDefaultLoadPlanner that adds multiple features on top of LoadPlanner.\nIn particular it adds the following:\nflatten_state_dict: Handle state_dict with nested dicts\nflatten_sharded_tensors: For FSDP in 2D parallel mode\nallow_partial_load: If False, will raise a runtime error if a key is present in state_dict, but not in the checkpoint.\nExtension from the planner interface to make it easy to extend the default planner.\nTensor\nExtension from the planner interface to make it easy to extend the default planner.\nDue to legacy design decisions, the state dictionaries ofFSDPandDDPmay have different keys or fully qualified names (e.g., layer1.weight) even when the original unparallelized model is identical. Moreover,FSDPoffers various types of model state dictionaries, such as full and sharded state dictionaries. Additionally, optimizer state dictionaries employ parameter IDs instead of fully qualified names to identify parameters, potentially causing issues when parallelisms are used (e.g., pipeline parallelism).\nFSDP\nDDP\nFSDP\nTo tackle these challenges, we offer a collection of APIs for users to easily manage state_dicts.get_model_state_dict()returns a model state dictionary with keys consistent with those returned by the unparallelized model state dictionary. Similarly,get_optimizer_state_dict()provides the optimizer state dictionary with keys uniform across all parallelisms applied. To achieve this consistency,get_optimizer_state_dict()converts parameter IDs to fully qualified names identical to those found in the unparallelized model state dictionary.\nget_model_state_dict()\nget_optimizer_state_dict()\nget_optimizer_state_dict()\nNote that results returned by these APIs can be used directly with thetorch.distributed.checkpoint.save()andtorch.distributed.checkpoint.load()methods without requiring any additional conversions.\ntorch.distributed.checkpoint.save()\ntorch.distributed.checkpoint.load()\nset_model_state_dict()andset_optimizer_state_dict()are provided to load the model and optimizer state_dict generated by by their respective getter APIs.\nset_model_state_dict()\nset_optimizer_state_dict()\nNote thatset_optimizer_state_dict()can only be called beforebackward()or afterstep()is called on optimizers.\nset_optimizer_state_dict()\nbackward()\nstep()\nNote that this feature is experimental, and API signatures might change in the future.\nReturn the model state_dict and optimizers state_dict.\nget_state_dictcan process any module that is parallelized by PyTorch\nFSDP/fully_shard, DDP/replicate, tensor_parallel/parallelize_module, and any\ncombination of these parallelisms. The main functions ofget_state_dictare: 1.) returning a model and optimizer state_dict that can be resharded\nwith a different number of trainers and/or different parallelisms.\n2.) hiding the parallelism-specific state_dict APIs. Users don\u2019t have to call\nthese APIs.\n3.) sanity checking the result state_dict.\nget_state_dict\nget_state_dict\nThe keys of the result state dictionary are the canonical FQNs (Fully\nQualified Names).  A canonical FQN refers to the FQN based on a parameter\u2019s\nposition in an nn.Module hierarchy. More specifically, a canonical FQN to a\nparameter is the FQN returned bymodule.named_parameters()ormodule.named_buffers()when the module is not distributed by any\nparallelisms. Since the optimizer internally uses parameter IDs to represent\na parameter, there will be a conversion from the parameter IDs to the\ncanonical FQNs when calling this API.\nmodule.named_parameters()\nmodule.named_buffers()\nget_state_dictcan also process a module that is not parallelized. In\nsuch a case,get_state_dictonly performs one function \u2013 converting the\noptimizer parameter IDs to the canonical FQNs.\nget_state_dict\nget_state_dict\nExample\n\n```python\n>>> import torch\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n>>> from torch.distributed.checkpoint.state_dict import get_state_dict\n\n```\n\n\n```python\n>>> fsdp_model = FSDP(copy.deepcopy(model))\n>>> fsdp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n>>> ddp_model = DDP(copy.deepcopy(model))\n>>> ddp_optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n```\n\n\n```python\n>>> ddp_state_dict, ddp_optim_state_dict = get_state_dict(ddp_model, ddp_optim)\n>>> fsdp_state_dict, fsdp_optim_state_dict = get_state_dict(\n...     fsdp_model, fsdp_optim\n... )\n\n```\n\n\n```python\n>>> # if we simply call ddp_model.state_dict() and fsdp_model.state_dict(),\n>>> # the asserts will fail.\n>>> assert ddp_state_dict == fsdp_state_dict\n>>> assert ddp_optim_state == fsdp_optim_state_dict\n\n```\n\nmodel(nn.Module) \u2013 the nn.Module to the model.\noptimizers(Union[None,Optimizer,Iterable[Optimizer]]) \u2013 The optimizers that are used to optimizemodel.\nmodel\nsubmodules(deprecated) \u2013 Optional[set[nn.Module]]: only return the model parameters\nthat belong to the submodules.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be returned. SeeStateDictOptionsfor the details.\nTuplethat contain model state_dict and optimizer state_dict.\nTuple\nTuple[Dict[str, ValueType], OptimizerStateType]\nReturn the model state_dict ofmodel.\nmodel\nSeeget_state_dictfor the detail usage.\nget_state_dict\nmodel(nn.Module) \u2013 the nn.Module to the model.\nsubmodules(deprecated) \u2013 Optional[set[nn.Module]]: only return the model parameters\nthat belong to the submodules.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be returned. SeeStateDictOptionsfor the details.\nThe state_dict formodel.\nmodel\nDict[str, ValueType]\nReturn the combined state_dict for optimizers.\nSeeget_state_dictfor the detail usage.\nget_state_dict\nmodel(nn.Module) \u2013 the nn.Module to the model.\noptimizers(Union[None,Optimizer,Iterable[Optimizer]]) \u2013 The optimizers that are used to optimizemodel.\nmodel\nsubmodules(deprecated) \u2013 Optional[set[nn.Module]]: only return the model parameters\nthat belong to the submodules.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be returned. SeeStateDictOptionsfor the details.\nThe state_dict foroptimizers.\noptimizers\nOptimizerStateType\nLoad the model state_dict and optimizers state_dict.\nThe counterpart ofget_state_dictto set the state_dict to the model and\noptimizers.  The givenmodel_state_dictandoptim_state_dictdo not\nhave to be returned byget_state_dictbut must meet the following\nrequirements: 1) all FQNs are canonical FQNs as defined inget_state_dict,\n2) if a tensor is sharded, it must be either a ShardedTensor or DTensor,\n3) optimizer state_dict cannot contain the parameter IDs; the keys should be\nthe canonical FQNs.\nget_state_dict\nmodel_state_dict\noptim_state_dict\nget_state_dict\nget_state_dict\nset_state_dict\nbackward()\nstep()\nis called on the optimizers. Otherwise, the optimizer states won\u2019t be initialized\ncorrectly.\nmodel(nn.Module) \u2013 the nn.Module to the model.\noptimizers(Union[Optimizer,Iterable[Optimizer]]) \u2013 The optimizers that are used to optimizemodel.\nmodel\nmodel_state_dict(Dict[str,ValueType]) \u2013 (Union[Dict[nn.Module, Dict[str, ValueType]], Dict[str, ValueType]]):\nthe model state_dict to load. If the key of themodel_state_dictis nn.Module, the key is a submodule ofmodeland the value should\nbe the state_dict of the submodule. When loading the state_dict,\nthe prefix of the submodule will be append to the state_dict.\nmodel_state_dict\nmodel\noptim_state_dict(OptimizerStateType) \u2013 OptimizerStateType:\nthe optimizer state_dict to load.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be loaded. SeeStateDictOptionsfor the details.\nmissing_keysis a list of str containing the missing keys of the model state_dict.unexpected_keysis a list of str containing the unexpected keys of the model state_dict.\nmissing_keysis a list of str containing the missing keys of the model state_dict.\nunexpected_keysis a list of str containing the unexpected keys of the model state_dict.\nNamedTuplewithmissing_keysandunexpected_keysfields\nNamedTuple\nmissing_keys\nunexpected_keys\nLoad the model state_dict.\nThe counterpart ofget_model_state_dictto set the state_dict to the\nmodel. Seeset_state_dictfor the detail usage.\nget_model_state_dict\nset_state_dict\nmodel(nn.Module) \u2013 the nn.Module to the model.\nmodel_state_dict(Dict[str,ValueType]) \u2013 (Dict[str, ValueType]):\nthe model state_dict to load. If the key of themodel_state_dictis nn.Module, the key is a submodule ofmodeland the value should\nbe the state_dict of the submodule. When loading the state_dict,\nthe prefix of the submodule will be append to the state_dict.\nmodel_state_dict\nmodel\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be loaded. SeeStateDictOptionsfor the details.\nmissing_keysis a list of str containing the missing keysunexpected_keysis a list of str containing the unexpected keys\nmissing_keysis a list of str containing the missing keys\nunexpected_keysis a list of str containing the unexpected keys\nNamedTuplewithmissing_keysandunexpected_keysfields\nNamedTuple\nmissing_keys\nunexpected_keys\nLoad the optimizers state_dict.\nThe counterpart ofget_optimizer_state_dictto set the state_dict to the\noptimizers. Seeset_state_dictfor the detail usage.\nget_optimizer_state_dict\nset_state_dict\nset_optimizer_state_dict\nbackward()\nstep()is called on the optimizers. Otherwise, the optimizer states won\u2019t be\ninitialized correctly.\nstep()\nmodel(nn.Module) \u2013 the nn.Module to the model.\noptimizers(Union[Optimizer,Iterable[Optimizer]]) \u2013 The optimizers that are used to optimizemodel.\nmodel\noptim_state_dict(OptimizerStateType) \u2013 OptimizerStateType:\nthe optimizer state_dict to load.\noptions(StateDictOptions) \u2013 the options to control how\nmodel state_dict and optimizer state_dict should be loaded. SeeStateDictOptionsfor the details.\nNone\nNone\nThis dataclass specifies how get_state_dict/set_state_dict will work.\nfull_state_dict: if this is set to True, all the tensors in the\nreturned state_dict will be gathered. No ShardedTensor and DTensor\nwill be in the returned state_dict.\nfull_state_dict\ncpu_offload: offload all the tensors to cpu. To prevent CPU OOM, iffull_state_dictis also true, then only the rank0 will get the\nstate_dict and all other ranks will get empty state_dict.\ncpu_offload\nfull_state_dict\nignore_frozen_params: if the value is True, the returned state_dict\nwon\u2019t contain any frozen parameters \u2013 therequires_gradis False.\nThe default value is False.\nignore_frozen_params\nrequires_grad\nkeep_submodule_prefixes(deprecated): whensubmodulesis not None, this option\nindicates whether to keep the submodule prefixes from the state_dict keys.\nor example, if the submodule ismodule.pretrainand the full FQN of\nthe parameter ispretrain.layer1.weightof the param. When this option\nis True, the parameter\u2019s key in the returned state_dict will bepretrain.layer1.weight. If the options is False, the key will belayer1.weight.\nNote that ifkeep_submodule_prefixesis False, there may be conflicted\nFQNs, hence there should be only one submodule insubmodules.\nkeep_submodule_prefixes\nsubmodules\nmodule.pretrain\npretrain.layer1.weight\npretrain.layer1.weight\nlayer1.weight\nkeep_submodule_prefixes\nsubmodules\nstrict: thestrictoption whenset_state_dictcalls\nmodel.load_state_dict().\nstrict\nstrict\nset_state_dict\nbroadcast_from_rank0\nfull state_dict and will broadcast the tensors in the state_dict/\noptim_state_dict one by one to other ranks. Other ranks will receive\nthe tensors and shard according to the local shards in the model and\noptimizer.full_state_dictmust be set to True when using this option.\nThis option currently only supports DTensor, not the legacy ShardedTensor.\nfull_state_dict\nFor users which are used to using and sharing models in thetorch.saveformat, the following methods are provided which provide offline utilities for converting betweeing formats.\ntorch.save\nGiven a directory containing a DCP checkpoint, this function will convert it into a\nTorch save file.\ndcp_checkpoint_dir(Union[str,PathLike]) \u2013 Directory containing the DCP checkpoint.\ntorch_save_path(Union[str,PathLike]) \u2013 Filename to store the converted Torch save file.\nWarning\nTo avoid OOM, it\u2019s recommended to only run this function on a single rank.\nGiven the location of a torch save file, converts it into a DCP checkpoint.\ntorch_save_path(Union[str,PathLike]) \u2013 Filename of the Torch save file.\ndcp_checkpoint_dir(Union[str,PathLike]) \u2013 Directory to store the DCP checkpoint.\nWarning\nTo avoid OOM, it\u2019s recommended to only run this function on a single rank.\nThe following classes can also be utilized for online loading and resharding of models from the torch.save format.\nStorageReader for reading a Torch Save file. This reader will read the entire checkpoint\non the coordinator rank, and then broadcast and shard each tensor to all ranks.\n. N.B. Intended to be used with DynamicMetaLoadPlanner\nWarning\nCurrent implementation only supports loading Tensors.\n\n```python\n>>> sd = {\"mode\": model}\n>>> dcp.load(\n>>>    sd,\n>>>    storage_reader=BroadcastingTorchSaveReader(),\n>>>    planner=DynamicMetaLoadPlanner(),\n>>>    checkpoint_id=\"path_to_model.pt\"\n>>> )\n\n```\n\nImplementation of the StorageReader method\nlist[torch.distributed.checkpoint.planner.LoadPlan]\nImplementation of the StorageReader method\nLoadPlan\nReads torch save data on the coordinator rank, and broadcast afterwards\nthis incurrs a communication cost, but avoids having to load\nthe entire checkpoint on each rank, hopefully preventing OOM issues\nFuture[None]\nExtends the default StorageReader to support building the metadata file\nMetadata\nImplementation of the StorageReader method\nImplementation of the StorageReader method\nImplementation of the StorageReader method\nbool\nExtension of DefaultLoadPlanner, which creates a new Metadata object based on the passed in state dict,\navoiding the need to read metadata from disk. This is useful when reading formats which don\u2019t have a\nmetadata file, like Torch Save files.\n. N.B. Intended to be used with BroadcastingTorchSaveReader\nWarning\nCurrent implementation only supports loading Tensors.\n\n```python\n>>> sd = {\"mode\": model}\n>>> dcp.load(\n>>>    sd,\n>>>    storage_reader=BroadcastingTorchSaveReader(),\n>>>    planner=DynamicMetaLoadPlanner(),\n>>>    checkpoint_id=\"path_to_model.pt\"\n>>> )\n\n```\n\nSetups of the planner, extnding default behavior by creating the Metadata object from the state dict\nThe following experimental interfaces are provided for improved observability in production environments:",
    "url": "https://pytorch.org/docs/stable/distributed.checkpoint.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "dca8befae22c99b43717da64f2102172",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/tensor_view.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "0e95e40a2ade712ce76aae7d7408e048",
    "source": "pytorch_docs",
    "title": "torch.dynamic-value \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.dynamic-value#\n\n\n## constrain_as_size_example#\n\nNote\nTags:torch.dynamic-value,torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass ConstrainAsSizeExample(torch.nn.Module):\n    \"\"\"\n    If the value is not known at tracing time, you can provide hint so that we\n    can trace further. Please look at torch._check and torch._check_is_size APIs.\n    torch._check_is_size is used for values that NEED to be used for constructing\n    tensor.\n    \"\"\"\n\n    def forward(self, x):\n        a = x.item()\n        torch._check_is_size(a)\n        torch._check(a <= 5)\n        return torch.zeros((a, 5))\n\n\nexample_args = (torch.tensor(4),)\ntags = {\n    \"torch.dynamic-value\",\n    \"torch.escape-hatch\",\n}\nmodel = ConstrainAsSizeExample()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"i64[]\"):\n                 item: \"Sym(u0)\" = torch.ops.aten.item.default(x);  x = None\n\n             #\n            sym_constrain_range_for_size_default = torch.ops.aten.sym_constrain_range_for_size.default(item);  sym_constrain_range_for_size_default = None\n\n                 ge_1: \"Sym(u0 >= 0)\" = item >= 0\n            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\");  ge_1 = _assert_scalar_default = None\n            le_1: \"Sym(u0 <= 5)\" = item <= 5\n            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\");  le_1 = _assert_scalar_default_1 = None\n\n                 zeros: \"f32[u0, 5]\" = torch.ops.aten.zeros.default([item, 5], device = device(type='cpu'), pin_memory = False);  item = None\n            return (zeros,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    zeros: USER_OUTPUT\n\nRange constraints: {u0: VR[0, 5], u1: VR[0, 5]}\n\n```\n\n\n## constrain_as_value_example#\n\nNote\nTags:torch.dynamic-value,torch.escape-hatch\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\n\nclass ConstrainAsValueExample(torch.nn.Module):\n    \"\"\"\n    If the value is not known at tracing time, you can provide hint so that we\n    can trace further. Please look at torch._check and torch._check_is_size APIs.\n    torch._check is used for values that don't need to be used for constructing\n    tensor.\n    \"\"\"\n\n    def forward(self, x, y):\n        a = x.item()\n        torch._check(a >= 0)\n        torch._check(a <= 5)\n\n        if a < 6:\n            return y.sin()\n        return y.cos()\n\n\nexample_args = (torch.tensor(4), torch.randn(5, 5))\ntags = {\n    \"torch.dynamic-value\",\n    \"torch.escape-hatch\",\n}\nmodel = ConstrainAsValueExample()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"i64[]\", y: \"f32[5, 5]\"):\n                 item: \"Sym(u0)\" = torch.ops.aten.item.default(x);  x = None\n            ge_1: \"Sym(u0 >= 0)\" = item >= 0\n            _assert_scalar_default = torch.ops.aten._assert_scalar.default(ge_1, \"Runtime assertion failed for expression u0 >= 0 on node 'ge_1'\");  ge_1 = _assert_scalar_default = None\n            le_1: \"Sym(u0 <= 5)\" = item <= 5;  item = None\n            _assert_scalar_default_1 = torch.ops.aten._assert_scalar.default(le_1, \"Runtime assertion failed for expression u0 <= 5 on node 'le_1'\");  le_1 = _assert_scalar_default_1 = None\n\n                 sin: \"f32[5, 5]\" = torch.ops.aten.sin.default(y);  y = None\n            return (sin,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n    y: USER_INPUT\n\n    # outputs\n    sin: USER_OUTPUT\n\nRange constraints: {u0: VR[0, 5], u1: VR[0, 5]}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/torch.dynamic-value.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "aac7c0f73c39dc2bcffa4eb21742aeed",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.custom_ops.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4a9ebb905fd6dd61d5cb22e693d540b3",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/storage.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "00704b7dc133e0890883eeebe04219d2",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/exportdb/index.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "601f7e8dd5b2132f8ed27dfbae50997d",
    "source": "pytorch_docs",
    "title": "torch.__future__ \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.__future__#\n\nCreated On: Feb 05, 2024 | Last Updated On: Jun 12, 2025\nSets whether to assign new tensors to the parameters instead of changing the\nexisting parameters in-place when converting annn.Module.\nnn.Module\nWhen enabled, the following methods will assign new parameters to the module:\nmodule.{device}()(e.g.nn.Module.cuda()) for moving a module between devices\nmodule.{device}()\nnn.Module.cuda()\nmodule.{dtype}()(e.g.nn.Module.float()) for converting a module to a different dtype\nmodule.{dtype}()\nnn.Module.float()\nnn.Module.to()\nnn.Module.to()\nnn.Module.to_empty()\nnn.Module.to_empty()\nvalue(bool) \u2013 Whether to assign new tensors or not.\nReturns whether to assign new tensors to the parameters instead of changing the\nexisting parameters in-place when converting antorch.nn.Module. Defaults toFalse.\ntorch.nn.Module\nFalse\nSeeset_overwrite_module_params_on_conversion()for more information.\nset_overwrite_module_params_on_conversion()\nbool\nSets whether to useswap_tensors()instead of setting.datato\nchange the existing parameters in-place when converting annn.Moduleand instead\nofparam.copy_(state_dict[key])when loading a state dict into annn.Module.\nswap_tensors()\n.data\nnn.Module\nparam.copy_(state_dict[key])\nnn.Module\nNote\nThis function takes precedence overget_overwrite_module_params_on_conversion()\nget_overwrite_module_params_on_conversion()\nWhen enabled, the following methods will swap the existing parameters in-place:\nmodule.{device}()(e.g.nn.Module.cuda()) for moving a module between devices\nmodule.{device}()\nnn.Module.cuda()\nmodule.{dtype}()(e.g.nn.Module.float()) for converting a module to a different dtype\nmodule.{dtype}()\nnn.Module.float()\nnn.Module.to()\nnn.Module.to()\nnn.Module.to_empty()\nnn.Module.to_empty()\nnn.Module.load_state_dict()\nnn.Module.load_state_dict()\nThe semantics forload_state_dict()when this is set are as follows:\nload_state_dict()\nFor each parameter/buffer, its correspondingstate_dict['key']is transformed viamodule_load()(i.e.res=param.module_load(state_dict['key']))\nstate_dict['key']\nmodule_load()\nres=param.module_load(state_dict['key'])\nIf necessary,reswill be wrapped in anParameter\nres\nParameter\nThe parameter/buffer in the module will be swapped viaswap_tensors()withres\nswap_tensors()\nres\nvalue(bool) \u2013 Whether to useswap_tensors()or not.\nswap_tensors()\nReturns whether to useswap_tensors()instead of setting .data to\nchange the existing parameters in-place when converting annn.Module. Defaults toFalse.\nswap_tensors()\nnn.Module\nFalse\nSeeset_swap_module_params_on_conversion()for more information.\nset_swap_module_params_on_conversion()\nbool",
    "url": "https://pytorch.org/docs/stable/future_mod.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1116fb413c2a1c85a72b2576aaf24f3c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/community/build_ci_governance.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f050ec57cddc7deb04ff5f917b6b63a4",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/torch.are_deterministic_algorithms_enabled.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "3d2210c102d4fb1cf9e24bf8b4cdd042",
    "source": "pytorch_docs",
    "title": "Reporting Issues \u2014 PyTorch 2.9 documentation",
    "text": "\n## Reporting Issues#\n\nCreated On: Jul 29, 2025 | Last Updated On: Jul 29, 2025\nIf the provided workarounds were not enough to gettorch.compileworking,\nthen you should consider reporting the issue to PyTorch.\nBut there are a few things that you can do to make our lives significantly easier.\ntorch.compile\n\n## Ablation#\n\nCheck which component of thetorch.compilestack is the one causing the issue using thebackend=option fortorch.compile.\nIn particular, try:\ntorch.compile\nbackend=\ntorch.compile\ntorch.compile(fn,backend=\"eager\"), which only runs TorchDynamo, the graph capture component oftorch.compile.\ntorch.compile(fn,backend=\"eager\")\ntorch.compile\ntorch.compile(fn,backend=\"aot_eager\"), which runs TorchDynamo and AOTAutograd, which additionally generates the backward graph during compilation.\ntorch.compile(fn,backend=\"aot_eager\")\ntorch.compile(fn,backend=\"aot_eager_decomp_partition\"), which runs TorchDynamo and AOTAutograd with operator decompositions/partitions.\ntorch.compile(fn,backend=\"aot_eager_decomp_partition\")\ntorch.compile(fn,backend=\"inductor\"), which runs TorchDynamo, AOTAutograd, and TorchInductor, the backend ML compiler that generates compiled kernels.\ntorch.compile(fn,backend=\"inductor\")\nIf you only fail with the Inductor backend, you can additionally test various Inductor modes:\ntorch.compile(fn,backend=\"inductor\",mode=\"default\")\ntorch.compile(fn,backend=\"inductor\",mode=\"default\")\ntorch.compile(fn,backend=\"inductor\",mode=\"reduce-overhead\")\ntorch.compile(fn,backend=\"inductor\",mode=\"reduce-overhead\")\ntorch.compile(fn,backend=\"inductor\",mode=\"max-autotune\")\ntorch.compile(fn,backend=\"inductor\",mode=\"max-autotune\")\nYou can also check if dynamic shapes is causing issues with any backend:\ntorch.compile(fn,dynamic=True)(always use dynamic shapes)\ntorch.compile(fn,dynamic=True)\ntorch.compile(fn,dynamic=False)(never use dynamic shapes)\ntorch.compile(fn,dynamic=False)\ntorch.compile(fn,dynamic=None)(automatic dynamic shapes)\ntorch.compile(fn,dynamic=None)\n\n## Bisecting#\n\nDid you try on the latest nightly? Did something work in the past but now no longer works?\nCan you bisect to determine the first nightly where your issue occurs?\nBisecting is especially helpful for performance, accuracy, or compile time regressions,\nwhere it is not immediately obvious where the problem originates from.\n\n## Creating a reproducer#\n\nCreating reproducers is a lot of work, and it is perfectly fine if you do not have the time to do it.\nHowever, if you are a motivated user unfamiliar with the internals oftorch.compile,\ncreating a standalone reproducer can have a huge impact on our ability to fix the bug.\nWithout a reproducer, your bug report must contain enough information for us to identify the root cause of the problem and write a reproducer from scratch.\ntorch.compile\nHere\u2019s a list of useful reproducers, ranked from most to least preferred:\nSelf-contained, small reproducer:A script with no external dependencies, under 100 lines of code, that reproduces the problem when run.\nSelf-contained, large reproducer:Even if it\u2019s large, being self-contained is a huge advantage!\nNon-self-contained reproducer with manageable dependencies:For example, if you can reproduce the problem by running a script afterpipinstalltransformers,\nthat\u2019s manageable. We can likely run it and investigate.\npipinstalltransformers\nNon-self-contained reproducer requiring substantial setup:This might involve downloading datasets,\nmultiple environment setup steps, or specific system library versions requiring a Docker image.\nThe more complex the setup, the harder it is for us to recreate the environment.\nNote\nDocker simplifies setup but complicates changes to the environment, so it\u2019s not a perfect solution, though we\u2019ll use it if necessary.\nIf possible, try to make your reproducer single-process, as those are easier to debug than a multi-process reproducer.\nAdditionally, below is a non-exhaustive list of aspects to check in your\nissue that you can attempt to replicate in your reproducer:\nAutograd. Did you have tensor inputs withrequires_grad=True? Did you callbackward()on the output?\nrequires_grad=True\nbackward()\nDynamic shapes. Did you setdynamic=True? Or did you run the test code multiple times with varying shapes?\ndynamic=True\nCustom operators. Is there a custom operator involved in the real workflow?\nCan you replicate some of its important characteristics using the Python custom operator API?\nConfiguration. Did you set all the same configuration?\nThis includestorch._dynamo.configandtorch._inductor.configsettings,\nas well as arguments totorch.compilelikebackend/mode.\ntorch._dynamo.config\ntorch._inductor.config\ntorch.compile\nbackend\nmode\nContext managers. Did you replicate any active context managers?\nThis could betorch.no_grad, automatic mixed precision,TorchFunctionMode/TorchDispatchMode,\nactivation checkpointing, compiled autograd etc.\ntorch.no_grad\nTorchFunctionMode\nTorchDispatchMode\nTensor subclasses. Is there a tensor subclass involved?",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.reporting_issues.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ce5d28cb65f22aecb6a9cb152e21d44b",
    "source": "pytorch_docs",
    "title": "Quantization \u2014 PyTorch 2.9 documentation",
    "text": "\n## Quantization#\n\nCreated On: Oct 09, 2019 | Last Updated On: Aug 19, 2025\nWe are cetralizing all quantization related development totorchao, please checkout our new doc page:https://docs.pytorch.org/ao/stable/index.html\nPlan for the existing quantization flows:\n1. Eager mode quantization (torch.ao.quantization.quantize,\ntorch.ao.quantization.quantize_dynamic), please migrate to use torchao eager modequantize_API instead\n2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx\ntorch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization\nAPI instead (torchao.quantization.pt2e.quantize_pt2e.prepare_pt2e,torchao.quantization.pt2e.quantize_pt2e.convert_pt2e)\n3. pt2e quantization has been migrated to torchao (pytorch/ao)\nseepytorch/ao#2259for more details\nWe plan to deletetorch.ao.quantizationin 2.10 if there are no blockers, or in the earliest PyTorch version until all the blockers are cleared.\n\n## Quantization API Reference (Kept since APIs are still public)#\n\nTheQuantization API Referencecontains documentation\nof quantization APIs, such as quantization passes, quantized tensor operations,\nand supported quantized modules and functions.",
    "url": "https://pytorch.org/docs/stable/quantization.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "fc8a56f452a04c21c89e78b8eb028fb5",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/tensors.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ae42e4a524fd56463b4f8ebe8dd5f639",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/distributed.tensor.parallel.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "55cff3adab1655d464e4768878299e24",
    "source": "pytorch_docs",
    "title": "CUDA Stream Sanitizer \u2014 PyTorch 2.9 documentation",
    "text": "\n## CUDA Stream Sanitizer#\n\nCreated On: Sep 09, 2022 | Last Updated On: Oct 31, 2022\nNote\nThis is a prototype feature, which means it is at an early stage\nfor feedback and testing, and its components are subject to change.\n\n## Overview#\n\nThis module introduces CUDA Sanitizer, a tool for detecting synchronization errors between kernels ran on different streams.\nIt stores information on accesses to tensors to determine if they are synchronized\nor not. When enabled in a python program and a possible data race is detected, a\ndetailed warning will be printed and the program will exit.\nIt can be enabled either by importing this module and callingenable_cuda_sanitizer()or by exporting theTORCH_CUDA_SANITIZERenvironment variable.\nenable_cuda_sanitizer()\nTORCH_CUDA_SANITIZER\n\n## Usage#\n\nHere is an example of a simple synchronization error in PyTorch:\n\n```python\nimport torch\n\na = torch.rand(4, 2, device=\"cuda\")\n\nwith torch.cuda.stream(torch.cuda.Stream()):\n    torch.mul(a, 5, out=a)\n\n```\n\nTheatensor is initialized on the default stream and, without any synchronization\nmethods, modified on a new stream. The two kernels will run concurrently on the same tensor,\nwhich might cause the second kernel to read uninitialized data before the first one was able\nto write it, or the first kernel might overwrite part of the result of the second.\nWhen this script is run on the commandline with:\na\n\n```python\nTORCH_CUDA_SANITIZER=1 python example_error.py\n\n```\n\nthe following output is printed by CSAN:\n\n```python\n============================\nCSAN detected a possible data race on tensor with data pointer 139719969079296\nAccess by stream 94646435460352 during kernel:\naten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)\nwriting to argument(s) self, out, and to the output\nWith stack trace:\n  File \"example_error.py\", line 6, in <module>\n    torch.mul(a, 5, out=a)\n  ...\n  File \"pytorch/torch/cuda/_sanitizer.py\", line 364, in _handle_kernel_launch\n    stack_trace = traceback.StackSummary.extract(\n\nPrevious access by stream 0 during kernel:\naten::rand(int[] size, *, int? dtype=None, Device? device=None) -> Tensor\nwriting to the output\nWith stack trace:\n  File \"example_error.py\", line 3, in <module>\n    a = torch.rand(10000, device=\"cuda\")\n  ...\n  File \"pytorch/torch/cuda/_sanitizer.py\", line 364, in _handle_kernel_launch\n    stack_trace = traceback.StackSummary.extract(\n\nTensor was allocated with stack trace:\n  File \"example_error.py\", line 3, in <module>\n    a = torch.rand(10000, device=\"cuda\")\n  ...\n  File \"pytorch/torch/cuda/_sanitizer.py\", line 420, in _handle_memory_allocation\n    traceback.StackSummary.extract(\n\n```\n\nThis gives extensive insight into the origin of the error:\nA tensor was incorrectly accessed from streams with ids: 0 (default stream) and 94646435460352 (new stream)\nThe tensor was allocated by invokinga=torch.rand(10000,device=\"cuda\")\na=torch.rand(10000,device=\"cuda\")\na=torch.rand(10000,device=\"cuda\")on stream 0\na=torch.rand(10000,device=\"cuda\")\ntorch.mul(a,5,out=a)on stream 94646435460352\ntorch.mul(a,5,out=a)\nThe error message also displays the schemas of the invoked operators, along with a note\nshowing which arguments of the operators correspond to the affected tensor.\nIn the example, it can be seen that tensoracorresponds to argumentsself,outand theoutputvalue of the invoked operatortorch.mul.\na\nself\nout\noutput\ntorch.mul\nSee also\nThe list of supported torch operators and their schemas can be viewedhere.\nThe bug can be fixed by forcing the new stream to wait for the default stream:\n\n```python\nwith torch.cuda.stream(torch.cuda.Stream()):\n    torch.cuda.current_stream().wait_stream(torch.cuda.default_stream())\n    torch.mul(a, 5, out=a)\n\n```\n\nWhen the script is run again, there are no errors reported.\n\n## API Reference#\n\nEnable CUDA Sanitizer.\nThe sanitizer will begin to analyze low-level CUDA calls invoked by torch functions\nfor synchronization errors. All data races found will be printed to the standard\nerror output along with stack traces of suspected causes. For best results, the\nsanitizer should be enabled at the very beginning of the program.",
    "url": "https://pytorch.org/docs/stable/cuda._sanitizer.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e47998840b23e8b541818f75d68414df",
    "source": "pytorch_docs",
    "title": "CUDAGraph \u2014 PyTorch 2.9 documentation",
    "text": "\n## CUDAGraph#\n\nWrapper around a CUDA graph.\nkeep_graph(bool,optional) \u2013 Ifkeep_graph=False, the\ncudaGraphExec_t will be instantiated on GPU at the end ofcapture_endand the underlying cudaGraph_t will be\ndestroyed. Users who want to query or otherwise modify the\nunderlying cudaGraph_t before instantiation can setkeep_graph=Trueand access it viaraw_cuda_graphaftercapture_end. Note that the cudaGraphExec_t will not be\ninstantiated at the end ofcapture_endin this\ncase. Instead, it will be instantiated via an explicit called\ntoinstantiateor automatically on the first call toreplayifinstantiatewas not already called. Callinginstantiatemanually beforereplayis recommended to\nprevent increased latency on the first call toreplay. It\nis allowed to modify the raw cudaGraph_t after first callinginstantiate, but the user must callinstantiateagain\nmanually to make sure the instantiated graph has these\nchanges. Pytorch has no means of tracking these changes.\nkeep_graph=False\ncapture_end\nkeep_graph=True\nraw_cuda_graph\ncapture_end\ncapture_end\ninstantiate\nreplay\ninstantiate\ninstantiate\nreplay\nreplay\ninstantiate\ninstantiate\nSelf\nWarning\nThis API is in beta and may change in future releases.\nBegin capturing CUDA work on the current stream.\nTypically, you shouldn\u2019t callcapture_beginyourself.\nUsegraphormake_graphed_callables(),\nwhich callcapture_begininternally.\ncapture_begin\ngraph\nmake_graphed_callables()\ncapture_begin\npool(optional) \u2013 Token (returned bygraph_pool_handle()orother_Graph_instance.pool()) that hints this graph may share memory\nwith the indicated pool.  SeeGraph memory management.\ngraph_pool_handle()\nother_Graph_instance.pool()\ncapture_error_mode(str,optional) \u2013 specifies the cudaStreamCaptureMode for the graph capture stream.\nCan be \u201cglobal\u201d, \u201cthread_local\u201d or \u201crelaxed\u201d. During cuda graph capture, some actions, such as cudaMalloc,\nmay be unsafe. \u201cglobal\u201d will error on actions in other threads, \u201cthread_local\u201d will only error for\nactions in the current thread, and \u201crelaxed\u201d will not error on these actions. Do NOT change this setting\nunless you\u2019re familiar withcudaStreamCaptureMode\nEnd CUDA graph capture on the current stream.\nAftercapture_end,replaymay be called on this instance.\ncapture_end\nreplay\nTypically, you shouldn\u2019t callcapture_endyourself.\nUsegraphormake_graphed_callables(),\nwhich callcapture_endinternally.\ncapture_end\ngraph\nmake_graphed_callables()\ncapture_end\ndebug_path(required) \u2013 Path to dump the graph to.\nCalls a debugging function to dump the graph if the debugging is\nenabled via CUDAGraph.enable_debug_mode()\nEnable debugging mode for CUDAGraph.debug_dump.\nInstantiate the CUDA graph. Will be called bycapture_endifkeep_graph=False, or byreplayifkeep_graph=Trueandinstantiatehas not already been\nexplicitly called. Does not destroy the cudaGraph_t returned\nbyraw_cuda_graph.\ncapture_end\nkeep_graph=False\nreplay\nkeep_graph=True\ninstantiate\nraw_cuda_graph\nReturn an opaque token representing the id of this graph\u2019s memory pool.\nThis id can optionally be passed to another graph\u2019scapture_begin,\nwhich hints the other graph may share the same memory pool.\ncapture_begin\n_POOL_HANDLE\nReturns the underlying cudaGraph_t.keep_graphmust be True.\nkeep_graph\nSee the following for APIs for how to manipulate this object:Graph Managmementandcuda-python Graph Management bindings\nint\nReturns the underlying cudaGraphExec_t.instantiatemust have been called ifkeep_graphis True, orcapture_endmust have been called ifkeep_graphis False. If you callinstantiate()afterraw_cuda_graph_exec(), the previously returned cudaGraphExec_t will be destroyed. It is your responsibility not to use this object after destruction.\ninstantiate\nkeep_graph\ncapture_end\nkeep_graph\ninstantiate()\nraw_cuda_graph_exec()\nSee the following for APIs for how to manipulate this object:Graph Executionandcuda-python Graph Execution bindings\nint\nReplay the CUDA work captured by this graph.\nDelete the graph currently held by this instance.",
    "url": "https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "daca9de6609df39a1f3a7166e7313d20",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/export/joint_with_descriptors.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "278c85fafc807bbba5e5add40b2553d8",
    "source": "pytorch_docs",
    "title": "torch.operator \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.operator#\n\n\n## unsupported_operator#\n\nNote\nTags:torch.operator\nSupport Level: NOT_SUPPORTED_YET\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\nfrom torch._export.db.case import SupportLevel\n\n\nclass TorchSymMin(torch.nn.Module):\n    \"\"\"\n    torch.sym_min operator is not supported in export.\n    \"\"\"\n\n    def forward(self, x):\n        return x.sum() + torch.sym_min(x.size(0), 100)\n\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"torch.operator\"}\nsupport_level = SupportLevel.NOT_SUPPORTED_YET\nmodel = TorchSymMin()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nUnsupported: torch.* op returned non-Tensor\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/torch.operator.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "30c032aa8a8ffaef0130e4c1341e9b6f",
    "source": "pytorch_docs",
    "title": "torch.utils.cpp_extension \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.cpp_extension#\n\nCreated On: Mar 07, 2018 | Last Updated On: Feb 16, 2025\nCreate asetuptools.Extensionfor C++.\nsetuptools.Extension\nConvenience method that creates asetuptools.Extensionwith the\nbare minimum (but often sufficient) arguments to build a C++ extension.\nsetuptools.Extension\nAll arguments are forwarded to thesetuptools.Extensionconstructor. Full list arguments can be found athttps://setuptools.pypa.io/en/latest/userguide/ext_modules.html#extension-api-reference\nsetuptools.Extension\nWarning\nThe PyTorch python API (as provided in libtorch_python) cannot be built\nwith the flagpy_limited_api=True.  When this flag is passed, it is\nthe user\u2019s responsibility in their library to not use APIs from\nlibtorch_python (in particular pytorch/python bindings) and to only use\nAPIs from libtorch (aten objects, operators and the dispatcher). For\nexample, to give access to custom ops from python, the library should\nregister the ops through the dispatcher.\npy_limited_api=True\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API\nas a compile flag when py_limited_api is specified as an option for\nthe \u201cbdist_wheel\u201d command insetup, PyTorch does! We will specify\n-DPy_LIMITED_API=min_supported_cpython to best enforce consistency,\nsafety, and sanity in order to encourage best practices. To target a\ndifferent version, set min_supported_cpython to the hexcode of the\nCPython version of choice.\nsetup\nExample\n\n```python\n>>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\n>>> setup(\n...     name='extension',\n...     ext_modules=[\n...         CppExtension(\n...             name='extension',\n...             sources=['extension.cpp'],\n...             extra_compile_args=['-g'],\n...             extra_link_args=['-Wl,--no-as-needed', '-lm'])\n...     ],\n...     cmdclass={\n...         'build_ext': BuildExtension\n...     })\n\n```\n\nCreate asetuptools.Extensionfor CUDA/C++.\nsetuptools.Extension\nConvenience method that creates asetuptools.Extensionwith the\nbare minimum (but often sufficient) arguments to build a CUDA/C++\nextension. This includes the CUDA include path, library path and runtime\nlibrary.\nsetuptools.Extension\nAll arguments are forwarded to thesetuptools.Extensionconstructor. Full list arguments can be found athttps://setuptools.pypa.io/en/latest/userguide/ext_modules.html#extension-api-reference\nsetuptools.Extension\nWarning\nThe PyTorch python API (as provided in libtorch_python) cannot be built\nwith the flagpy_limited_api=True.  When this flag is passed, it is\nthe user\u2019s responsibility in their library to not use APIs from\nlibtorch_python (in particular pytorch/python bindings) and to only use\nAPIs from libtorch (aten objects, operators and the dispatcher). For\nexample, to give access to custom ops from python, the library should\nregister the ops through the dispatcher.\npy_limited_api=True\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API\nas a compile flag when py_limited_api is specified as an option for\nthe \u201cbdist_wheel\u201d command insetup, PyTorch does! We will specify\n-DPy_LIMITED_API=min_supported_cpython to best enforce consistency,\nsafety, and sanity in order to encourage best practices. To target a\ndifferent version, set min_supported_cpython to the hexcode of the\nCPython version of choice.\nsetup\nExample\n\n```python\n>>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n>>> setup(\n...     name='cuda_extension',\n...     ext_modules=[\n...         CUDAExtension(\n...                 name='cuda_extension',\n...                 sources=['extension.cpp', 'extension_kernel.cu'],\n...                 extra_compile_args={'cxx': ['-g'],\n...                                     'nvcc': ['-O2']},\n...                 extra_link_args=['-Wl,--no-as-needed', '-lcuda'])\n...     ],\n...     cmdclass={\n...         'build_ext': BuildExtension\n...     })\n\n```\n\nCompute capabilities:\nBy default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension, plus PTX. If down the road a new card is installed the\nextension may need to be recompiled. If a visible card has a compute capability (CC) that\u2019s\nnewer than the newest version for which your nvcc can build fully-compiled binaries, PyTorch\nwill make nvcc fall back to building kernels with the newest version of PTX your nvcc does\nsupport (see below for details on PTX).\nYou can override the default behavior usingTORCH_CUDA_ARCH_LISTto explicitly specify which\nCCs you want the extension to support:\nTORCH_CUDA_ARCH_LIST=\"6.18.6\"pythonbuild_my_extension.pyTORCH_CUDA_ARCH_LIST=\"5.26.06.17.07.58.08.6+PTX\"pythonbuild_my_extension.py\nTORCH_CUDA_ARCH_LIST=\"6.18.6\"pythonbuild_my_extension.py\nTORCH_CUDA_ARCH_LIST=\"5.26.06.17.07.58.08.6+PTX\"pythonbuild_my_extension.py\nThe +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better.\nNote that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch.\nNote that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows.\nTo workaround the issue, move python binding logic to pure C++ file.\n#include <ATen/ATen.h>\nat::Tensor SigmoidAlphaBlendForwardCuda(\u2026.)\n#include <torch/extension.h>\ntorch::Tensor SigmoidAlphaBlendForwardCuda(\u2026)\nCurrently open issue for nvcc bug:pytorch/pytorch#69460Complete workaround code example:facebookresearch/pytorch3d\nRelocatable device code linking:\nIf you want to reference device symbols across compilation units (across object files),\nthe object files need to be built withrelocatable device code(-rdc=true or -dc).\nAn exception to this rule is \u201cdynamic parallelism\u201d (nested kernel launches)  which is not used a lot anymore.Relocatable device codeis less optimized so it needs to be used only on object files that need it.\nUsing-dlto(Device Link Time Optimization) at the device code compilation step anddlinkstep\nhelps reduce the protentional perf degradation of-rdc.\nNote that it needs to be used at both steps to be useful.\nIf you haverdcobjects you need to have an extra-dlink(device linking) step before the CPU symbol linking step.\nThere is also a case where-dlinkis used without-rdc:\nwhen an extension is linked against a static lib containing rdc-compiled objects\nlike the [NVSHMEM library](https://developer.nvidia.com/nvshmem).\nNote: Ninja is required to build a CUDA Extension with RDC linking.\nExample\n\n```python\n>>> CUDAExtension(\n...        name='cuda_extension',\n...        sources=['extension.cpp', 'extension_kernel.cu'],\n...        dlink=True,\n...        dlink_libraries=[\"dlink_lib\"],\n...        extra_compile_args={'cxx': ['-g'],\n...                            'nvcc': ['-O2', '-rdc=true']})\n\n```\n\nCreates asetuptools.Extensionfor SYCL/C++.\nsetuptools.Extension\nConvenience method that creates asetuptools.Extensionwith the\nbare minimum (but often sufficient) arguments to build a SYCL/C++\nextension.\nsetuptools.Extension\nAll arguments are forwarded to thesetuptools.Extensionconstructor.\nsetuptools.Extension\nWarning\nThe PyTorch python API (as provided in libtorch_python) cannot be built\nwith the flagpy_limited_api=True.  When this flag is passed, it is\nthe user\u2019s responsibility in their library to not use APIs from\nlibtorch_python (in particular pytorch/python bindings) and to only use\nAPIs from libtorch (aten objects, operators and the dispatcher). For\nexample, to give access to custom ops from python, the library should\nregister the ops through the dispatcher.\npy_limited_api=True\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API\nas a compile flag when py_limited_api is specified as an option for\nthe \u201cbdist_wheel\u201d command insetup, PyTorch does! We will specify\n-DPy_LIMITED_API=min_supported_cpython to best enforce consistency,\nsafety, and sanity in order to encourage best practices. To target a\ndifferent version, set min_supported_cpython to the hexcode of the\nCPython version of choice.\nsetup\nExample\n\n```python\n>>> from torch.utils.cpp_extension import BuildExtension, SyclExtension\n>>> setup(\n...     name='xpu_extension',\n...     ext_modules=[\n...     SyclExtension(\n...                 name='xpu_extension',\n...                 sources=['extension.cpp', 'extension_kernel.cpp'],\n...                 extra_compile_args={'cxx': ['-g', '-std=c++20', '-fPIC']})\n...     ],\n...     cmdclass={\n...         'build_ext': BuildExtension\n...     })\n\n```\n\nBy default the extension will be compiled to run on all archs of the cards visible during the\nbuilding process of the extension. If down the road a new card is installed the\nextension may need to be recompiled. You can override the default behavior usingTORCH_XPU_ARCH_LISTto explicitly specify which device architectures you want the extension\nto support:\nTORCH_XPU_ARCH_LIST=\"pvc,xe-lpg\"pythonbuild_my_extension.py\nTORCH_XPU_ARCH_LIST=\"pvc,xe-lpg\"pythonbuild_my_extension.py\nNote that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch.\nNote: Ninja is required to build SyclExtension.\nA customsetuptoolsbuild extension .\nsetuptools\nThissetuptools.build_extsubclass takes care of passing the\nminimum required compiler flags (e.g.-std=c++17) as well as mixed\nC++/CUDA/SYCL compilation (and support for CUDA/SYCL files in general).\nsetuptools.build_ext\n-std=c++17\nWhen usingBuildExtension, it is allowed to supply a dictionary\nforextra_compile_args(rather than the usual list) that maps from\nlanguages/compilers (the only expected values arecxx,nvccorsycl) to a list of additional compiler flags to supply to the compiler.\nThis makes it possible to supply different flags to the C++, CUDA and SYCL\ncompiler during mixed compilation.\nBuildExtension\nextra_compile_args\ncxx\nnvcc\nsycl\nuse_ninja(bool): Ifuse_ninjaisTrue(default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standardsetuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available.\nuse_ninja\nuse_ninja\nTrue\nsetuptools.build_ext\nNote\nBy default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting theMAX_JOBSenvironment\nvariable to a non-negative number.\nLoad a PyTorch C++ extension just-in-time (JIT).\nTo load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.\nBy default, the directory to which the build file is emitted and the\nresulting library compiled to is<tmp>/torch_extensions/<name>, where<tmp>is the temporary folder on the current platform and<name>the name of the extension. This location can be overridden in two ways.\nFirst, if theTORCH_EXTENSIONS_DIRenvironment variable is set, it\nreplaces<tmp>/torch_extensionsand all extensions will be compiled\ninto subfolders of this directory. Second, if thebuild_directoryargument to this function is supplied, it overrides the entire path, i.e.\nthe library will be compiled into that folder directly.\n<tmp>/torch_extensions/<name>\n<tmp>\n<name>\nTORCH_EXTENSIONS_DIR\n<tmp>/torch_extensions\nbuild_directory\nTo compile the sources, the default system compiler (c++) is used,\nwhich can be overridden by setting theCXXenvironment variable. To pass\nadditional arguments to the compilation process,extra_cflagsorextra_ldflagscan be provided. For example, to compile your extension\nwith optimizations, passextra_cflags=['-O3']. You can also useextra_cflagsto pass further include directories.\nc++\nCXX\nextra_cflags\nextra_ldflags\nextra_cflags=['-O3']\nextra_cflags\nCUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cuor.cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linkingcudart. You can pass additional flags to nvcc viaextra_cuda_cflags, just like withextra_cflagsfor C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting theCUDA_HOMEenvironment variable is the\nsafest option.\n.cu\n.cuh\ncudart\nextra_cuda_cflags\nextra_cflags\nCUDA_HOME\nSYCL support with mixed compilation is provided. Simply pass SYCL source\nfiles (.sycl) along with other sources. Such files will be detected\nand compiled with SYCL compiler (such as Intel DPC++ Compiler) rather\nthan the C++ compiler. You can pass additional flags to SYCL compiler\nviaextra_sycl_cflags, just like withextra_cflagsfor C++.\nSYCL compiler is expected to be found via system PATH environment\nvariable.\n.sycl\nextra_sycl_cflags\nextra_cflags\nname\u2013 The name of the extension to build. This MUST be the same as the\nname of the pybind11 module!\nsources(Union[str,list[str]]) \u2013 A list of relative or absolute paths to C++ source files.\nextra_cflags\u2013 optional list of compiler flags to forward to the build.\nextra_cuda_cflags\u2013 optional list of compiler flags to forward to nvcc\nwhen building CUDA sources.\nextra_sycl_cflags\u2013 optional list of compiler flags to forward to SYCL\ncompiler when building SYCL sources.\nextra_ldflags\u2013 optional list of linker flags to forward to the build.\nextra_include_paths\u2013 optional list of include directories to forward\nto the build.\nbuild_directory\u2013 optional path to use as build workspace.\nverbose\u2013 IfTrue, turns on verbose logging of load steps.\nTrue\nwith_cuda(Optional[bool]) \u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set toNone(default), this value is\nautomatically determined based on the existence of.cuor.cuhinsources. Set it toTrue`to force CUDA headers\nand libraries to be included.\nNone\n.cu\n.cuh\nsources\nwith_sycl(Optional[bool]) \u2013 Determines whether SYCL headers and libraries are added to\nthe build. If set toNone(default), this value is\nautomatically determined based on the existence of.syclinsources. Set it toTrue`to force SYCL headers and\nlibraries to be included.\nNone\n.sycl\nsources\nis_python_module\u2013 IfTrue(default), imports the produced shared\nlibrary as a Python module. IfFalse, behavior depends onis_standalone.\nTrue\nFalse\nis_standalone\nis_standalone\u2013 IfFalse(default) loads the constructed extension\ninto the process as a plain dynamic library. IfTrue, build a\nstandalone executable.\nFalse\nTrue\nReturns the loaded PyTorch extension as a Python module.Ifis_python_moduleisFalseandis_standaloneisFalse:Returns nothing. (The shared library is loaded into the process as\na side effect.)Ifis_standaloneisTrue.Return the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.)\nReturns the loaded PyTorch extension as a Python module.\nis_python_module\nFalse\nis_standalone\nFalse\nReturns nothing. (The shared library is loaded into the process as\na side effect.)\nis_standalone\nTrue\nReturn the path to the executable. (On Windows, TORCH_LIB_PATH is\nadded to the PATH environment variable as a side effect.)\nIfis_python_moduleisTrue\nis_python_module\nTrue\nExample\n\n```python\n>>> from torch.utils.cpp_extension import load\n>>> module = load(\n...     name='extension',\n...     sources=['extension.cpp', 'extension_kernel.cu'],\n...     extra_cflags=['-O2'],\n...     verbose=True)\n\n```\n\nLoad a PyTorch C++ extension just-in-time (JIT) from string sources.\nThis function behaves exactly likeload(), but takes its sources as\nstrings rather than filenames. These strings are stored to files in the\nbuild directory, after which the behavior ofload_inline()is\nidentical toload().\nload()\nload_inline()\nload()\nSeethe\ntestsfor good examples of using this function.\nSources may omit two required parts of a typical non-inline C++ extension:\nthe necessary header includes, as well as the (pybind11) binding code. More\nprecisely, strings passed tocpp_sourcesare first concatenated into a\nsingle.cppfile. This file is then prepended with#include<torch/extension.h>\ncpp_sources\n.cpp\n#include<torch/extension.h>\nFurthermore, if thefunctionsargument is supplied, bindings will be\nautomatically generated for each function specified.functionscan\neither be a list of function names, or a dictionary mapping from function\nnames to docstrings. If a list is given, the name of each function is used\nas its docstring.\nfunctions\nfunctions\nThe sources incuda_sourcesare concatenated into a separate.cufile and  prepended withtorch/types.h,cuda.handcuda_runtime.hincludes. The.cppand.cufiles are compiled\nseparately, but ultimately linked into a single library. Note that no\nbindings are generated for functions incuda_sourcesper se. To bind\nto a CUDA kernel, you must create a C++ function that calls it, and either\ndeclare or define this C++ function in one of thecpp_sources(and\ninclude its name infunctions).\ncuda_sources\n.cu\ntorch/types.h\ncuda.h\ncuda_runtime.h\n.cpp\n.cu\ncuda_sources\ncpp_sources\nfunctions\nThe sources insycl_sourcesare concatenated into a separate.syclfile and  prepended withtorch/types.h,sycl/sycl.hppincludes.\nThe.cppand.syclfiles are compiled separately, but ultimately\nlinked into a single library. Note that no bindings are generated for\nfunctions insycl_sourcesper se. To bind to a SYCL kernel, you must\ncreate a C++ function that calls it, and either declare or define this\nC++ function in one of thecpp_sources(and include its name\ninfunctions).\nsycl_sources\n.sycl\ntorch/types.h\nsycl/sycl.hpp\n.cpp\n.sycl\nsycl_sources\ncpp_sources\nfunctions\nSeeload()for a description of arguments omitted below.\nload()\ncpp_sources\u2013 A string, or list of strings, containing C++ source code.\ncuda_sources\u2013 A string, or list of strings, containing CUDA source code.\nsycl_sources\u2013 A string, or list of strings, containing SYCL source code.\nfunctions\u2013 A list of function names for which to generate function\nbindings. If a dictionary is given, it should map function names to\ndocstrings (which are otherwise just the function names).\nwith_cuda\u2013 Determines whether CUDA headers and libraries are added to\nthe build. If set toNone(default), this value is\nautomatically determined based on whethercuda_sourcesis\nprovided. Set it toTrueto force CUDA headers\nand libraries to be included.\nNone\ncuda_sources\nTrue\nwith_sycl\u2013 Determines whether SYCL headers and libraries are added to\nthe build. If set toNone(default), this value is\nautomatically determined based on whethersycl_sourcesis\nprovided. Set it toTrueto force SYCL headers\nand libraries to be included.\nNone\nsycl_sources\nTrue\nwith_pytorch_error_handling\u2013 Determines whether pytorch error and\nwarning macros are handled by pytorch instead of pybind. To do\nthis, each functionfoois called via an intermediary_safe_foofunction. This redirection might cause issues in obscure cases\nof cpp. This flag should be set toFalsewhen this redirect\ncauses issues.\nfoo\n_safe_foo\nFalse\nno_implicit_headers\u2013 IfTrue, skips automatically adding headers, most notably#include<torch/extension.h>and#include<torch/types.h>lines.\nUse this option to improve cold start times when you\nalready include the necessary headers in your source code. Default:False.\nTrue\n#include<torch/extension.h>\n#include<torch/types.h>\nFalse\nExample\n\n```python\n>>> from torch.utils.cpp_extension import load_inline\n>>> source = \"\"\"\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\n  return x.sin() + y.sin();\n}\n\"\"\"\n>>> module = load_inline(name='inline_extension',\n...                      cpp_sources=[source],\n...                      functions=['sin_add'])\n\n```\n\nNote\nSince load_inline will just-in-time compile the source code, please ensure\nthat you have the right toolchains installed in the runtime. For example,\nwhen loading C++, make sure a C++ compiler is available. If you\u2019re loading\na CUDA extension, you will need to additionally install the corresponding CUDA\ntoolkit (nvcc and any other dependencies your code has). Compiling toolchains\nare not included when you install torch and must be additionally installed.\nDuring compiling, by default, the Ninja backend uses #CPUS + 2 workers to build\nthe extension. This may use up too many resources on some systems. One\ncan control the number of workers by setting theMAX_JOBSenvironment\nvariable to a non-negative number.\nGet the include paths required to build a C++ or CUDA or SYCL extension.\ndevice_type(str) \u2013 Defaults to \u201ccpu\u201d.\nA list of include path strings.\nlist[str]\nDetermine if the given compiler is ABI-compatible with PyTorch alongside its version.\ncompiler(str) \u2013 The compiler executable name to check (e.g.g++).\nMust be executable in a shell process.\ng++\nA tuple that contains a boolean that defines if the compiler is (likely) ABI-incompatible with PyTorch,\nfollowed by aTorchVersionstring that contains the compiler version separated by dots.\ntuple[bool, torch.torch_version.TorchVersion]\nRaiseRuntimeErrorifninjabuild system is not available on the system, does nothing otherwise.\nRuntimeError\nReturnTrueif theninjabuild system is available on the system,Falseotherwise.\nTrue\nFalse",
    "url": "https://pytorch.org/docs/stable/cpp_extension.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e4f12efc5570705d431331e6fb639203",
    "source": "pytorch_docs",
    "title": "torch.xpu \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.xpu#\n\nCreated On: Feb 01, 2024 | Last Updated On: Jun 06, 2025\nThis package introduces support for the XPU backend, specifically tailored for\nIntel GPU optimization.\nThis package is lazily initialized, so you can always import it, and useis_available()to determine if your system supports XPU.\nis_available()\nStreamContext\n\nStreamContext\nContext-manager that selects a given stream.\ncurrent_device\n\ncurrent_device\nReturn the index of a currently selected device.\ncurrent_stream\n\ncurrent_stream\nReturn the currently selectedStreamfor a given device.\nStream\ndevice\n\ndevice\nContext-manager that changes the selected device.\ndevice_count\n\ndevice_count\nReturn the number of XPU device available.\ndevice_of\n\ndevice_of\nContext-manager that changes the current device to that of given object.\nget_arch_list\n\nget_arch_list\nReturn list XPU architectures this library was compiled for.\nget_device_capability\n\nget_device_capability\nGet the xpu capability of a device.\nget_device_name\n\nget_device_name\nGet the name of a device.\nget_device_properties\n\nget_device_properties\nGet the properties of a device.\nget_gencode_flags\n\nget_gencode_flags\nReturn XPU AOT(ahead-of-time) build flags this library was compiled with.\nget_stream_from_external\n\nget_stream_from_external\nReturn aStreamfrom an external SYCL queue.\nStream\ninit\n\ninit\nInitialize PyTorch's XPU state.\nis_available\n\nis_available\nReturn a bool indicating if XPU is currently available.\nis_initialized\n\nis_initialized\nReturn whether PyTorch's XPU state has been initialized.\nset_device\n\nset_device\nSet the current device.\nset_stream\n\nset_stream\nSet the current stream.This is a wrapper API to set the stream.\nstream\n\nstream\nWrap around the Context-manager StreamContext that selects a given stream.\nsynchronize\n\nsynchronize\nWait for all kernels in all streams on a XPU device to complete.\n\n## Random Number Generator#\n\nget_rng_state\n\nget_rng_state\nReturn the random number generator state of the specified GPU as a ByteTensor.\nget_rng_state_all\n\nget_rng_state_all\nReturn a list of ByteTensor representing the random number states of all devices.\ninitial_seed\n\ninitial_seed\nReturn the current random seed of the current GPU.\nmanual_seed\n\nmanual_seed\nSet the seed for generating random numbers for the current GPU.\nmanual_seed_all\n\nmanual_seed_all\nSet the seed for generating random numbers on all GPUs.\nseed\n\nseed\nSet the seed for generating random numbers to a random number for the current GPU.\nseed_all\n\nseed_all\nSet the seed for generating random numbers to a random number on all GPUs.\nset_rng_state\n\nset_rng_state\nSet the random number generator state of the specified GPU.\nset_rng_state_all\n\nset_rng_state_all\nSet the random number generator state of all devices.\n\n## Streams and events#\n\nEvent\n\nEvent\nWrapper around a XPU event.\nStream\n\nStream\nWrapper around a XPU stream.\n\n## Memory management#\n\nempty_cache\n\nempty_cache\nRelease all unoccupied cached memory currently held by the caching allocator so that those can be used in other XPU application.\nmax_memory_allocated\n\nmax_memory_allocated\nReturn the maximum GPU memory occupied by tensors in bytes for a given device.\nmax_memory_reserved\n\nmax_memory_reserved\nReturn the maximum GPU memory managed by the caching allocator in bytes for a given device.\nmem_get_info\n\nmem_get_info\nReturn the global free and total GPU memory for a given device.\nmemory_allocated\n\nmemory_allocated\nReturn the current GPU memory occupied by tensors in bytes for a given device.\nmemory_reserved\n\nmemory_reserved\nReturn the current GPU memory managed by the caching allocator in bytes for a given device.\nmemory_stats\n\nmemory_stats\nReturn a dictionary of XPU memory allocator statistics for a given device.\nmemory_stats_as_nested_dict\n\nmemory_stats_as_nested_dict\nReturn the result ofmemory_stats()as a nested dictionary.\nmemory_stats()\nreset_accumulated_memory_stats\n\nreset_accumulated_memory_stats\nReset the \"accumulated\" (historical) stats tracked by the XPU memory allocator.\nreset_peak_memory_stats\n\nreset_peak_memory_stats\nReset the \"peak\" stats tracked by the XPU memory allocator.",
    "url": "https://pytorch.org/docs/stable/xpu.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e638bdafc8b2a8a15f33ff56c8cb19ed",
    "source": "pytorch_docs",
    "title": "torch.nested \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.nested#\n\nCreated On: Mar 02, 2022 | Last Updated On: Jun 14, 2025\n\n## Introduction#\n\nWarning\nThe PyTorch API of nested tensors is in prototype stage and will change in the near future.\nNested tensors allow for ragged-shaped data to be contained within and operated upon as a\nsingle tensor. Such data is stored underneath in an efficient packed representation, while exposing\na standard PyTorch tensor interface for applying operations.\nA common application of nested tensors is for expressing batches of variable-length sequential data\npresent in various domains, such as varying sentence lengths, image sizes, and audio / video clip\nlengths. Traditionally, such data has been handled by padding sequences to that of the max length\nwithin a batch, performing computation on the padded form, and subsequently masking to remove\npadding. This is inefficient and error-prone, and nested tensors exist to address these problems.\nThe API for calling operations on a nested tensor is no different from that of a regulartorch.Tensor, allowing for seamless integration with existing models, with the main\ndifference beingconstruction of the inputs.\ntorch.Tensor\nAs this is a prototype feature, the set ofoperations supportedis\nlimited, but growing. We welcome issues, feature requests, and contributions.\nMore information on contributing can be foundin this Readme.\n\n## Construction#\n\nNote\nThere are two forms of nested tensors present within PyTorch, distinguished by layout as\nspecified during construction. Layout can be one oftorch.stridedortorch.jagged.\nWe recommend utilizing thetorch.jaggedlayout whenever possible. While it currently only\nsupports a single ragged dimension, it has better op coverage, receives active development, and\nintegrates well withtorch.compile. These docs adhere to this recommendation and refer to\nnested tensors with thetorch.jaggedlayout as \u201cNJTs\u201d for brevity throughout.\ntorch.strided\ntorch.jagged\ntorch.jagged\ntorch.compile\ntorch.jagged\nConstruction is straightforward and involves passing a list of tensors to thetorch.nested.nested_tensorconstructor. A nested tensor with thetorch.jaggedlayout\n(AKA an \u201cNJT\u201d) supports a single ragged dimension. This constructor will copy the input tensors\ninto a packed, contiguous block of memory according to the layout described in thedata_layout_\nsection below.\ntorch.nested.nested_tensor\ntorch.jagged\ndata_layout\n\n```python\n>>> a, b = torch.arange(3), torch.arange(5) + 3\n>>> a\ntensor([0, 1, 2])\n>>> b\ntensor([3, 4, 5, 6, 7])\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> print([component for component in nt])\n[tensor([0, 1, 2]), tensor([3, 4, 5, 6, 7])]\n\n```\n\nEach tensor in the list must have the same number of dimensions, but the shapes can otherwise vary\nalong a single dimension. If the dimensionalities of the input components don\u2019t match, the\nconstructor throws an error.\n\n```python\n>>> a = torch.randn(50, 128) # 2D tensor\n>>> b = torch.randn(2, 50, 128) # 3D tensor\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n...\nRuntimeError: When constructing a nested tensor, all tensors in list must have the same dim\n\n```\n\nDuring construction, dtype, device, and whether gradients are required can be chosen via the\nusual keyword arguments.\n\n```python\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32, device=\"cuda\", requires_grad=True)\n>>> print([component for component in nt])\n[tensor([0., 1., 2.], device='cuda:0',\n       grad_fn=<UnbindBackwardAutogradNestedTensor0>), tensor([3., 4., 5., 6., 7.], device='cuda:0',\n       grad_fn=<UnbindBackwardAutogradNestedTensor0>)]\n\n```\n\ntorch.nested.as_nested_tensorcan be used to preserve autograd history from the tensors passed\nto the constructor. When this constructor is utilized, gradients will flow through the nested tensor\nback into the original components. Note that this constructor still copies the input components into\na packed, contiguous block of memory.\ntorch.nested.as_nested_tensor\n\n```python\n>>> a = torch.randn(12, 512, requires_grad=True)\n>>> b = torch.randn(23, 512, requires_grad=True)\n>>> nt = torch.nested.as_nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.sum().backward()\n>>> a.grad\ntensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])\n>>> b.grad\ntensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])\n\n```\n\nThe above functions all create contiguous NJTs, where a chunk of memory is allocated to store\na packed form of the underlying components (see thedata_layout_ section below for more\ndetails).\ndata_layout\nIt is also possible to create a non-contiguous NJT view over a pre-existing dense tensor\nwith padding, avoiding the memory allocation and copying.torch.nested.narrow()is the tool\nfor accomplishing this.\ntorch.nested.narrow()\n\n```python\n>>> padded = torch.randn(3, 5, 4)\n>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)\n>>> nt = torch.nested.narrow(padded, dim=1, start=0, length=seq_lens, layout=torch.jagged)\n>>> nt.shape\ntorch.Size([3, j1, 4])\n>>> nt.is_contiguous()\nFalse\n\n```\n\nNote that the nested tensor acts as a view over the original padded dense tensor, referencing the\nsame memory without copying / allocation. Operation support for non-contiguous NJTs is somewhat more\nlimited, so if you run into support gaps, it\u2019s always possible to convert to a contiguous NJT\nusingcontiguous().\ncontiguous()\n\n## Data Layout and Shape#\n\nFor efficiency, nested tensors generally pack their tensor components into a contiguous chunk of\nmemory and maintain additional metadata to specify batch item boundaries. For thetorch.jaggedlayout, the contiguous chunk of memory is stored in thevaluescomponent, with theoffsetscomponent delineating batch item boundaries for the ragged dimension.\ntorch.jagged\nvalues\noffsets\n\nIt\u2019s possible to directly access the underlying NJT components when necessary.\n\n```python\n>>> a = torch.randn(50, 128) # text 1\n>>> b = torch.randn(32, 128) # text 2\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.values().shape  # note the \"packing\" of the ragged dimension; no padding needed\ntorch.Size([82, 128])\n>>> nt.offsets()\ntensor([ 0, 50, 82])\n\n```\n\nIt can also be useful to construct an NJT from the jaggedvaluesandoffsetsconstituents directly; thetorch.nested.nested_tensor_from_jagged()constructor serves\nthis purpose.\nvalues\noffsets\ntorch.nested.nested_tensor_from_jagged()\n\n```python\n>>> values = torch.randn(82, 128)\n>>> offsets = torch.tensor([0, 50, 82], dtype=torch.int64)\n>>> nt = torch.nested.nested_tensor_from_jagged(values=values, offsets=offsets)\n\n```\n\nAn NJT has a well-defined shape with dimensionality 1 greater than that of its components. The\nunderlying structure of the ragged dimension is represented by a symbolic value (j1in the\nexample below).\nj1\n\n```python\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.dim()\n3\n>>> nt.shape\ntorch.Size([2, j1, 128])\n\n```\n\nNJTs must have the same ragged structure to be compatible with each other. For example, to run a\nbinary operation involving two NJTs, the ragged structures must match (i.e. they must have the\nsame ragged shape symbol in their shapes). In the details, each symbol corresponds with an exactoffsetstensor, so both NJTs must have the sameoffsetstensor to be compatible with\neach other.\noffsets\noffsets\n\n```python\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt2 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt1.offsets() is nt2.offsets()\nFalse\n>>> nt3 = nt1 + nt2\nRuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)\n\n```\n\nIn the above example, even though the conceptual shapes of the two NJTs are the same, they don\u2019t\nshare a reference to the sameoffsetstensor, so their shapes differ, and they are not\ncompatible. We recognize that this behavior is unintuitive and are working hard to relax this\nrestriction for the beta release of nested tensors. For a workaround, see theTroubleshootingsection of this document.\noffsets\nIn addition to theoffsetsmetadata, NJTs can also compute and cache the minimum and maximum\nsequence lengths for its components, which can be useful for invoking particular kernels (e.g. SDPA).\nThere are currently no public APIs for accessing these, but this will change for the beta release.\noffsets\n\n## Supported Operations#\n\nThis section contains a list of common operations over nested tensors that you may find useful.\nIt is not comprehensive, as there are on the order of a couple thousand ops within PyTorch. While\na sizeable subset of these are supported for nested tensors today, full support is a large task.\nThe ideal state for nested tensors is full support of all PyTorch operations that are available\nfor non-nested tensors. To help us accomplish this, please consider:\nRequesting particular ops needed for your use casehereto help us prioritize.\nContributing! It\u2019s not too hard to add nested tensor support for a given PyTorch op; see\ntheContributionssection below for details.\n\n## Viewing nested tensor constituents#\n\nunbind()allows you to retrieve a view of the nested tensor\u2019s constituents.\nunbind()\n\n```python\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(3, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt.unbind()\n(tensor([[-0.9916, -0.3363, -0.2799],\n        [-2.3520, -0.5896, -0.4374]]), tensor([[-2.0969, -1.0104,  1.4841],\n        [ 2.0952,  0.2973,  0.2516],\n        [ 0.9035,  1.3623,  0.2026]]))\n>>> nt.unbind()[0] is not a\nTrue\n>>> nt.unbind()[0].mul_(3)\ntensor([[ 3.6858, -3.7030, -4.4525],\n        [-2.3481,  2.0236,  0.1975]])\n>>> nt.unbind()\n(tensor([[-2.9747, -1.0089, -0.8396],\n        [-7.0561, -1.7688, -1.3122]]), tensor([[-2.0969, -1.0104,  1.4841],\n        [ 2.0952,  0.2973,  0.2516],\n        [ 0.9035,  1.3623,  0.2026]]))\n\n```\n\nNote thatnt.unbind()[0]is not a copy, but rather a slice of the underlying memory, which\nrepresents the first entry or constituent of the nested tensor.\nnt.unbind()[0]\ntorch.nested.to_padded_tensor()converts an NJT to a padded dense tensor with the specified\npadding value. The ragged dimension will be padded out to the size of the maximum sequence length.\ntorch.nested.to_padded_tensor()\n\n```python\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(6, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> padded = torch.nested.to_padded_tensor(nt, padding=4.2)\n>>> padded\ntensor([[[ 1.6107,  0.5723,  0.3913],\n         [ 0.0700, -0.4954,  1.8663],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000]],\n        [[-0.0479, -0.7610, -0.3484],\n         [ 1.1345,  1.0556,  0.3634],\n         [-1.7122, -0.5921,  0.0540],\n         [-0.5506,  0.7608,  2.0606],\n         [ 1.5658, -1.1934,  0.3041],\n         [ 0.1483, -1.1284,  0.6957]]])\n\n```\n\nThis can be useful as an escape hatch to work around NJT support gaps, but ideally such\nconversions should be avoided when possible for optimal memory usage and performance, as the\nmore efficient nested tensor layout does not materialize padding.\nThe reverse conversion can be accomplished usingtorch.nested.narrow(), which applies\nragged structure to a given dense tensor to produce an NJT. Note that by default, this operation\ndoes not copy the underlying data, and thus the output NJT is generally non-contiguous. It may be\nuseful to explicitly callcontiguous()here if a contiguous NJT is desired.\ntorch.nested.narrow()\ncontiguous()\n\n```python\n>>> padded = torch.randn(3, 5, 4)\n>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)\n>>> nt = torch.nested.narrow(padded, dim=1, length=seq_lens, layout=torch.jagged)\n>>> nt.shape\ntorch.Size([3, j1, 4])\n>>> nt = nt.contiguous()\n>>> nt.shape\ntorch.Size([3, j2, 4])\n\n```\n\n\n## Shape manipulations#\n\nNested tensors support a wide array of operations for shape manipulation, including views.\n\n```python\n>>> a = torch.randn(2, 6)\n>>> b = torch.randn(4, 6)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt.shape\ntorch.Size([2, j1, 6])\n>>> nt.unsqueeze(-1).shape\ntorch.Size([2, j1, 6, 1])\n>>> nt.unflatten(-1, [2, 3]).shape\ntorch.Size([2, j1, 2, 3])\n>>> torch.cat([nt, nt], dim=2).shape\ntorch.Size([2, j1, 12])\n>>> torch.stack([nt, nt], dim=2).shape\ntorch.Size([2, j1, 2, 6])\n>>> nt.transpose(-1, -2).shape\ntorch.Size([2, 6, j1])\n\n```\n\n\n## Attention mechanisms#\n\nAs variable-length sequences are common inputs to attention mechanisms, nested tensors support\nimportant attention operatorsScaled Dot Product Attention (SDPA)andFlexAttention.\nSeeherefor usage examples of NJT with SDPA andherefor usage examples of NJT with FlexAttention.\n\n## Usage with torch.compile#\n\nNJTs are designed to be used withtorch.compile()for optimal performance, and we always\nrecommend utilizingtorch.compile()with NJTs when possible. NJTs work out-of-the-box and\ngraph-break-free both when passed as inputs to a compiled function or module OR when\ninstantiated in-line within the function.\ntorch.compile()\ntorch.compile()\nNote\n\n```python\nIf you're not able to utilize ``torch.compile()`` for your use case, performance and memory\nusage may still benefit from the use of NJTs, but it's not as clear-cut whether this will be\nthe case. It is important that the tensors being operated on are large enough so the\nperformance gains are not outweighed by the overhead of python tensor subclasses.\n\n```\n\n\n```python\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(4, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> def f(x): return x.sin() + 1\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output = compiled_f(nt)\n>>> output.shape\ntorch.Size([2, j1, 3])\n>>> def g(values, offsets): return torch.nested.nested_tensor_from_jagged(values, offsets) * 2.\n...\n>>> compiled_g = torch.compile(g, fullgraph=True)\n>>> output2 = compiled_g(nt.values(), nt.offsets())\n>>> output2.shape\ntorch.Size([2, j1, 3])\n\n```\n\nNote that NJTs supportDynamic Shapesto avoid unnecessary recompiles with changing ragged structure.\n\n```python\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(4, 3)\n>>> c = torch.randn(5, 3)\n>>> d = torch.randn(6, 3)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt2 = torch.nested.nested_tensor([c, d], layout=torch.jagged)\n>>> def f(x): return x.sin() + 1\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output1 = compiled_f(nt1)\n>>> output2 = compiled_f(nt2)  # NB: No recompile needed even though ragged structure differs\n\n```\n\nIf you run into problems or arcane errors when utilizing NJT +torch.compile, please file a\nPyTorch issue. Full subclass support withintorch.compileis a long-term effort and there may\nbe some rough edges at this time.\ntorch.compile\ntorch.compile\n\n## Troubleshooting#\n\nThis section contains common errors that you may run into when utilizing nested tensors, alongside\nthe reason for these errors and suggestions for how to address them.\n\n## Unimplemented ops#\n\nThis error is becoming rarer as nested tensor op support grows, but it\u2019s still possible to hit it\ntoday given that there are a couple thousand ops within PyTorch.\n\n```python\n    NotImplementedError: aten.view_as_real.default\n\n```\n\nThe error is straightforward; we haven\u2019t gotten around to adding op support for this particular op\nyet. If you\u2019d like, you cancontributean implementation yourself OR simplyrequestthat we add support for this op\nin a future PyTorch release.\n\n## Ragged structure incompatibility#\n\n\n```python\n    RuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)\n\n```\n\nThis error occurs when calling an op that operates over multiple NJTs with incompatible ragged\nstructures. Currently, it is required that input NJTs have the exact sameoffsetsconstituent\nin order to have the same symbolic ragged structure symbol (e.g.j1).\noffsets\nj1\nAs a workaround for this situation, it is possible to construct NJTs from thevaluesandoffsetscomponents directly. With both NJTs referencing the sameoffsetscomponents, they\nare considered to have the same ragged structure and are thus compatible.\nvalues\noffsets\noffsets\n\n```python\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt2 = torch.nested.nested_tensor_from_jagged(values=torch.randn(82, 128), offsets=nt1.offsets())\n>>> nt3 = nt1 + nt2\n>>> nt3.shape\ntorch.Size([2, j1, 128])\n\n```\n\n\n## Data dependent operation within torch.compile#\n\n\n```python\n    torch._dynamo.exc.Unsupported: data dependent operator: aten._local_scalar_dense.default; to enable, set torch._dynamo.config.capture_scalar_outputs = True\n\n```\n\nThis error occurs when calling an op that does data-dependent operation within torch.compile; this\ncommonly occurs for ops that need to examine the values of the NJT\u2019soffsetsto determine the\noutput shape. For example:\noffsets\n\n```python\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> def f(nt): return nt.chunk(2, dim=0)[0]\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output = compiled_f(nt)\n\n```\n\nIn this example, callingchunk()on the batch dimension of the NJT requires examination of the\nNJT\u2019soffsetsdata to delineate batch item boundaries within the packed ragged dimension. As a\nworkaround, there are a couple torch.compile flags that can be set:\nchunk()\noffsets\n\n```python\n>>> torch._dynamo.config.capture_dynamic_output_shape_ops = True\n>>> torch._dynamo.config.capture_scalar_outputs = True\n\n```\n\nIf, after setting these, you still see data-dependent operator errors, please file an issue with\nPyTorch. This area oftorch.compile()is still in heavy development and certain aspects of\nNJT support may be incomplete.\ntorch.compile()\n\n## Contributions#\n\nIf you\u2019d like to contribute to nested tensor development, one of the most impactful ways to do\nso is to add nested tensor support for a currently-unsupported PyTorch op. This process generally\nconsists of a couple simple steps:\nDetermine the name of the op to add; this should be something likeaten.view_as_real.default.\nThe signature for this op can be found inaten/src/ATen/native/native_functions.yaml.\naten.view_as_real.default\naten/src/ATen/native/native_functions.yaml\nRegister an op implementation intorch/nested/_internal/ops.py, following the pattern\nestablished there for other ops. Use the signature fromnative_functions.yamlfor schema\nvalidation.\ntorch/nested/_internal/ops.py\nnative_functions.yaml\nThe most common way to implement an op is to unwrap the NJT into its constituents, redispatch the\nop on the underlyingvaluesbuffer, and propagate the relevant NJT metadata (includingoffsets) to a new output NJT. If the output of the op is expected to have a different shape\nfrom the input, newoffsets, etc. metadata must be computed.\nvalues\noffsets\noffsets\nWhen an op is applied over the batch or ragged dimension, these tricks can help quickly get a\nworking implementation:\nFornon-batchwiseoperation, anunbind()-based fallback should work.\nunbind()\nFor operation on the ragged dimension, consider converting to padded dense with a properly-selected\npadding value that won\u2019t negatively bias the output, running the op, and converting back to NJT.\nWithintorch.compile, these conversions can be fused to avoid materializing the padded\nintermediate.\ntorch.compile\n\n## Detailed Docs for Construction and Conversion Functions#\n\nConstructs a nested tensor with no autograd history (also known as a \u201cleaf tensor\u201d, seeAutograd mechanics) fromtensor_lista list of tensors.\ntensor_list\ntensor_list(List[array_like]) \u2013 a list of tensors, or anything that can be passed to torch.tensor,\ndimensionality.(where each elementofthe list has the same) \u2013\ndtype(torch.dtype, optional) \u2013 the desired type of returned nested tensor.\nDefault: if None, sametorch.dtypeas leftmost tensor in the list.\ntorch.dtype\ntorch.dtype\nlayout(torch.layout, optional) \u2013 the desired layout of returned nested tensor.\nOnly strided and jagged layouts are supported. Default: if None, the strided layout.\ntorch.layout\ndevice(torch.device, optional) \u2013 the desired device of returned nested tensor.\nDefault: if None, sametorch.deviceas leftmost tensor in the list\ntorch.device\ntorch.device\nrequires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned nested tensor. Default:False.\nFalse\npin_memory(bool,optional) \u2013 If set, returned nested tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default:False.\nFalse\nTensor\nExample:\n\n```python\n>>> a = torch.arange(3, dtype=torch.float, requires_grad=True)\n>>> b = torch.arange(5, dtype=torch.float, requires_grad=True)\n>>> nt = torch.nested.nested_tensor([a, b], requires_grad=True)\n>>> nt.is_leaf\nTrue\n\n```\n\nConstructs a jagged layout nested tensor from the given jagged components. The jagged layout\nconsists of a required values buffer with the jagged dimension packed into a single dimension.\nThe offsets / lengths metadata determines how this dimension is split into batch elements\nand are expected to be allocated on the same device as the values buffer.\noffsets: Indices within the packed dimension splitting it into heterogeneously-sized\nbatch elements. Example: [0, 2, 3, 6] indicates that a packed jagged dim of size 6\nshould be conceptually split into batch elements of length [2, 1, 3]. Note that both the\nbeginning and ending offsets are required for kernel convenience (i.e. shape batch_size + 1).\nlengths: Lengths of the individual batch elements; shape == batch_size. Example: [2, 1, 3]\nindicates that a packed jagged dim of size 6 should be conceptually split into batch\nelements of length [2, 1, 3].\nNote that it can be useful to provide both offsets and lengths. This describes a nested tensor\nwith \u201choles\u201d, where the offsets indicate the start position of each batch item and the length\nspecifies the total number of elements (see example below).\nThe returned jagged layout nested tensor will be a view of the input values tensor.\nvalues(torch.Tensor) \u2013 The underlying buffer in the shape of\n(sum_B(*), D_1, \u2026, D_N). The jagged dimension is packed into a single dimension,\nwith the offsets / lengths metadata used to distinguish batch elements.\ntorch.Tensor\noffsets(optionaltorch.Tensor) \u2013 Offsets into the jagged dimension of shape B + 1.\ntorch.Tensor\nlengths(optionaltorch.Tensor) \u2013 Lengths of the batch elements of shape B.\ntorch.Tensor\njagged_dim(optional python:int) \u2013 Indicates which dimension in values is the packed jagged\ndimension. Must be >= 1 as the batch dimension (dim=0) cannot be ragged.\nIf None, this is set to dim=1 (i.e. the dimension immediately following the batch dimension). Default: None\nmin_seqlen(optional python:int) \u2013 If set, uses the specified value as the cached minimum sequence\nlength for the returned nested tensor. This can be a useful alternative to computing\nthis value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\nmax_seqlen(optional python:int) \u2013 If set, uses the specified value as the cached maximum sequence\nlength for the returned nested tensor. This can be a useful alternative to computing\nthis value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\nTensor\nExample:\n\n```python\n>>> values = torch.randn(12, 5)\n>>> offsets = torch.tensor([0, 3, 5, 6, 10, 12])\n>>> nt = nested_tensor_from_jagged(values, offsets)\n>>> # 3D shape with the middle dimension jagged\n>>> nt.shape\ntorch.Size([5, j2, 5])\n>>> # Length of each item in the batch:\n>>> offsets.diff()\ntensor([3, 2, 1, 4, 2])\n\n>>> values = torch.randn(6, 5)\n>>> offsets = torch.tensor([0, 2, 3, 6])\n>>> lengths = torch.tensor([1, 1, 2])\n>>> # NT with holes\n>>> nt = nested_tensor_from_jagged(values, offsets, lengths)\n>>> a, b, c = nt.unbind()\n>>> # Batch item 1 consists of indices [0, 1)\n>>> torch.equal(a, values[0:1, :])\nTrue\n>>> # Batch item 2 consists of indices [2, 3)\n>>> torch.equal(b, values[2:3, :])\nTrue\n>>> # Batch item 3 consists of indices [3, 5)\n>>> torch.equal(c, values[3:5, :])\nTrue\n\n```\n\nConstructs a nested tensor preserving autograd history from a tensor or a list / tuple of\ntensors.\nIf a nested tensor is passed, it will be returned directly unless the device / dtype / layout\ndiffer. Note that converting device / dtype will result in a copy, while converting layout\nis not currently supported by this function.\nIf a non-nested tensor is passed, it is treated as a batch of constituents of consistent size.\nA copy will be incurred if the passed device / dtype differ from those of the input OR if\nthe input is non-contiguous. Otherwise, the input\u2019s storage will be used directly.\nIf a tensor list is provided, tensors in the list are always copied during construction of\nthe nested tensor.\nts(TensororList[Tensor] orTuple[Tensor]) \u2013 a tensor to treat as a nested tensor OR a\nlist / tuple of tensors with the same ndim\ndtype(torch.dtype, optional) \u2013 the desired type of returned nested tensor.\nDefault: if None, sametorch.dtypeas leftmost tensor in the list.\ntorch.dtype\ntorch.dtype\ndevice(torch.device, optional) \u2013 the desired device of returned nested tensor.\nDefault: if None, sametorch.deviceas leftmost tensor in the list\ntorch.device\ntorch.device\nlayout(torch.layout, optional) \u2013 the desired layout of returned nested tensor.\nOnly strided and jagged layouts are supported. Default: if None, the strided layout.\ntorch.layout\nTensor\nExample:\n\n```python\n>>> a = torch.arange(3, dtype=torch.float, requires_grad=True)\n>>> b = torch.arange(5, dtype=torch.float, requires_grad=True)\n>>> nt = torch.nested.as_nested_tensor([a, b])\n>>> nt.is_leaf\nFalse\n>>> fake_grad = torch.nested.nested_tensor([torch.ones_like(a), torch.zeros_like(b)])\n>>> nt.backward(fake_grad)\n>>> a.grad\ntensor([1., 1., 1.])\n>>> b.grad\ntensor([0., 0., 0., 0., 0.])\n>>> c = torch.randn(3, 5, requires_grad=True)\n>>> nt2 = torch.nested.as_nested_tensor(c)\n\n```\n\nReturns a new (non-nested) Tensor by padding theinputnested tensor.\nThe leading entries will be filled with the nested data,\nwhile the trailing entries will be padded.\ninput\nWarning\nto_padded_tensor()always copies the underlying data,\nsince the nested and the non-nested tensors differ in memory layout.\nto_padded_tensor()\npadding(float) \u2013 The padding value for the trailing entries.\noutput_size(Tuple[int]) \u2013 The size of the output tensor.\nIf given, it must be large enough to contain all nested data;\nelse, will infer by taking the max size of each nested sub-tensor along each dimension.\nout(Tensor,optional) \u2013 the output tensor.\nExample:\n\n```python\n>>> nt = torch.nested.nested_tensor([torch.randn((2, 5)), torch.randn((3, 4))])\nnested_tensor([\n  tensor([[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276],\n          [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995]]),\n  tensor([[-1.8546, -0.7194, -0.2918, -0.1846],\n          [ 0.2773,  0.8793, -0.5183, -0.6447],\n          [ 1.8009,  1.8468, -0.9832, -1.5272]])\n])\n>>> pt_infer = torch.nested.to_padded_tensor(nt, 0.0)\ntensor([[[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276],\n         [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995],\n         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n        [[-1.8546, -0.7194, -0.2918, -0.1846,  0.0000],\n         [ 0.2773,  0.8793, -0.5183, -0.6447,  0.0000],\n         [ 1.8009,  1.8468, -0.9832, -1.5272,  0.0000]]])\n>>> pt_large = torch.nested.to_padded_tensor(nt, 1.0, (2, 4, 6))\ntensor([[[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276,  1.0000],\n         [-1.9967, -1.0054,  1.8972,  0.9174, -1.4995,  1.0000],\n         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]],\n        [[-1.8546, -0.7194, -0.2918, -0.1846,  1.0000,  1.0000],\n         [ 0.2773,  0.8793, -0.5183, -0.6447,  1.0000,  1.0000],\n         [ 1.8009,  1.8468, -0.9832, -1.5272,  1.0000,  1.0000],\n         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]])\n>>> pt_small = torch.nested.to_padded_tensor(nt, 2.0, (2, 2, 2))\nRuntimeError: Value in output_size is less than NestedTensor padded size. Truncation is not supported.\n\n```\n\nConstructs a nested tensor given a strided tensor input and a strided mask, the resulting jagged layout nested tensor\nwill have values retain values where the mask is equal to True. The dimensionality of the mask is preserved and is\nrepresented with the offsets, this is unlikemasked_select()where the output is collapsed to a 1D tensor.\nmasked_select()\nArgs:\ntensor (torch.Tensor): a strided tensor from which the jagged layout nested tensor is constructed from.\nmask (torch.Tensor): a strided mask tensor which is applied to the tensor input\ntorch.Tensor\ntorch.Tensor\nExample:\n\n```python\n>>> tensor = torch.randn(3, 3)\n>>> mask = torch.tensor([[False, False, True], [True, False, True], [False, False, True]])\n>>> nt = torch.nested.masked_select(tensor, mask)\n>>> nt.shape\ntorch.Size([3, j4])\n>>> # Length of each item in the batch:\n>>> nt.offsets().diff()\ntensor([1, 2, 1])\n\n>>> tensor = torch.randn(6, 5)\n>>> mask = torch.tensor([False])\n>>> nt = torch.nested.masked_select(tensor, mask)\n>>> nt.shape\ntorch.Size([6, j5])\n>>> # Length of each item in the batch:\n>>> nt.offsets().diff()\ntensor([0, 0, 0, 0, 0, 0])\n\n```\n\nTensor\nConstructs a nested tensor (which might be a view) fromtensor, a strided tensor. This follows\nsimilar semantics to torch.Tensor.narrow, where in thedim-th dimension the new nested tensor\nshows only the elements in the interval[start, start+length). As nested representations\nallow for a differentstartandlengthat each \u2018row\u2019 of that dimension,startandlengthcan also be tensors of shapetensor.shape[0].\ntensor\ndim\nstart\nlength\nThere\u2019s some differences depending on the layout you use for the nested tensor. If using strided layout,\ntorch.narrow will do a copy of the narrowed data into a contiguous NT with strided layout, while\njagged layout narrow() will create a non-contiguous view of your original strided tensor. This particular\nrepresentation is really useful for representing kv-caches in Transformer models, as specialized\nSDPA kernels can deal with format easily, resulting in performance improvements.\ntensor(torch.Tensor) \u2013 a strided tensor, which will be used as the underlying data\nfor the nested tensor if using the jagged layout or will be copied for the strided layout.\ntorch.Tensor\ndim(int) \u2013 the dimension where narrow will be applied. Onlydim=1is supported for the\njagged layout, while strided supports all dim\nstart(Union[int,torch.Tensor]) \u2013 starting element for the narrow operation\ntorch.Tensor\nlength(Union[int,torch.Tensor]) \u2013 number of elements taken during the narrow op\ntorch.Tensor\nlayout(torch.layout, optional) \u2013 the desired layout of returned nested tensor.\nOnly strided and jagged layouts are supported. Default: if None, the strided layout.\ntorch.layout\nTensor\nExample:\n\n```python\n>>> starts = torch.tensor([0, 1, 2, 3, 4], dtype=torch.int64)\n>>> lengths = torch.tensor([3, 2, 2, 1, 5], dtype=torch.int64)\n>>> narrow_base = torch.randn(5, 10, 20)\n>>> nt_narrowed = torch.nested.narrow(narrow_base, 1, starts, lengths, layout=torch.jagged)\n>>> nt_narrowed.is_contiguous()\nFalse\n\n```\n\nSee also\nAccelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile",
    "url": "https://pytorch.org/docs/stable/nested.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "fb72f9de4030942757b0c4a370023680",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/cuda._sanitizer.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4f3f60e56c0e574d14b6a0c189eacb0c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/torch.load.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e34e5ddc62d22ef8db87f6d02b575fc4",
    "source": "pytorch_docs",
    "title": "python.control-flow \u2014 PyTorch 2.9 documentation",
    "text": "\n## python.control-flow#\n\n\n## dynamic_shape_if_guard#\n\nNote\nTags:torch.dynamic-shape,python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass DynamicShapeIfGuard(torch.nn.Module):\n    \"\"\"\n    `if` statement with backed dynamic shape predicate will be specialized into\n    one particular branch and generate a guard. However, export will fail if the\n    the dimension is marked as dynamic shape from higher level API.\n    \"\"\"\n\n    def forward(self, x):\n        if x.shape[0] == 3:\n            return x.cos()\n\n        return x.sin()\n\nexample_args = (torch.randn(3, 2, 2),)\ntags = {\"torch.dynamic-shape\", \"python.control-flow\"}\nmodel = DynamicShapeIfGuard()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2, 2]\"):\n                 cos: \"f32[3, 2, 2]\" = torch.ops.aten.cos.default(x);  x = None\n            return (cos,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    cos: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## list_unpack#\n\nNote\nTags:python.data-structure,python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\n\nimport torch\n\nclass ListUnpack(torch.nn.Module):\n    \"\"\"\n    Lists are treated as static construct, therefore unpacking should be\n    erased after tracing.\n    \"\"\"\n\n    def forward(self, args: list[torch.Tensor]):\n        \"\"\"\n        Lists are treated as static construct, therefore unpacking should be\n        erased after tracing.\n        \"\"\"\n        x, *y = args\n        return x + y[0]\n\nexample_args = ([torch.randn(3, 2), torch.tensor(4), torch.tensor(5)],)\ntags = {\"python.control-flow\", \"python.data-structure\"}\nmodel = ListUnpack()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, args_0: \"f32[3, 2]\", args_1: \"i64[]\", args_2: \"i64[]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(args_0, args_1);  args_0 = args_1 = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    args_0: USER_INPUT\n    args_1: USER_INPUT\n    args_2: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## static_for_loop#\n\nNote\nTags:python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass StaticForLoop(torch.nn.Module):\n    \"\"\"\n    A for loop with constant number of iterations should be unrolled in the exported graph.\n    \"\"\"\n\n    def forward(self, x):\n        # constant\n        ret = [i + x for i in range(10)]\n        return ret\n\nexample_args = (torch.randn(3, 2),)\ntags = {\"python.control-flow\"}\nmodel = StaticForLoop()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 0)\n            add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1)\n            add_2: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 2)\n            add_3: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 3)\n            add_4: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 4)\n            add_5: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 5)\n            add_6: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 6)\n            add_7: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 7)\n            add_8: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 8)\n            add_9: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 9);  x = None\n            return (add, add_1, add_2, add_3, add_4, add_5, add_6, add_7, add_8, add_9)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n    add_1: USER_OUTPUT\n    add_2: USER_OUTPUT\n    add_3: USER_OUTPUT\n    add_4: USER_OUTPUT\n    add_5: USER_OUTPUT\n    add_6: USER_OUTPUT\n    add_7: USER_OUTPUT\n    add_8: USER_OUTPUT\n    add_9: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## static_if#\n\nNote\nTags:python.control-flow\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass StaticIf(torch.nn.Module):\n    \"\"\"\n    `if` statement with static predicate value should be traced through with the\n    taken branch.\n    \"\"\"\n\n    def forward(self, x):\n        if len(x.shape) == 3:\n            return x + torch.ones(1, 1, 1)\n\n        return x\n\nexample_args = (torch.randn(3, 2, 2),)\ntags = {\"python.control-flow\"}\nmodel = StaticIf()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 2, 2]\"):\n                 ones: \"f32[1, 1, 1]\" = torch.ops.aten.ones.default([1, 1, 1], device = device(type='cpu'), pin_memory = False)\n            add: \"f32[3, 2, 2]\" = torch.ops.aten.add.Tensor(x, ones);  x = ones = None\n            return (add,)\n\nGraph signature:\n    # inputs\n    x: USER_INPUT\n\n    # outputs\n    add: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/python.control-flow.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1f1c1d4cbc0f28179357937777d06d34",
    "source": "pytorch_docs",
    "title": "Community \u2014 PyTorch 2.9 documentation",
    "text": "\n## Community#\n\nCreated On: Apr 16, 2025 | Last Updated On: May 12, 2025\nPyTorch is more than just a deep learning framework\u2014it\u2019s a vibrant\necosystem powered by a diverse global community. The PyTorch community\nbrings together researchers, developers, students, and industry\nprofessionals who collaborate to advance the state of machine learning.\nCheck out the resources below to learn how to contribute code to the\ncore framework, report and fix bugs, improve documentation, and much more.",
    "url": "https://pytorch.org/docs/stable/community/index.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "9cd7777350f225259c341bf9d0c07a29",
    "source": "pytorch_docs",
    "title": "torch.mtia \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.mtia#\n\nCreated On: Jul 11, 2023 | Last Updated On: Jun 08, 2025\nThe MTIA backend is implemented out of the tree, only interfaces are be defined here.\nThis package enables an interface for accessing MTIA backend in python\nStreamContext\n\nStreamContext\nContext-manager that selects a given stream.\ncurrent_device\n\ncurrent_device\nReturn the index of a currently selected device.\ncurrent_stream\n\ncurrent_stream\nReturn the currently selectedStreamfor a given device.\nStream\ndefault_stream\n\ndefault_stream\nReturn the defaultStreamfor a given device.\nStream\ndevice_count\n\ndevice_count\nReturn the number of MTIA devices available.\ninit\n\ninit\n\nis_available\n\nis_available\nReturn true if MTIA device is available\nis_initialized\n\nis_initialized\nReturn whether PyTorch's MTIA state has been initialized.\nmemory_stats\n\nmemory_stats\nReturn a dictionary of MTIA memory allocator statistics for a given device.\nget_device_capability\n\nget_device_capability\nReturn capability of a given device as a tuple of (major version, minor version).\nempty_cache\n\nempty_cache\nEmpty the MTIA device cache.\nrecord_memory_history\n\nrecord_memory_history\nEnable/Disable the memory profiler on MTIA allocator\nsnapshot\n\nsnapshot\nReturn a dictionary of MTIA memory allocator history\nattach_out_of_memory_observer\n\nattach_out_of_memory_observer\nAttach an out-of-memory observer to MTIA memory allocator\nset_device\n\nset_device\nSet the current device.\nset_stream\n\nset_stream\nSet the current stream.This is a wrapper API to set the stream.\nstream\n\nstream\nWrap around the Context-manager StreamContext that selects a given stream.\nsynchronize\n\nsynchronize\nWaits for all jobs in all streams on a MTIA device to complete.\ndevice\n\ndevice\nContext-manager that changes the selected device.\nset_rng_state\n\nset_rng_state\nSets the random number generator state.\nget_rng_state\n\nget_rng_state\nReturns the random number generator state as a ByteTensor.\nDeferredMtiaCallError\n\nDeferredMtiaCallError\n\n\n## Streams and events#\n\nEvent\n\nEvent\nQuery and record Stream status to identify or control dependencies across Stream and measure timing.\nStream\n\nStream\nAn in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.",
    "url": "https://pytorch.org/docs/stable/mtia.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f3941a62bf5106cf0187bcc988169dc4",
    "source": "pytorch_docs",
    "title": "Working with Graph Breaks \u2014 PyTorch 2.9 documentation",
    "text": "\n## Working with Graph Breaks#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nAs you might remember from (Dynamo Core Concepts)[programming_model.dynamo_core_concepts] that Dynamo performs a graph break when\nit encounters code that can\u2019t be traced. In the defaulttorch.compilesettings, Dynamo compiles the FX graph\nthat has been determined up to that point, executes the unsupported code in regular Python, and then resumes tracing.\ntorch.compile\nGraph breaks enable Dynamo to trace through arbitrary Python code and carve out functional\nsubgraphs that can each be individually optimized.\nHowever, graph breaks may cause unexpected slowness intorch.compile.\nIf you\u2019re not seeing the expected speedups, we recommend checking for graph breaks and removing them.\ntorch.compile\nThe following sections outline strategies for addressing graph breaks.\nfullgraph=True\ntorch._dynamo.nonstrict_trace\nfullgraph=False\ncompile(model)\nmodel.compile()\nerror_on_graph_break\nerror_on_graph_break(False)\nerror_on_graph_break(True)\nerror_on_graph_break\nfullgraph\nfullgraph=True/False\nerror_on_graph_break",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.graph_breaks_index.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ccf5f1ef779277bb256e4911b9243185",
    "source": "pytorch_docs",
    "title": "python.closure \u2014 PyTorch 2.9 documentation",
    "text": "\n## python.closure#\n\n\n## cond_closed_over_variable#\n\nNote\nTags:python.closure,torch.cond\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nfrom functorch.experimental.control_flow import cond\n\nclass CondClosedOverVariable(torch.nn.Module):\n    \"\"\"\n    torch.cond() supports branches closed over arbitrary variables.\n    \"\"\"\n\n    def forward(self, pred, x):\n        def true_fn(val):\n            return x * 2\n\n        def false_fn(val):\n            return x - 2\n\n        return cond(pred, true_fn, false_fn, [x + 1])\n\nexample_args = (torch.tensor(True), torch.randn(3, 2))\ntags = {\"torch.cond\", \"python.closure\"}\nmodel = CondClosedOverVariable()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, pred: \"b8[]\", x: \"f32[3, 2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(x, 1);  add = None\n\n                 true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            cond = torch.ops.higher_order.cond(pred, true_graph_0, false_graph_0, (x,));  pred = true_graph_0 = false_graph_0 = x = None\n            getitem: \"f32[3, 2]\" = cond[0];  cond = None\n            return (getitem,)\n\n        class true_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(x, 2);  x = None\n                return (mul,)\n\n        class false_graph_0(torch.nn.Module):\n            def forward(self, x: \"f32[3, 2]\"):\n                         sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(x, 2);  x = None\n                return (sub,)\n\nGraph signature:\n    # inputs\n    pred: USER_INPUT\n    x: USER_INPUT\n\n    # outputs\n    getitem: USER_OUTPUT\n\nRange constraints: {}\n\n```\n\n\n## nested_function#\n\nNote\nTags:python.closure\nSupport Level: SUPPORTED\nOriginal source code:\n\n```python\n# mypy: allow-untyped-defs\nimport torch\n\nclass NestedFunction(torch.nn.Module):\n    \"\"\"\n    Nested functions are traced through. Side effects on global captures\n    are not supported though.\n    \"\"\"\n\n    def forward(self, a, b):\n        x = a + b\n        z = a - b\n\n        def closure(y):\n            nonlocal x\n            x += 1\n            return x * y + z\n\n        return closure(x)\n\nexample_args = (torch.randn(3, 2), torch.randn(2))\ntags = {\"python.closure\"}\nmodel = NestedFunction()\n\n\ntorch.export.export(model, example_args)\n\n```\n\nResult:\n\n```python\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, a: \"f32[3, 2]\", b: \"f32[2]\"):\n                 add: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(a, b)\n\n                 sub: \"f32[3, 2]\" = torch.ops.aten.sub.Tensor(a, b);  a = b = None\n\n                 add_: \"f32[3, 2]\" = torch.ops.aten.add_.Tensor(add, 1);  add = None\n\n                 mul: \"f32[3, 2]\" = torch.ops.aten.mul.Tensor(add_, add_);  add_ = None\n            add_1: \"f32[3, 2]\" = torch.ops.aten.add.Tensor(mul, sub);  mul = sub = None\n            return (add_1,)\n\nGraph signature:\n    # inputs\n    a: USER_INPUT\n    b: USER_INPUT\n\n    # outputs\n    add_1: USER_OUTPUT\n\nRange constraints: {}\n\n```\n",
    "url": "https://pytorch.org/docs/stable/generated/exportdb/python.closure.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "cca3e6cce3905d5c0055522dcdc02ae3",
    "source": "pytorch_docs",
    "title": "Modules \u2014 PyTorch 2.9 documentation",
    "text": "\n## Modules#\n\nCreated On: Feb 04, 2021 | Last Updated On: Nov 08, 2024\nPyTorch uses modules to represent neural networks. Modules are:\nBuilding blocks of stateful computation.PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for\neasy construction of elaborate, multi-layer neural networks.\nTightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update.\nEasy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more.\nThis note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well.\nA Simple Custom Module\nModules as Building Blocks\nNeural Network Training with Modules\nModule State\nModule Initialization\nModule Hooks\nAdvanced Features\nDistributed Training\nProfiling Performance\nImproving Performance with Quantization\nImproving Memory Usage with Pruning\nParametrizations\nTransforming Modules with FX\n\n## A Simple Custom Module#\n\nTo get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input.\nLinear\n\n```python\nimport torch\nfrom torch import nn\n\nclass MyLinear(nn.Module):\n  def __init__(self, in_features, out_features):\n    super().__init__()\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n    self.bias = nn.Parameter(torch.randn(out_features))\n\n  def forward(self, input):\n    return (input @ self.weight) + self.bias\n\n```\n\nThis simple module has the following fundamental characteristics of modules:\nIt inherits from the base Module class.All modules should subclassModulefor composability with other modules.\nModule\nIt defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless.\nweight\nbias\nParameter\nparameters()\nIt defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs.\nweight\n@\nbias\nforward()\nThis simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:\n\n```python\nm = MyLinear(4, 3)\nsample_input = torch.randn(4)\nm(sample_input)\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)\n\n```\n\nNote that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules.\nforward()\nbackward()\nThe full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name:\nparameters()\nnamed_parameters()\n\n```python\nfor parameter in m.named_parameters():\n  print(parameter)\n: ('weight', Parameter containing:\ntensor([[ 1.0597,  1.1796,  0.8247],\n        [-0.5080, -1.2635, -1.1045],\n        [ 0.0593,  0.2469, -1.4299],\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n('bias', Parameter containing:\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))\n\n```\n\nIn general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another.\n\n## Modules as Building Blocks#\n\nModules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules:\nSequential\n\n```python\nnet = nn.Sequential(\n  MyLinear(4, 3),\n  nn.ReLU(),\n  MyLinear(3, 1)\n)\n\nsample_input = torch.randn(4)\nnet(sample_input)\n: tensor([-0.6749], grad_fn=<AddBackward0>)\n\n```\n\nNote thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules with a single input and output.\nSequential\nMyLinear\nReLU\nMyLinear\nIn general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation.\nFor example, here\u2019s a simple neural network implemented as a custom module:\n\n```python\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l0 = MyLinear(4, 3)\n    self.l1 = MyLinear(3, 1)\n  def forward(self, x):\n    x = self.l0(x)\n    x = F.relu(x)\n    x = self.l1(x)\n    return x\n\n```\n\nThis module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0andl1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019sforward()method. Immediate\nchildren of a module can be iterated through via a call tochildren()ornamed_children():\nl0\nl1\nforward()\nchildren()\nnamed_children()\n\n```python\nnet = Net()\nfor child in net.named_children():\n  print(child)\n: ('l0', MyLinear())\n('l1', MyLinear())\n\n```\n\nTo go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules:\nmodules()\nnamed_modules()\n\n```python\nclass BigNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l1 = MyLinear(5, 4)\n    self.net = Net()\n  def forward(self, x):\n    return self.net(self.l1(x))\n\nbig_net = BigNet()\nfor module in big_net.named_modules():\n  print(module)\n: ('', BigNet(\n  (l1): MyLinear()\n  (net): Net(\n    (l0): MyLinear()\n    (l1): MyLinear()\n  )\n))\n('l1', MyLinear())\n('net', Net(\n  (l0): MyLinear()\n  (l1): MyLinear()\n))\n('net.l0', MyLinear())\n('net.l1', MyLinear())\n\n```\n\nSometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict:\nModuleList\nModuleDict\n\n```python\nclass DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n      x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu')\n\n```\n\nFor any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:\nparameters()\nnamed_parameters()\n\n```python\nfor parameter in dynamic_net.named_parameters():\n  print(parameter)\n: ('linears.0.weight', Parameter containing:\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\n        [-0.4446,  0.4628,  0.8774,  1.6848],\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n        [-0.0550,  1.5317,  1.1064, -0.5562],\n        [-0.4028, -0.6942,  1.5793, -1.0140],\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n        [-0.3526,  0.8756, -1.5847, -0.6016],\n        [-0.3269, -0.1608,  0.2897, -2.0829],\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n('final.weight', Parameter containing:\ntensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\n('final.bias', Parameter containing:\ntensor([0.3381], requires_grad=True))\n\n```\n\nIt\u2019s also easy to move all parameters to a different device or change their precision usingto():\nto()\n\n```python\n# Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n\n```\n\nMore generally, an arbitrary function can be applied to a module and its submodules recursively by\nusing theapply()function. For example, to apply custom initialization to parameters\nof a module and its submodules:\napply()\n\n```python\n# Define a function to initialize Linear weights.\n# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.\n@torch.no_grad()\ndef init_weights(m):\n  if isinstance(m, nn.Linear):\n    nn.init.xavier_normal_(m.weight)\n    m.bias.fill_(0.0)\n\n# Apply the function recursively on the module and its submodules.\ndynamic_net.apply(init_weights)\n\n```\n\nThese examples show how elaborate neural networks can be formed through module composition and conveniently\nmanipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch\nprovides a large library of performant modules within thetorch.nnnamespace that perform common neural\nnetwork operations like pooling, convolutions, loss functions, etc.\ntorch.nn\nIn the next section, we give a full example of training a neural network.\nFor more information, check out:\nLibrary of PyTorch-provided modules:torch.nn\nDefining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html\n\n## Neural Network Training with Modules#\n\nOnce a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:\ntorch.optim\n\n```python\n# Create the network (from previous section) and optimizer\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n# Run a sample training loop that \"teaches\" the network\n# to output the constant zero function\nfor _ in range(10000):\n  input = torch.randn(4)\n  output = net(input)\n  loss = torch.abs(output)\n  net.zero_grad()\n  loss.backward()\n  optimizer.step()\n\n# After training, switch the module to eval mode to do inference, compute performance metrics, etc.\n# (see discussion below for a description of training and evaluation modes)\n...\nnet.eval()\n...\n\n```\n\nIn this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present:\ntorch.abs()\nA network is created.\nAn optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it.\nacquires an input,\nruns the network,\ncomputes a loss,\nzeros the network\u2019s parameters\u2019 gradients,\ncalls loss.backward() to update the parameters\u2019 gradients,\ncalls optimizer.step() to apply the gradients to the parameters.\nAfter the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2018sweightparameter shows that its values are now much closer to 0 (as may be expected):\nl1\nweight\n\n```python\nprint(net.l1.weight)\n: Parameter containing:\ntensor([[-0.0013],\n        [ 0.0030],\n        [-0.0008]], requires_grad=True)\n\n```\n\nNote that the above process is done entirely while the network module is in \u201ctraining mode\u201d. Modules default to\ntraining mode and can be switched between training and evaluation modes usingtrain()andeval(). They can behave differently depending on which mode they are in. For example, theBatchNormmodule maintains a running mean and variance during training that are not updated\nwhen the module is in evaluation mode. In general, modules should be in training mode during training\nand only switched to evaluation mode for inference or evaluation. Below is an example of a custom module\nthat behaves differently between the two modes:\ntrain()\neval()\nBatchNorm\n\n```python\nclass ModalModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n\n  def forward(self, x):\n    if self.training:\n      # Add a constant only in training mode.\n      return x + 1.\n    else:\n      return x\n\n\nm = ModalModule()\nx = torch.randn(4)\n\nprint('training mode output: {}'.format(m(x)))\n: tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481])\n\nm.eval()\nprint('evaluation mode output: {}'.format(m(x)))\n: tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])\n\n```\n\nTraining neural networks can often be tricky. For more information, check out:\nUsing Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html.\nNeural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\nIntroduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n\n## Module State#\n\nIn the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d):\nstate_dict\n\n```python\n# Save the module\ntorch.save(net.state_dict(), 'net.pt')\n\n...\n\n# Load the module later on\nnew_net = Net()\nnew_net.load_state_dict(torch.load('net.pt'))\n: <All keys matched successfully>\n\n```\n\nA module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have:\nstate_dict\nParameters: learnable aspects of computation; contained within thestate_dict\nstate_dict\nBuffers: non-learnable aspects of computation\nPersistentbuffers: contained within thestate_dict(i.e. serialized when saving & loading)\nstate_dict\nNon-persistentbuffers: not contained within thestate_dict(i.e. left out of serialization)\nstate_dict\nAs a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this:\nstate_dict\nregister_buffer()\n\n```python\nclass RunningMean(nn.Module):\n  def __init__(self, num_features, momentum=0.9):\n    super().__init__()\n    self.momentum = momentum\n    self.register_buffer('mean', torch.zeros(num_features))\n  def forward(self, x):\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n    return self.mean\n\n```\n\nNow, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk:\nstate_dict\n\n```python\nm = RunningMean(4)\nfor _ in range(10):\n  input = torch.randn(4)\n  m(input)\n\nprint(m.state_dict())\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\n\n# Serialized form will contain the 'mean' tensor\ntorch.save(m.state_dict(), 'mean.pt')\n\nm_loaded = RunningMean(4)\nm_loaded.load_state_dict(torch.load('mean.pt'))\nassert(torch.all(m.mean == m_loaded.mean))\n\n```\n\nAs mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent:\nstate_dict\n\n```python\nself.register_buffer('unserialized_thing', torch.randn(5), persistent=False)\n\n```\n\nBoth persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto():\nto()\n\n```python\n# Moves all module parameters and buffers to the specified device / dtype\nm.to(device='cuda', dtype=torch.float64)\n\n```\n\nBuffers of a module can be iterated over usingbuffers()ornamed_buffers().\nbuffers()\nnamed_buffers()\n\n```python\nfor buffer in m.named_buffers():\n  print(buffer)\n\n```\n\nThe following class demonstrates the various ways of registering parameters and buffers within a module:\n\n```python\nclass StatefulModule(nn.Module):\n  def __init__(self):\n    super().__init__()\n    # Setting a nn.Parameter as an attribute of the module automatically registers the tensor\n    # as a parameter of the module.\n    self.param1 = nn.Parameter(torch.randn(2))\n\n    # Alternative string-based way to register a parameter.\n    self.register_parameter('param2', nn.Parameter(torch.randn(3)))\n\n    # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything\n    # except a parameter. \"None\" entries like this will not be present in the module's state_dict.\n    self.register_parameter('param3', None)\n\n    # Registers a list of parameters.\n    self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])\n\n    # Registers a dictionary of parameters.\n    self.param_dict = nn.ParameterDict({\n      'foo': nn.Parameter(torch.randn(3)),\n      'bar': nn.Parameter(torch.randn(4))\n    })\n\n    # Registers a persistent buffer (one that appears in the module's state_dict).\n    self.register_buffer('buffer1', torch.randn(4), persistent=True)\n\n    # Registers a non-persistent buffer (one that does not appear in the module's state_dict).\n    self.register_buffer('buffer2', torch.randn(5), persistent=False)\n\n    # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything\n    # except a buffer. \"None\" entries like this will not be present in the module's state_dict.\n    self.register_buffer('buffer3', None)\n\n    # Adding a submodule registers its parameters as parameters of the module.\n    self.linear = nn.Linear(2, 3)\n\nm = StatefulModule()\n\n# Save and load state_dict.\ntorch.save(m.state_dict(), 'state.pt')\nm_loaded = StatefulModule()\nm_loaded.load_state_dict(torch.load('state.pt'))\n\n# Note that non-persistent buffer \"buffer2\" and reserved attributes \"param3\" and \"buffer3\" do\n# not appear in the state_dict.\nprint(m_loaded.state_dict())\n: OrderedDict([('param1', tensor([-0.0322,  0.9066])),\n               ('param2', tensor([-0.4472,  0.1409,  0.4852])),\n               ('buffer1', tensor([ 0.6949, -0.1944,  1.2911, -2.1044])),\n               ('param_list.0', tensor([ 0.4202, -0.1953])),\n               ('param_list.1', tensor([ 1.5299, -0.8747])),\n               ('param_list.2', tensor([-1.6289,  1.4898])),\n               ('param_dict.bar', tensor([-0.6434,  1.5187,  0.0346, -0.4077])),\n               ('param_dict.foo', tensor([-0.0845, -1.4324,  0.7022])),\n               ('linear.weight', tensor([[-0.3915, -0.6176],\n                                         [ 0.6062, -0.5992],\n                                         [ 0.4452, -0.2843]])),\n               ('linear.bias', tensor([-0.3710, -0.0795, -0.3947]))])\n\n```\n\nFor more information, check out:\nSaving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html\nSerialization semantics:https://pytorch.org/docs/main/notes/serialization.html\nWhat is a state dict?https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html\n\n## Module Initialization#\n\nBy default, parameters and floating-point buffers for modules provided bytorch.nnare initialized during\nmodule instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to\nperform well historically for the module type. For certain use cases, it may be desired to initialize with a different\ndtype, device (e.g. GPU), or initialization technique.\ntorch.nn\nExamples:\n\n```python\n# Initialize module directly onto GPU.\nm = nn.Linear(5, 3, device='cuda')\n\n# Initialize module with 16-bit floating point parameters.\nm = nn.Linear(5, 3, dtype=torch.half)\n\n# Skip default parameter initialization and perform custom (e.g. orthogonal) initialization.\nm = torch.nn.utils.skip_init(nn.Linear, 5, 3)\nnn.init.orthogonal_(m.weight)\n\n```\n\nNote that the device and dtype options demonstrated above also apply to any floating-point buffers registered\nfor the module:\n\n```python\nm = nn.BatchNorm2d(3, dtype=torch.half)\nprint(m.running_mean)\n: tensor([0., 0., 0.], dtype=torch.float16)\n\n```\n\nWhile module writers can use any device or dtype to initialize parameters in their custom modules, good practice is\nto usedtype=torch.floatanddevice='cpu'by default as well. Optionally, you can provide full flexibility\nin these areas for your custom module by conforming to the convention demonstrated above that alltorch.nnmodules follow:\ndtype=torch.float\ndevice='cpu'\ntorch.nn\nProvide adeviceconstructor kwarg that applies to any parameters / buffers registered by the module.\ndevice\nProvide adtypeconstructor kwarg that applies to any parameters / floating-point buffers registered by\nthe module.\ndtype\nOnly use initialization functions (i.e. functions fromtorch.nn.init) on parameters and buffers within the\nmodule\u2019s constructor. Note that this is only required to useskip_init(); seethis pagefor an explanation.\ntorch.nn.init\nskip_init()\nFor more information, check out:\nSkipping module parameter initialization:https://pytorch.org/tutorials/prototype/skip_param_init.html\n\n## Module Hooks#\n\nInNeural Network Training with Modules, we demonstrated the training process for a module, which iteratively\nperforms forward and backward passes, updating module parameters each iteration. For more control\nover this process, PyTorch provides \u201chooks\u201d that can perform arbitrary computation during a forward or backward\npass, even modifying how the pass is done if desired. Some useful examples for this functionality include\ndebugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules\nyou haven\u2019t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules.\nPyTorch provides two types of hooks for modules:\nForward hooksare called during the forward pass. They can be installed for a given module withregister_forward_pre_hook()andregister_forward_hook().\nThese hooks will be called respectively just before the forward function is called and just after it is called.\nAlternatively, these hooks can be installed globally for all modules with the analogousregister_module_forward_pre_hook()andregister_module_forward_hook()functions.\nregister_forward_pre_hook()\nregister_forward_hook()\nregister_module_forward_pre_hook()\nregister_module_forward_hook()\nBackward hooksare called during the backward pass. They can be installed withregister_full_backward_pre_hook()andregister_full_backward_hook().\nThese hooks will be called when the backward for this Module has been computed.register_full_backward_pre_hook()will allow the user to access the gradients for outputs\nwhileregister_full_backward_hook()will allow the user to access the gradients\nboth the inputs and outputs. Alternatively, they can be installed globally for all modules withregister_module_full_backward_hook()andregister_module_full_backward_pre_hook().\nregister_full_backward_pre_hook()\nregister_full_backward_hook()\nregister_full_backward_pre_hook()\nregister_full_backward_hook()\nregister_module_full_backward_hook()\nregister_module_full_backward_pre_hook()\nAll hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function.\nforward()\nBelow is an example demonstrating usage of forward and backward hooks:\n\n```python\ntorch.manual_seed(1)\n\ndef forward_pre_hook(m, inputs):\n  # Allows for examination and modification of the input before the forward pass.\n  # Note that inputs are always wrapped in a tuple.\n  input = inputs[0]\n  return input + 1.\n\ndef forward_hook(m, inputs, output):\n  # Allows for examination of inputs / outputs and modification of the outputs\n  # after the forward pass. Note that inputs are always wrapped in a tuple while outputs\n  # are passed as-is.\n\n  # Residual computation a la ResNet.\n  return output + inputs[0]\n\ndef backward_hook(m, grad_inputs, grad_outputs):\n  # Allows for examination of grad_inputs / grad_outputs and modification of\n  # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and\n  # grad_outputs are always wrapped in tuples.\n  new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]\n  return new_grad_inputs\n\n# Create sample module & input.\nm = nn.Linear(3, 3)\nx = torch.randn(2, 3, requires_grad=True)\n\n# ==== Demonstrate forward hooks. ====\n# Run input through module before and after adding hooks.\nprint('output with no forward hooks: {}'.format(m(x)))\n: output with no forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                        [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)\n\n# Note that the modified input results in a different output.\nforward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)\nprint('output with forward pre hook: {}'.format(m(x)))\n: output with forward pre hook: tensor([[-0.5752, -0.7421,  0.4942],\n                                        [-0.0736,  0.5461,  0.0838]], grad_fn=<AddmmBackward>)\n\n# Note the modified output.\nforward_hook_handle = m.register_forward_hook(forward_hook)\nprint('output with both forward hooks: {}'.format(m(x)))\n: output with both forward hooks: tensor([[-1.0980,  0.6396,  0.4666],\n                                          [ 0.3634,  0.6538,  1.0256]], grad_fn=<AddBackward0>)\n\n# Remove hooks; note that the output here matches the output before adding hooks.\nforward_pre_hook_handle.remove()\nforward_hook_handle.remove()\nprint('output after removing forward hooks: {}'.format(m(x)))\n: output after removing forward hooks: tensor([[-0.5059, -0.8158,  0.2390],\n                                               [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)\n\n# ==== Demonstrate backward hooks. ====\nm(x).sum().backward()\nprint('x.grad with no backwards hook: {}'.format(x.grad))\n: x.grad with no backwards hook: tensor([[ 0.4497, -0.5046,  0.3146],\n                                         [ 0.4497, -0.5046,  0.3146]])\n\n# Clear gradients before running backward pass again.\nm.zero_grad()\nx.grad.zero_()\n\nm.register_full_backward_hook(backward_hook)\nm(x).sum().backward()\nprint('x.grad with backwards hook: {}'.format(x.grad))\n: x.grad with backwards hook: tensor([[42., 42., 42.],\n                                      [42., 42., 42.]])\n\n```\n\n\n## Advanced Features#\n\nPyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare available for custom-written modules, with the small caveat that certain features may require modules to conform\nto particular constraints in order to be supported. In-depth discussion of these features and the corresponding\nrequirements can be found in the links below.\n\n## Distributed Training#\n\nVarious methods for distributed training exist within PyTorch, both for scaling up training using multiple GPUs\nas well as training across multiple machines. Check out thedistributed training overview pagefor\ndetailed information on how to utilize these.\n\n## Profiling Performance#\n\nThePyTorch Profilercan be useful for identifying\nperformance bottlenecks within your models. It measures and outputs performance characteristics for\nboth memory usage and time spent.\n\n## Improving Performance with Quantization#\n\nApplying quantization techniques to modules can improve performance and memory usage by utilizing lower\nbitwidths than floating-point precision. Check out the various PyTorch-provided mechanisms for quantizationhere.\n\n## Improving Memory Usage with Pruning#\n\nLarge deep learning models are often over-parametrized, resulting in high memory usage. To combat this, PyTorch\nprovides mechanisms for model pruning, which can help reduce memory usage while maintaining task accuracy. ThePruning tutorialdescribes how to utilize\nthe pruning techniques PyTorch provides or define custom pruning techniques as necessary.\n\n## Parametrizations#\n\nFor certain applications, it can be beneficial to constrain the parameter space during model training. For example,\nenforcing orthogonality of the learned parameters can improve convergence for RNNs. PyTorch provides a mechanism for\napplyingparametrizationssuch as this, and\nfurther allows for custom constraints to be defined.\n\n## Transforming Modules with FX#\n\nTheFXcomponent of PyTorch provides a flexible way to transform\nmodules by operating directly on module computation graphs. This can be used to programmatically generate or\nmanipulate modules for a broad array of use cases. To explore FX, check out these examples of using FX forconvolution + batch norm fusionandCPU performance analysis.",
    "url": "https://pytorch.org/docs/stable/notes/modules.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "58f4f40256d7b3e208ac29880d7a1053",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/hier_tags.png",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ec2290ab12ed5325178a4663ee6a0e26",
    "source": "pytorch_docs",
    "title": "PyTorch Governance | Maintainers \u2014 PyTorch 2.9 documentation",
    "text": "\n## PyTorch Governance | Maintainers#\n\nCreated On: Mar 11, 2019 | Last Updated On: Jul 23, 2025\n\n## Responsibilities#\n\nTriage and fix high priority issues assigned to the module or library\nTriage, review, and land high priority pull requests assigned to the module or library\nAnswer module or library questions ondiscuss.pytorch.organddev-discuss.pytorch.org\nMaintain public user and development documentation\nRun meetings and share minutes plus roadmap on a half or quarterly basis\n\n## Lead Core Maintainer (BDFL)#\n\nSoumith Chintala (soumith)\n\n## Core Maintainers#\n\nSoumith Chintala (soumith)\nEdward Yang (ezyang)\nGreg Chanan (gchanan)\nDmytro Dzhulgakov (dzhulgakov)\nNikita Shulga (malfet)\nAlban Desmaison (albanD)\nPiotr Bialecki (ptrblck)\n\n## Module-level maintainers#\n\n\n## NN APIs (torch.nn)#\n\nMikayla Gawarecki (mikaylagawarecki)\nAlban Desmaison (albanD)\nJoel Schlosser (jbschlosser)\n(emeritus) Greg Chanan (gchanan)\n(emeritus) Soumith Chintala (soumith)\n(emeritus) Sam Gross (colesbury)\n(emeritus) Adam Paszke (apaszke)\n\n## Optimizers (torch.optim)#\n\nJane Xu (janeyx99)\nAlban Desmaison (albanD)\nJoel Schlosser (jbschlosser)\n(emeritus) Soumith Chintala (soumith)\n(emeritus) Ilqar Ramazanli (iramazanli)\n(emeritus) Vincent Quenneville-Belair (vincentqb)\n\n## Autograd (torch.autograd)#\n\nJeffrey Wan (soulitzer)\nAlban Desmaison (alband)\nEdward Yang (ezyang)\n(emeritus) Adam Paszke (apaszke)\n\n## TorchDynamo#\n\nAnimesh Jain (anijain2305)\nJason Ansel (jansel)\nEdward Yang (ezyang)\n\n## TorchInductor#\n\nElias Ellison (eellison)\nHorace He (Chillee)\nShunting Zhang (shunting314)\nJason Ansel (jansel)\nJiong Gong (jgong5)\n\n## Cudagraph Tree#\n\nElias Ellison (eellison)\n\n## PT2 Dispatcher#\n\nBrian Hirsh (bdhirsh)\nRichard Zou (zou3519)\nHorace He (Chillee)\nEdward Yang (ezyang)\n\n## PT2 Export (torch.export)#\n\nAvik Chaudhuri (avikchaudhuri)\nYanan Cao (gmagogsfm)\n\n## AOT Inductor (AOTI) & AOTI Runtime#\n\nBin Bao (desertfire)\nAngela Yi (angelayi)\nYang Chen (chenyang78)\n\n## Compilers (JIT / TorchScript / Package / Deploy)#\n\n(emeritus) Elias Ellison (eellison)\n(emeritus) Michael Suo (suo)\n(emeritus) Yanan Cao (gmagogsfm)\n(emeritus) James Reed (jamesr66a)\n(emeritus) Jason Ansel (jansel)\n(emeritus) Jiong Gong (jgong5)\n(emeritus) Zach Devito (zdevito)\n\n## Distributions & RNG#\n\nFritz Obermeyer (fritzo)\nNeeraj Pradhan (neerajprad)\nAlican Bozkurt (alicanb)\n(emeritus) Vishwak Srinivasan (vishwakftw)\n\n## Distributed#\n\nWill Constable (wconstab)\nHoward Huang (H-Huang)\nWanchao Liang (wanchaol)\nKe Wen (kwen2501)\nChien-Chin Huang (fegin)\nTristan Rice (d4l3k)\nJunjie Wang (fduwjj)\nWei Feng (weifengpy)\n(emeritus) Shen Li (mrshenli)\n(emeritus) Pritam Damania (pritamdamania87)\n(emeritus) Yanli Zhao (zhaojuanmao)\n(emeritus) Rohan Varma (rohan-varma)\n(emeritus) Alisson Azzolini (aazzolini)\n(emeritus) James Reed (jamesr66a)\n(emeritus) Kiuk Chung (kiukchung)\n(emeritus) Pieter Noordhuis (pietern)\n(emeritus) Mingzhe Li (mingzhe09088)\n(emeritus) Omkar Salpekar (osalpekar)\n\n## Multiprocessing#\n\n(emeritus) Simon Wang (SsnL)\n(emeritus) Vitaly Fedyunin (VitalyFedyunin)\n(emeritus) Adam Paszke (apaszke)\n\n## Linear Algebra (torch.linalg)#\n\nMario Lezcano (lezcano)\n(emeritus) Mike Ruberry (mruberry)\n(emeritus) Ivan Yashchuk (IvanYashchuk)\n(emeritus) Vishwak Srinivasan (vishwakftw)\n(emeritus) Nikita Vedeneev (nikitaved)\n\n## Sparse (torch.sparse)#\n\n(emeritus) Pearu Peterson (pearu)\n(emeritus) Nikita Vedeneev (nikitaved)\n(emeritus) Ivan Yashchuk (IvanYashchuk)\n(emeritus) Christian Puhrsch (cpuhrsch)\n(emeritus) Andrew James (amjames)\n\n## NestedTensor (torch.nested)#\n\nJoel Schlosser (jbschlosser)\nChristian Puhrsch (cpuhrsch)\nDriss Guessous (drisspg)\nMikayla Gawarecki (mikaylagawarecki)\nAlban Desmaison (albanD)\n(emeritus) Natalia Gimelshein (ngimel)\n\n## MaskedTensor (torch.masked)#\n\nChristian Puhrsch (cpuhrsch)\n(emeritus) George Qi (george-qi)\n\n## Fast Fourier Transform (torch.fft)#\n\n(emeritus) Mike Ruberry (mruberry)\n(emeritus) Peter Bell (peterbell10)\n\n## MKLDNN#\n\nXiaobing Zhang (XiaobingSuper)\nMingfei Ma (mingfeima)\nJiong Gong (jgong5)\n(emeritus) Xiaoqiang Zheng (zheng-xq)\n(emeritus) Sam Gross (colesbury)\n(emeritus) Christian Puhrsch (cpuhrsch)\n(emeritus) Ilia Cherniavskii (ilia-cher)\n(emeritus) Junjie Bai (bddppq)\n(emeritus) Yinghai Lu (yinghai)\n(emeritus) Vitaly Fedyunin (VitalyFedyunin)\n(emeritus) Jianhui Li (Jianhui-Li)\n\n## CUDA#\n\nNatalia Gimelshein (ngimel)\nEdward Yang (ezyang)\nPiotr Bialecki (ptrblck)\nChristian Sarofeen (csarofeen)\n(emeritus) Andrew Tulloch (ajtulloch)\n(emeritus) Xiaoqiang Zheng (zheng-xq)\n\n## AMD/ROCm/HIP#\n\nJeff Daily (jeffdaily)\nJithun Nair (jithunnair-amd)\n(emeritus) Junjie Bai (bddppq)\n\n## Build + CI#\n\nNikita Shulga (malfet)\nEli Uriegas (seemethere)\nAlban Desmaison (alband)\nAndrey Talman (atalman)\nZain Rizvi (ZainRizvi)\n(emeritus) Mikey Dagitses (dagitses)\n(emeritus) Omkar Salpekar (osalpekar)\n(emeritus) Nirav Mehta (mehtanirav)\n(emeritus) Zhuojie Zhou (zhouzhuojie)\n(emeritus) Edward Yang (ezyang)\n(emeritus) Karl Ostmo (kostmo)\n\n## Performance Tools#\n\nTaylor Robie (robieta)\nXu Zhao (xuzhao9)\n(emeritus) Victor Bittorf (bitfort)\n(emeritus) Gisle Dankel (gdankel)\n(emeritus) Natalia Gimelshein (ngimel)\n(emeritus) Mingzhe Li (mingzhe09088)\n\n## C++ API#\n\n(emeritus) Joel Schlosser (jbschlosser)\n(emeritus) Will Feng (yf225)\n\n## C10 utils and operator dispatch#\n\nBrian Hirsh (bdhirsh)\nEdward Yang (ezyang)\n(emeritus) Dmytro Dzhulgakov (dzhulgakov)\n(emeritus) Sebastian Messmer (smessmer)\n\n## ONNX exporter#\n\nShubham Bhokare (shubhambhokare1)\nJustin Chu (justinchuby)\nXavier Dupr\u00e9 (xadupre)\nTitai Wang (titaiwangms)\n(emeritus) Bowen Bao (BowenBao)\n(emeritus) Thiago Crepaldi (thiagocrepaldi)\n(emeritus) Aaron Bockover (abock)\n(emeritus) Gary Miguel (garymm)\n(emeritus) Lara Haidar (lara-hdr)\n(emeritus) Lu Fang (houseroad)\n(emeritus) Negin Raoof (neginraoof)\n(emeritus) Spandan Tiwari (spandantiwari)\n\n## LiteInterpreter#\n\n(emeritus) David Reiss (dreiss)\n(emeritus) Raziel Guevara (raziel)\n(emeritus) Linbin Yu (linbinyu)\n(emeritus) Ivan Kobzarev (IvanKobzarev)\n(emeritus) Tao Xu (xta0)\n\n## Quantization (torch/ao)#\n\nMark Saroufim (msaroufim)\nVasiliy Kuznetsov (vkuzo)\nJerry Zhang (jerryzh168)\n(emeritus) Zafar Takhirov (z-a-f)\n(emeritus) Raghuraman Krishnamoorthi (raghuramank100)\n\n## Windows#\n\n(emeritus) Guoliang Hua (nbcsm)\n(emeritus) Teng Gao (gaoteng-git)\n(emeritus) Peter Johnson (peterjc123)\n\n## Apple M1/MPS/Metal#\n\nKulin Seth (kulinseth)\nAlban Desmaison (alband)\nNikita Shulga (malfet)\n(emeritus) Ramin Azarmehr (razarmehr)\n\n## PowerPC#\n\n(emeritus) Alfredo Mendoza (avmgithub)\n\n## x86 CPU#\n\nMingfei Ma (mingfeima)\nJiong Gong (jgong5)\n\n## AArch64 CPU#\n\nSunita Nadampalli (snadampal)\n\n## Docs / Tutorials#\n\nSvetlana Karslioglu (svekars)\n\n## Library-level maintainers#\n\n\n## XLA#\n\nJack Cao (JackCaoG)\nDaniel Sohn (jysohn23)\nZach Cain (zcain117)\nBrian Hirsh (bdhirsh)\nGregory Chanan (gchanan)\n(emeritus) Ailing Zhang (ailzhang)\n(emeritus) Davide Libenzi (dlibenzi)\n(emeritus) Alex Suhan (asuhan)\n\n## TorchServe#\n\n(emeritus) Li Ning (lxning)\n(emeritus) Ankith Gunapal (agunapal)\n(emeritus) Hamid Shojanazeri (HamidShojanazeri)\n(emeritus) Mark Saroufim (msaroufIm)\n(emeritus) Manoj Rao (mycpuorg)\n(emeritus) Vamshi Dantu (vdantu)\n(emeritus) Dhanasekar Karuppasamy (dhanainme)\n\n## TorchVision#\n\nNicolas Hug (NicolasHug)\nPhilip Meier (pmeier)\nVictor Fomin (vfdev-5)\n(emeritus) Francisco Massa (fmassa)\n(emeritus) Vasilis Vryniotis (datumbox)\n(emeritus) Yosua Michael Maranatha (YosuaMichael)\n(emeritus) Joao Gomes (jdsgomes)\n\n## TorchText#\n\n(emeritus) Nayef Ahmed (Nayef211)\n(emeritus) Parmeet Singh Bhatia (parmeet)\n(emeritus) Guanheng George Zhang (zhangguanheng66)\n(emeritus) Christian Puhrsch (cpuhrsch)\n\n## TorchAudio#\n\nMoto Hira (mthrok)\n(emeritus) Jeff Hwang (hwangjeff)\n(emeritus) Caroline Chen (carolineechen)\n(emeritus) Xiaohui Zhang (xiaohui-zhang)\n(emeritus) Zhaoheng Ni (nateanl)\n(emeritus) Christian Puhrsch (cpuhrsch)\n(emeritus) Vincent QB (vincentqb)\n\n## TorchRec#\n\nColin Taylor (colin2328)\nPaul Zhang (PaulZhang12)\n(emeritus) Dmytro Ivchenko (divchenko)\n\n## TorchX#\n\n(emeritus) Tristan Rice (d4l3k)\n(emeritus) Kiuk Chung (kiukchung)\n\n## TorchData#\n\nAndrew Ho (andrewkho)\nDivyansh Khanna (divyanshk)\n\n## TorchArrow#\n\n(emeritus) Wenlei Xie (wenleix)\n(emeritus) Vitaly Fedyunin (VitalyFedyunin)\n\n## ExecuTorch (Edge, Mobile)#\n\nMergen Nachin (mergennachin)\nKimish Patel (kimishpatel)\nDave Bort (dbort)\nMartin Yuan (iseeyuan)\n\n## TorchTune#\n\nKartikay Khandelwal (kartikayk)\nEvan Smothers (ebsmothers)\nJoe Cummings (joecummings)\n\n## TorchChat#\n\nJack Khuu (Jack-Khuu)\nJesse White (byjlw)\n(emeritus) Michael Gschwind (mikekgfb)\n\n## TorchCodec#\n\nNicolas Hug (nicolashug)\nAhmad Sharif (ahmadsharif1)\nScott Schneider (scotts)",
    "url": "https://pytorch.org/docs/stable/community/persons_of_interest.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "abeca6bb35921a3e06a9a21e966939a6",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/symmetric_memory.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ffc046ebdc946b5578ac0f3812587ec5",
    "source": "pytorch_docs",
    "title": "Error Propagation \u2014 PyTorch 2.9 documentation",
    "text": "\n## Error Propagation#\n\nCreated On: May 04, 2021 | Last Updated On: Jul 08, 2021\nEach host in a distributed PyTorch job runs with a single TorchElastic agent,\nand multiple workers (as children processes of the TorchElastic agent).\nSince the workers are user-provided (your PyTorch script/job), TorchElastic\nhas a way to propagate errors on the trainers through the agent and up to the\nscheduler, which ultimately informs the end-user about the state of the job\nand applies any retry policies.\nTorchElastic categorizes errors into 3 categories:\nCategory\nSub-Category\nDescription\nUser Error\nInput Error\ninvalid inputs to TorchElastic APIs (e.g. min > max nodes)\nWorker Failure\nany failures on the worker child process\nPlatform Error\nn/a\nfailures caused by the agent\nInfra Error\nn/a\nfailures outside the domain of the agent and workers\n(e.g. host failures)\nAll errors other than \u201cWorker Failure\u201d are either raised canonically from the\nagent process or implicitly or explicitly crash the agent process. So the\nstandard language (python) provided exception handling strategies apply.\nWorker Failures are special because the exception/failure originates on a different\nprocess from the agent so the error needs to be propagated inter-process\n(e.g. the agent cannot simplytry-catchan exception raised on the worker process).\ntry-catch\nTorchElastic agents usetorch.distributed.elastic.multiprocessing.start_processes()to launch the workers which has a simple file based inter-process error propagation\nbuilt-in.\ntorch.distributed.elastic.multiprocessing.start_processes()\nAny function or binary entrypoint decorated withrecord()will write uncaught exceptions (with the trace information) to a file specified by the\nenvironment variableTORCHELASTIC_ERROR_FILE. The parent process (e.g. agent)\nsets this env var on each child it launches, then aggregates the error files for all\nchildren, and propagates the one with thesmallesttimestamp (e.g. thefirsterror).\nrecord()\nTORCHELASTIC_ERROR_FILE\n\n## Methods and Classes#\n\nSyntactic sugar to record errors/exceptions that happened in the decorated\nfunction using the providederror_handler.\nerror_handler\nUsing this decorator is equivalent to:\n\n```python\nerror_handler = get_error_handler()\nerror_handler.initialize()\ntry:\n    foobar()\nexcept ChildFailedError as e:\n    _, failure = e.get_first_failure()\n    error_handler.dump_error_file(failure.error_file, failure.exitcode)\n    raise\nexcept Exception as e:\n    error_handler.record_exception(e)\n    raise\n\n```\n\nImportant\nuse this decorator once per process at the top level method,\ntypically this is the main method.\nExample\n\n```python\n@record\ndef main():\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\nCallable[[~_P],Optional[_R]]\nSpecial exception type that can be raised from a function annotated with the@recorddecorator to have the child process\u2019 (root exception) propagate\nup the stack as-is (e.g. without being wrapped in the parent\u2019s traceback).\n@record\nUseful in cases where the parent is a simple nanny process\nand the child (worker) processes are actually doing meaningful compute.\nIn this case, errors typically occur on the child process as the parent\nis not doing anything non-trivial, and child errors should be propagated\nto the scheduler for accurate root cause diagnostics.\nNote\nThe propagation relies on error files rather than exception handling to\nsupport both function and binary launches.\nExample:\n\n```python\n# process tree on a host (container)\n0: scheduler-init-process:\n           |- 1: torchelastic_agent:\n                    |- 2: trainer_0 (ok)\n                    |- 3: trainer_1 (fail) -> error.json\n                    |- ...\n                    |- n+2: trainer_n (ok)\n           |- n+3: other processes\n           |- ...\n\n```\n\nIn the example above, trainer 1\u2019s failure (written into error.json) is\nthe root cause and should be reported to the scheduler\u2019s init process.\nThe torchelastic agent raises aChildFailedError(\"trainer\",{1:\"trainer_1/error.json\"})upon detecting trainer 1\u2019s failure which would propagate the contents\nof trainer 1\u2019s error file to the scheduler\u2019s init process.\nChildFailedError(\"trainer\",{1:\"trainer_1/error.json\"})\nWrite the provided exception object along with some other metadata about\nthe error in a structured way in JSON format to an error file specified by the\nenvironment variable:TORCHELASTIC_ERROR_FILE. If this environment\nvariable is not set, then simply logs the contents of what would have been\nwritten to the error file.\nTORCHELASTIC_ERROR_FILE\nThis handler may be subclassed to customize the handling of the error.\nSubclasses should overrideinitialize()andrecord_exception().\ninitialize()\nrecord_exception()\nRepresent the failed process result. When the worker process fails, it may record failure root cause into the file.\nTries to read the failure timestamp from the providederror_file,\nif theerror_filedoes not exist, the timestamp is the current\ntimestamp (seconds since epoch).\nerror_file\nerror_file\nThemessagefield is a concise explanation of the failure. If\nthe error file exists then the message is obtained from the error file.\nOtherwise one is generated based on the failure signature.\nmessage\nNote\nIt is assumed that theerror_fileis written bytorch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler.\nOtherwise the behavior is undefined.\nerror_file\ntorch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler",
    "url": "https://pytorch.org/docs/stable/elastic/errors.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "63b1b452e29aeeed276703a60ed7b081",
    "source": "pytorch_docs",
    "title": "Common Graph Breaks \u2014 PyTorch 2.9 documentation",
    "text": "\n## Common Graph Breaks#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 28, 2025\nBelow are some common graph breaks and some workarounds.\n\n## Incorrect Code#\n\nYour code might contain errors (meaning it doesn\u2019t execute even withouttorch.compile). In the example below, there\u2019s a typo in thetorch.sincall due to an extra argument.Always disabletorch.compileto check if the code runs correctly.\ntorch.compile\ntorch.sin\ntorch.compile\n\n```python\n@torch.compile\ndef fn(x):\n    y = torch.sin(x, x)\n    return y\n\ntry:\n    fn(torch.ones(3, 3))\nexcept Exception as e:\n    pass\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_142/343837593.py:3\nGraph Break Reason: TypeError when making fake tensor call\n  Explanation: \n\n\n  Developer debug context: TypeError <built-in method sin of type object at 0x7f271f582260>: sin() takes 1 positional argument but 2 were given\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0112.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_142/343837593.py\", line 7, in <module>\n    fn(torch.ones(3, 3))\n  File \"/tmp/ipykernel_142/343837593.py\", line 3, in fn\n    y = torch.sin(x, x)\n\n```\n\nDynamo makes a best-effort attempt to hint if a graph break is caused by your code.\nBut it can still sometimes be difficult to tell from the logs if the graph break is caused by an error in your code,\nis a more complicated graph break, or is atorch.compilebug. In order to differentiate, we recommend trying to run your code withouttorch.compileto see if you still get the error reported by the graph break.\ntorch.compile\ntorch.compile\n\n## Data-dependent operations#\n\ntorch.compilegraph breaks on data-dependent operations such as data-dependent control flow (if-statements, loops with tensors) and direct tensor data accesses (.item,.data_ptr).\ntorch.compile\n.item\n.data_ptr\n\n```python\n@torch.compile\ndef fn(x):\n    y = x.sum()\n    if y > 0:\n        return x + y.item()\n    return x - y.item()\n\nprint(fn(torch.ones(3, 3)))\n\n```\n\n\n```python\ntensor([[10., 10., 10.],\n        [10., 10., 10.],\n        [10., 10., 10.]])\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_142/3495555842.py:4\nGraph Break Reason: Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() > 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with TensorVariable()\n\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_142/3495555842.py\", line 8, in <module>\n    print(fn(torch.ones(3, 3)))\n  File \"/tmp/ipykernel_142/3495555842.py\", line 4, in fn\n    if y > 0:\n\nGraph break from `Tensor.item()`, consider setting:\n    torch._dynamo.config.capture_scalar_outputs = True\nor:\n    env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\nto include these operations in the captured graph.\n\nGraph break: from user code at:\n  File \"/tmp/ipykernel_142/3495555842.py\", line 5, in torch_dynamo_resume_in_fn_at_4\n    return x + y.item()\n\n\nGraph break in user code at /tmp/ipykernel_142/3495555842.py:5\nGraph Break Reason: Unsupported Tensor.item() call with capture_scalar_outputs=False\n  Explanation: Dynamo does not support tracing `Tensor.item()` with config.capture_scalar_outputs=False.\n  Hint: Set `torch._dynamo.config.capture_scalar_outputs = True` or `export TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` to include these operations in the captured graph.\n\n  Developer debug context: call_method TensorVariable() item () {}\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0124.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_142/3495555842.py\", line 8, in <module>\n    print(fn(torch.ones(3, 3)))\n  File \"/tmp/ipykernel_142/3495555842.py\", line 5, in fn\n    return x + y.item()\n\n```\n\nThe general workaround for these graph breaks is to avoid doing data-dependent operations. Some specific workarounds are:\nIf your control flow doesn\u2019t actually depend on data values, consider modifying your code to perform control flow on constants.\n\n```python\n# old\nx = torch.randn(3, 3)\n@torch.compile\ndef fn(y):\n    if x.sum() > 0:\n        return y + x\n    else:\n        return y - x\n\nprint(fn(torch.ones(3, 3)))\n\n```\n\n\n```python\ntensor([[ 1.4090,  0.3393,  2.4898],\n        [-0.8570,  2.4646,  1.2680],\n        [ 0.8819,  1.5492,  2.2382]])\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_142/2410325100.py:5\nGraph Break Reason: Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() > 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with TensorVariable()\n\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_142/2410325100.py\", line 10, in <module>\n    print(fn(torch.ones(3, 3)))\n  File \"/tmp/ipykernel_142/2410325100.py\", line 5, in fn\n    if x.sum() > 0:\n\n```\n\n\n```python\n# new\nx = torch.randn(3, 3)\ncond = (x.sum() > 0).item()\n@torch.compile\ndef fn(y):\n    if cond:\n        return y + x\n    else:\n        return y - x\n\nprint(fn(torch.ones(3, 3)))\n\n```\n\n\n```python\ntensor([[1.1334, 1.6834, 1.5118],\n        [1.3258, 1.0107, 2.2545],\n        [1.6032, 1.2765, 0.6417]])\n\n```\n\nUse higher-order ops likeControl Flow - Condin place of data-dependent control flow\n\n```python\n# old\n@torch.compile\ndef fn(x):\n    if x.sum() > 0:\n        return x + 1\n    return x - 1\n\nprint(fn(torch.ones(3, 3)))\n\n```\n\n\n```python\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n```\n\n\n```python\nGraph break in user code at /tmp/ipykernel_142/520574912.py:4\nGraph Break Reason: Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() > 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with TensorVariable()\n\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_142/520574912.py\", line 8, in <module>\n    print(fn(torch.ones(3, 3)))\n  File \"/tmp/ipykernel_142/520574912.py\", line 4, in fn\n    if x.sum() > 0:\n\n```\n\n\n```python\n# new\n@torch.compile\ndef fn(x):\n    return torch.cond(\n        x.sum() > 0,\n        lambda x: x + 1,\n        lambda x: x - 1,\n        (x,),\n    )\n\nprint(fn(torch.ones(3, 3)))\n\n```\n\n\n```python\ntensor([[2., 2., 2.],\n        [2., 2., 2.],\n        [2., 2., 2.]])\n\n```\n\nIf you have a.item()call, trytorch._dynamo.config.capture_scalar_outputs=TrueorTORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1.\n.item()\ntorch._dynamo.config.capture_scalar_outputs=True\nTORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\nWrap problematic parts of the function in a custom operator\n\n## Printing and logging#\n\nPrinting/logging/issuing warnings will result in a graph break.\nYou can try working around this by usingtorch._dynamo.config.reorderable_logging_functions.\nThis config is used to reorder logging functions so that they are called at the end of the\ntraced function, thus avoiding a graph break.\nHowever, the logged contents may differ if, for example, a mutation occurs.\ntorch._dynamo.config.reorderable_logging_functions\n\n```python\ntorch._dynamo.config.reorderable_logging_functions.add(print)\n\n@torch.compile\ndef fn(x):\n    x += 1\n    print(\"log!\")\n    return torch.sin(x)\n\nprint(fn(torch.ones(3, 3)))\n\n```\n\n\n```python\nlog!\ntensor([[0.9093, 0.9093, 0.9093],\n        [0.9093, 0.9093, 0.9093],\n        [0.9093, 0.9093, 0.9093]])\n\n```\n",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.common_graph_breaks.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "10abd86624fe1f89fc66ec397de49125",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/export/draft_export.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "39351e4cf1b2661940675e2978623af5",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch_cuda_memory.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f7861b12b95713c957eae022574f56eb",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/fx.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "f611ff4de79a3df0ff0eb998c5fdefd9",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.observability.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b05e0af0d165989f65454821e6884c6f",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/multiprocessing.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4eff7f3a5ff01e29dd179b3cf93f09ec",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/profiler.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d4fe7d0f4359a76814b73cb4146c01b8",
    "source": "pytorch_docs",
    "title": "Remote Reference Protocol \u2014 PyTorch 2.9 documentation",
    "text": "\n## Remote Reference Protocol#\n\nCreated On: Nov 20, 2019 | Last Updated On: Apr 27, 2025\nThis note describes the design details of Remote Reference protocol and walks\nthrough message flows in different scenarios. Make sure you\u2019re familiar with theDistributed RPC Frameworkbefore proceeding.\n\n## Background#\n\nRRef stands for Remote REFerence. It is a reference of an object which is\nlocated on the local or remote worker, and transparently handles reference\ncounting under the hood. Conceptually, it can be considered as a distributed\nshared pointer. Applications can create an RRef by callingremote(). Each RRef is owned by the callee worker\nof theremote()call (i.e., owner) and can be used\nby multiple users. The owner stores the real data and keeps track of the global\nreference count. Every RRef can be uniquely identified by a globalRRefId,\nwhich is assigned at the time of creation on the caller of theremote()call.\nremote()\nremote()\nRRefId\nremote()\nOn the owner worker, there is only oneOwnerRRefinstance, which contains\nthe real data, while on user workers, there can be as manyUserRRefsas\nnecessary, andUserRRefdoes not hold the data. All usage on the owner will\nretrieve the uniqueOwnerRRefinstance using the globally uniqueRRefId.\nAUserRRefwill be created when it is used as an argument or return value inrpc_sync(),rpc_async()orremote()invocation, and the owner will be notified\naccording to update the reference count. AnOwnerRRefand its data will be\ndeleted when there is noUserRRefinstances globally and there are no\nreference to theOwnerRRefon the owner as well.\nOwnerRRef\nUserRRefs\nUserRRef\nOwnerRRef\nRRefId\nUserRRef\nrpc_sync()\nrpc_async()\nremote()\nOwnerRRef\nUserRRef\nOwnerRRef\n\n## Assumptions#\n\nRRef protocol is designed with the following assumptions.\nTransient Network Failures: The RRef design handles transient\nnetwork failures by retrying messages. It cannot handle node crashes or\npermanent network partitions. When those incidents occur, the application\nshould take down all workers, revert to the previous checkpoint, and resume\ntraining.\nNon-idempotent UDFs: We assume the user functions (UDF) provided torpc_sync(),rpc_async()orremote()are not idempotent and therefore\ncannot be retried. However, internal RRef control messages are idempotent and\nretried upon message failure.\nrpc_sync()\nrpc_async()\nremote()\nOut of Order Message Delivery: We do not assume message delivery order\nbetween any pair of nodes, because both sender and receiver are using multiple\nthreads. There is no guarantee on which message will be processed first.\n\n## RRef Lifetime#\n\nThe goal of the protocol is to delete anOwnerRRefat an appropriate time.\nThe right time to delete anOwnerRRefis when there are no livingUserRRefinstances and user code is not holding references to theOwnerRRefeither. The tricky part is to determine if there are any livingUserRRefinstances.\nOwnerRRef\nOwnerRRef\nUserRRef\nOwnerRRef\nUserRRef\n\n## Design Reasoning#\n\nA user can get aUserRRefin three situations:\nUserRRef\nReceiving aUserRReffrom the owner.\nUserRRef\nReceiving aUserRReffrom another user.\nUserRRef\nCreating a newUserRRefowned by another worker.\nUserRRef\nCase 1 is the simplest where the owner passes its RRef to a user, where the\nowner callsrpc_sync(),rpc_async(), orremote()and uses its RRef as an argument. In this\ncase a newUserRRefwill be created on the user. As the owner is the caller,\nit can easily update its local reference count on theOwnerRRef.\nrpc_sync()\nrpc_async()\nremote()\nUserRRef\nOwnerRRef\nThe only requirement is that anyUserRRefmust notify the owner upon destruction. Hence, we need the first\nguarantee:\nUserRRef\nG1. The owner will be notified when any UserRRef is deleted.\nAs messages might come delayed or out-of-order, we need one more guarantee to\nmake sure the delete message is not processed too soon. If A sends a message to\nB that involves an RRef, we call the RRef on A (the parent RRef) and the RRef on B\n(the child RRef).\nG2. Parent RRef will NOT be deleted until the child RRef is confirmed by the\nowner.\nIn cases 2 and 3, it is possible that the owner has only partial or no knowledge\nat all about the RRef fork graph. For example, an RRef could be\nconstructed on a user, and before the owner receives any RPC call, the\ncreator user might have already shared the RRef with other users, and those\nusers could further share the RRef. One invariant is that the fork graph of\nany RRef is always a tree, because forking an RRef always\ncreates a newUserRRefinstance on the callee (except if the callee is the\nowner), and hence every RRef has a single parent.\nUserRRef\nThe owner\u2019s view on anyUserRRefin the tree has three stages:\nUserRRef\n\n```python\n1) unknown -> 2) known -> 3) deleted.\n\n```\n\nThe owner\u2019s view of the entire tree keeps changing. The owner deletes itsOwnerRRefinstance when it thinks there are no livingUserRRefinstances, i.e.,\nwhenOwnerRRefis deleted, allUserRRefinstances could be either indeed\ndeleted or unknown. The dangerous case is when some forks are unknown and others\nare deleted.\nOwnerRRef\nUserRRef\nOwnerRRef\nUserRRef\nG2trivially guarantees that no parentUserRRefcan be deleted before\nthe owner knows all of its childrenUserRRefinstances. However, it is\npossible that the childUserRRefmay be deleted before the owner knows its\nparentUserRRef.\nUserRRef\nUserRRef\nUserRRef\nUserRRef\nConsider the following example, where theOwnerRRefforks to A, then A forks\nto Y, and Y forks to Z:\nOwnerRRef\n\n```python\nOwnerRRef -> A -> Y -> Z\n\n```\n\nIf all of Z\u2019s messages, including the delete message, are processed by the\nowner before Y\u2019s messages. the owner will learn of Z\u2019s deletion before\nknowing Y exists. Nevertheless, this does not cause any problem. Because, at least\none of Y\u2019s ancestors will be alive (A) and it will\nprevent the owner from deleting theOwnerRRef. More specifically, if the\nowner does not know Y, A cannot be deleted due toG2, and the owner knows A\nsince it is A\u2019s parent.\nOwnerRRef\nThings get a little trickier if the RRef is created on a user:\n\n```python\nOwnerRRef\n    ^\n    |\n    A -> Y -> Z\n\n```\n\nIf Z callsto_here()on theUserRRef, the\nowner at least knows A when Z is deleted, because otherwise,to_here()wouldn\u2019t finish. If Z does not callto_here(), it is possible that the owner\nreceives all messages from Z before any message from A and Y. In this case, as\nthe real data of theOwnerRRefhas not been created yet, there is nothing to\nbe deleted either. It is the same as Z does not exist at all. Hence, it\u2019s still\nOK.\nto_here()\nUserRRef\nto_here()\nto_here()\nOwnerRRef\n\n## Implementation#\n\nG1is implemented by sending out a delete message inUserRRefdestructor. To provideG2, the parentUserRRefis put into a context\nwhenever it is forked, indexed by the newForkId. The parentUserRRefis\nonly removed from the context when it receives an acknowledgement message (ACK)\nfrom the child, and the child will only send out the ACK when it is confirmed by\nthe owner.\nUserRRef\nUserRRef\nForkId\nUserRRef\n\n## Protocol Scenarios#\n\nLet\u2019s now discuss how the above designs translate to the protocol in four\nscenarios.\n\n## User Share RRef with Owner as Return Value#\n\n\n```python\nimport torch\nimport torch.distributed.rpc as rpc\n\n# on worker A\nrref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n# say the rref has RRefId 100 and ForkId 1\nrref.to_here()\n\n```\n\nIn this case, theUserRRefis created on the user worker A, then it is\npassed to the owner worker B together with the remote message, and then B\ncreates theOwnerRRef. The methodremote()returns immediately, meaning that theUserRRefcan be forked/used before\nthe owner knows about it.\nUserRRef\nOwnerRRef\nremote()\nUserRRef\nOn the owner, when receiving theremote()call, it\nwill create theOwnerRRef, and returns an ACK to acknowledge{100,1}(RRefId,ForkId). Only after receiving this ACK, can A delete itsUserRRef. This involves bothG1andG2.G1is obvious. ForG2, theOwnerRRefis a child of theUserRRef, and theUserRRefis not deleted until it receives the ACK from the owner.\nremote()\nOwnerRRef\n{100,1}\nRRefId\nForkId\nUserRRef\nOwnerRRef\nUserRRef\nUserRRef\nThe diagram above shows the message flow, where solid arrow contains user\nfunction and dashed arrow are builtin messages. Note that the first two messages\nfrom A to B (remote()andto_here()) may\narrive at B in any order, but the final delete message will only be sent out\nwhen:\nremote()\nto_here()\nB acknowledgesUserRRef{100,1}(G2), and\nUserRRef{100,1}\nPython GC agrees to delete the localUserRRefinstance. This occurs when\nthe RRef is no longer in scope and is eligible for garbage collection.\nUserRRef\n\n## User Share RRef with Owner as Argument#\n\n\n```python\nimport torch\nimport torch.distributed.rpc as rpc\n\n# on worker A and worker B\ndef func(rref):\n  pass\n\n# on worker A\nrref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n# say the rref has RRefId 100 and ForkId 1\nrpc.rpc_async('B', func, args=(rref, ))\n\n```\n\nIn this case, after creating theUserRRefon A, A uses it as an argument in\na followup RPC call to B. A will keepUserRRef{100,1}alive until it\nreceives the acknowledge from B (G2, not the return value of the RPC call).\nThis is necessary because A should not send out the delete message until all\nprevious messages are received, otherwise, theOwnerRRefcould be\ndeleted before usage as we do not guarantee message delivery order. This is done\nby creating a childForkIdof RRef, holding them in a map until receives the\nowner confirms the childForkId. The figure below shows the message flow.\nUserRRef\nUserRRef{100,1}\nOwnerRRef\nForkId\nForkId\nNote that theUserRRefcould be deleted on B before func finishes or even\nstarts. However this is OK, as at the time B sends out ACK for the childForkId, it already acquired theOwnerRRefinstance, which would prevent\nit been deleted too soon.\nUserRRef\nForkId\nOwnerRRef\n\n## Owner Share RRef with User#\n\nOwner to user is the simplest case, where the owner can update reference\ncounting locally, and does not need any additional control message to notify\nothers. RegardingG2, it is same as the parent receives the ACK from the\nowner immediately, as the parent is the owner.\n\n```python\nimport torch\nimport torch.distributed.rpc as RRef, rpc\n\n# on worker B and worker C\ndef func(rref):\n  pass\n\n# on worker B, creating a local RRef\nrref = RRef(\"data\")\n# say the rref has RRefId 100\ndist.rpc_async('C', func, args=(rref, ))\n\n```\n\nThe figure above shows the message flow. Note that when theOwnerRRefexits\nscope after the rpc_async call, it will not be deleted, because internally\nthere is a map to hold it alive if there is any known forks, in which case isUserRRef{100,1}. (G2)\nOwnerRRef\nUserRRef{100,1}\n\n## User Share RRef with User#\n\nThis is the most complicated case where caller user (parentUserRRef),\ncallee user (childUserRRef), and the owner all need to get involved.\nUserRRef\nUserRRef\n\n```python\nimport torch\nimport torch.distributed.rpc as rpc\n\n# on worker A and worker C\ndef func(rref):\n  pass\n\n# on worker A\nrref = rpc.remote('B', torch.add, args=(torch.ones(2), 1))\n# say the rref has RRefId 100 and ForkId 1\nrpc.rpc_async('C', func, args=(rref, ))\n\n```\n\nWhen C receives the childUserRReffrom A, it sends out a fork request to\nthe owner B. Later, when the B confirms theUserRRefon C, C will perform\ntwo actions in parallel: 1) send out the child ACK to A ,and 2) run the user\nprovided function. During this time, the parent (A) will hold itsUserRRef{100,1}alive to achieveG2.\nUserRRef\nUserRRef\nUserRRef{100,1}",
    "url": "https://pytorch.org/docs/stable/rpc/rref.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "74dd563d26aa5737ae6d378c17154604",
    "source": "pytorch_docs",
    "title": "Multiprocessing package - torch.multiprocessing \u2014 PyTorch 2.9 documentation",
    "text": "\n## Multiprocessing package - torch.multiprocessing#\n\nCreated On: Dec 23, 2016 | Last Updated On: Jun 08, 2025\ntorch.multiprocessing is a wrapper around the nativemultiprocessingmodule.\nmultiprocessing\nIt registers custom reducers, that use shared memory to provide shared\nviews on the same data in different processes. Once the tensor/storage is moved\nto shared_memory (seeshare_memory_()), it will be possible\nto send it to other processes without making any copies.\nshare_memory_()\nThe API is 100% compatible with the original module - it\u2019s enough to changeimportmultiprocessingtoimporttorch.multiprocessingto have all the\ntensors sent through the queues or shared via other mechanisms, moved to shared\nmemory.\nimportmultiprocessing\nimporttorch.multiprocessing\nBecause of the similarity of APIs we do not document most of this package\ncontents, and we recommend referring to very good docs of the original module.\nWarning\nIf the main process exits abruptly (e.g. because of an incoming signal),\nPython\u2019smultiprocessingsometimes fails to clean up its children.\nIt\u2019s a known caveat, so if you\u2019re seeing any resource leaks after\ninterrupting the interpreter, it probably means that this has just happened\nto you.\nmultiprocessing\n\n## Strategy management#\n\nReturn a set of sharing strategies supported on a current system.\nReturn the current strategy for sharing CPU tensors.\nSet the strategy for sharing CPU tensors.\nnew_strategy(str) \u2013 Name of the selected strategy. Should be one of\nthe values returned byget_all_sharing_strategies().\nget_all_sharing_strategies()\n\n## Sharing CUDA tensors#\n\nSharing CUDA tensors between processes is supported only in Python 3, using\naspawnorforkserverstart methods.\nspawn\nforkserver\nUnlike CPU tensors, the sending process is required to keep the original tensor\nas long as the receiving process retains a copy of the tensor. The refcounting is\nimplemented under the hood but requires users to follow the next best practices.\nWarning\nIf the consumer process dies abnormally to a fatal signal, the shared tensor\ncould be forever kept in memory as long as the sending process is running.\nRelease memory ASAP in the consumer.\n\n```python\n## Good\nx = queue.get()\n# do somethings with x\ndel x\n\n```\n\n\n```python\n## Bad\nx = queue.get()\n# do somethings with x\n# do everything else (producer have to keep x in memory)\n\n```\n\nKeep producer process running until all consumers exits. This will prevent\nthe situation when the producer process releasing memory which is still in use\nby the consumer.\n\n```python\n## producer\n# send tensors, do something\nevent.wait()\n\n```\n\n\n```python\n## consumer\n# receive tensors and use them\nevent.set()\n\n```\n\nDon\u2019t pass received tensors.\n\n```python\n# not going to work\nx = queue.get()\nqueue_2.put(x)\n\n```\n\n\n```python\n# you need to create a process-local copy\nx = queue.get()\nx_clone = x.clone()\nqueue_2.put(x_clone)\n\n```\n\n\n```python\n# putting and getting from the same queue in the same process will likely end up with segfault\nqueue.put(tensor)\nx = queue.get()\n\n```\n\n\n## Sharing strategies#\n\nThis section provides a brief overview into how different sharing strategies\nwork. Note that it applies only to CPU tensor - CUDA tensors will always use\nthe CUDA API, as that\u2019s the only way they can be shared.\n\n## File descriptor -file_descriptor#\n\nfile_descriptor\nNote\nThis is the default strategy (except for macOS and OS X where it\u2019s not\nsupported).\nThis strategy will use file descriptors as shared memory handles. Whenever a\nstorage is moved to shared memory, a file descriptor obtained fromshm_openis cached with the object, and when it\u2019s going to be sent to other processes,\nthe file descriptor will be transferred (e.g. via UNIX sockets) to it. The\nreceiver will also cache the file descriptor andmmapit, to obtain a shared\nview onto the storage data.\nshm_open\nmmap\nNote that if there will be a lot of tensors shared, this strategy will keep a\nlarge number of file descriptors open most of the time. If your system has low\nlimits for the number of open file descriptors, and you can\u2019t raise them, you\nshould use thefile_systemstrategy.\nfile_system\n\n## File system -file_system#\n\nfile_system\nThis strategy will use file names given toshm_opento identify the shared\nmemory regions. This has a benefit of not requiring the implementation to cache\nthe file descriptors obtained from it, but at the same time is prone to shared\nmemory leaks. The file can\u2019t be deleted right after its creation, because other\nprocesses need to access it to open their views. If the processes fatally\ncrash, or are killed, and don\u2019t call the storage destructors, the files will\nremain in the system. This is very serious, because they keep using up the\nmemory until the system is restarted, or they\u2019re freed manually.\nshm_open\nTo counter the problem of shared memory file leaks,torch.multiprocessingwill spawn a daemon namedtorch_shm_managerthat will isolate itself from\nthe current process group, and will keep track of all shared memory allocations.\nOnce all processes connected to it exit, it will wait a moment to ensure there\nwill be no new connections, and will iterate over all shared memory files\nallocated by the group. If it finds that any of them still exist, they will be\ndeallocated. We\u2019ve tested this method and it proved to be robust to various\nfailures. Still, if your system has high enough limits, andfile_descriptoris a supported strategy, we do not recommend switching to this one.\ntorch.multiprocessing\ntorch_shm_manager\nfile_descriptor\n\n## Spawning subprocesses#\n\nNote\nAvailable for Python >= 3.4.\nThis depends on thespawnstart method in Python\u2019smultiprocessingpackage.\nspawn\nmultiprocessing\nSpawning a number of subprocesses to perform some function can be done\nby creatingProcessinstances and callingjointo wait for\ntheir completion. This approach works fine when dealing with a single\nsubprocess but presents potential issues when dealing with multiple\nprocesses.\nProcess\njoin\nNamely, joining processes sequentially implies they will terminate\nsequentially. If they don\u2019t, and the first process does not terminate,\nthe process termination will go unnoticed. Also, there are no native\nfacilities for error propagation.\nThespawnfunction below addresses these concerns and takes care\nof error propagation, out of order termination, and will actively\nterminate processes upon detecting an error in one of them.\nspawn\nSpawnsnprocsprocesses that runfnwithargs.\nnprocs\nfn\nargs\nIf one of the processes exits with a non-zero exit status, the\nremaining processes are killed and an exception is raised with the\ncause of termination. In the case an exception was caught in the\nchild process, it is forwarded and its traceback is included in\nthe exception raised in the parent process.\nfn(function) \u2013Function is called as the entrypoint of the\nspawned process. This function must be defined at the top\nlevel of a module so it can be pickled and spawned. This\nis a requirement imposed by multiprocessing.The function is called asfn(i,*args), whereiis\nthe process index andargsis the passed through tuple\nof arguments.\nFunction is called as the entrypoint of the\nspawned process. This function must be defined at the top\nlevel of a module so it can be pickled and spawned. This\nis a requirement imposed by multiprocessing.\nThe function is called asfn(i,*args), whereiis\nthe process index andargsis the passed through tuple\nof arguments.\nfn(i,*args)\ni\nargs\nargs(tuple) \u2013 Arguments passed tofn.\nfn\nnprocs(int) \u2013 Number of processes to spawn.\njoin(bool) \u2013 Perform a blocking join on all processes.\ndaemon(bool) \u2013 The spawned processes\u2019 daemon flag. If set to True,\ndaemonic processes will be created.\nstart_method(str) \u2013 (deprecated) this method will always usespawnas the start method. To use a different start method\nusestart_processes().\nspawn\nstart_processes()\nNone ifjoinisTrue,ProcessContextifjoinisFalse\njoin\nTrue\nProcessContext\njoin\nFalse\nReturned byspawn()when called withjoin=False.\nspawn()\njoin=False\nJoin one or more processes within spawn context.\nAttempt to join one or more processes in this spawn context.\nIf one of them exited with a non-zero exit status, this function\nkills the remaining processes (optionally with a grace period)\nand raises an exception with the cause of the first process exiting.\nReturnsTrueif all processes have been joined successfully,Falseif there are more processes that need to be joined.\nTrue\nFalse\ntimeout(float) \u2013 Wait this long (in seconds) before giving up on waiting.\ngrace_period(float) \u2013 When any processes fail, wait this long (in seconds)\nfor others to shutdown gracefully before terminating them. If they\nstill don\u2019t exit, wait another grace period before killing them.",
    "url": "https://pytorch.org/docs/stable/multiprocessing.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5646b6498671b4fc397e634285fc6dd6",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_cudagraph_trees.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "41b98469435111d2fec1bdee1973be97",
    "source": "pytorch_docs",
    "title": "torch.signal \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.signal#\n\nCreated On: Oct 14, 2022 | Last Updated On: Jun 18, 2025\nThetorch.signalmodule, modeled after SciPy\u2019ssignalmodule.\ntorch.signal\n\n## torch.signal.windows#\n\nbartlett\n\nbartlett\nComputes the Bartlett window.\nblackman\n\nblackman\nComputes the Blackman window.\ncosine\n\ncosine\nComputes a window with a simple cosine waveform, following the same implementation as SciPy.\nexponential\n\nexponential\nComputes a window with an exponential waveform.\ngaussian\n\ngaussian\nComputes a window with a gaussian waveform.\ngeneral_cosine\n\ngeneral_cosine\nComputes the general cosine window.\ngeneral_hamming\n\ngeneral_hamming\nComputes the general Hamming window.\nhamming\n\nhamming\nComputes the Hamming window.\nhann\n\nhann\nComputes the Hann window.\nkaiser\n\nkaiser\nComputes the Kaiser window.\nnuttall\n\nnuttall\nComputes the minimum 4-term Blackman-Harris window according to Nuttall.",
    "url": "https://pytorch.org/docs/stable/signal.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "75570fee6e3a399be93f203b414c283c",
    "source": "pytorch_docs",
    "title": "Distributed Autograd Design \u2014 PyTorch 2.9 documentation",
    "text": "\n## Distributed Autograd Design#\n\nCreated On: Nov 12, 2019 | Last Updated On: Sep 03, 2021\nThis note will present the detailed design for distributed autograd and walk\nthrough the internals of the same. Make sure you\u2019re familiar withAutograd mechanicsand theDistributed RPC Frameworkbefore\nproceeding.\n\n## Background#\n\nLet\u2019s say you have two nodes and a very simple model partitioned across two\nnodes. This can be implemented usingtorch.distributed.rpcas follows:\ntorch.distributed.rpc\n\n```python\nimport torch\nimport torch.distributed.rpc as rpc\n\ndef my_add(t1, t2):\n  return torch.add(t1, t2)\n\n# On worker 0:\nt1 = torch.rand((3, 3), requires_grad=True)\nt2 = torch.rand((3, 3), requires_grad=True)\n\n# Perform some computation remotely.\nt3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2))\n\n# Perform some computation locally based on remote result.\nt4 = torch.rand((3, 3), requires_grad=True)\nt5 = torch.mul(t3, t4)\n\n# Compute some loss.\nloss = t5.sum()\n\n```\n\nThe main motivation behind distributed autograd is to enable running a backward\npass on such distributed models with thelossthat we\u2019ve computed and\nrecord appropriate gradients for all tensors that require gradients.\nloss\n\n## Autograd recording during the forward pass#\n\nPyTorch builds the autograd graph during the forward pass and this graph is\nused to execute the backward pass. For more details seeHow autograd encodes the history.\nFor distributed autograd, we need to keep track of all RPCs during the forward\npass to ensure the backward pass is executed appropriately. For this purpose,\nwe attachsendandrecvfunctions to the autograd graph when we perform\nan RPC.\nsend\nrecv\nThesendfunction is attached to the source of the RPC and its output\nedges point to the autograd function for the input tensors of the RPC.\nThe input for this function during the backward pass is received from the\ndestination as the output of the appropriaterecvfunction.\nsend\nrecv\nTherecvfunction is attached to the destination of the RPC and its\ninputs are retrieved from operators executed on the destination using the\ninput tensors. The output gradients of this function are sent to the source\nnode to the appropriatesendfunction during the backward pass.\nrecv\nsend\nEachsend-recvpair is assigned a globally uniqueautograd_message_idto uniquely identify the pair. This is useful to look up the corresponding\nfunction on a remote node during the backward pass.\nsend-recv\nautograd_message_id\nForRRef, whenever we calltorch.distributed.rpc.RRef.to_here()we attach an appropriatesend-recvpair for the tensors involved.\ntorch.distributed.rpc.RRef.to_here()\nsend-recv\nAs an example, this is what the autograd graph for our example above would look\nlike (t5.sum() excluded for simplicity):\n\n## Distributed Autograd Context#\n\nEach forward and backward pass that uses distributed autograd is assigned a\nuniquetorch.distributed.autograd.contextand this context has a\nglobally uniqueautograd_context_id. This context is created on each node\nas needed.\ntorch.distributed.autograd.context\nautograd_context_id\nThis context serves the following purpose:\nMultiple nodes running distributed backward passes might accumulate\ngradients on the same tensor and as a result the.gradfield of the\ntensor would have gradients from a variety of distributed backward passes\nbefore we have the opportunity to run the optimizer. This is similar to\ncallingtorch.autograd.backward()multiple times locally. In order to\nprovide a way of separating out the gradients for each backward pass, the\ngradients are accumulated in thetorch.distributed.autograd.contextfor each backward pass.\n.grad\ntorch.autograd.backward()\ntorch.distributed.autograd.context\nDuring the forward pass we store thesendandrecvfunctions for\neach autograd pass in this context. This ensures we hold references to the\nappropriate nodes in the autograd graph to keep it alive. In addition to\nthis, it is easy to look up the appropriatesendandrecvfunctions\nduring the backward pass.\nsend\nrecv\nsend\nrecv\nIn general we also use this context to store some metadata for each\ndistributed autograd pass.\nFrom the user\u2019s perspective the autograd context is setup as follows:\n\n```python\nimport torch.distributed.autograd as dist_autograd\nwith dist_autograd.context() as context_id:\n  loss = model.forward()\n  dist_autograd.backward(context_id, loss)\n\n```\n\nIt is important to note that your model\u2019s forward pass must be invoked within\nthe distributed autograd context manager, as a valid context is needed in\norder to ensure that allsendandrecvfunctions are stored properly\nto run the backward pass across all participating nodes.\nsend\nrecv\n\n## Distributed Backward Pass#\n\nIn this section we outline the challenge of computing dependencies accurately\nduring a distributed backward pass and describe a couple of algorithms (with\ntradeoffs) on how we can execute a distributed backward pass.\n\n## Computing dependencies#\n\nConsider the following piece of code being run on a single machine\n\n```python\nimport torch\na = torch.rand((3, 3), requires_grad=True)\nb = torch.rand((3, 3), requires_grad=True)\nc = torch.rand((3, 3), requires_grad=True)\nd = a + b\ne = b * c\nd.sum.().backward()\n\n```\n\nThis is what the autograd graph for the code above would look like:\nThe first step the autograd engine performs as part of the backward pass is\ncomputing the number of dependencies for each node in the autograd graph. This\nhelps the autograd engine know when a node in the graph is ready for execution.\nThe numbers in brackets foradd(1)andmul(0)denote the number of\ndependencies. As you can see, this means during the backward pass theaddnode needs 1 input and themulnode doesn\u2019t need any inputs (in other\nwords doesn\u2019t need to be executed). The local autograd engine computes these\ndependencies by traversing the graph from the root nodes (din this case).\nadd(1)\nmul(0)\nadd\nmul\nd\nThe fact that certain nodes in the autograd graph might not be executed in the\nbackward pass poses a challenge for distributed autograd. Consider this piece\nof code which uses RPC.\n\n```python\nimport torch\nimport torch.distributed.rpc as rpc\n\na = torch.rand((3, 3), requires_grad=True)\nb = torch.rand((3, 3), requires_grad=True)\nc = torch.rand((3, 3), requires_grad=True)\n\nd = rpc.rpc_sync(\"worker1\", torch.add, args=(a, b))\ne = rpc.rpc_sync(\"worker1\", torch.mul, args=(b, c))\nloss = d.sum()\n\n```\n\nThe associated autograd graph for the code above would be:\nComputing dependencies of this distributed autograd graph is much more\nchallenging and requires some overhead (either in terms of computation or\nnetwork communication).\nFor performance sensitive applications we can avoid a\nlot of overhead by assuming everysendandrecvfunction are valid as\npart of the backward pass (most applications don\u2019t perform RPCs that aren\u2019t\nused). This simplifies the distributed autograd algorithm and is much more\nefficient, but at the cost that the application needs to be aware of the\nlimitations. This algorithm is called theFAST mode algorithmand is\ndescribed in detail below.\nsend\nrecv\nIn the general case it might not be necessary that everysendandrecvfunction is valid as part of the backward pass. To address this, we have\nproposed aSMART mode algorithmwhich is described in a later section.\nPlease note that currently, only theFASTmode algorithm is implemented.\nsend\nrecv\n\n## FAST mode algorithm#\n\nThe key assumption of this algorithm is that eachsendfunction has a\ndependency of 1 when we run a backward pass. In other words, we assume we\u2019ll\nreceive a gradient over RPC from another node.\nsend\nThe algorithm is as follows:\nWe start from the worker which has the roots for the backward pass\n(all roots must be local).\nLookup all thesendfunctions for the currentDistributed Autograd Context.\nsend\nCompute dependencies locally starting from the provided roots and all thesendfunctions we retrieved.\nsend\nAfter computing dependencies, kick off the local autograd engine with the\nprovided roots.\nWhen the autograd engine executes therecvfunction, therecvfunction sends the input gradients via RPC to the appropriate worker.\nEachrecvfunction knows the destination worker id since it is recorded\nas part of the forward pass. Therecvfunction also sends over theautograd_context_idandautograd_message_idto the remote host.\nrecv\nrecv\nrecv\nrecv\nautograd_context_id\nautograd_message_id\nWhen this request is received on the remote host, we use theautograd_context_idandautograd_message_idto look up the\nappropriatesendfunction.\nautograd_context_id\nautograd_message_id\nsend\nIf this is the first time a worker has received a request for the givenautograd_context_id, it will compute dependencies locally as described\nin points 1-3 above.\nautograd_context_id\nThesendfunction retrieved in 6. is then enqueued for execution on the\nlocal autograd engine for that worker.\nsend\nFinally, instead of accumulating the gradients on the.gradfield of the\nTensor, we accumulate the gradients separately perDistributed Autograd Context. The gradients are stored in aDict[Tensor,Tensor], which is basically a map from Tensor to its\nassociated gradient and this map can be retrieved using theget_gradients()API.\n.grad\nDict[Tensor,Tensor]\nget_gradients()\nAs an example the complete code with distributed autograd would be as follows:\n\n```python\nimport torch\nimport torch.distributed.autograd as dist_autograd\nimport torch.distributed.rpc as rpc\n\ndef my_add(t1, t2):\n  return torch.add(t1, t2)\n\n# On worker 0:\n\n# Setup the autograd context. Computations that take\n# part in the distributed backward pass must be within\n# the distributed autograd context manager.\nwith dist_autograd.context() as context_id:\n  t1 = torch.rand((3, 3), requires_grad=True)\n  t2 = torch.rand((3, 3), requires_grad=True)\n\n  # Perform some computation remotely.\n  t3 = rpc.rpc_sync(\"worker1\", my_add, args=(t1, t2))\n\n  # Perform some computation locally based on remote result.\n  t4 = torch.rand((3, 3), requires_grad=True)\n  t5 = torch.mul(t3, t4)\n\n  # Compute some loss.\n  loss = t5.sum()\n\n  # Run the backward pass.\n  dist_autograd.backward(context_id, [loss])\n\n  # Retrieve the gradients from the context.\n  dist_autograd.get_gradients(context_id)\n\n```\n\nThe distributed autograd graph with dependencies would be as follows (t5.sum() excluded for simplicity):\nTheFAST mode algorithmapplied to the above example would be as follows:\nOnWorker0we start from the rootslossandsend1to compute\ndependencies. As a resultsend1is marked with a dependency of 1 andmulonWorker0is marked with a dependency of 1.\nWorker0\nloss\nsend1\nsend1\nmul\nWorker0\nNow, we kickoff the local autograd engine onWorker0. We first execute\nthemulfunction, accumulate its output in the autograd context as the\ngradient fort4. Then, we executerecv2which sends the gradients toWorker1.\nWorker0\nmul\nt4\nrecv2\nWorker1\nSince this is the first timeWorker1has heard about this backward pass,\nit starts dependency computation and marks the dependencies forsend2,addandrecv1appropriately.\nWorker1\nsend2\nadd\nrecv1\nNext, we enqueuesend2on the local autograd engine ofWorker1, which\nin turn executesaddandrecv1.\nsend2\nWorker1\nadd\nrecv1\nWhenrecv1is executed it sends the gradients over toWorker0.\nrecv1\nWorker0\nSinceWorker0has already computed dependencies for this backward pass,\nit just enqueues and executessend1locally.\nWorker0\nsend1\nFinally, gradients fort1,t2andt4are accumulated in theDistributed Autograd Context.\nt1\nt2\nt4\n\n## SMART mode algorithm#\n\nFull details of this algorithm are still in the works, but for the general idea\nyou can refer toDistributed Autograd Algorithm Smart modesection in theRFC.\n\n## Distributed Optimizer#\n\nTheDistributedOptimizeroperates as follows:\nDistributedOptimizer\nTakes a list of remote parameters (RRef) to\noptimize. These could also be local parameters wrapped within a localRRef.\nRRef\nRRef\nTakes aOptimizerclass as the local\noptimizer to run on all distinctRRefowners.\nOptimizer\nRRef\nThe distributed optimizer creates an instance of the localOptimizeron\neach of the worker nodes and holds anRRefto them.\nOptimizer\nRRef\nWhentorch.distributed.optim.DistributedOptimizer.step()is invoked,\nthe distributed optimizer uses RPC to remotely execute all the local\noptimizers on the appropriate remote workers. A distributed autogradcontext_idmust be provided as input totorch.distributed.optim.DistributedOptimizer.step(). This is used\nby local optimizers to apply gradients stored in the corresponding\ncontext.\ntorch.distributed.optim.DistributedOptimizer.step()\ncontext_id\ntorch.distributed.optim.DistributedOptimizer.step()\nIf multiple concurrent distributed optimizers are updating the same\nparameters on a worker, these updates are serialized via a lock.\n\n## Simple end to end example#\n\nPutting it all together, the following is a simple end to end example using\ndistributed autograd and the distributed optimizer. If the code is placed into a\nfile called \u201cdist_autograd_simple.py\u201d, it can be run with the commandMASTER_ADDR=\"localhost\"MASTER_PORT=29500pythondist_autograd_simple.py:\nMASTER_ADDR=\"localhost\"MASTER_PORT=29500pythondist_autograd_simple.py\n\n```python\nimport torch\nimport torch.multiprocessing as mp\nimport torch.distributed.autograd as dist_autograd\nfrom torch.distributed import rpc\nfrom torch import optim\nfrom torch.distributed.optim import DistributedOptimizer\n\ndef random_tensor():\n    return torch.rand((3, 3), requires_grad=True)\n\ndef _run_process(rank, dst_rank, world_size):\n    name = \"worker{}\".format(rank)\n    dst_name = \"worker{}\".format(dst_rank)\n\n    # Initialize RPC.\n    rpc.init_rpc(\n        name=name,\n        rank=rank,\n        world_size=world_size\n    )\n\n    # Use a distributed autograd context.\n    with dist_autograd.context() as context_id:\n        # Forward pass (create references on remote nodes).\n        rref1 = rpc.remote(dst_name, random_tensor)\n        rref2 = rpc.remote(dst_name, random_tensor)\n        loss = rref1.to_here() + rref2.to_here()\n\n        # Backward pass (run distributed autograd).\n        dist_autograd.backward(context_id, [loss.sum()])\n\n        # Build DistributedOptimizer.\n        dist_optim = DistributedOptimizer(\n        optim.SGD,\n        [rref1, rref2],\n        lr=0.05,\n        )\n\n        # Run the distributed optimizer step.\n        dist_optim.step(context_id)\n\ndef run_process(rank, world_size):\n    dst_rank = (rank + 1) % world_size\n    _run_process(rank, dst_rank, world_size)\n    rpc.shutdown()\n\nif __name__ == '__main__':\n  # Run world_size workers\n  world_size = 2\n  mp.spawn(run_process, args=(world_size,), nprocs=world_size)\n\n```\n",
    "url": "https://pytorch.org/docs/stable/rpc/distributed_autograd.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "5b8c0aeef7316c9beae1c8f711ef90fd",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/elastic/errors.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e796edc0640de42ceae4960a0e706d0a",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/compile/programming_model.recompilation.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "d61cc1e0bc0bb484d5f77e9d34fd003b",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/add_histogram.png",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "278511eef8901180455cf38e3823b141",
    "source": "pytorch_docs",
    "title": "Skipped Functions \u2014 PyTorch 2.9 documentation",
    "text": "\n## Skipped Functions#\n\nCreated On: Jul 28, 2025 | Last Updated On: Jul 29, 2025\nSummary:\nSometimes,torch.compilecompletely gives up compiling a function and runs it eagerly instead,\nresulting in potentially lost optimization opportunities.\ntorch.compile\nThere are ways to work around skipped functions in order to re-enable tracing around the problematic code.\nSometimes,torch.compilewithfullgraph=Falseis unable to resume tracing when encountering a graph break\nor other compiler error. In many of these cases,torch.compilewill skip compiling the function entirely and run it eagerly.\ntorch.compile\nfullgraph=False\ntorch.compile\nNote that the skip is only applied to the current function and NOT any nested function calls.torch.compilewill still attempt to compile nested calls.\ntorch.compile\n\n```python\ndef inner1(x):\n    return x + 1\ndef inner2(x):\n    return x + 2\n@torch.compile\ndef fn(x):\n    x = inner1(x)\n    torch._dynamo.skip_frame()\n    x = inner2(x)\nfn(torch.randn(3))\n\n```\n\n\n```python\nChromiumEventLogger initialized with id 23519bca-29f9-476e-84a4-2aecfc50de2d\ntorchdynamo start compiling fn /tmp/ipykernel_532/2126697152.py:5, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/2126697152.py\", line 10, in <module>\n    fn(torch.randn(3))\n\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/2126697152.py:5\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/2126697152.py:7 in fn\n        x = inner1(x)\nTRACE LOAD_GLOBAL inner1 []\nTRACE LOAD_FAST x [LazyVariableTracker()]\nTRACE CALL_FUNCTION 1 [LazyVariableTracker(), LazyVariableTracker()]\nINLINING <code object inner1 at 0x7fabe826a340, file \"/tmp/ipykernel_532/2126697152.py\", line 1>, inlined according trace_rules.lookup inlined by default\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE starts_line /tmp/ipykernel_532/2126697152.py:2 in inner1 (inline depth: 1)\n        return x + 1\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE RETURN_VALUE None [TensorVariable()]\nDONE INLINING <code object inner1 at 0x7fabe826a340, file \"/tmp/ipykernel_532/2126697152.py\", line 1>\nTRACE STORE_FAST x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/2126697152.py:8 in fn\n        torch._dynamo.skip_frame()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR skip_frame [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nSkipping frame Skip frame due to `torch._dynamo.skip_frame()`. Message: None fn                 /tmp/ipykernel_532/2126697152.py 5\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\ntorchdynamo start compiling inner1 /tmp/ipykernel_532/2126697152.py:1, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/2126697152.py\", line 10, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n\nStep 1: torchdynamo start tracing inner1 /tmp/ipykernel_532/2126697152.py:1\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/2126697152.py:2 in inner1\n        return x + 1\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 1 [LazyVariableTracker()]\nTRACE BINARY_ADD None [LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE RETURN_VALUE None [TensorVariable()]\nStep 1: torchdynamo done tracing inner1 (RETURN_VALUE)\nRETURN_VALUE triggered compile\nCOMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_532/2126697152.py, line 2 in inner1>], graph_break=False)\nTRACED GRAPH\n ===== __compiled_fn_2_9a47d8a0_0722_48b1_8b6d_8892f4ce7d34 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/2126697152.py:2 in inner1, code: return x + 1\n        add: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        return (add,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # return x + 1  # mp/ipykernel_532/2126697152.py:2 in inner1\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x + 1  # mp/ipykernel_532/2126697152.py:2 in inner1\n\nGuard eval latency = 72.12 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\nskipping: inner (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_compile.py)\nskipping: disable (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/decorators.py)\nskipping: innermost_fn (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py)\nskipping: __init__ (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py)\nskipping: __init__ (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py)\nskipping: nothing (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py)\nskipping: __call__ (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py)\nskipping: _fn (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py)\nskipping: skip_frame (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/decorators.py)\ntorchdynamo start compiling inner2 /tmp/ipykernel_532/2126697152.py:3, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/2126697152.py\", line 10, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n\nStep 1: torchdynamo start tracing inner2 /tmp/ipykernel_532/2126697152.py:3\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/2126697152.py:4 in inner2\n        return x + 2\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 2 [LazyVariableTracker()]\nTRACE BINARY_ADD None [LazyVariableTracker(), ConstantVariable(int: 2)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE RETURN_VALUE None [TensorVariable()]\nStep 1: torchdynamo done tracing inner2 (RETURN_VALUE)\nRETURN_VALUE triggered compile\nCOMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_532/2126697152.py, line 4 in inner2>], graph_break=False)\nTRACED GRAPH\n ===== __compiled_fn_4_c777bb3e_3a95_481d_9fa6_aca3bc776228 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/2126697152.py:4 in inner2, code: return x + 2\n        add: \"f32[3][1]cpu\" = l_x_ + 2;  l_x_ = None\n        return (add,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # return x + 2  # mp/ipykernel_532/2126697152.py:4 in inner2\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x + 2  # mp/ipykernel_532/2126697152.py:4 in inner2\n\nGuard eval latency = 66.29 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\nskipping: remove (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/utils/weak.py)\nskipping: __hash__ (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/utils/weak.py)\n\n```\n\nIn the above example,torch.compilewill tracefn(includinginner1) up until theskip_frame.\nThenfnis skipped and run eagerly -inner1andinner2are compiled when they are called.\ntorch.compile\nfn\ninner1\nskip_frame\nfn\ninner1\ninner2\nSkipping functions may result in lost optimization opportunities,\nso it is important to check if code you want compiled is being skipped, and if so, to work around the skip.\n\n## Graph Break in a Loop#\n\ntorch.compilecannot resume tracing if a graph break occurs in a loop:\ntorch.compile\n\n```python\n@torch.compile\ndef fn(x):\n    for i in range(5):\n        x = x + 1\n        if i == 3:\n            torch._dynamo.graph_break()\n    return x\nfn(torch.randn(3))\n\n```\n\n\n```python\ntorchdynamo start compiling fn /tmp/ipykernel_532/2044822433.py:1, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/2044822433.py\", line 8, in <module>\n    fn(torch.randn(3))\n\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/2044822433.py:1\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:3 in fn\n        for i in range(5):\nTRACE LOAD_GLOBAL range []\nTRACE LOAD_CONST 5 [LazyVariableTracker()]\nTRACE CALL_FUNCTION 1 [LazyVariableTracker(), ConstantVariable(int: 5)]\nTRACE GET_ITER None [RangeVariable()]\nTRACE FOR_ITER 40 [RangeIteratorVariable()]\nTRACE STORE_FAST i [RangeIteratorVariable(), ConstantVariable(int: 0)]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:4 in fn\n            x = x + 1\nTRACE LOAD_FAST x [RangeIteratorVariable()]\nTRACE LOAD_CONST 1 [RangeIteratorVariable(), LazyVariableTracker()]\nTRACE BINARY_ADD None [RangeIteratorVariable(), LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_FAST x [RangeIteratorVariable(), TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:5 in fn\n            if i == 3:\nTRACE LOAD_FAST i [RangeIteratorVariable()]\nTRACE LOAD_CONST 3 [RangeIteratorVariable(), ConstantVariable(int: 0)]\nTRACE COMPARE_OP == [RangeIteratorVariable(), ConstantVariable(int: 0), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 38 [RangeIteratorVariable(), ConstantVariable(bool: False)]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:6 in fn\n                torch._dynamo.graph_break()\nTRACE JUMP_ABSOLUTE 8 [RangeIteratorVariable()]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:3 in fn\n        for i in range(5):\nTRACE FOR_ITER 40 [RangeIteratorVariable()]\nTRACE STORE_FAST i [RangeIteratorVariable(), ConstantVariable(int: 1)]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:4 in fn\n            x = x + 1\nTRACE LOAD_FAST x [RangeIteratorVariable()]\nTRACE LOAD_CONST 1 [RangeIteratorVariable(), TensorVariable()]\nTRACE BINARY_ADD None [RangeIteratorVariable(), TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_FAST x [RangeIteratorVariable(), TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:5 in fn\n            if i == 3:\nTRACE LOAD_FAST i [RangeIteratorVariable()]\nTRACE LOAD_CONST 3 [RangeIteratorVariable(), ConstantVariable(int: 1)]\nTRACE COMPARE_OP == [RangeIteratorVariable(), ConstantVariable(int: 1), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 38 [RangeIteratorVariable(), ConstantVariable(bool: False)]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:6 in fn\n                torch._dynamo.graph_break()\nTRACE JUMP_ABSOLUTE 8 [RangeIteratorVariable()]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:3 in fn\n        for i in range(5):\nTRACE FOR_ITER 40 [RangeIteratorVariable()]\nTRACE STORE_FAST i [RangeIteratorVariable(), ConstantVariable(int: 2)]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:4 in fn\n            x = x + 1\nTRACE LOAD_FAST x [RangeIteratorVariable()]\nTRACE LOAD_CONST 1 [RangeIteratorVariable(), TensorVariable()]\nTRACE BINARY_ADD None [RangeIteratorVariable(), TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_FAST x [RangeIteratorVariable(), TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:5 in fn\n            if i == 3:\nTRACE LOAD_FAST i [RangeIteratorVariable()]\nTRACE LOAD_CONST 3 [RangeIteratorVariable(), ConstantVariable(int: 2)]\nTRACE COMPARE_OP == [RangeIteratorVariable(), ConstantVariable(int: 2), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 38 [RangeIteratorVariable(), ConstantVariable(bool: False)]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:6 in fn\n                torch._dynamo.graph_break()\nTRACE JUMP_ABSOLUTE 8 [RangeIteratorVariable()]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:3 in fn\n        for i in range(5):\nTRACE FOR_ITER 40 [RangeIteratorVariable()]\nTRACE STORE_FAST i [RangeIteratorVariable(), ConstantVariable(int: 3)]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:4 in fn\n            x = x + 1\nTRACE LOAD_FAST x [RangeIteratorVariable()]\nTRACE LOAD_CONST 1 [RangeIteratorVariable(), TensorVariable()]\nTRACE BINARY_ADD None [RangeIteratorVariable(), TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_FAST x [RangeIteratorVariable(), TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:5 in fn\n            if i == 3:\nTRACE LOAD_FAST i [RangeIteratorVariable()]\nTRACE LOAD_CONST 3 [RangeIteratorVariable(), ConstantVariable(int: 3)]\nTRACE COMPARE_OP == [RangeIteratorVariable(), ConstantVariable(int: 3), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 38 [RangeIteratorVariable(), ConstantVariable(bool: True)]\nTRACE starts_line /tmp/ipykernel_532/2044822433.py:6 in fn\n                torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch [RangeIteratorVariable()]\nTRACE LOAD_ATTR _dynamo [RangeIteratorVariable(), LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [RangeIteratorVariable(), LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [RangeIteratorVariable(), LazyVariableTracker()]\nGraph break in user code at /tmp/ipykernel_532/2044822433.py:6\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/2044822433.py\", line 8, in <module>\n    fn(torch.randn(3))\n  File \"/tmp/ipykernel_532/2044822433.py\", line 6, in fn\n    torch._dynamo.graph_break()\n\nSkipping frame because there is a graph break in a for/while loop\n<FrameSummary file /tmp/ipykernel_532/2044822433.py, line 6 in fn>\nSkipping frame Skipping frame because there is a graph break in a for/while loop\n<FrameSummary file /tmp/ipykernel_532/2044822433.py, line 6 in fn> fn                 /tmp/ipykernel_532/2044822433.py 1\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\nskipping: graph_break (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/decorators.py)\n\n```\n\n\n```python\ntensor([4.9228, 6.1472, 2.6696])\n\n```\n\nIn this example, we can avoid skipping by unrolling the loop:\n\n```python\n@torch.compile\ndef fn(x):\n    def inner(i):\n        nonlocal x\n        x = x + 1\n        if i == 3:\n            torch._dynamo.graph_break()\n    inner(0)\n    inner(1)\n    inner(2)\n    inner(3)\n    inner(4)\n    return x\nfn(torch.randn(3))\n\n```\n\n\n```python\ntorchdynamo start compiling fn /tmp/ipykernel_532/617960493.py:1, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/617960493.py\", line 14, in <module>\n    fn(torch.randn(3))\n\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/617960493.py:1\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/617960493.py:3 in fn\n        def inner(i):\nTRACE LOAD_CLOSURE x []\nTRACE BUILD_TUPLE 1 [CellVariable()]\nTRACE LOAD_CONST <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3> [TupleVariable(length=1)]\nTRACE LOAD_CONST fn.<locals>.inner [TupleVariable(length=1), ConstantVariable(code: <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>)]\nTRACE MAKE_FUNCTION 8 [TupleVariable(length=1), ConstantVariable(code: <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>), ConstantVariable(str: 'fn.<locals>.inner')]\nTRACE STORE_FAST inner [NestedUserFunctionVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:8 in fn\n        inner(0)\nTRACE LOAD_FAST inner []\nTRACE LOAD_CONST 0 [NestedUserFunctionVariable()]\nTRACE CALL_FUNCTION 1 [NestedUserFunctionVariable(), ConstantVariable(int: 0)]\nINLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner (inline depth: 1)\n            x = x + 1\nTRACE LOAD_DEREF x []\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=True), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner (inline depth: 1)\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [ConstantVariable(int: 0)]\nTRACE COMPARE_OP == [ConstantVariable(int: 0), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: False)]\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>\nTRACE POP_TOP None [ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:9 in fn\n        inner(1)\nTRACE LOAD_FAST inner []\nTRACE LOAD_CONST 1 [NestedUserFunctionVariable()]\nTRACE CALL_FUNCTION 1 [NestedUserFunctionVariable(), ConstantVariable(int: 1)]\nINLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner (inline depth: 1)\n            x = x + 1\nTRACE LOAD_DEREF x []\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner (inline depth: 1)\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [ConstantVariable(int: 1)]\nTRACE COMPARE_OP == [ConstantVariable(int: 1), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: False)]\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>\nTRACE POP_TOP None [ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:10 in fn\n        inner(2)\nTRACE LOAD_FAST inner []\nTRACE LOAD_CONST 2 [NestedUserFunctionVariable()]\nTRACE CALL_FUNCTION 1 [NestedUserFunctionVariable(), ConstantVariable(int: 2)]\nINLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner (inline depth: 1)\n            x = x + 1\nTRACE LOAD_DEREF x []\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner (inline depth: 1)\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [ConstantVariable(int: 2)]\nTRACE COMPARE_OP == [ConstantVariable(int: 2), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: False)]\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>\nTRACE POP_TOP None [ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:11 in fn\n        inner(3)\nTRACE LOAD_FAST inner []\nTRACE LOAD_CONST 3 [NestedUserFunctionVariable()]\nTRACE CALL_FUNCTION 1 [NestedUserFunctionVariable(), ConstantVariable(int: 3)]\nINLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner (inline depth: 1)\n            x = x + 1\nTRACE LOAD_DEREF x []\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner (inline depth: 1)\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [ConstantVariable(int: 3)]\nTRACE COMPARE_OP == [ConstantVariable(int: 3), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: True)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:7 in inner (inline depth: 1)\n                torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nempty checkpoint\nFAILED INLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>\nGraph break in user code at /tmp/ipykernel_532/617960493.py:7\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/617960493.py\", line 14, in <module>\n    fn(torch.randn(3))\n  File \"/tmp/ipykernel_532/617960493.py\", line 11, in fn\n    inner(3)\n  File \"/tmp/ipykernel_532/617960493.py\", line 7, in inner\n    torch._dynamo.graph_break()\n\nRestarting analysis due to _dynamo/symbolic_convert.py:249 in fail_and_restart_analysis\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/617960493.py:1\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/617960493.py:3 in fn\n        def inner(i):\nTRACE LOAD_CLOSURE x []\nTRACE BUILD_TUPLE 1 [CellVariable()]\nTRACE LOAD_CONST <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3> [TupleVariable(length=1)]\nTRACE LOAD_CONST fn.<locals>.inner [TupleVariable(length=1), ConstantVariable(code: <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>)]\nTRACE MAKE_FUNCTION 8 [TupleVariable(length=1), ConstantVariable(code: <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>), ConstantVariable(str: 'fn.<locals>.inner')]\nTRACE STORE_FAST inner [NestedUserFunctionVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:8 in fn\n        inner(0)\nTRACE LOAD_FAST inner []\nTRACE LOAD_CONST 0 [NestedUserFunctionVariable()]\nTRACE CALL_FUNCTION 1 [NestedUserFunctionVariable(), ConstantVariable(int: 0)]\nINLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner (inline depth: 1)\n            x = x + 1\nTRACE LOAD_DEREF x []\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=True), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner (inline depth: 1)\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [ConstantVariable(int: 0)]\nTRACE COMPARE_OP == [ConstantVariable(int: 0), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: False)]\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>\nTRACE POP_TOP None [ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:9 in fn\n        inner(1)\nTRACE LOAD_FAST inner []\nTRACE LOAD_CONST 1 [NestedUserFunctionVariable()]\nTRACE CALL_FUNCTION 1 [NestedUserFunctionVariable(), ConstantVariable(int: 1)]\nINLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner (inline depth: 1)\n            x = x + 1\nTRACE LOAD_DEREF x []\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner (inline depth: 1)\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [ConstantVariable(int: 1)]\nTRACE COMPARE_OP == [ConstantVariable(int: 1), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: False)]\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>\nTRACE POP_TOP None [ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:10 in fn\n        inner(2)\nTRACE LOAD_FAST inner []\nTRACE LOAD_CONST 2 [NestedUserFunctionVariable()]\nTRACE CALL_FUNCTION 1 [NestedUserFunctionVariable(), ConstantVariable(int: 2)]\nINLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner (inline depth: 1)\n            x = x + 1\nTRACE LOAD_DEREF x []\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner (inline depth: 1)\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [ConstantVariable(int: 2)]\nTRACE COMPARE_OP == [ConstantVariable(int: 2), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: False)]\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>\nTRACE POP_TOP None [ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:11 in fn\n        inner(3)\nTRACE LOAD_FAST inner []\nTRACE LOAD_CONST 3 [NestedUserFunctionVariable()]\nTRACE CALL_FUNCTION 1 [NestedUserFunctionVariable(), ConstantVariable(int: 3)]\nCOMPILING GRAPH due to GraphCompileReason(reason='Call to `torch._dynamo.graph_break()`\\n  Explanation: User-inserted graph break. Message: None\\n  Hint: Remove the `torch._dynamo.graph_break()` call.\\n\\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\\n\\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html', user_stack=[<FrameSummary file /tmp/ipykernel_532/617960493.py, line 11 in fn>, <FrameSummary file /tmp/ipykernel_532/617960493.py, line 7 in inner>], graph_break=True)\nTRACED GRAPH\n ===== __compiled_fn_8_0472935d_d975_47e2_b34e_9d149161b952 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/617960493.py:5 in inner, code: x = x + 1\n        x: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        \n         # File: /tmp/ipykernel_532/617960493.py:5 in inner, code: x = x + 1\n        x_1: \"f32[3][1]cpu\" = x + 1;  x = None\n        \n         # File: /tmp/ipykernel_532/617960493.py:5 in inner, code: x = x + 1\n        x_2: \"f32[3][1]cpu\" = x_1 + 1;  x_1 = None\n        return (x_2,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=2), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # x = x + 1  # mp/ipykernel_532/617960493.py:5 in inner\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # x = x + 1  # mp/ipykernel_532/617960493.py:5 in inner\n\nGuard eval latency = 53.83 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\nskipping: _create_nested_fn (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py)\ntorchdynamo start compiling inner /tmp/ipykernel_532/617960493.py:3, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/617960493.py\", line 14, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n\nStep 1: torchdynamo start tracing inner /tmp/ipykernel_532/617960493.py:3\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner\n            x = x + 1\nTRACE LOAD_DEREF x []\nTRACE LOAD_CONST 1 [LazyVariableTracker()]\nTRACE BINARY_ADD None [LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=False, dynamism=None, is_derefed_cell_contents=True), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [LazyVariableTracker()]\nTRACE COMPARE_OP == [LazyVariableTracker(), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: True)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:7 in inner\n                torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nGraph break (user stack suppressed due to duplicate graph break) in user code at /tmp/ipykernel_532/617960493.py:7\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nRestarting analysis due to _dynamo/symbolic_convert.py:249 in fail_and_restart_analysis\nStep 1: torchdynamo start tracing inner /tmp/ipykernel_532/617960493.py:3\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner\n            x = x + 1\nTRACE LOAD_DEREF x []\nTRACE LOAD_CONST 1 [LazyVariableTracker()]\nTRACE BINARY_ADD None [LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=False, dynamism=None, is_derefed_cell_contents=True), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [LazyVariableTracker()]\nTRACE COMPARE_OP == [LazyVariableTracker(), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: True)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:7 in inner\n                torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nCOMPILING GRAPH due to GraphCompileReason(reason='Call to `torch._dynamo.graph_break()`\\n  Explanation: User-inserted graph break. Message: None\\n  Hint: Remove the `torch._dynamo.graph_break()` call.\\n\\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\\n\\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html', user_stack=[<FrameSummary file /tmp/ipykernel_532/617960493.py, line 7 in inner>], graph_break=True)\nTRACED GRAPH\n ===== __compiled_fn_12_c9702b2e_a86b_41a2_9a4d_e0823204f4ae =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/617960493.py:5 in inner, code: x = x + 1\n        x: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        return (x,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['i'], accessed_by=FrameLocalsGuardAccessor(key='i', framelocals_idx=0), type=<class 'int'>, tag_safe=(True, False)\n| | +- EQUALS_MATCH: L['i'] == 3                                                   # if i == 3:  # mp/ipykernel_532/617960493.py:6 in inner\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=1), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # x = x + 1  # mp/ipykernel_532/617960493.py:5 in inner\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # x = x + 1  # mp/ipykernel_532/617960493.py:5 in inner\n| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor, type=<class 'dict'>, tag_safe=(False, False)\n| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch'), type=<class 'module'>, tag_safe=(False, False)\n| | | +- ID_MATCH: ___check_obj_id(G['torch'], 140376310990704)                  # torch._dynamo.graph_break()  # mp/ipykernel_532/617960493.py:7 in inner\n| | | +- GuardManager: source=G['torch']._dynamo, accessed_by=GetAttrGuardAccessor(_dynamo), type=<class 'module'>, tag_safe=(False, False)\n| | | | +- ID_MATCH: ___check_obj_id(G['torch']._dynamo, 140374889071936)          # torch._dynamo.graph_break()  # mp/ipykernel_532/617960493.py:7 in inner\n| | | | +- GuardManager: source=G['torch']._dynamo.graph_break, accessed_by=GetAttrGuardAccessor(graph_break), type=<class 'function'>, tag_safe=(True, False)\n| | | | | +- GuardManager: source=G['torch']._dynamo.graph_break.__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)\n| | | | | | +- ID_MATCH: ___check_obj_id(G['torch']._dynamo.graph_break.__code__, 140374703207360)  # torch._dynamo.graph_break()  # mp/ipykernel_532/617960493.py:7 in inner\n\nGuard eval latency = 368.80 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\ntorchdynamo start compiling torch_dynamo_resume_in_inner_at_7 /tmp/ipykernel_532/617960493.py:7, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/617960493.py\", line 14, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"/tmp/ipykernel_532/617960493.py\", line 11, in fn\n    inner(3)\n\nStep 1: torchdynamo start tracing torch_dynamo_resume_in_inner_at_7 /tmp/ipykernel_532/617960493.py:7\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/617960493.py:7 in torch_dynamo_resume_in_inner_at_7\n                torch._dynamo.graph_break()\nTRACE LOAD_CONST True []\nTRACE STORE_FAST __is_tracing_resume_prologue [ConstantVariable(bool: True)]\nTRACE LOAD_FAST ___stack0 []\nTRACE LOAD_CONST False [LazyVariableTracker()]\nTRACE STORE_FAST __is_tracing_resume_prologue [LazyVariableTracker(), ConstantVariable(bool: False)]\nTRACE JUMP_ABSOLUTE 36 [LazyVariableTracker()]\nTRACE POP_TOP None [LazyVariableTracker()]\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nSkipping frame because no content in function call torch_dynamo_resume_in_inner_at_7                 /tmp/ipykernel_532/617960493.py 7\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\ntorchdynamo start compiling torch_dynamo_resume_in_fn_at_11 /tmp/ipykernel_532/617960493.py:11, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/617960493.py\", line 14, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n\nStep 1: torchdynamo start tracing torch_dynamo_resume_in_fn_at_11 /tmp/ipykernel_532/617960493.py:11\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/617960493.py:11 in torch_dynamo_resume_in_fn_at_11\n        inner(3)\nTRACE LOAD_CONST True []\nTRACE STORE_FAST __is_tracing_resume_prologue [ConstantVariable(bool: True)]\nTRACE LOAD_FAST ___stack0 []\nTRACE LOAD_CONST False [LazyVariableTracker()]\nTRACE STORE_FAST __is_tracing_resume_prologue [LazyVariableTracker(), ConstantVariable(bool: False)]\nTRACE JUMP_ABSOLUTE 54 [LazyVariableTracker()]\nTRACE POP_TOP None [LazyVariableTracker()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:12 in torch_dynamo_resume_in_fn_at_11\n        inner(4)\nTRACE LOAD_FAST inner []\nTRACE LOAD_CONST 4 [LazyVariableTracker()]\nTRACE CALL_FUNCTION 1 [LazyVariableTracker(), ConstantVariable(int: 4)]\nINLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/617960493.py:5 in inner (inline depth: 1)\n            x = x + 1\nTRACE LOAD_DEREF x []\nTRACE LOAD_CONST 1 [LazyVariableTracker()]\nTRACE BINARY_ADD None [LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=False, dynamism=None, is_derefed_cell_contents=True), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_DEREF x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:6 in inner (inline depth: 1)\n            if i == 3:\nTRACE LOAD_FAST i []\nTRACE LOAD_CONST 3 [ConstantVariable(int: 4)]\nTRACE COMPARE_OP == [ConstantVariable(int: 4), ConstantVariable(int: 3)]\nTRACE POP_JUMP_IF_FALSE 30 [ConstantVariable(bool: False)]\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object inner at 0x7fab86007aa0, file \"/tmp/ipykernel_532/617960493.py\", line 3>\nTRACE POP_TOP None [ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/617960493.py:13 in torch_dynamo_resume_in_fn_at_11\n        return x\nTRACE LOAD_DEREF x []\nTRACE RETURN_VALUE None [TensorVariable()]\nStep 1: torchdynamo done tracing torch_dynamo_resume_in_fn_at_11 (RETURN_VALUE)\nRETURN_VALUE triggered compile\nCOMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_532/617960493.py, line 13 in torch_dynamo_resume_in_fn_at_11>], graph_break=False)\nTRACED GRAPH\n ===== __compiled_fn_16_240112e8_d62a_444c_a9ef_5bde0a078bbd =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/617960493.py:5 in inner, code: x = x + 1\n        x: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        return (x,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=6), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # x = x + 1  # mp/ipykernel_532/617960493.py:5 in inner\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # x = x + 1  # mp/ipykernel_532/617960493.py:5 in inner\n| +- GuardManager: source=L['inner'], accessed_by=FrameLocalsGuardAccessor(key='inner', framelocals_idx=3), type=<class 'function'>, tag_safe=(True, False)\n| | +- GuardManager: source=L['inner'].__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)\n| | | +- ID_MATCH: ___check_obj_id(L['inner'].__code__, 140374664313504)         # inner(4)  # mp/ipykernel_532/617960493.py:12 in torch_dynamo_resume_in_fn_at_11\n\nGuard eval latency = 1147.51 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\nskipping because no torch.* remove             /opt/conda/envs/py_3.10/lib/python3.10/weakref.py 370\n\n```\n\n\n```python\ntensor([4.3159, 5.4236, 5.5645])\n\n```\n\nIn general, resolving the graph break causing the skip will also resolve the skip.\n\n## Graph Break in a Context Manager#\n\nAnother common example of an unresumable graph break is a graph break in most context managers:\n\n```python\nclass CustomCtxManager:\n    def __enter__(self):\n        pass\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n@torch.compile\ndef fn(x):\n    with CustomCtxManager():\n        x = x + 1\n        torch._dynamo.graph_break()\n        return x + 1\nfn(torch.randn(3))\n\n```\n\n\n```python\ntorchdynamo start compiling fn /tmp/ipykernel_532/4148913404.py:6, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/4148913404.py\", line 12, in <module>\n    fn(torch.randn(3))\n\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/4148913404.py:6\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:8 in fn\n        with CustomCtxManager():\nTRACE LOAD_GLOBAL CustomCtxManager []\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nTRACE SETUP_WITH 48 [GenericContextWrappingVariable(CustomCtxManager)]\nINLINING <code object __enter__ at 0x7fabee5323f0, file \"/tmp/ipykernel_532/4148913404.py\", line 2>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:3 in __enter__ (inline depth: 1)\n            pass\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object __enter__ at 0x7fabee5323f0, file \"/tmp/ipykernel_532/4148913404.py\", line 2>\nTRACE POP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:9 in fn\n            x = x + 1\nTRACE LOAD_FAST x [WithExitFunctionVariable()]\nTRACE LOAD_CONST 1 [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE BINARY_ADD None [WithExitFunctionVariable(), LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_FAST x [WithExitFunctionVariable(), TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:10 in fn\n            torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch [WithExitFunctionVariable()]\nTRACE LOAD_ATTR _dynamo [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [WithExitFunctionVariable(), LazyVariableTracker()]\nempty checkpoint\nrun_gc_after_compile: running gc\nGraph break: skip: from user code at:\n  File \"/tmp/ipykernel_532/4148913404.py\", line 10, in fn\n    torch._dynamo.graph_break()\nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 904, in wrapper\n    return inner_fn(self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2333, in CALL_FUNCTION\n    self.call_function(fn, args, {})\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1266, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 212, in realize_and_forward\n    return getattr(self.realize(), name)(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 1515, in call_function\n    unimplemented_v2(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 596, in unimplemented_v2\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1625, in __call__\n    result = self._inner_convert(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 688, in __call__\n    result = _compile(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1434, in _compile\n    guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py\", line 92, in wrapper_function\n    return function(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1117, in compile_inner\n    return _compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1151, in _compile_inner\n    dynamo_output = compile_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1032, in compile_frame\n    bytecode, tracer_output = transform_code_object(code, transform)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1592, in transform_code_object\n    tracer_output = transformations(instructions, code_options)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1004, in transform\n    tracer_output = trace_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 312, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 815, in trace_frame\n    run_tracer()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 797, in run_tracer\n    tracer.run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1500, in run\n    while self.step():\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1348, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 910, in wrapper\n    unimplemented_v2(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 595, in unimplemented_v2\n    raise Unsupported(msg) from from_exc\ntorch._dynamo.exc.Unsupported: Graph break under GenericContextWrappingVariable\n  Explanation: Attempted to graph break in an active context manager(s) that doesn't support graph breaking.\n  Hint: Move the offending context manager(s) to outside the compiled region.\n  Hint: This graph break may have been caused by an earlier graph break. Resolving the earlier graph break may resolve this one.\n\n  Developer debug context: Active generic context managers: [GenericContextWrappingVariable(CustomCtxManager)]\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0066.html\n\nfrom user code:\n   File \"/tmp/ipykernel_532/4148913404.py\", line 10, in fn\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\nWON'T CONVERT fn /tmp/ipykernel_532/4148913404.py line 6 \ndue to: \nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 904, in wrapper\n    return inner_fn(self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2333, in CALL_FUNCTION\n    self.call_function(fn, args, {})\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1266, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 212, in realize_and_forward\n    return getattr(self.realize(), name)(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 1515, in call_function\n    unimplemented_v2(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 596, in unimplemented_v2\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1625, in __call__\n    result = self._inner_convert(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 688, in __call__\n    result = _compile(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1434, in _compile\n    guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py\", line 92, in wrapper_function\n    return function(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1117, in compile_inner\n    return _compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1151, in _compile_inner\n    dynamo_output = compile_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1032, in compile_frame\n    bytecode, tracer_output = transform_code_object(code, transform)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1592, in transform_code_object\n    tracer_output = transformations(instructions, code_options)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1004, in transform\n    tracer_output = trace_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 312, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 815, in trace_frame\n    run_tracer()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 797, in run_tracer\n    tracer.run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1500, in run\n    while self.step():\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1348, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 910, in wrapper\n    unimplemented_v2(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 595, in unimplemented_v2\n    raise Unsupported(msg) from from_exc\ntorch._dynamo.exc.Unsupported: Graph break under GenericContextWrappingVariable\n  Explanation: Attempted to graph break in an active context manager(s) that doesn't support graph breaking.\n  Hint: Move the offending context manager(s) to outside the compiled region.\n  Hint: This graph break may have been caused by an earlier graph break. Resolving the earlier graph break may resolve this one.\n\n  Developer debug context: Active generic context managers: [GenericContextWrappingVariable(CustomCtxManager)]\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0066.html\n\nfrom user code:\n   File \"/tmp/ipykernel_532/4148913404.py\", line 10, in fn\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 904, in wrapper\n    return inner_fn(self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2333, in CALL_FUNCTION\n    self.call_function(fn, args, {})\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1266, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 212, in realize_and_forward\n    return getattr(self.realize(), name)(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 1515, in call_function\n    unimplemented_v2(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 596, in unimplemented_v2\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1625, in __call__\n    result = self._inner_convert(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 688, in __call__\n    result = _compile(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1434, in _compile\n    guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py\", line 92, in wrapper_function\n    return function(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1117, in compile_inner\n    return _compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1151, in _compile_inner\n    dynamo_output = compile_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1032, in compile_frame\n    bytecode, tracer_output = transform_code_object(code, transform)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1592, in transform_code_object\n    tracer_output = transformations(instructions, code_options)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1004, in transform\n    tracer_output = trace_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 312, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 815, in trace_frame\n    run_tracer()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 797, in run_tracer\n    tracer.run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1500, in run\n    while self.step():\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1348, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 910, in wrapper\n    unimplemented_v2(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 595, in unimplemented_v2\n    raise Unsupported(msg) from from_exc\ntorch._dynamo.exc.Unsupported: Graph break under GenericContextWrappingVariable\n  Explanation: Attempted to graph break in an active context manager(s) that doesn't support graph breaking.\n  Hint: Move the offending context manager(s) to outside the compiled region.\n  Hint: This graph break may have been caused by an earlier graph break. Resolving the earlier graph break may resolve this one.\n\n  Developer debug context: Active generic context managers: [GenericContextWrappingVariable(CustomCtxManager)]\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0066.html\n\nfrom user code:\n   File \"/tmp/ipykernel_532/4148913404.py\", line 10, in fn\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\nskipping because no torch.* __enter__             /tmp/ipykernel_532/4148913404.py 2\nskipping because no torch.* __exit__             /tmp/ipykernel_532/4148913404.py 4\n\n```\n\n\n```python\ntensor([2.9449, 2.7372, 1.0708])\n\n```\n\nWe can avoid skipping by moving the graph break outside of the context manager:\n\n```python\n@torch.compile\ndef fn(x):\n    with CustomCtxManager():\n        x = x + 1\n    torch._dynamo.graph_break()\n    with CustomCtxManager():\n        return x + 1\nfn(torch.randn(3))\n\n```\n\n\n```python\ntorchdynamo start compiling fn /tmp/ipykernel_532/2124425154.py:1, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/2124425154.py\", line 8, in <module>\n    fn(torch.randn(3))\n\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/2124425154.py:1\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:3 in fn\n        with CustomCtxManager():\nTRACE LOAD_GLOBAL CustomCtxManager []\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nTRACE SETUP_WITH 30 [GenericContextWrappingVariable(CustomCtxManager)]\nINLINING <code object __enter__ at 0x7fabee5323f0, file \"/tmp/ipykernel_532/4148913404.py\", line 2>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:3 in __enter__ (inline depth: 1)\n            pass\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object __enter__ at 0x7fabee5323f0, file \"/tmp/ipykernel_532/4148913404.py\", line 2>\nTRACE POP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:4 in fn\n            x = x + 1\nTRACE LOAD_FAST x [WithExitFunctionVariable()]\nTRACE LOAD_CONST 1 [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE BINARY_ADD None [WithExitFunctionVariable(), LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_FAST x [WithExitFunctionVariable(), TensorVariable()]\nTRACE POP_BLOCK None [WithExitFunctionVariable()]\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:3 in fn\n        with CustomCtxManager():\nTRACE LOAD_CONST None [WithExitFunctionVariable()]\nTRACE DUP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE DUP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nTRACE CALL_FUNCTION 3 [WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nINLINING <code object __exit__ at 0x7fabe8268c90, file \"/tmp/ipykernel_532/4148913404.py\", line 4>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:5 in __exit__ (inline depth: 1)\n            pass\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object __exit__ at 0x7fabe8268c90, file \"/tmp/ipykernel_532/4148913404.py\", line 4>\nTRACE POP_TOP None [ConstantVariable(NoneType: None)]\nTRACE JUMP_FORWARD 46 []\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:5 in fn\n        torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nGraph break in user code at /tmp/ipykernel_532/2124425154.py:5\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/2124425154.py\", line 8, in <module>\n    fn(torch.randn(3))\n  File \"/tmp/ipykernel_532/2124425154.py\", line 5, in fn\n    torch._dynamo.graph_break()\n\nRestarting analysis due to _dynamo/symbolic_convert.py:249 in fail_and_restart_analysis\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/2124425154.py:1\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:3 in fn\n        with CustomCtxManager():\nTRACE LOAD_GLOBAL CustomCtxManager []\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nTRACE SETUP_WITH 30 [GenericContextWrappingVariable(CustomCtxManager)]\nINLINING <code object __enter__ at 0x7fabee5323f0, file \"/tmp/ipykernel_532/4148913404.py\", line 2>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:3 in __enter__ (inline depth: 1)\n            pass\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object __enter__ at 0x7fabee5323f0, file \"/tmp/ipykernel_532/4148913404.py\", line 2>\nTRACE POP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:4 in fn\n            x = x + 1\nTRACE LOAD_FAST x [WithExitFunctionVariable()]\nTRACE LOAD_CONST 1 [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE BINARY_ADD None [WithExitFunctionVariable(), LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_FAST x [WithExitFunctionVariable(), TensorVariable()]\nTRACE POP_BLOCK None [WithExitFunctionVariable()]\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:3 in fn\n        with CustomCtxManager():\nTRACE LOAD_CONST None [WithExitFunctionVariable()]\nTRACE DUP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE DUP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nTRACE CALL_FUNCTION 3 [WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nINLINING <code object __exit__ at 0x7fabe8268c90, file \"/tmp/ipykernel_532/4148913404.py\", line 4>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:5 in __exit__ (inline depth: 1)\n            pass\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object __exit__ at 0x7fabe8268c90, file \"/tmp/ipykernel_532/4148913404.py\", line 4>\nTRACE POP_TOP None [ConstantVariable(NoneType: None)]\nTRACE JUMP_FORWARD 46 []\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:5 in fn\n        torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nCOMPILING GRAPH due to GraphCompileReason(reason='Call to `torch._dynamo.graph_break()`\\n  Explanation: User-inserted graph break. Message: None\\n  Hint: Remove the `torch._dynamo.graph_break()` call.\\n\\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\\n\\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html', user_stack=[<FrameSummary file /tmp/ipykernel_532/2124425154.py, line 5 in fn>], graph_break=True)\nTRACED GRAPH\n ===== __compiled_fn_20_763153f3_7654_433a_8e55_bfa1b1276b54 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/2124425154.py:4 in fn, code: x = x + 1\n        x: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        return (x,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # x = x + 1  # mp/ipykernel_532/2124425154.py:4 in fn\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # x = x + 1  # mp/ipykernel_532/2124425154.py:4 in fn\n| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor, type=<class 'dict'>, tag_safe=(False, False)\n| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch'), type=<class 'module'>, tag_safe=(False, False)\n| | | +- ID_MATCH: ___check_obj_id(G['torch'], 140376310990704)                  # torch._dynamo.graph_break()  # mp/ipykernel_532/2124425154.py:5 in fn\n| | | +- GuardManager: source=G['torch']._dynamo, accessed_by=GetAttrGuardAccessor(_dynamo), type=<class 'module'>, tag_safe=(False, False)\n| | | | +- ID_MATCH: ___check_obj_id(G['torch']._dynamo, 140374889071936)          # torch._dynamo.graph_break()  # mp/ipykernel_532/2124425154.py:5 in fn\n| | | | +- GuardManager: source=G['torch']._dynamo.graph_break, accessed_by=GetAttrGuardAccessor(graph_break), type=<class 'function'>, tag_safe=(True, False)\n| | | | | +- GuardManager: source=G['torch']._dynamo.graph_break.__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)\n| | | | | | +- ID_MATCH: ___check_obj_id(G['torch']._dynamo.graph_break.__code__, 140374703207360)  # torch._dynamo.graph_break()  # mp/ipykernel_532/2124425154.py:5 in fn\n| | +- GuardManager: source=G['CustomCtxManager'], accessed_by=DictGetItemGuardAccessor('CustomCtxManager'), type=<class 'type'>, tag_safe=(True, False)\n| | | +- ID_MATCH: ___check_obj_id(G['CustomCtxManager'], 93956540677728)        # with CustomCtxManager():  # mp/ipykernel_532/2124425154.py:3 in fn\n\nGuard eval latency = 48.80 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\ntorchdynamo start compiling torch_dynamo_resume_in_fn_at_5 /tmp/ipykernel_532/2124425154.py:5, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/2124425154.py\", line 8, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n\nStep 1: torchdynamo start tracing torch_dynamo_resume_in_fn_at_5 /tmp/ipykernel_532/2124425154.py:5\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:5 in torch_dynamo_resume_in_fn_at_5\n        torch._dynamo.graph_break()\nTRACE LOAD_CONST True []\nTRACE STORE_FAST __is_tracing_resume_prologue [ConstantVariable(bool: True)]\nTRACE LOAD_FAST ___stack0 []\nTRACE LOAD_CONST False [LazyVariableTracker()]\nTRACE STORE_FAST __is_tracing_resume_prologue [LazyVariableTracker(), ConstantVariable(bool: False)]\nTRACE JUMP_ABSOLUTE 66 [LazyVariableTracker()]\nTRACE POP_TOP None [LazyVariableTracker()]\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:6 in torch_dynamo_resume_in_fn_at_5\n        with CustomCtxManager():\nTRACE LOAD_GLOBAL CustomCtxManager []\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nTRACE SETUP_WITH 98 [GenericContextWrappingVariable(CustomCtxManager)]\nINLINING <code object __enter__ at 0x7fabee5323f0, file \"/tmp/ipykernel_532/4148913404.py\", line 2>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:3 in __enter__ (inline depth: 1)\n            pass\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object __enter__ at 0x7fabee5323f0, file \"/tmp/ipykernel_532/4148913404.py\", line 2>\nTRACE POP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:7 in torch_dynamo_resume_in_fn_at_5\n            return x + 1\nTRACE LOAD_FAST x [WithExitFunctionVariable()]\nTRACE LOAD_CONST 1 [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE BINARY_ADD None [WithExitFunctionVariable(), LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE starts_line /tmp/ipykernel_532/2124425154.py:6 in torch_dynamo_resume_in_fn_at_5\n        with CustomCtxManager():\nTRACE POP_BLOCK None [WithExitFunctionVariable(), TensorVariable()]\nTRACE ROT_TWO None [WithExitFunctionVariable(), TensorVariable()]\nTRACE LOAD_CONST None [TensorVariable(), WithExitFunctionVariable()]\nTRACE DUP_TOP None [TensorVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE DUP_TOP None [TensorVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nTRACE CALL_FUNCTION 3 [TensorVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nINLINING <code object __exit__ at 0x7fabe8268c90, file \"/tmp/ipykernel_532/4148913404.py\", line 4>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/4148913404.py:5 in __exit__ (inline depth: 1)\n            pass\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nDONE INLINING <code object __exit__ at 0x7fabe8268c90, file \"/tmp/ipykernel_532/4148913404.py\", line 4>\nTRACE POP_TOP None [TensorVariable(), ConstantVariable(NoneType: None)]\nTRACE RETURN_VALUE None [TensorVariable()]\nStep 1: torchdynamo done tracing torch_dynamo_resume_in_fn_at_5 (RETURN_VALUE)\nRETURN_VALUE triggered compile\nCOMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_532/2124425154.py, line 6 in torch_dynamo_resume_in_fn_at_5>], graph_break=False)\nTRACED GRAPH\n ===== __compiled_fn_23_df525bd6_d853_4d00_8aac_8dccfb55e2ea =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/2124425154.py:7 in torch_dynamo_resume_in_fn_at_5, code: return x + 1\n        add: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        return (add,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=3), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # return x + 1  # mp/ipykernel_532/2124425154.py:7 in torch_dynamo_resume_in_fn_at_5\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x + 1  # mp/ipykernel_532/2124425154.py:7 in torch_dynamo_resume_in_fn_at_5\n| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)\n| | +- GuardManager: source=G['CustomCtxManager'], accessed_by=DictGetItemGuardAccessor('CustomCtxManager'), type=<class 'type'>, tag_safe=(True, False)\n| | | +- ID_MATCH: ___check_obj_id(G['CustomCtxManager'], 93956540677728)        # with CustomCtxManager():  # mp/ipykernel_532/2124425154.py:6 in torch_dynamo_resume_in_fn_at_5\n\nGuard eval latency = 1087.49 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\n\n```\n\n\n```python\ntensor([1.5408, 1.1547, 1.4015])\n\n```\n\nThere are some context managers where Dynamo can resume after a graph break.\nSome of these can be found insupported_ctx_manager_classesintorch/_dynamo/variables/torch.py.\nIn general, any context manager represented by aContextWrappingVariablesubclass intorch/_dynamo/variables/ctx_manager.pysupport resuming after a graph break. For example:\nsupported_ctx_manager_classes\ntorch/_dynamo/variables/torch.py\nContextWrappingVariable\ntorch/_dynamo/variables/ctx_manager.py\n\n```python\nimport contextlib\n@torch.compile\ndef fn(x):\n    with contextlib.nullcontext():\n        with torch.no_grad():\n            x = x + 1\n            torch._dynamo.graph_break()\n            return x + 1\nfn(torch.randn(3))\n\n```\n\n\n```python\ntorchdynamo start compiling fn /tmp/ipykernel_532/3152636365.py:2, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/3152636365.py\", line 9, in <module>\n    fn(torch.randn(3))\n\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/3152636365.py:2\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:4 in fn\n        with contextlib.nullcontext():\nTRACE LOAD_GLOBAL contextlib []\nTRACE LOAD_ATTR nullcontext [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nTRACE SETUP_WITH 106 [NullContextVariable()]\nTRACE POP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:5 in fn\n            with torch.no_grad():\nTRACE LOAD_GLOBAL torch [WithExitFunctionVariable()]\nTRACE LOAD_ATTR no_grad [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE SETUP_WITH 74 [WithExitFunctionVariable(), GradModeVariable()]\nTRACE POP_TOP None [WithExitFunctionVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:6 in fn\n                x = x + 1\nTRACE LOAD_FAST x [WithExitFunctionVariable(), WithExitFunctionVariable()]\nTRACE LOAD_CONST 1 [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE BINARY_ADD None [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_FAST x [WithExitFunctionVariable(), WithExitFunctionVariable(), TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:7 in fn\n                torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch [WithExitFunctionVariable(), WithExitFunctionVariable()]\nTRACE LOAD_ATTR _dynamo [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nGraph break in user code at /tmp/ipykernel_532/3152636365.py:7\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/3152636365.py\", line 9, in <module>\n    fn(torch.randn(3))\n  File \"/tmp/ipykernel_532/3152636365.py\", line 7, in fn\n    torch._dynamo.graph_break()\n\nRestarting analysis due to _dynamo/symbolic_convert.py:249 in fail_and_restart_analysis\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/3152636365.py:2\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:4 in fn\n        with contextlib.nullcontext():\nTRACE LOAD_GLOBAL contextlib []\nTRACE LOAD_ATTR nullcontext [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nTRACE SETUP_WITH 106 [NullContextVariable()]\nTRACE POP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:5 in fn\n            with torch.no_grad():\nTRACE LOAD_GLOBAL torch [WithExitFunctionVariable()]\nTRACE LOAD_ATTR no_grad [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE SETUP_WITH 74 [WithExitFunctionVariable(), GradModeVariable()]\nTRACE POP_TOP None [WithExitFunctionVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:6 in fn\n                x = x + 1\nTRACE LOAD_FAST x [WithExitFunctionVariable(), WithExitFunctionVariable()]\nTRACE LOAD_CONST 1 [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE BINARY_ADD None [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_FAST x [WithExitFunctionVariable(), WithExitFunctionVariable(), TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:7 in fn\n                torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch [WithExitFunctionVariable(), WithExitFunctionVariable()]\nTRACE LOAD_ATTR _dynamo [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nCOMPILING GRAPH due to GraphCompileReason(reason='Call to `torch._dynamo.graph_break()`\\n  Explanation: User-inserted graph break. Message: None\\n  Hint: Remove the `torch._dynamo.graph_break()` call.\\n\\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\\n\\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html', user_stack=[<FrameSummary file /tmp/ipykernel_532/3152636365.py, line 7 in fn>], graph_break=True)\nTRACED GRAPH\n ===== __compiled_fn_26_b931600f_47fa_4e1d_8c6b_321de5fcc179 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n        # No stacktrace found for following nodes\n        _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None\n        \n         # File: /tmp/ipykernel_532/3152636365.py:6 in fn, code: x = x + 1\n        x: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        \n        # No stacktrace found for following nodes\n        _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None\n        return (x,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # x = x + 1  # mp/ipykernel_532/3152636365.py:6 in fn\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # x = x + 1  # mp/ipykernel_532/3152636365.py:6 in fn\n| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor, type=<class 'dict'>, tag_safe=(False, False)\n| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch'), type=<class 'module'>, tag_safe=(False, False)\n| | | +- ID_MATCH: ___check_obj_id(G['torch'], 140376310990704)                  # with torch.no_grad():  # mp/ipykernel_532/3152636365.py:5 in fn\n| | | +- GuardManager: source=G['torch']._dynamo, accessed_by=GetAttrGuardAccessor(_dynamo), type=<class 'module'>, tag_safe=(False, False)\n| | | | +- ID_MATCH: ___check_obj_id(G['torch']._dynamo, 140374889071936)          # torch._dynamo.graph_break()  # mp/ipykernel_532/3152636365.py:7 in fn\n| | | | +- GuardManager: source=G['torch']._dynamo.graph_break, accessed_by=GetAttrGuardAccessor(graph_break), type=<class 'function'>, tag_safe=(True, False)\n| | | | | +- GuardManager: source=G['torch']._dynamo.graph_break.__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)\n| | | | | | +- ID_MATCH: ___check_obj_id(G['torch']._dynamo.graph_break.__code__, 140374703207360)  # torch._dynamo.graph_break()  # mp/ipykernel_532/3152636365.py:7 in fn\n| | | +- GuardManager: source=G['torch'].no_grad, accessed_by=GetAttrGuardAccessor(no_grad), type=<class 'type'>, tag_safe=(True, False)\n| | | | +- ID_MATCH: ___check_obj_id(G['torch'].no_grad, 93956503623376)           # with torch.no_grad():  # mp/ipykernel_532/3152636365.py:5 in fn\n| | +- GuardManager: source=G['contextlib'], accessed_by=DictGetItemGuardAccessor('contextlib'), type=<class 'module'>, tag_safe=(False, False)\n| | | +- ID_MATCH: ___check_obj_id(G['contextlib'], 140376423420832)             # with contextlib.nullcontext():  # mp/ipykernel_532/3152636365.py:4 in fn\n| | | +- GuardManager: source=G['contextlib'].nullcontext, accessed_by=GetAttrGuardAccessor(nullcontext), type=<class 'abc.ABCMeta'>, tag_safe=(True, False)\n| | | | +- ID_MATCH: ___check_obj_id(G['contextlib'].nullcontext, 93956428443184)  # with contextlib.nullcontext():  # mp/ipykernel_532/3152636365.py:4 in fn\n\nGuard eval latency = 261.20 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\nskipping because no torch.* __init__             /opt/conda/envs/py_3.10/lib/python3.10/contextlib.py 732\nskipping because no torch.* __enter__             /opt/conda/envs/py_3.10/lib/python3.10/contextlib.py 735\nskipping: __init__ (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py)\nskipping: __enter__ (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py)\nskipping: __exit__ (reason: in skipfiles, file: /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py)\nskipping because no torch.* __exit__             /opt/conda/envs/py_3.10/lib/python3.10/contextlib.py 738\ntorchdynamo start compiling torch_dynamo_resume_in_fn_at_7 /tmp/ipykernel_532/3152636365.py:7, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/3152636365.py\", line 9, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n\nStep 1: torchdynamo start tracing torch_dynamo_resume_in_fn_at_7 /tmp/ipykernel_532/3152636365.py:7\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:7 in torch_dynamo_resume_in_fn_at_7\n                torch._dynamo.graph_break()\nTRACE LOAD_CONST True []\nTRACE STORE_FAST __is_tracing_resume_prologue [ConstantVariable(bool: True)]\nTRACE LOAD_FAST ___stack0 []\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nTRACE NOP None [NullContextVariable()]\nTRACE SETUP_WITH 96 [NullContextVariable()]\nTRACE POP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE NOP None [WithExitFunctionVariable()]\nTRACE LOAD_FAST ___stack1 [WithExitFunctionVariable()]\nTRACE LOAD_CONST False [WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE CALL_FUNCTION 1 [WithExitFunctionVariable(), LazyVariableTracker(), ConstantVariable(bool: False)]\nTRACE NOP None [WithExitFunctionVariable(), GradModeVariable()]\nTRACE SETUP_WITH 56 [WithExitFunctionVariable(), GradModeVariable()]\nTRACE POP_TOP None [WithExitFunctionVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE NOP None [WithExitFunctionVariable(), WithExitFunctionVariable()]\nTRACE LOAD_FAST ___stack2 [WithExitFunctionVariable(), WithExitFunctionVariable()]\nTRACE LOAD_CONST False [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE STORE_FAST __is_tracing_resume_prologue [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker(), ConstantVariable(bool: False)]\nTRACE JUMP_ABSOLUTE 158 [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE POP_TOP None [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:8 in torch_dynamo_resume_in_fn_at_7\n                return x + 1\nTRACE LOAD_FAST x [WithExitFunctionVariable(), WithExitFunctionVariable()]\nTRACE LOAD_CONST 1 [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker()]\nTRACE BINARY_ADD None [WithExitFunctionVariable(), WithExitFunctionVariable(), LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:5 in torch_dynamo_resume_in_fn_at_7\n            with torch.no_grad():\nTRACE POP_BLOCK None [WithExitFunctionVariable(), WithExitFunctionVariable(), TensorVariable()]\nTRACE ROT_TWO None [WithExitFunctionVariable(), WithExitFunctionVariable(), TensorVariable()]\nTRACE LOAD_CONST None [WithExitFunctionVariable(), TensorVariable(), WithExitFunctionVariable()]\nTRACE DUP_TOP None [WithExitFunctionVariable(), TensorVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE DUP_TOP None [WithExitFunctionVariable(), TensorVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nTRACE CALL_FUNCTION 3 [WithExitFunctionVariable(), TensorVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nTRACE POP_TOP None [WithExitFunctionVariable(), TensorVariable(), ConstantVariable(NoneType: None)]\nTRACE starts_line /tmp/ipykernel_532/3152636365.py:4 in torch_dynamo_resume_in_fn_at_7\n        with contextlib.nullcontext():\nTRACE POP_BLOCK None [WithExitFunctionVariable(), TensorVariable()]\nTRACE ROT_TWO None [WithExitFunctionVariable(), TensorVariable()]\nTRACE LOAD_CONST None [TensorVariable(), WithExitFunctionVariable()]\nTRACE DUP_TOP None [TensorVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None)]\nTRACE DUP_TOP None [TensorVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nTRACE CALL_FUNCTION 3 [TensorVariable(), WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]\nTRACE POP_TOP None [TensorVariable(), ConstantVariable(NoneType: None)]\nTRACE RETURN_VALUE None [TensorVariable()]\nStep 1: torchdynamo done tracing torch_dynamo_resume_in_fn_at_7 (RETURN_VALUE)\nRETURN_VALUE triggered compile\nCOMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_532/3152636365.py, line 4 in torch_dynamo_resume_in_fn_at_7>], graph_break=False)\nTRACED GRAPH\n ===== __compiled_fn_31_b2a5c450_5d0e_4895_be05_d0def17b67d7 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n        # No stacktrace found for following nodes\n        _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None\n        \n         # File: /tmp/ipykernel_532/3152636365.py:8 in torch_dynamo_resume_in_fn_at_7, code: return x + 1\n        add: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        \n        # No stacktrace found for following nodes\n        _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None\n        return (add,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=5), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # return x + 1  # mp/ipykernel_532/3152636365.py:8 in torch_dynamo_resume_in_fn_at_7\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x + 1  # mp/ipykernel_532/3152636365.py:8 in torch_dynamo_resume_in_fn_at_7\n| +- GuardManager: source=L['___stack0'], accessed_by=FrameLocalsGuardAccessor(key='___stack0', framelocals_idx=2), type=<class 'abc.ABCMeta'>, tag_safe=(True, False)\n| | +- ID_MATCH: ___check_obj_id(L['___stack0'], 93956428443184)               # torch._dynamo.graph_break()  # mp/ipykernel_532/3152636365.py:7 in torch_dynamo_resume_in_fn_at_7\n| +- GuardManager: source=L['___stack1'], accessed_by=FrameLocalsGuardAccessor(key='___stack1', framelocals_idx=3), type=<class 'type'>, tag_safe=(True, False)\n| | +- ID_MATCH: ___check_obj_id(L['___stack1'], 93956503628432)               # torch._dynamo.graph_break()  # mp/ipykernel_532/3152636365.py:7 in torch_dynamo_resume_in_fn_at_7\n\nGuard eval latency = 79.83 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\n\n```\n\n\n```python\ntensor([1.3265, 1.4369, 2.3154])\n\n```\n\n\n## Graph Break in a Try Block#\n\nA graph break in a try block cannot be resumed:\n\n```python\n@torch.compile\ndef fn(x):\n    try:\n        x = x + 1\n        torch._dynamo.graph_break()\n        return x + 1\n    except Exception as e:\n        pass\nfn(torch.randn(3))\n\n```\n\n\n```python\ntorchdynamo start compiling fn /tmp/ipykernel_532/2546874632.py:1, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/2546874632.py\", line 9, in <module>\n    fn(torch.randn(3))\n\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/2546874632.py:1\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/2546874632.py:3 in fn\n        try:\nTRACE SETUP_FINALLY 30 []\nTRACE starts_line /tmp/ipykernel_532/2546874632.py:4 in fn\n            x = x + 1\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 1 [LazyVariableTracker()]\nTRACE BINARY_ADD None [LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_FAST x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/2546874632.py:5 in fn\n            torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nempty checkpoint\nrun_gc_after_compile: running gc\nGraph break: skip: from user code at:\n  File \"/tmp/ipykernel_532/2546874632.py\", line 5, in fn\n    torch._dynamo.graph_break()\nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1625, in __call__\n    result = self._inner_convert(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 688, in __call__\n    result = _compile(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1434, in _compile\n    guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py\", line 92, in wrapper_function\n    return function(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1117, in compile_inner\n    return _compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1151, in _compile_inner\n    dynamo_output = compile_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1032, in compile_frame\n    bytecode, tracer_output = transform_code_object(code, transform)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1592, in transform_code_object\n    tracer_output = transformations(instructions, code_options)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1004, in transform\n    tracer_output = trace_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 312, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 815, in trace_frame\n    run_tracer()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 797, in run_tracer\n    tracer.run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1500, in run\n    while self.step():\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1348, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 904, in wrapper\n    return inner_fn(self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2333, in CALL_FUNCTION\n    self.call_function(fn, args, {})\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1266, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 212, in realize_and_forward\n    return getattr(self.realize(), name)(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 1515, in call_function\n    unimplemented_v2(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 596, in unimplemented_v2\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_532/2546874632.py\", line 5, in fn\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\nWON'T CONVERT fn /tmp/ipykernel_532/2546874632.py line 1 \ndue to: \nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1625, in __call__\n    result = self._inner_convert(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 688, in __call__\n    result = _compile(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1434, in _compile\n    guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py\", line 92, in wrapper_function\n    return function(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1117, in compile_inner\n    return _compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1151, in _compile_inner\n    dynamo_output = compile_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1032, in compile_frame\n    bytecode, tracer_output = transform_code_object(code, transform)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1592, in transform_code_object\n    tracer_output = transformations(instructions, code_options)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1004, in transform\n    tracer_output = trace_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 312, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 815, in trace_frame\n    run_tracer()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 797, in run_tracer\n    tracer.run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1500, in run\n    while self.step():\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1348, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 904, in wrapper\n    return inner_fn(self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2333, in CALL_FUNCTION\n    self.call_function(fn, args, {})\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1266, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 212, in realize_and_forward\n    return getattr(self.realize(), name)(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 1515, in call_function\n    unimplemented_v2(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 596, in unimplemented_v2\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_532/2546874632.py\", line 5, in fn\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1625, in __call__\n    result = self._inner_convert(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 688, in __call__\n    result = _compile(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1434, in _compile\n    guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_utils_internal.py\", line 92, in wrapper_function\n    return function(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1117, in compile_inner\n    return _compile_inner(code, one_graph, hooks)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1151, in _compile_inner\n    dynamo_output = compile_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1032, in compile_frame\n    bytecode, tracer_output = transform_code_object(code, transform)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1592, in transform_code_object\n    tracer_output = transformations(instructions, code_options)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 1004, in transform\n    tracer_output = trace_frame(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 312, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 815, in trace_frame\n    run_tracer()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 797, in run_tracer\n    tracer.run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1500, in run\n    while self.step():\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1348, in step\n    self.dispatch_table[inst.opcode](self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 904, in wrapper\n    return inner_fn(self, inst)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 2333, in CALL_FUNCTION\n    self.call_function(fn, args, {})\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1266, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/lazy.py\", line 212, in realize_and_forward\n    return getattr(self.realize(), name)(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py\", line 1515, in call_function\n    unimplemented_v2(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 596, in unimplemented_v2\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n\nfrom user code:\n   File \"/tmp/ipykernel_532/2546874632.py\", line 5, in fn\n    torch._dynamo.graph_break()\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n\n```\n\n\n```python\ntensor([1.4041, 2.4074, 2.5800])\n\n```\n\nWe can avoid skipping by moving the graph break outside of the try block:\n\n```python\n@torch.compile\ndef fn(x):\n    try:\n        x = x + 1\n    except Exception as e:\n        pass\n    torch._dynamo.graph_break()\n    try:\n        return x + 1\n    except Exception as e:\n        pass\nfn(torch.randn(3))\n\n```\n\n\n```python\ntorchdynamo start compiling fn /tmp/ipykernel_532/3015389759.py:1, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/3015389759.py\", line 12, in <module>\n    fn(torch.randn(3))\n\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/3015389759.py:1\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/3015389759.py:3 in fn\n        try:\nTRACE SETUP_FINALLY 14 []\nTRACE starts_line /tmp/ipykernel_532/3015389759.py:4 in fn\n            x = x + 1\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 1 [LazyVariableTracker()]\nTRACE BINARY_ADD None [LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_FAST x [TensorVariable()]\nTRACE POP_BLOCK None []\nTRACE JUMP_FORWARD 50 []\nTRACE starts_line /tmp/ipykernel_532/3015389759.py:7 in fn\n        torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nGraph break in user code at /tmp/ipykernel_532/3015389759.py:7\nGraph Break Reason: Call to `torch._dynamo.graph_break()`\n  Explanation: User-inserted graph break. Message: None\n  Hint: Remove the `torch._dynamo.graph_break()` call.\n\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/3015389759.py\", line 12, in <module>\n    fn(torch.randn(3))\n  File \"/tmp/ipykernel_532/3015389759.py\", line 7, in fn\n    torch._dynamo.graph_break()\n\nRestarting analysis due to _dynamo/symbolic_convert.py:249 in fail_and_restart_analysis\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/3015389759.py:1\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/3015389759.py:3 in fn\n        try:\nTRACE SETUP_FINALLY 14 []\nTRACE starts_line /tmp/ipykernel_532/3015389759.py:4 in fn\n            x = x + 1\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 1 [LazyVariableTracker()]\nTRACE BINARY_ADD None [LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE STORE_FAST x [TensorVariable()]\nTRACE POP_BLOCK None []\nTRACE JUMP_FORWARD 50 []\nTRACE starts_line /tmp/ipykernel_532/3015389759.py:7 in fn\n        torch._dynamo.graph_break()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR graph_break [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nCOMPILING GRAPH due to GraphCompileReason(reason='Call to `torch._dynamo.graph_break()`\\n  Explanation: User-inserted graph break. Message: None\\n  Hint: Remove the `torch._dynamo.graph_break()` call.\\n\\n  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\\n\\n For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html', user_stack=[<FrameSummary file /tmp/ipykernel_532/3015389759.py, line 7 in fn>], graph_break=True)\nTRACED GRAPH\n ===== __compiled_fn_35_9cf6feae_253b_490a_9640_de9cdf09ab17 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/3015389759.py:4 in fn, code: x = x + 1\n        x: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        return (x,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # x = x + 1  # mp/ipykernel_532/3015389759.py:4 in fn\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # x = x + 1  # mp/ipykernel_532/3015389759.py:4 in fn\n| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor, type=<class 'dict'>, tag_safe=(False, False)\n| | +- GuardManager: source=G['torch'], accessed_by=DictGetItemGuardAccessor('torch'), type=<class 'module'>, tag_safe=(False, False)\n| | | +- ID_MATCH: ___check_obj_id(G['torch'], 140376310990704)                  # torch._dynamo.graph_break()  # mp/ipykernel_532/3015389759.py:7 in fn\n| | | +- GuardManager: source=G['torch']._dynamo, accessed_by=GetAttrGuardAccessor(_dynamo), type=<class 'module'>, tag_safe=(False, False)\n| | | | +- ID_MATCH: ___check_obj_id(G['torch']._dynamo, 140374889071936)          # torch._dynamo.graph_break()  # mp/ipykernel_532/3015389759.py:7 in fn\n| | | | +- GuardManager: source=G['torch']._dynamo.graph_break, accessed_by=GetAttrGuardAccessor(graph_break), type=<class 'function'>, tag_safe=(True, False)\n| | | | | +- GuardManager: source=G['torch']._dynamo.graph_break.__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)\n| | | | | | +- ID_MATCH: ___check_obj_id(G['torch']._dynamo.graph_break.__code__, 140374703207360)  # torch._dynamo.graph_break()  # mp/ipykernel_532/3015389759.py:7 in fn\n\nGuard eval latency = 78.68 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\ntorchdynamo start compiling torch_dynamo_resume_in_fn_at_7 /tmp/ipykernel_532/3015389759.py:7, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/3015389759.py\", line 12, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n\nStep 1: torchdynamo start tracing torch_dynamo_resume_in_fn_at_7 /tmp/ipykernel_532/3015389759.py:7\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/3015389759.py:7 in torch_dynamo_resume_in_fn_at_7\n        torch._dynamo.graph_break()\nTRACE LOAD_CONST True []\nTRACE STORE_FAST __is_tracing_resume_prologue [ConstantVariable(bool: True)]\nTRACE LOAD_FAST ___stack0 []\nTRACE LOAD_CONST False [LazyVariableTracker()]\nTRACE STORE_FAST __is_tracing_resume_prologue [LazyVariableTracker(), ConstantVariable(bool: False)]\nTRACE JUMP_ABSOLUTE 70 [LazyVariableTracker()]\nTRACE POP_TOP None [LazyVariableTracker()]\nTRACE starts_line /tmp/ipykernel_532/3015389759.py:8 in torch_dynamo_resume_in_fn_at_7\n        try:\nTRACE SETUP_FINALLY 84 []\nTRACE starts_line /tmp/ipykernel_532/3015389759.py:9 in torch_dynamo_resume_in_fn_at_7\n            return x + 1\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 1 [LazyVariableTracker()]\nTRACE BINARY_ADD None [LazyVariableTracker(), ConstantVariable(int: 1)]\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE POP_BLOCK None [TensorVariable()]\nTRACE RETURN_VALUE None [TensorVariable()]\nStep 1: torchdynamo done tracing torch_dynamo_resume_in_fn_at_7 (RETURN_VALUE)\nRETURN_VALUE triggered compile\nCOMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_532/3015389759.py, line 9 in torch_dynamo_resume_in_fn_at_7>], graph_break=False)\nTRACED GRAPH\n ===== __compiled_fn_38_5cdd28f0_bd4c_41fe_864e_fe626167383a =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/3015389759.py:9 in torch_dynamo_resume_in_fn_at_7, code: return x + 1\n        add: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        return (add,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=3), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # return x + 1  # mp/ipykernel_532/3015389759.py:9 in torch_dynamo_resume_in_fn_at_7\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x + 1  # mp/ipykernel_532/3015389759.py:9 in torch_dynamo_resume_in_fn_at_7\n\nGuard eval latency = 61.02 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\n\n```\n\n\n```python\ntensor([1.5745, 2.3647, 1.8617])\n\n```\n\n\n## Hitting a Recompilation Limit#\n\nSeeChanging the Cache Size Limit.\n\n## Compiler Errors#\n\nSome compiler errors will result in skipped functions.\nOther compiler errors will result in a hard error rather than a skipped function.\n\n## Dealing with Skipped Functions#\n\nIn general, you can resolve a skipped function by fixing the underlying graph break or error that\nis causing the function to be skipped.\nIf the graph break/error causing the skipped function is difficult to fix,\nthen consider isolating the graph break/error in its own function so that minimal things are skipped.\n\n```python\ndef inner1(x):\n    return x + 1\ndef inner2(x):\n    return x + 2\n@torch.compile\ndef fn(x):\n    x = inner1(x)\n    def problematic_code():\n        torch._dynamo.skip_frame()\n    problematic_code()\n    x = inner2(x)\nfn(torch.randn(3))\n\n```\n\n\n```python\ntorchdynamo start compiling fn /tmp/ipykernel_532/273153676.py:5, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/273153676.py\", line 12, in <module>\n    fn(torch.randn(3))\n\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/273153676.py:5\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/273153676.py:7 in fn\n        x = inner1(x)\nTRACE LOAD_GLOBAL inner1 []\nTRACE LOAD_FAST x [LazyVariableTracker()]\nTRACE CALL_FUNCTION 1 [LazyVariableTracker(), LazyVariableTracker()]\nINLINING <code object inner1 at 0x7fabee5514d0, file \"/tmp/ipykernel_532/273153676.py\", line 1>, inlined according trace_rules.lookup inlined by default\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE starts_line /tmp/ipykernel_532/273153676.py:2 in inner1 (inline depth: 1)\n        return x + 1\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE RETURN_VALUE None [TensorVariable()]\nDONE INLINING <code object inner1 at 0x7fabee5514d0, file \"/tmp/ipykernel_532/273153676.py\", line 1>\nTRACE STORE_FAST x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/273153676.py:8 in fn\n        def problematic_code():\nTRACE LOAD_CONST <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8> []\nTRACE LOAD_CONST fn.<locals>.problematic_code [ConstantVariable(code: <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8>)]\nTRACE MAKE_FUNCTION 0 [ConstantVariable(code: <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8>), ConstantVariable(str: 'fn.<locals>.problematic_code')]\nTRACE STORE_FAST problematic_code [NestedUserFunctionVariable()]\nTRACE starts_line /tmp/ipykernel_532/273153676.py:10 in fn\n        problematic_code()\nTRACE LOAD_FAST problematic_code []\nTRACE CALL_FUNCTION 0 [NestedUserFunctionVariable()]\nINLINING <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8>, inlined according trace_rules.lookup inlined by default\nTRACE starts_line /tmp/ipykernel_532/273153676.py:9 in problematic_code (inline depth: 1)\n            torch._dynamo.skip_frame()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR skip_frame [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nSKIPPED INLINING <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8>: Skip frame due to `torch._dynamo.skip_frame()`. Message: None\nGraph break in user code at /tmp/ipykernel_532/273153676.py:10\nGraph Break Reason: SKIPPED INLINING <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8>: Skip frame due to `torch._dynamo.skip_frame()`. Message: None\nUser code traceback:\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/273153676.py\", line 12, in <module>\n    fn(torch.randn(3))\n  File \"/tmp/ipykernel_532/273153676.py\", line 10, in fn\n    problematic_code()\n\nRestarting analysis due to _dynamo/symbolic_convert.py:249 in fail_and_restart_analysis\nStep 1: torchdynamo start tracing fn /tmp/ipykernel_532/273153676.py:5\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/273153676.py:7 in fn\n        x = inner1(x)\nTRACE LOAD_GLOBAL inner1 []\nTRACE LOAD_FAST x [LazyVariableTracker()]\nTRACE CALL_FUNCTION 1 [LazyVariableTracker(), LazyVariableTracker()]\nINLINING <code object inner1 at 0x7fabee5514d0, file \"/tmp/ipykernel_532/273153676.py\", line 1>, inlined according trace_rules.lookup inlined by default\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE starts_line /tmp/ipykernel_532/273153676.py:2 in inner1 (inline depth: 1)\n        return x + 1\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 1 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 1)]\nTRACE RETURN_VALUE None [TensorVariable()]\nDONE INLINING <code object inner1 at 0x7fabee5514d0, file \"/tmp/ipykernel_532/273153676.py\", line 1>\nTRACE STORE_FAST x [TensorVariable()]\nTRACE starts_line /tmp/ipykernel_532/273153676.py:8 in fn\n        def problematic_code():\nTRACE LOAD_CONST <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8> []\nTRACE LOAD_CONST fn.<locals>.problematic_code [ConstantVariable(code: <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8>)]\nTRACE MAKE_FUNCTION 0 [ConstantVariable(code: <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8>), ConstantVariable(str: 'fn.<locals>.problematic_code')]\nTRACE STORE_FAST problematic_code [NestedUserFunctionVariable()]\nTRACE starts_line /tmp/ipykernel_532/273153676.py:10 in fn\n        problematic_code()\nTRACE LOAD_FAST problematic_code []\nTRACE CALL_FUNCTION 0 [NestedUserFunctionVariable()]\nCOMPILING GRAPH due to GraphCompileReason(reason='SKIPPED INLINING <code object problematic_code at 0x7fab85cc7e10, file \"/tmp/ipykernel_532/273153676.py\", line 8>: Skip frame due to `torch._dynamo.skip_frame()`. Message: None', user_stack=[<FrameSummary file /tmp/ipykernel_532/273153676.py, line 10 in fn>], graph_break=True)\nTRACED GRAPH\n ===== __compiled_fn_41_48d91d4a_4ea8_4f47_a3d3_109b14872333 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/273153676.py:2 in inner1, code: return x + 1\n        x: \"f32[3][1]cpu\" = l_x_ + 1;  l_x_ = None\n        return (x,)\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=0), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # x = inner1(x)  # mp/ipykernel_532/273153676.py:7 in fn\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # x = inner1(x)  # mp/ipykernel_532/273153676.py:7 in fn\n| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)\n| | +- GuardManager: source=G['inner1'], accessed_by=DictGetItemGuardAccessor('inner1'), type=<class 'function'>, tag_safe=(True, False)\n| | | +- GuardManager: source=G['inner1'].__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)\n| | | | +- ID_MATCH: ___check_obj_id(G['inner1'].__code__, 140376414688464)        # x = inner1(x)  # mp/ipykernel_532/273153676.py:7 in fn\n\nGuard eval latency = 1067.03 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\ntorchdynamo start compiling problematic_code /tmp/ipykernel_532/273153676.py:8, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/273153676.py\", line 12, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n\nStep 1: torchdynamo start tracing problematic_code /tmp/ipykernel_532/273153676.py:8\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/273153676.py:9 in problematic_code\n            torch._dynamo.skip_frame()\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR _dynamo [LazyVariableTracker()]\nTRACE LOAD_ATTR skip_frame [LazyVariableTracker()]\nTRACE CALL_FUNCTION 0 [LazyVariableTracker()]\nSkipping frame Skip frame due to `torch._dynamo.skip_frame()`. Message: None problematic_code                 /tmp/ipykernel_532/273153676.py 8\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\ntorchdynamo start compiling torch_dynamo_resume_in_fn_at_10 /tmp/ipykernel_532/273153676.py:10, stack (elided 5 frames):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    app.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 758, in start\n    self.io_loop.start()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n    self._run_once()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n    handle._run()\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/utils.py\", line 71, in preserve_context\n    return await f(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n    await self.dispatch_shell(msg, subshell_id=subshell_id)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n    await result\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n    await super().execute_request(stream, ident, parent)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3006, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3061, in _run_cell\n    result = runner(coro)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3266, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3445, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_532/273153676.py\", line 12, in <module>\n    fn(torch.randn(3))\n  File \"/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 832, in compile_wrapper\n    return fn(*args, **kwargs)\n\nStep 1: torchdynamo start tracing torch_dynamo_resume_in_fn_at_10 /tmp/ipykernel_532/273153676.py:10\ncreate_env\nTRACE starts_line /tmp/ipykernel_532/273153676.py:10 in torch_dynamo_resume_in_fn_at_10\n        problematic_code()\nTRACE LOAD_CONST True []\nTRACE STORE_FAST __is_tracing_resume_prologue [ConstantVariable(bool: True)]\nTRACE LOAD_FAST ___stack0 []\nTRACE LOAD_CONST False [LazyVariableTracker()]\nTRACE STORE_FAST __is_tracing_resume_prologue [LazyVariableTracker(), ConstantVariable(bool: False)]\nTRACE JUMP_ABSOLUTE 32 [LazyVariableTracker()]\nTRACE POP_TOP None [LazyVariableTracker()]\nTRACE starts_line /tmp/ipykernel_532/273153676.py:11 in torch_dynamo_resume_in_fn_at_10\n        x = inner2(x)\nTRACE LOAD_GLOBAL inner2 []\nTRACE LOAD_FAST x [LazyVariableTracker()]\nTRACE CALL_FUNCTION 1 [LazyVariableTracker(), LazyVariableTracker()]\nINLINING <code object inner2 at 0x7fab86277b50, file \"/tmp/ipykernel_532/273153676.py\", line 3>, inlined according trace_rules.lookup inlined by default\nwrap_to_fake L['x'] (3,) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>], dynamic_strides=[<DimDynamic.INFER_STRIDE: 4>], constraint_sizes=[None], constraint_strides=[None], specialize_on=[[]], view_base_context=None, tensor_source=LocalSource(local_name='x', is_input=True, dynamism=None, is_derefed_cell_contents=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>\ncreate_graph_input L_x_ L['x'] FakeTensor(..., size=(3,)) at debug_level 0 before=False\nTRACE starts_line /tmp/ipykernel_532/273153676.py:4 in inner2 (inline depth: 1)\n        return x + 2\nTRACE LOAD_FAST x []\nTRACE LOAD_CONST 2 [TensorVariable()]\nTRACE BINARY_ADD None [TensorVariable(), ConstantVariable(int: 2)]\nTRACE RETURN_VALUE None [TensorVariable()]\nDONE INLINING <code object inner2 at 0x7fab86277b50, file \"/tmp/ipykernel_532/273153676.py\", line 3>\nTRACE STORE_FAST x [TensorVariable()]\nTRACE LOAD_CONST None []\nTRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]\nStep 1: torchdynamo done tracing torch_dynamo_resume_in_fn_at_10 (RETURN_VALUE)\nRETURN_VALUE triggered compile\nCOMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /tmp/ipykernel_532/273153676.py, line 11 in torch_dynamo_resume_in_fn_at_10>], graph_break=False)\nTRACED GRAPH\n ===== __compiled_fn_45_b13efbe6_821e_4bde_a769_2f883cd09f46 =====\n /opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3][1]cpu\"):\n        l_x_ = L_x_\n        \n         # File: /tmp/ipykernel_532/273153676.py:4 in inner2, code: return x + 2\n        x: \"f32[3][1]cpu\" = l_x_ + 2;  l_x_ = x = None\n        return ()\n        \n\nStep 2: calling compiler function eager\nStep 2: done compiler function eager\nproduce_guards\ntrack_symint L['x'].size()[0] 3 None\ntrack_symint L['x'].stride()[0] 1 None\ntrack_symint L['x'].storage_offset() 0 None\nSkipping guard L['x'].size()[0] == 3\nSkipping guard L['x'].stride()[0] == 1\nSkipping guard L['x'].storage_offset() == 0\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- LAMBDA_GUARD: torch._functorch.aot_autograd.utils.top_saved_tensors_hooks ids == None  # _dynamo/output_graph.py:688 in init_ambient_guards\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:676 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=FrameLocalsGuardAccessor(key='x', framelocals_idx=3), type=<class 'torch.Tensor'>, tag_safe=(True, False)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3], stride=[1])  # x = inner2(x)  # mp/ipykernel_532/273153676.py:11 in torch_dynamo_resume_in_fn_at_10\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # x = inner2(x)  # mp/ipykernel_532/273153676.py:11 in torch_dynamo_resume_in_fn_at_10\n| +- GuardManager: source=G, accessed_by=GlobalsGuardAccessor, type=<class 'dict'>, tag_safe=(True, False)\n| | +- GuardManager: source=G['inner2'], accessed_by=DictGetItemGuardAccessor('inner2'), type=<class 'function'>, tag_safe=(True, False)\n| | | +- GuardManager: source=G['inner2'].__code__, accessed_by=CodeGuardAccessor, type=<class 'code'>, tag_safe=(True, False)\n| | | | +- ID_MATCH: ___check_obj_id(G['inner2'].__code__, 140374666869584)        # x = inner2(x)  # mp/ipykernel_532/273153676.py:11 in torch_dynamo_resume_in_fn_at_10\n\nGuard eval latency = 1082.61 us\nput_code_state: no cache key, skipping\nrun_gc_after_compile: running gc\n\n```\n",
    "url": "https://pytorch.org/docs/stable/compile/programming_model.skipped_functions.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "10e040f8bc046961151219fa7f092110",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_dynamo_overview.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "501f17aa69d4b9299a39518c0502397f",
    "source": "pytorch_docs",
    "title": "Extending torch.func with autograd.Function \u2014 PyTorch 2.9 documentation",
    "text": "\n## Extending torch.func with autograd.Function#\n\nCreated On: Jan 03, 2023 | Last Updated On: Sep 14, 2023\nSo you\u2019d like to usetorch.autograd.Functionwith thetorch.functransforms liketorch.vmap(),torch.func.grad(), etc.\ntorch.autograd.Function\ntorch.func\ntorch.vmap()\ntorch.func.grad()\nThere are two main use cases:\nyou wish to call code that does not contain PyTorch operations and\nhave it work with function transforms. That is, thetorch.autograd.Function\u2019s\nforward/backward/etc calls into functions from other systems like C++, CUDA, numpy.\ntorch.autograd.Function\nyou wish to specify custom gradient rules, like\nJAX\u2019scustom_vjp/custom_jvp\nPyTorch combines both of these concepts intotorch.autograd.Function.\ntorch.autograd.Function\n\n## Basic Usage#\n\nThis guide assumes you are familiar withExtending torch.autograd,\nwhich explains how to usetorch.autograd.Function.\ntorch.autograd.Function\ntorch.autograd.Functioncan either have aforward()that accepts a ctx object,\nor it can have separateforward()(that does not acceptctx) and asetup_context()staticmethod that modifies thectxobject.\ntorch.autograd.Function\nforward()\nforward()\nctx\nsetup_context()\nctx\nOnly the latter is supported with function transforms:\nforward()is the code that performs the operation and it should not accept\nactxobject.\nforward()\nctx\nsetup_context(ctx,inputs,output)is the code where you can\ncall methods onctx. Here is where you should save Tensors for backward\n(by callingctx.save_for_backward(*tensors)), or save non-Tensors\n(by assigning them to thectxobject).\nsetup_context(ctx,inputs,output)\nctx\nctx.save_for_backward(*tensors)\nctx\nBecausesetup_context()accepts onlyinputsandoutput,\nthe only quantities that can be saved are either objects (such as Tensors) in\nthe inputs or outputs or quantities (likeTensor.shape) derived from them.\nIf you wish to save a non-input intermediate activation fromFunction.forward()for backward, then you\u2019ll need to return it as an\noutput fromforward()so that it gets passed tosetup_context().\nsetup_context()\ninputs\noutput\nTensor.shape\nFunction.forward()\nforward()\nsetup_context()\nDepending on the transform,\nto support reverse-mode AD (torch.func.grad(),torch.func.vjp()),\nthetorch.autograd.Functionneeds abackward()staticmethod.\ntorch.func.grad()\ntorch.func.vjp()\ntorch.autograd.Function\nbackward()\nto supporttorch.vmap(), thetorch.autograd.Functionneeds avmap()staticmethod.\ntorch.vmap()\ntorch.autograd.Function\nvmap()\nto supporttorch.func.jvp(), thetorch.autograd.Functionneeds ajvp()staticmethod.\ntorch.func.jvp()\ntorch.autograd.Function\njvp()\nto support compositions of transforms (liketorch.func.jacrev(),torch.func.jacfwd(),torch.func.hessian()) \u2013 you may need multiple\nof the above.\ntorch.func.jacrev()\ntorch.func.jacfwd()\ntorch.func.hessian()\nIn order for thetorch.autograd.Functionto be arbitrarily composable with function\ntransforms, we recommend that all other staticmethods other thanforward()andsetup_context()must be transformable: that is, they must consist of only PyTorch\noperators or call othertorch.autograd.Function(that may call into C++/CUDA/etc).\ntorch.autograd.Function\nforward()\nsetup_context()\ntorch.autograd.Function\nLet\u2019s go over some examples of common use cases.\n\n## Example 1: autograd.Function calls into another system#\n\nA common case is atorch.autograd.Functionwith both forward() and backward() calling\ninto another system (like C++, CUDA, numpy, triton).\ntorch.autograd.Function\n\n```python\nimport torch\nimport numpy as np\n\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    # Note that forward does not take ctx\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        # Any intermediates to be saved in backward must be returned as\n        # outputs.\n        return (\n            # The desired output\n            torch.tensor(result, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind, device=device),\n            # intermediate to save for backward\n            torch.tensor(ind_inv, device=device),\n        )\n\n    # setup_context is responsible for calling methods and/or assigning to\n    # the ctx object. Please do not do additional compute (e.g. add\n    # Tensors together) in setup_context.\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        # Note that output is whatever you returned from forward.\n        # If you returned multiple values, then output is a Tuple of multiple values.\n        # If you returned a single Tensor, then output is a Tensor.\n        # If you returned a Tuple with a single Tensor, then output is a\n        # Tuple with a single Tensor.\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        # Tensors must be saved via ctx.save_for_backward. Please do not\n        # assign them directly onto the ctx object.\n        ctx.save_for_backward(ind, ind_inv)\n        # Non-tensors may be saved by assigning them as attributes on the ctx object.\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        # For the autograd.Function to be arbitrarily composable with function\n        # transforms, all staticmethod other than forward and setup_context\n        # must be implemented in a \"transformable\" way; that is, they must\n        # only consist of PyTorch operations or autograd.Function.\n        #\n        # For example, this allows us to do double backwards and/or compute\n        # second order gradients.\n        #\n        # We've written the backward pass of NumpySort in terms of another\n        # autograd.Function, NumpyTake.\n        ind, ind_inv = ctx.saved_tensors\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None\n\n```\n\nNow, to make it easier to useNumpySort(to hide away the intermediates we\nreturned as outputs, as well as allow default args and kwargs), we create a new\nfunction that invokes it:\nNumpySort\n\n```python\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result\n\n```\n\nAnd here\u2019s a sanity check:\n\n```python\nx = torch.randn(2, 3)\ngrad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)\nassert torch.allclose(grad_x, torch.ones_like(x))\n\n```\n\n\n## Example 2: autograd.Function specifies custom gradient rules#\n\nAnother common case is antorch.autograd.Functionthat is implemented with PyTorch\noperations. PyTorch is able to compute gradients for PyTorch operations automatically,\nbut perhaps we wish to customize how the gradients are computed. Some reasons why\nwe may want a custom backward different from the one PyTorch gives us are:\ntorch.autograd.Function\nimproving numeric stability\nchanging the performance characteristics of the backward\nchanging how edge cases are handled (e.g. nans, inf)\nmodifying the gradient (e.g. gradient clipping)\nHere\u2019s an example of antorch.autograd.Functionfor the functiony=x**3where we\nchange the performance characteristics (some computation that would normally happen\nduring the backward pass, computing dx, happens in the forward pass).\ntorch.autograd.Function\ny=x**3\n\n```python\nclass MyCube(torch.autograd.Function):\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        # In regular PyTorch, if we had just run y = x ** 3, then the backward\n        # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done\n        # that computation here in the forward pass instead.\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`.\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\n```\n\nNow, to make it easier to useNumpySort(and hide away the intermediates we\nreturned as outputs) we create a new function that invokes it:\nNumpySort\n\n```python\ndef my_cube(x):\n    result, _ = MyCube.apply(x)\n    return result\n\n```\n\nHere\u2019s a sanity check computing the second-order gradients:\n\n```python\nx = torch.randn([])\nggx = torch.func.grad(torch.func.grad(my_cube))(x)\nassert torch.allclose(ggx, 6 * x)\n\n```\n\n\n## Limitations and gotchas#\n\nWarning\nPlease read these limitations oftorch.autograd.Functionwith torch.func transforms\ncarefully. We are not able to catch many of these situations and error out\ngracefully so they will lead to undefined behavior.\ntorch.autograd.Function\nPlease do not capture Tensors that are being transformed over, have\nrequires_grad=True, or are dual tensors, into the methods of thetorch.autograd.Function. The way to be completely safe is to ensure that the only\nTensors being used inside any method of thetorch.autograd.Functionmust be directly\npassed as inputs (or via the ctx object) rather than come from outside\nthetorch.autograd.Function.\ntorch.autograd.Function\ntorch.autograd.Function\ntorch.autograd.Function\ntorch.autograd.Functiondoes not handle Tensors in pytrees (arbitrary nested\nPython data structures that may or may not contain Tensors). For\nthose Tensors to be tracked by autograd, they must be passed directly as\nan argument totorch.autograd.Function. This is in contrast to\njax.{custom_vjp, custom_jvp}, which do accept pytrees.\ntorch.autograd.Function\ntorch.autograd.Function\nPlease only usesave_for_backward()orsave_for_forward()to save Tensors.\nPlease do not assign Tensors or collections of Tensors directly onto the ctx object -\nthese Tensors will not get tracked\nsave_for_backward()\nsave_for_forward()\n\n## torch.vmap()Support#\n\ntorch.vmap()\nTo use antorch.autograd.Functionwithtorch.vmap(), you must either:\ntorch.autograd.Function\ntorch.vmap()\nprovide avmap()staticmethod that tells us the behavior of thetorch.autograd.Functionundertorch.vmap()\nvmap()\ntorch.autograd.Function\ntorch.vmap()\nask us to autogenerate it by settinggenerate_vmap_rule=True.\ngenerate_vmap_rule=True\n\n## Automatically generate a vmap rule#\n\nIf yourtorch.autograd.Functionfulfills the following additional constraints, then we\nare able to generate a vmap rule for it. If it doesn\u2019t fulfill the constraints or if you\nwant custom behavior under vmap, please manually define a vmap staticmethod (see next section).\ntorch.autograd.Function\nWarning\nWe are not easily able to check for the following constraints and error\nout gracefully. Violation of the constraints may lead to undefined\nbehavior.\nThetorch.autograd.Function\u2019sforward(),backward()(if it exists) andjvp()(if it exists) staticmethods must be transformable viatorch.vmap(). That\nis, they must consist of only PyTorch operations (as opposed to e.g. NumPy or custom\nCUDA kernels).\ntorch.autograd.Function\nforward()\nbackward()\njvp()\ntorch.vmap()\nExample:\n\n```python\nclass MyCube(torch.autograd.Function):\n    # Set generate_vmap_rule to True to ask PyTorch to automatically generate\n    # a vmap rule.\n    generate_vmap_rule = True\n\n    @staticmethod\n    def forward(x):\n        result = x ** 3\n        dx = 3 * x ** 2\n        return result, dx\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, = inputs\n        result, dx = output\n        ctx.save_for_backward(x, dx)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_dx):\n        x, dx = ctx.saved_tensors\n        result = grad_output * dx + grad_dx * 6 * x\n        return result\n\ndef my_cube(x):\n    result, dx = MyCube.apply(x)\n    return result\n\nx = torch.randn(3)\nresult = torch.vmap(my_cube)(x)\nassert torch.allclose(result, x ** 3)\n\n```\n\n\n## Defining the vmap staticmethod#\n\nIf yourtorch.autograd.Functioncalls into another system (like NumPy, C++, CUDA, triton),\nthen to get it to work withtorch.vmap()or transforms that use it, you\u2019ll\nneed to manually define avmap()staticmethod.\ntorch.autograd.Function\ntorch.vmap()\nvmap()\nDepending on what transforms you want to use and your use case, you may not need\nto add avmap()staticmethod to all of yourtorch.autograd.Function:\nvmap()\ntorch.autograd.Function\nFor example,torch.func.jacrev()performsvmap()over the backward pass.\nSo if you\u2019re only interested in usingtorch.func.jacrev(), only\nthebackward()staticmethod needs to be vmappable.\ntorch.func.jacrev()\nvmap()\ntorch.func.jacrev()\nbackward()\nWe do recommend ensuring all of yourtorch.autograd.Functionhave support fortorch.vmap()though, especially if you are writing a third-party library and you want yourtorch.autograd.Functionto work with all combinations oftorch.func()transforms.\ntorch.autograd.Function\ntorch.vmap()\ntorch.autograd.Function\ntorch.func()\nConceptually, the vmap staticmethod is responsible for defining how theforward()should behave undertorch.vmap(). That is, it defines how to transform\ntheforward()to run over inputs with an additional dimension (the dimension\nbeing vmapped over). This is similar to howtorch.vmap()is implemented over\nPyTorch operations: for each operation, we define a vmap rule (sometimes also\nreferred to as a \u201cbatching rule\u201d).\nforward()\ntorch.vmap()\nforward()\ntorch.vmap()\nHere\u2019s how to define thevmap()staticmethod:\nvmap()\nthe signature isvmap(info,in_dims:Tuple[Optional[int]],*args), where*argsis the same as the args toforward().\nvmap(info,in_dims:Tuple[Optional[int]],*args)\n*args\nforward()\nThe vmap staticmethod is responsible for defining how theforward()should behave\nundertorch.vmap(). That is, given inputs with an additional dimension\n(specified byin_dims), how do we compute the batched version offorward()?\nforward()\ntorch.vmap()\nin_dims\nforward()\nFor each arg inargs,in_dimshas a correspondingOptional[int].\nIt isNoneif the arg is not a Tensor or if the arg is not being vmapped over,\notherwise, it is an integer specifying what dimension of the Tensor is being vmapped\nover.\nargs\nin_dims\nOptional[int]\nNone\ninfois a collection of additional metadata that may be helpful:info.batch_sizespecifies the size of the dimension being vmapped over, whileinfo.randomnessis therandomnessoption that was passed totorch.vmap().\ninfo\ninfo.batch_size\ninfo.randomness\nrandomness\ntorch.vmap()\nThe return of the vmap staticmethod is a tuple of(output,out_dims). Similar\ntoin_dims,out_dimsshould be of the same structure asoutputand contain\noneout_dimper output that specifies if the output has the vmapped\ndimension and what index it is in.\n(output,out_dims)\nin_dims\nout_dims\noutput\nout_dim\nExample:\n\n```python\ndef to_numpy(tensor):\n    return tensor.cpu().numpy()\n\nclass NumpySort(torch.autograd.Function):\n    @staticmethod\n    def forward(x, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = np.argsort(x, axis=dim)\n        ind_inv = np.argsort(ind, axis=dim)\n        result = np.take_along_axis(x, ind, axis=dim)\n        return (\n            torch.tensor(result, device=device),\n            torch.tensor(ind, device=device),\n            torch.tensor(ind_inv, device=device),\n        )\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, dim = inputs\n        _, ind, ind_inv = output\n        ctx.mark_non_differentiable(ind, ind_inv)\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output, _0, _1):\n        ind, ind_inv = ctx.saved_tensors\n        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None\n\n    # The signature of the vmap staticmethod is:\n    # vmap(info, in_dims: Tuple[Optional[int]], *args)\n    # where *args is the same as the arguments to `forward`.\n    @staticmethod\n    def vmap(info, in_dims, x, dim):\n        # For every input (x and dim), in_dims stores an Optional[int]\n        # that is:\n        # - None if the input is not being vmapped over or if the input\n        #   is not a Tensor\n        # - an integer if the input is being vmapped over that represents\n        #   the index of the dimension being vmapped over.\n        x_bdim, _ = in_dims\n\n        # A \"vmap rule\" is the logic of how to perform the operation given\n        # inputs with one additional dimension. In NumpySort, x has an\n        # additional dimension (x_bdim). The vmap rule is simply\n        # to call NumpySort again but pass it a different `dim`.\n        x = x.movedim(x_bdim, 0)\n        # Handle negative dims correctly\n        dim = dim if dim >= 0 else dim + x.dim() - 1\n        result = NumpySort.apply(x, dim + 1)\n\n        # The vmap rule must return a tuple of two things\n        # 1. the output. Should be the same amount of things\n        #    as returned by the forward().\n        # 2. one Optional[int] for each output specifying if each output\n        # is being vmapped over, and if so, the index of the\n        # dimension being vmapped over.\n        #\n        # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the\n        # dimension being vmapped over to the front of `x`, that appears at\n        # dimension 0 of all outputs.\n        # The return is (output, out_dims) -- output is a tuple of 3 Tensors\n        # and out_dims is a Tuple of 3 Optional[int]\n        return NumpySort.apply(x, dim + 1), (0, 0, 0)\n\nclass NumpyTake(torch.autograd.Function):\n    @staticmethod\n    def forward(x, ind, ind_inv, dim):\n        device = x.device\n        x = to_numpy(x)\n        ind = to_numpy(ind)\n        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)\n\n    @staticmethod\n    def setup_context(ctx, inputs, output):\n        x, ind, ind_inv, dim = inputs\n        ctx.save_for_backward(ind, ind_inv)\n        ctx.dim = dim\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        ind, ind_inv = ctx.saved_tensors\n        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)\n        return result, None, None, None\n\n    @staticmethod\n    def vmap(info, in_dims, x, ind, ind_inv, dim):\n        x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims\n\n        # The strategy is: expand {x, ind, ind_inv} to all have the dimension\n        # being vmapped over.\n        # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim).\n\n        # Handle negative dims by wrapping them to be positive\n        logical_dim = x.dim() if x_bdim is None else x_bdim - 1\n        dim = dim if dim >= 0 else dim + logical_dim\n\n        def maybe_expand_bdim_at_front(x, x_bdim):\n            if x_bdim is None:\n                return x.expand(info.batch_size, *x.shape)\n            return x.movedim(x_bdim, 0)\n\n        # If the Tensor doesn't have the dimension being vmapped over,\n        # expand it out. Otherwise, move it to the front of the Tensor\n        x = maybe_expand_bdim_at_front(x, x_bdim)\n        ind = maybe_expand_bdim_at_front(ind, ind_bdim)\n        ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)\n\n        # The return is a tuple (output, out_dims). Since output is a Tensor,\n        # then out_dims is an Optional[int] (instead of being a Tuple).\n        return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0\n\ndef numpy_sort(x, dim=-1):\n    result, _, _ = NumpySort.apply(x, dim)\n    return result\n\nx = torch.randn(2, 3)\nresult = torch.vmap(numpy_sort)(x)\nassert torch.allclose(result, numpy_sort(result, 1))\n\n```\n\nNote\nThe vmap staticmethod should aim to preserve the semantics of the\nentireFunction. That is, (pseudocode)grad(vmap(MyFunc))should be replaceable with agrad(map(MyFunc)).\nFunction\ngrad(vmap(MyFunc))\ngrad(map(MyFunc))\nIf your autograd.Function has any custom behavior in the backward pass, please\nkeep this in mind.\nNote\nIt is a legitimate use case to write a custom vmap staticmethod for aFunctionthat PyTorch is able to generate a vmap\nrule for viagenerate_vmap_rule=True. You may wish to do this if the\ngenerated vmap rule doesn\u2019t have the semantics you\u2019re looking for.\nFunction\ngenerate_vmap_rule=True\n\n## torch.func.jvp()Support#\n\ntorch.func.jvp()\nTo support forward-mode AD, atorch.autograd.Functionmust have ajvp()staticmethod.\nPlease seeForward mode ADfor details.\ntorch.autograd.Function\njvp()",
    "url": "https://pytorch.org/docs/stable/notes/extending.func.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "634b5e38737ecb5f23bacedd9f1dd226",
    "source": "pytorch_docs",
    "title": "torch.fft \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.fft#\n\nCreated On: Aug 06, 2020 | Last Updated On: Jun 13, 2025\nDiscrete Fourier transforms and related functions.\n\n## Fast Fourier Transforms#\n\nfft\n\nfft\nComputes the one dimensional discrete Fourier transform ofinput.\ninput\nifft\n\nifft\nComputes the one dimensional inverse discrete Fourier transform ofinput.\ninput\nfft2\n\nfft2\nComputes the 2 dimensional discrete Fourier transform ofinput.\ninput\nifft2\n\nifft2\nComputes the 2 dimensional inverse discrete Fourier transform ofinput.\ninput\nfftn\n\nfftn\nComputes the N dimensional discrete Fourier transform ofinput.\ninput\nifftn\n\nifftn\nComputes the N dimensional inverse discrete Fourier transform ofinput.\ninput\nrfft\n\nrfft\nComputes the one dimensional Fourier transform of real-valuedinput.\ninput\nirfft\n\nirfft\nComputes the inverse ofrfft().\nrfft()\nrfft2\n\nrfft2\nComputes the 2-dimensional discrete Fourier transform of realinput.\ninput\nirfft2\n\nirfft2\nComputes the inverse ofrfft2().\nrfft2()\nrfftn\n\nrfftn\nComputes the N-dimensional discrete Fourier transform of realinput.\ninput\nirfftn\n\nirfftn\nComputes the inverse ofrfftn().\nrfftn()\nhfft\n\nhfft\nComputes the one dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.\ninput\nihfft\n\nihfft\nComputes the inverse ofhfft().\nhfft()\nhfft2\n\nhfft2\nComputes the 2-dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.\ninput\nihfft2\n\nihfft2\nComputes the 2-dimensional inverse discrete Fourier transform of realinput.\ninput\nhfftn\n\nhfftn\nComputes the n-dimensional discrete Fourier transform of a Hermitian symmetricinputsignal.\ninput\nihfftn\n\nihfftn\nComputes the N-dimensional inverse discrete Fourier transform of realinput.\ninput\n\n## Helper Functions#\n\nfftfreq\n\nfftfreq\nComputes the discrete Fourier Transform sample frequencies for a signal of sizen.\nn\nrfftfreq\n\nrfftfreq\nComputes the sample frequencies forrfft()with a signal of sizen.\nrfft()\nn\nfftshift\n\nfftshift\nReorders n-dimensional FFT data, as provided byfftn(), to have negative frequency terms first.\nfftn()\nifftshift\n\nifftshift\nInverse offftshift().\nfftshift()",
    "url": "https://pytorch.org/docs/stable/fft.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "4563e111ee3fb3011f8c76cbb73c5664",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/export/ir_spec.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "e88c7054a1655c25a969fc975ae5ac27",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/torch.compiler_dynamic_shapes.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b6aa4856e9582163bb5e12b15c893c6a",
    "source": "pytorch_docs",
    "title": "CPU threading and TorchScript inference \u2014 PyTorch 2.9 documentation",
    "text": "\n## CPU threading and TorchScript inference#\n\nCreated On: Jul 29, 2019 | Last Updated On: Jul 15, 2025\nWarning\nTorchScript is deprecated, please usetorch.exportinstead.",
    "url": "https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "1ccc01ae43031e0f23cd23919be9838c",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/generated/torch.nn.functional.scaled_dot_product_attention.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "cbdcf55def4ee4991519f7b13b42fbb4",
    "source": "pytorch_docs",
    "title": "Serialization semantics \u2014 PyTorch 2.9 documentation",
    "text": "\n## Serialization semantics#\n\nCreated On: Feb 26, 2017 | Last Updated On: Jun 23, 2025\nThis note describes how you can save and load PyTorch tensors and module states\nin Python, and how to serialize Python modules so they can be loaded in C++.\nTable of Contents\nSerialization semantics\nSaving and loading tensors\nSaving and loading tensors preserves views\nSaving and loading torch.nn.Modules\nSerialized file format fortorch.save\ntorch.save\nLayout Control\ntorch.loadwithweights_only=True\ntorch.load\nweights_only=True\nTroubleshootingweights_only\nweights_only\nGetting unsafe globals\nEnvironment Variables\nUtility functions\nConfig\n\n## Saving and loading tensors#\n\ntorch.save()andtorch.load()let you easily save and load tensors:\ntorch.save()\ntorch.load()\n\n```python\n>>> t = torch.tensor([1., 2.])\n>>> torch.save(t, 'tensor.pt')\n>>> torch.load('tensor.pt')\ntensor([1., 2.])\n\n```\n\nBy convention, PyTorch files are typically written with a \u2018.pt\u2019 or \u2018.pth\u2019 extension.\ntorch.save()andtorch.load()use Python\u2019s pickle by default,\nso you can also save multiple tensors as part of Python objects like tuples,\nlists, and dicts:\ntorch.save()\ntorch.load()\n\n```python\n>>> d = {'a': torch.tensor([1., 2.]), 'b': torch.tensor([3., 4.])}\n>>> torch.save(d, 'tensor_dict.pt')\n>>> torch.load('tensor_dict.pt')\n{'a': tensor([1., 2.]), 'b': tensor([3., 4.])}\n\n```\n\nCustom data structures that include PyTorch tensors can also be saved if the\ndata structure is pickle-able.\n\n## Saving and loading tensors preserves views#\n\nSaving tensors preserves their view relationships:\n\n```python\n>>> numbers = torch.arange(1, 10)\n>>> evens = numbers[1::2]\n>>> torch.save([numbers, evens], 'tensors.pt')\n>>> loaded_numbers, loaded_evens = torch.load('tensors.pt')\n>>> loaded_evens *= 2\n>>> loaded_numbers\ntensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])\n\n```\n\nBehind the scenes, these tensors share the same \u201cstorage.\u201d SeeTensor Viewsfor more\non views and storage.\nWhen PyTorch saves tensors it saves their storage objects and tensor\nmetadata separately. This is an implementation detail that may change in the\nfuture, but it typically saves space and lets PyTorch easily\nreconstruct the view relationships between the loaded tensors. In the above\nsnippet, for example, only a single storage is written to \u2018tensors.pt\u2019.\nIn some cases, however, saving the current storage objects may be unnecessary\nand create prohibitively large files. In the following snippet a storage much\nlarger than the saved tensor is written to a file:\n\n```python\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small, 'small.pt')\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n999\n\n```\n\nInstead of saving only the five values in thesmalltensor to \u2018small.pt,\u2019\nthe 999 values in the storage it shares withlargewere saved and loaded.\nWhen saving tensors with fewer elements than their storage objects, the size of\nthe saved file can be reduced by first cloning the tensors. Cloning a tensor\nproduces a new tensor with a new storage object containing only the values\nin the tensor:\n\n```python\n>>> large = torch.arange(1, 1000)\n>>> small = large[0:5]\n>>> torch.save(small.clone(), 'small.pt')  # saves a clone of small\n>>> loaded_small = torch.load('small.pt')\n>>> loaded_small.storage().size()\n5\n\n```\n\nSince the cloned tensors are independent of each other, however, they have\nnone of the view relationships the original tensors did. If both file size and\nview relationships are important when saving tensors smaller than their\nstorage objects, then care must be taken to construct new tensors that minimize\nthe size of their storage objects but still have the desired view relationships\nbefore saving.\n\n## Saving and loading torch.nn.Modules#\n\nSee also:Tutorial: Saving and loading modules\nIn PyTorch, a module\u2019s state is frequently serialized using a \u2018state dict.\u2019\nA module\u2019s state dict contains all of its parameters and persistent buffers:\n\n```python\n>>> bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> list(bn.named_parameters())\n[('weight', Parameter containing: tensor([1., 1., 1.], requires_grad=True)),\n ('bias', Parameter containing: tensor([0., 0., 0.], requires_grad=True))]\n\n>>> list(bn.named_buffers())\n[('running_mean', tensor([0., 0., 0.])),\n ('running_var', tensor([1., 1., 1.])),\n ('num_batches_tracked', tensor(0))]\n\n>>> bn.state_dict()\nOrderedDict([('weight', tensor([1., 1., 1.])),\n             ('bias', tensor([0., 0., 0.])),\n             ('running_mean', tensor([0., 0., 0.])),\n             ('running_var', tensor([1., 1., 1.])),\n             ('num_batches_tracked', tensor(0))])\n\n```\n\nInstead of saving a module directly, for compatibility reasons it is recommended\nto instead save only its state dict. Python modules even have a function,load_state_dict(), to restore their states from a state dict:\nload_state_dict()\n\n```python\n>>> torch.save(bn.state_dict(), 'bn.pt')\n>>> bn_state_dict = torch.load('bn.pt')\n>>> new_bn = torch.nn.BatchNorm1d(3, track_running_stats=True)\n>>> new_bn.load_state_dict(bn_state_dict)\n<All keys matched successfully>\n\n```\n\nNote that the state dict is first loaded from its file withtorch.load()and the state then restored withload_state_dict().\ntorch.load()\nload_state_dict()\nEven custom modules and modules containing other modules have state dicts and\ncan use this pattern:\n\n```python\n# A module with two linear layers\n>>> class MyModule(torch.nn.Module):\n      def __init__(self):\n        super().__init__()\n        self.l0 = torch.nn.Linear(4, 2)\n        self.l1 = torch.nn.Linear(2, 1)\n\n      def forward(self, input):\n        out0 = self.l0(input)\n        out0_relu = torch.nn.functional.relu(out0)\n        return self.l1(out0_relu)\n\n>>> m = MyModule()\n>>> m.state_dict()\nOrderedDict([('l0.weight', tensor([[ 0.1400, 0.4563, -0.0271, -0.4406],\n                                   [-0.3289, 0.2827, 0.4588, 0.2031]])),\n             ('l0.bias', tensor([ 0.0300, -0.1316])),\n             ('l1.weight', tensor([[0.6533, 0.3413]])),\n             ('l1.bias', tensor([-0.1112]))])\n\n>>> torch.save(m.state_dict(), 'mymodule.pt')\n>>> m_state_dict = torch.load('mymodule.pt')\n>>> new_m = MyModule()\n>>> new_m.load_state_dict(m_state_dict)\n<All keys matched successfully>\n\n```\n\n\n## Serialized file format fortorch.save#\n\ntorch.save\nSince PyTorch 1.6.0,torch.savedefaults to returning an uncompressed ZIP64\narchive unless the user sets_use_new_zipfile_serialization=False.\ntorch.save\n_use_new_zipfile_serialization=False\nIn this archive, the files are ordered as such\n\n```python\ncheckpoint.pth\n\u251c\u2500\u2500 data.pkl\n\u251c\u2500\u2500 byteorder  # added in PyTorch 2.1.0\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 0\n\u2502   \u251c\u2500\u2500 1\n\u2502   \u251c\u2500\u2500 2\n\u2502   \u2514\u2500\u2500 \u2026\n\u2514\u2500\u2500 version\n\n```\n\ndata.pklis the result of pickling the object passed totorch.saveexcludingtorch.Storageobjects that it contains\ndata.pkl\ntorch.save\ntorch.Storage\nbyteordercontains a string with thesys.byteorderwhen saving (\u201clittle\u201d or \u201cbig\u201d)\nbyteorder\nsys.byteorder\ndata/contains all the storages in the object, where each storage is a separate file\ndata/\nversioncontains a version number at save time that can be used at load time\nversion\nWhen saving, PyTorch will ensure that the local file header of each file is padded\nto an offset that is a multiple of 64 bytes, ensuring that the offset of each file\nis 64-byte aligned.\nNote\nTensors on certain devices such as XLA are serialized as pickled numpy arrays. As\nsuch, their storages are not serialized. In these casesdata/might not exist\nin the checkpoint.\ndata/\n\n## Layout Control#\n\nThemmapargument intorch.load()allows for lazy loading of tensor storages.\nmmap\ntorch.load()\nIn addition, there are some advanced features that allow for more fine-grained\ncontrol and manipulation of atorch.savecheckpoint.\ntorch.save\ntorch.serialization.skip_data\nSaving a checkpoint withtorch.savethat includes empty space for data bytes\nto be written later.\ntorch.save\nLoading a checkpoint withtorch.loadand filling in the data bytes of tensors later.\ntorch.load\nTo inspect tensor metadata in atorch.savecheckpoint without allocating memory for storage\ndata, usetorch.loadwithin theFakeTensorModecontext manager. On top of skipping loading\nstorage data similar toskip_dataabove, it additionally tags storages with their offset within\nthe checkpoint, enabling direct checkpoint manipulation.\ntorch.save\ntorch.load\nFakeTensorMode\nskip_data\n\n```python\nimport torch.nn as nn\nfrom torch._subclasses.fake_tensor import FakeTensorMode\n\nm = nn.Linear(10, 10)\ntorch.save(m.state_dict(), \"checkpoint.pt\")\n\nwith FakeTensorMode() as mode:\n    fake_sd = torch.load(\"checkpoint.pt\")\n\nfor k, v in fake_sd.items():\n    print(f\"key={k}, dtype={v.dtype}, shape={v.shape}, stride={v.stride()}, storage_offset={v.storage_offset()}\")\n    # offset of the storage in the checkpoint\n    print(f\"key={k}, checkpoint_offset={v.untyped_storage()._checkpoint_offset}\")\n\n```\n\nFor more information,this tutorialoffers a comprehensive example of using these features to manipulate a checkpoint.\n\n## torch.loadwithweights_only=True#\n\ntorch.load\nweights_only=True\nStarting in version 2.6,torch.loadwill useweights_only=Trueif thepickle_moduleargument is not passed.\ntorch.load\nweights_only=True\npickle_module\nAs discussed in the documentation fortorch.load(),weights_only=Truerestricts\nthe unpickler used intorch.loadto only executing functions/building classes required forstate_dictsof plaintorch.Tensorsas well as some other primitive types. Further,\nunlike the defaultUnpicklerprovided by thepicklemodule, theweights_onlyUnpickler\nis not allowed to dynamically import anything during unpickling.\ntorch.load()\nweights_only=True\ntorch.load\nstate_dicts\ntorch.Tensors\nUnpickler\npickle\nweights_only\nAs mentioned above, saving a module\u2019sstate_dictis a best practice when usingtorch.save. If loading an old\ncheckpoint that contains annn.Module, we recommendweights_only=False. When loading a checkpoint that contains\ntensor subclasses, there will likely be functions/classes that need to be allowlisted, see below for further details.\nstate_dict\ntorch.save\nnn.Module\nweights_only=False\nIf theweights_onlyUnpickler encounters a function or class that is not allowlisted\nby default within the pickle file, you should see an actionable error like such\nweights_only\n\n```python\n_pickle.UnpicklingError: Weights only load failed. This file can still be loaded,\nto do so you have two options, do those steps only if you trust the source of the checkpoint.\n    1. Re-running `torch.load` with `weights_only` set to `False` will likely succeed,\n        but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n    2. Alternatively, to load with `weights_only=True` please check the recommended\n       steps in the following error message.\n       WeightsUnpickler error: Unsupported global: GLOBAL {__module__}.{__name__} was not an allowed global by\n       default. Please use `torch.serialization.add_safe_globals([{__name__}])` or the\n       `torch.serialization.safe_globals([{__name__}])` context manager to allowlist this global\n       if you trust this class/function.\n\n```\n\nPlease follow the steps in the error message and allowlist the functions or classes only if you trust them.\nTo get all GLOBALs (functions/classes) in the checkpoint that are not yet allowlisted you can usetorch.serialization.get_unsafe_globals_in_checkpoint()which will return a list of strings of the form{__module__}.{__name__}. If you trust these functions/classes, you can import them and allowlist them per\nthe error message either viatorch.serialization.add_safe_globals()or the context managertorch.serialization.safe_globals.\ntorch.serialization.get_unsafe_globals_in_checkpoint()\n{__module__}.{__name__}\ntorch.serialization.add_safe_globals()\ntorch.serialization.safe_globals\nTo access the list of user-allowlisted functions/classes you can usetorch.serialization.get_safe_globals()and\nto clear the current list seetorch.serialization.clear_safe_globals().\ntorch.serialization.get_safe_globals()\ntorch.serialization.clear_safe_globals()\n\n## Troubleshootingweights_only#\n\nweights_only\nA caveat is thattorch.serialization.get_unsafe_globals_in_checkpoint()analyzes the checkpoint statically,\nsome types might be built dynamically during the unpickling process and hence will not be reported bytorch.serialization.get_unsafe_globals_in_checkpoint(). One such example isdtypesin numpy. Innumpy<1.25after allowlisting all the functions/classes reported bytorch.serialization.get_unsafe_globals_in_checkpoint()you might see an error like\ntorch.serialization.get_unsafe_globals_in_checkpoint()\ntorch.serialization.get_unsafe_globals_in_checkpoint()\ndtypes\nnumpy<1.25\ntorch.serialization.get_unsafe_globals_in_checkpoint()\n\n```python\nWeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,\nbut got <class 'numpy.dtype[float32]'>\n\n```\n\nThis can be allowlisted via{add_}safe_globals([type(np.dtype(np.float32))]).\n{add_}safe_globals([type(np.dtype(np.float32))])\nInnumpy>=1.25you would see\nnumpy>=1.25\n\n```python\nWeightsUnpickler error: Can only build Tensor, Parameter, OrderedDict or types allowlisted via `add_safe_globals`,\nbut got <class 'numpy.dtypes.Float32DType'>\n\n```\n\nThis can be allowlisted via{add_}safe_globals([np.dtypes.Float32DType]).\n{add_}safe_globals([np.dtypes.Float32DType])\nThere are two environment variables that will influence the behavior oftorch.load. These can be helpful\nif one does not have access to thetorch.loadcallsites.\ntorch.load\ntorch.load\nTORCH_FORCE_WEIGHTS_ONLY_LOAD=1will override alltorch.loadcallsites to useweights_only=True.\nTORCH_FORCE_WEIGHTS_ONLY_LOAD=1\ntorch.load\nweights_only=True\nTORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1will maketorch.loadcallsites useweights_only=Falseonlyifweights_onlywas not passed as an argument.\nTORCH_FORCE_NO_WEIGHTS_ONLY_LOAD=1\ntorch.load\nweights_only=False\nweights_only\n\n## Utility functions#\n\nThe following utility functions are related to serialization:\nRegisters callables for tagging and deserializing storage objects with an associated priority.\nTagging associates a device with a storage object at save time while deserializing moves a\nstorage object to an appropriate device at load time.taggeranddeserializerare run in the order given by theirpriorityuntil a tagger/deserializer returns a\nvalue that is notNone.\ntagger\ndeserializer\npriority\nTo override the deserialization behavior for a device in the global registry, one can register a\ntagger with a higher priority than the existing tagger.\nThis function can also be used to register a tagger and deserializer for new devices.\npriority(int) \u2013 Indicates the priority associated with the tagger and deserializer, where a lower\nvalue indicates higher priority.\ntagger(Callable[[Union[Storage,TypedStorage,UntypedStorage]],Optional[str]]) \u2013 Callable that takes in a storage object and returns its tagged device as a string\nor None.\ndeserializer(Callable[[Union[Storage,TypedStorage,UntypedStorage],str],Optional[Union[Storage,TypedStorage,UntypedStorage]]]) \u2013 Callable that takes in storage object and a device string and returns a storage\nobject on the appropriate device or None.\nNone\nExample\n\n```python\n>>> def ipu_tag(obj):\n>>>     if obj.device.type == 'ipu':\n>>>         return 'ipu'\n>>> def ipu_deserialize(obj, location):\n>>>     if location.startswith('ipu'):\n>>>         ipu = getattr(torch, \"ipu\", None)\n>>>         assert ipu is not None, \"IPU device module is not loaded\"\n>>>         assert torch.ipu.is_available(), \"ipu is not available\"\n>>>         return obj.ipu(location)\n>>> torch.serialization.register_package(11, ipu_tag, ipu_deserialize)\n\n```\n\nGet whethertorch.save()computes and writes crc32 for each record.\ntorch.save()\nDefaults toTrue.\nTrue\nbool\nSet whethertorch.save()computes and writes crc32 for each record.\ntorch.save()\nNote\nSetting this toFalsemay make unzipping of thetorch.saveoutput\nfail or warn due to corrupted CRC32. Howevertorch.loadwill be\nable to load the file.\nFalse\ntorch.save\ntorch.load\ncompute_crc32(bool) \u2013 set crc32 computation flag\nGet fallback byte order for loading files\nIf byteorder mark is not present in saved checkpoint,\nthis byte order is used as fallback.\nBy default, it\u2019s \u201cnative\u201d byte order.\nOptional[LoadEndianness]\ndefault_load_endian\nSet fallback byte order for loading files\nIf byteorder mark is not present in saved checkpoint,\nthis byte order is used as fallback.\nBy default, it\u2019s \u201cnative\u201d byte order.\nendianness\u2013 the new fallback byte order\nGet default mmap options fortorch.load()withmmap=True.\ntorch.load()\nmmap=True\nDefaults tommap.MAP_PRIVATE.\nmmap.MAP_PRIVATE\nint\ndefault_mmap_options\nContext manager or function to set default mmap options fortorch.load()withmmap=Trueto flags.\ntorch.load()\nmmap=True\nFor now, only eithermmap.MAP_PRIVATEormmap.MAP_SHAREDare supported.\nPlease open an issue if you need any other option to be added here.\nmmap.MAP_PRIVATE\nmmap.MAP_SHARED\nNote\nThis feature is currently not supported for Windows.\nflags(int) \u2013mmap.MAP_PRIVATEormmap.MAP_SHARED\nmmap.MAP_PRIVATE\nmmap.MAP_SHARED\nMarks the given globals as safe forweights_onlyload. For example, functions\nadded to this list can be called during unpickling, classes could be instantiated\nand have state set.\nweights_only\nEach item in the list can either be a function/class or a tuple of the form\n(function/class, string) where string is the full path of the function/class.\nWithin the serialized format, each function is identified with its full\npath as{__module__}.{__qualname__}. When calling this API, you can provide this\nfull path that should match the one in the checkpoint otherwise the default{fn.__module__}.{fn.__qualname__}will be used.\n{__module__}.{__qualname__}\n{fn.__module__}.{fn.__qualname__}\nsafe_globals(List[Union[Callable,Tuple[Callable,str]]]) \u2013 list of globals to mark as safe\nExample\n\n```python\n>>> import tempfile\n>>> class MyTensor(torch.Tensor):\n...     pass\n>>> t = MyTensor(torch.randn(2, 3))\n>>> with tempfile.NamedTemporaryFile() as f:\n...     torch.save(t, f.name)\n# Running `torch.load(f.name, weights_only=True)` will fail with\n# Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.\n# Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.\n...     torch.serialization.add_safe_globals([MyTensor])\n...     torch.load(f.name, weights_only=True)\n# MyTensor([[-0.5024, -1.8152, -0.5455],\n#          [-0.8234,  2.0500, -0.3657]])\n\n```\n\nClears the list of globals that are safe forweights_onlyload.\nweights_only\nReturns the list of user-added globals that are safe forweights_onlyload.\nweights_only\nlist[Union[Callable,tuple[Callable,str]]]\nReturns a list of strings of functions/classes in atorch.saveobject that are not safe forweights_only.\ntorch.save\nweights_only\nFor a given function or classf, the corresponding string will be of the form{f.__module__}.{f.__name__}.\nf\n{f.__module__}.{f.__name__}\nThis function will return any GLOBALs in the checkpoint that are not in the set marked safe\nforweights_only(either viaadd_safe_globals()orsafe_globalscontext or\nallowlisted bytorchby default).\nweights_only\nadd_safe_globals()\nsafe_globals\ntorch\nNote\nThis function will statically disassemble the pickle file in the checkpoint.\nThe implication is any classes dynamically pushed onto the stack during unpickling\nwill not be included in the output.\nf(Union[str,PathLike[str],IO[bytes]]) \u2013 File-like object or string containing the checkpoint object saved viatorch.save\ntorch.save\nA list of strings of pickle GLOBALs in the checkpoint that are not allowlisted forweights_only.\nweights_only\nlist[str]\nContext-manager that adds certain globals as safe forweights_onlyload.\nweights_only\nsafe_globals(list[Union[Callable,tuple[Callable,str]]]) \u2013 List of globals for weights_only load.\nExample\n\n```python\n>>> import tempfile\n>>> class MyTensor(torch.Tensor):\n...     pass\n>>> t = MyTensor(torch.randn(2, 3))\n>>> with tempfile.NamedTemporaryFile() as f:\n...     torch.save(t, f.name)\n# Running `torch.load(f.name, weights_only=True)` will fail with\n# Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.\n# Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.\n...     with torch.serialization.safe_globals([MyTensor]):\n...         torch.load(f.name, weights_only=True)\n# MyTensor([[-0.5024, -1.8152, -0.5455],\n#          [-0.8234,  2.0500, -0.3657]])\n>>> assert torch.serialization.get_safe_globals() == []\n\n```\n\nContext-manager that skips writing/reading storage bytes fortorch.save/torch.loadcalls.\ntorch.save\ntorch.load\nFor the save path, storages will still be saved, but the space that their bytes would usually be written to\nwill be empty space. The storage bytes can then be populated in a separate pass.\nFor the load path, tensors will be loaded per the checkpoint but their storages will not be populated with data.\nWarning\nTheskip_datacontext manager is an early prototype and is subject to change.\nskip_data\nmaterialize_fake_tensors(bool) \u2013 Whether to materialize FakeTensors during save. This is a no-op for the load path.\nExample\n\n```python\n>>> import tempfile\n>>> t = torch.randn(2, 3)\n>>> with tempfile.NamedTemporaryFile() as f:\n...     with torch.serialization.skip_data():\n...         torch.save(t, f.name)\n...     torch.load(f.name, weights_only=True)\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n```\n\n\n## Config#\n\ntorch.utils.serialization.configprovides a global config that can control the behavior oftorch.saveandtorch.load.\ntorch.utils.serialization.config\ntorch.save\ntorch.load\ntorch.utils.serialization.config.savecontains options that control the behavior oftorch.save.\ntorch.utils.serialization.config.save\ntorch.save\ncompute_crc32: whether to compute and write the zip file checksum (Default :True).\nSeeset_crc32_options().\ncompute_crc32\nTrue\nset_crc32_options()\nuse_pinned_memory_for_d2h: for storages that are on an accelerator when passed totorch.save, whether to\nmove storage to pinned memory or pageable memory on CPU withintorch.save. (Default:False(i.e. pageable))\nuse_pinned_memory_for_d2h\ntorch.save\ntorch.save\nFalse\nstorage_alignment: alignment of storages in the checkpoint duringtorch.savein bytes. (Default64)\nstorage_alignment\ntorch.save\n64\ntorch.utils.serialization.config.loadcontains options that control the behavior oftorch.load.\ntorch.utils.serialization.config.load\ntorch.load\nmmap: See the documentation formmapargument intorch.load().\nThis config will set the behavior ofmmapfortorch.loadif it is not\nalready explicitly passed to thetorch.loadcall (Default :False).\nmmap\nmmap\ntorch.load()\nmmap\ntorch.load\ntorch.load\nFalse\nendianness: Seeset_default_load_endianness().\n(Default :torch.serialization.LoadEndianness.NATIVE)\nendianness\nset_default_load_endianness()\ntorch.serialization.LoadEndianness.NATIVE\nmmap_flags: Seeset_default_mmap_options.\n(Default :MAP_PRIVATE)\nmmap_flags\nset_default_mmap_options\nMAP_PRIVATE\ncalculate_storage_offsets: If this config is set toTrue, offsets for storages will be\ncalculated rather than read via random reads when usingtorch.load(mmap=True). This minimizes\nrandom reads, which can be helpful when the file is being loaded over a network. (Default :False)\ncalculate_storage_offsets\nTrue\ntorch.load(mmap=True)\nFalse",
    "url": "https://pytorch.org/docs/stable/notes/serialization.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b76dd3c092a1610184738f2c093075fb",
    "source": "pytorch_docs",
    "title": "torch.compile has different autograd semantics \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.compilehas different autograd semantics#\n\ntorch.compile\nCreated On: Jun 26, 2025 | Last Updated On: Jun 26, 2025\nWhen you applytorch.compileto a function in your model\u2019s forward pass,\nit will automatically generate a backward pass for the compiled function.\nDuring compilation, it will trace out a graph for the backward pass that\nis used whenever autograd is invoked. We refer to the component insidetorch.compilethat is responsible for this asAOTDispatcher(sometimes known asAOTAutograd).\ntorch.compile\ntorch.compile\nAOTDispatcher\nAOTAutograd\nAs so,torch.compilebakes in details of the computation into the\ntraced-out backward graph during compilation of the function\nin the forward pass.\nHowever, in eager-mode PyTorch, the backward computation is dynamic:\noutside of the forward pass, you can wrap the call totensor.backward()ortorch.autograd.grad(...)in a context manager that may change its behavior.\ntorch.compile\ntensor.backward()\ntorch.autograd.grad(...)\nThis page documents howtorch.compile\u2019s autograd semantics differ from\neager-mode PyTorch and how to work around it.\ntorch.compile\n\n## Autocastbehavior#\n\nAutocast\ntorch.compilebakes in an assumption on if the backward pass will be\nrun under an ambient autocast context manager. By default,\nUsetorch._functorch.config.backward_pass_autocastto control that assumption; an incorrect assumption may lead to silent\nincorrectness.\ntorch.compile\ntorch._functorch.config.backward_pass_autocast\nThe options are either:\n\"same_as_forward\"(default).\nWe assume that the backward of thetorch.compile\u2019ed region\nwill be run under the same autocast context manager that the region was run\nunder (if any). Use this if your code looks like the following:\n\"same_as_forward\"\ntorch.compile\n\n```python\nwith torch.amp.autocast(...):\n    y = torch.compile(region)(x)\n    ...\n    # backward pass run under the same autocast context as the compiled region\n    z.backward()\n\n```\n\n\"off\". We assume that the backward of the torch.compile\u2019d region will\nnot be run under any autocast context managers.\nUse this if your code looks like the following:\n\"off\"\n\n```python\nwith torch.amp.autocast(...):\n    y = torch.compile(region)(x)\n    ...\n# Backward pass runs under no autocast.\nz.backward()\n\n```\n\nThere is a third option. If you settorch._functorch.config.backward_pass_autocastto a list of kwargs, we will assume the backward pass runs under an autocast context\nconstructed by those kwargs.\ntorch._functorch.config.backward_pass_autocast\nFor example, if your code looks like the following:\n\n```python\ny = torch.compile(region)(x)\n...\n# Backward pass runs under special context manager\nwith torch.amp.autocast(**kwargs):\n    z.backward()\n\n```\n\nthen settorch._functorch.config.backward_pass_autocast=kwargs.\ntorch._functorch.config.backward_pass_autocast=kwargs\nUsepatchto apply the option to a specifictorch.compilecall:\npatch\ntorch.compile\n\n```python\nwith torch.amp.autocast(...):\n    with torch._functorch.config.patch(backward_pass_autocast=\"same_as_forward\")\n    y = torch.compile(region)(x)\n    ...\n    # backward pass run under the same autocast context as the compiled region\n    z.backward()\n\n```\n",
    "url": "https://pytorch.org/docs/stable/torch.compiler_backward.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "b073a940d873dfb5756cbcc23615371b",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/config_mod.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "ec55f5344bbcff13a20732f19f7518a9",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/utils.md.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "cff1ca30881a1ae6d72499f5122cdd9b",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_images/local_dependencies.png",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "046a70fdf79941038a119ea219c5b6dc",
    "source": "pytorch_docs",
    "title": "",
    "text": "",
    "url": "https://pytorch.org/docs/stable/_sources/bottleneck.rst.txt",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "fcb48a1f87a43f09fdc8dee162e32d88",
    "source": "pytorch_docs",
    "title": "torch.utils.tensorboard \u2014 PyTorch 2.9 documentation",
    "text": "\n## torch.utils.tensorboard#\n\nCreated On: Apr 25, 2019 | Last Updated On: Mar 10, 2022\nBefore going further, more details on TensorBoard can be found athttps://www.tensorflow.org/tensorboard/\nOnce you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.\nThe SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example:\n\n```python\nimport torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\n\n# Writer will output to ./runs/ directory by default\nwriter = SummaryWriter()\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\nmodel = torchvision.models.resnet50(False)\n# Have ResNet model take in grayscale rather than RGB\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nimages, labels = next(iter(trainloader))\n\ngrid = torchvision.utils.make_grid(images)\nwriter.add_image('images', grid, 0)\nwriter.add_graph(model, images)\nwriter.close()\n\n```\n\nThis can then be visualized with TensorBoard, which should be installable\nand runnable with:\n\n```python\npip install tensorboard\ntensorboard --logdir=runs\n\n```\n\nLots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface.\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nwriter = SummaryWriter()\n\nfor n_iter in range(100):\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n\n```\n\nExpected result:\nWrites entries directly to event files in the log_dir to be consumed by TensorBoard.\nTheSummaryWriterclass provides a high-level API to create an event file\nin a given directory and add summaries and events to it. The class updates the\nfile contents asynchronously. This allows a training program to call methods\nto add data to the file directly from the training loop, without slowing down\ntraining.\nCreate aSummaryWriterthat will write out events and summaries to the event file.\nlog_dir(str) \u2013 Save directory location. Default is\nruns/CURRENT_DATETIME_HOSTNAME, which changes after each run.\nUse hierarchical folder structure to compare\nbetween runs easily. e.g. pass in \u2018runs/exp1\u2019, \u2018runs/exp2\u2019, etc.\nfor each new experiment to compare across them.\ncomment(str) \u2013 Comment log_dir suffix appended to the defaultlog_dir. Iflog_diris assigned, this argument has no effect.\nlog_dir\nlog_dir\npurge_step(int) \u2013 When logging crashes at stepT+XT+XT+Xand restarts at stepTTT,\nany events whose global_step larger or equal toTTTwill be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the samelog_dir.\nlog_dir\nmax_queue(int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items.\nflush_secs(int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes.\nfilename_suffix(str) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter.\nExamples:\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\n\n# create a summary writer with automatically generated folder name.\nwriter = SummaryWriter()\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n# create a summary writer using the specified folder name.\nwriter = SummaryWriter(\"my_experiment\")\n# folder location: my_experiment\n\n# create a summary writer with comment appended.\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n\n```\n\nAdd scalar data to summary.\ntag(str) \u2013 Data identifier\nscalar_value(floatorstring/blobname) \u2013 Value to save\nglobal_step(int) \u2013 Global step value to record\nwalltime(float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event\nnew_style(boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading.\nExamples:\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nx = range(100)\nfor i in x:\n    writer.add_scalar('y=2x', i * 2, i)\nwriter.close()\n\n```\n\nExpected result:\nAdd many scalar data to summary.\nmain_tag(str) \u2013 The parent name for the tags\ntag_scalar_dict(dict) \u2013 Key-value pair storing the tag and corresponding values\nglobal_step(int) \u2013 Global step value to record\nwalltime(float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\nExamples:\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                    'xcosx':i*np.cos(i/r),\n                                    'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.\n\n```\n\nExpected result:\nAdd histogram to summary.\ntag(str) \u2013 Data identifier\nvalues(torch.Tensor,numpy.ndarray, orstring/blobname) \u2013 Values to build histogram\nglobal_step(int) \u2013 Global step value to record\nbins(str) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in:https://numpy.org/doc/stable/reference/generated/numpy.histogram.html\nwalltime(float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\nExamples:\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nwriter = SummaryWriter()\nfor i in range(10):\n    x = np.random.random(1000)\n    writer.add_histogram('distribution centers', x + i, i)\nwriter.close()\n\n```\n\nExpected result:\nAdd image data to summary.\nNote that this requires thepillowpackage.\npillow\ntag(str) \u2013 Data identifier\nimg_tensor(torch.Tensor,numpy.ndarray, orstring/blobname) \u2013 Image data\nglobal_step(int) \u2013 Global step value to record\nwalltime(float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\ndataformats(str) \u2013 Image data format specification of the form\nCHW, HWC, HW, WH, etc.\nimg_tensor: Default is(3,H,W)(3, H, W)(3,H,W). You can usetorchvision.utils.make_grid()to\nconvert a batch of tensor into 3xHxW format or calladd_imagesand let us do the job.\nTensor with(1,H,W)(1, H, W)(1,H,W),(H,W)(H, W)(H,W),(H,W,3)(H, W, 3)(H,W,3)is also suitable as long as\ncorrespondingdataformatsargument is passed, e.g.CHW,HWC,HW.\ntorchvision.utils.make_grid()\nadd_images\ndataformats\nCHW\nHWC\nHW\nExamples:\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nimg_HWC = np.zeros((100, 100, 3))\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter = SummaryWriter()\nwriter.add_image('my_image', img, 0)\n\n# If you have non-default dimension setting, set the dataformats argument.\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\nwriter.close()\n\n```\n\nExpected result:\nAdd batched image data to summary.\nNote that this requires thepillowpackage.\npillow\ntag(str) \u2013 Data identifier\nimg_tensor(torch.Tensor,numpy.ndarray, orstring/blobname) \u2013 Image data\nglobal_step(int) \u2013 Global step value to record\nwalltime(float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\ndataformats(str) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc.\nimg_tensor: Default is(N,3,H,W)(N, 3, H, W)(N,3,H,W). Ifdataformatsis specified, other shape will be\naccepted. e.g. NCHW or NHWC.\ndataformats\nExamples:\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nimg_batch = np.zeros((16, 3, 100, 100))\nfor i in range(16):\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\nwriter = SummaryWriter()\nwriter.add_images('my_image_batch', img_batch, 0)\nwriter.close()\n\n```\n\nExpected result:\nRender matplotlib figure into an image and add it to summary.\nNote that this requires thematplotlibpackage.\nmatplotlib\ntag(str) \u2013 Data identifier\nfigure(Union[Figure,list['Figure']]) \u2013 Figure or a list of figures\nglobal_step(Optional[int]) \u2013 Global step value to record\nclose(bool) \u2013 Flag to automatically close the figure\nwalltime(Optional[float]) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\nAdd video data to summary.\nNote that this requires themoviepypackage.\nmoviepy\ntag(str) \u2013 Data identifier\nvid_tensor(torch.Tensor) \u2013 Video data\nglobal_step(int) \u2013 Global step value to record\nfps(floatorint) \u2013 Frames per second\nwalltime(float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\nvid_tensor:(N,T,C,H,W)(N, T, C, H, W)(N,T,C,H,W). The values should lie in [0, 255] for typeuint8or [0, 1] for typefloat.\nAdd audio data to summary.\ntag(str) \u2013 Data identifier\nsnd_tensor(torch.Tensor) \u2013 Sound data\nglobal_step(int) \u2013 Global step value to record\nsample_rate(int) \u2013 sample rate in Hz\nwalltime(float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\nsnd_tensor:(1,L)(1, L)(1,L). The values should lie between [-1, 1].\nAdd text data to summary.\ntag(str) \u2013 Data identifier\ntext_string(str) \u2013 String to save\nglobal_step(int) \u2013 Global step value to record\nwalltime(float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\nExamples:\n\n```python\nwriter.add_text('lstm', 'This is an lstm', 0)\nwriter.add_text('rnn', 'This is an rnn', 10)\n\n```\n\nAdd graph data to summary.\nmodel(torch.nn.Module) \u2013 Model to draw.\ninput_to_model(torch.Tensororlistoftorch.Tensor) \u2013 A variable or a tuple of\nvariables to be fed.\nverbose(bool) \u2013 Whether to print graph structure in console.\nuse_strict_trace(bool) \u2013 Whether to pass keyword argumentstricttotorch.jit.trace. Pass False when you want the tracer to\nrecord your mutable container types (list, dict)\nAdd embedding projector data to summary.\nmat(torch.Tensorornumpy.ndarray) \u2013 A matrix which each row is the feature vector of the data point\nmetadata(list) \u2013 A list of labels, each element will be converted to string\nlabel_img(torch.Tensor) \u2013 Images correspond to each data point\nglobal_step(int) \u2013 Global step value to record\ntag(str) \u2013 Name for the embedding\nmetadata_header(list) \u2013 A list of headers for multi-column metadata. If given, each metadata must be\na list with values corresponding to headers.\nmat:(N,D)(N, D)(N,D), where N is number of data and D is feature dimension\nlabel_img:(N,C,H,W)(N, C, H, W)(N,C,H,W)\nExamples:\n\n```python\nimport keyword\nimport torch\nmeta = []\nwhile len(meta)<100:\n    meta = meta+keyword.kwlist # get some strings\nmeta = meta[:100]\n\nfor i, v in enumerate(meta):\n    meta[i] = v+str(i)\n\nlabel_img = torch.rand(100, 3, 10, 32)\nfor i in range(100):\n    label_img[i]*=i/100.0\n\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)\n\n```\n\nNote\nCategorical (i.e. non-numeric) metadata cannot have more than 50 unique values if they are to be used for\ncoloring in the embedding projector.\nAdd precision recall curve.\nPlotting a precision-recall curve lets you understand your model\u2019s\nperformance under different threshold settings. With this function,\nyou provide the ground truth labeling (T/F) and prediction confidence\n(usually the output of your model) for each target. The TensorBoard UI\nwill let you choose the threshold interactively.\ntag(str) \u2013 Data identifier\nlabels(torch.Tensor,numpy.ndarray, orstring/blobname) \u2013 Ground truth data. Binary label for each element.\npredictions(torch.Tensor,numpy.ndarray, orstring/blobname) \u2013 The probability that an element be classified as true.\nValue should be in [0, 1]\nglobal_step(int) \u2013 Global step value to record\nnum_thresholds(int) \u2013 Number of thresholds used to draw the curve.\nwalltime(float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\nExamples:\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nlabels = np.random.randint(2, size=100)  # binary label\npredictions = np.random.rand(100)\nwriter = SummaryWriter()\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\nwriter.close()\n\n```\n\nCreate special chart by collecting charts tags in \u2018scalars\u2019.\nNOTE: This function can only be called once for each SummaryWriter() object.\nBecause it only provides metadata to tensorboard, the function can be called before or after the training loop.\nlayout(dict) \u2013 {categoryName:charts}, wherechartsis also a dictionary\n{chartName:ListOfProperties}. The first element inListOfPropertiesis the chart\u2019s type\n(one ofMultilineorMargin) and the second element should be a list containing the tags\nyou have used in add_scalar function, which will be collected into the new chart.\nExamples:\n\n```python\nlayout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n\nwriter.add_custom_scalars(layout)\n\n```\n\nAdd meshes or 3D point clouds to TensorBoard.\nThe visualization is based on Three.js,\nso it allows users to interact with the rendered object. Besides the basic definitions\nsuch as vertices, faces, users can further provide camera parameter, lighting condition, etc.\nPlease checkhttps://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scenefor\nadvanced usage.\ntag(str) \u2013 Data identifier\nvertices(torch.Tensor) \u2013 List of the 3D coordinates of vertices.\ncolors(torch.Tensor) \u2013 Colors for each vertex\nfaces(torch.Tensor) \u2013 Indices of vertices within each triangle. (Optional)\nconfig_dict\u2013 Dictionary with ThreeJS classes names and configuration.\nglobal_step(int) \u2013 Global step value to record\nwalltime(float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event\nvertices:(B,N,3)(B, N, 3)(B,N,3). (batch, number_of_vertices, channels)\ncolors:(B,N,3)(B, N, 3)(B,N,3). The values should lie in [0, 255] for typeuint8or [0, 1] for typefloat.\nfaces:(B,N,3)(B, N, 3)(B,N,3). The values should lie in [0, number_of_vertices] for typeuint8.\nExamples:\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()\n\n```\n\nAdd a set of hyperparameters to be compared in TensorBoard.\nhparam_dict(dict) \u2013 Each key-value pair in the dictionary is the\nname of the hyper parameter and it\u2019s corresponding value.\nThe type of the value can be one ofbool,string,float,int, orNone.\nmetric_dict(dict) \u2013 Each key-value pair in the dictionary is the\nname of the metric and it\u2019s corresponding value. Note that the key used\nhere should be unique in the tensorboard record. Otherwise the value\nyou added byadd_scalarwill be displayed in hparam plugin. In most\ncases, this is unwanted.\nadd_scalar\nhparam_domain_discrete\u2013 (Optional[Dict[str, List[Any]]]) A dictionary that\ncontains names of the hyperparameters and all discrete values they can hold\nrun_name(str) \u2013 Name of the run, to be included as part of the logdir.\nIf unspecified, will use current timestamp.\nglobal_step(int) \u2013 Global step value to record\nExamples:\n\n```python\nfrom torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\n\n```\n\nExpected result:\nFlushes the event file to disk.\nCall this method to make sure that all pending events have been written to\ndisk.",
    "url": "https://pytorch.org/docs/stable/tensorboard.html",
    "metadata": {
      "section": null,
      "issue_number": null,
      "labels": null,
      "answer_author": null
    }
  },
  {
    "doc_id": "22d1c15b11337f14bdb4357ea2c7be78",
    "source": "github_issue",
    "title": "Update attention.cpp to remove warning about preferring torch.bool type",
    "text": "Question:\n\r\nUpdate attention.cpp to remove warning about preferring torch.bool data type \r\n\r\nFixes #100469 #97532\r\n\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/5539163457\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/103362",
    "metadata": {
      "section": null,
      "issue_number": 103362,
      "labels": [
        "better-engineering",
        "Merged",
        "ciflow/trunk",
        "release notes: nn"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "c2ec5acadbb36050950b9e9b69474c4f",
    "source": "github_issue",
    "title": "[FlexAttention] Fix broken eager tracing",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #143103\r\n* __->__ #143344\r\n* #143299\r\n\r\n\r\nFixes #143331\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov @Chillee @yanboliang @BoyuanFeng\n\nAnswer:\nThis PR (#143344) was merged in 5160a725c84827e2ff062feadf7b5ab66b389039 but it is still open, likely due to a Github bug, so mergebot is closing it manually.  If you think this is a mistake, please feel free to reopen and contact Dev Infra.",
    "url": "https://github.com/pytorch/pytorch/pull/143344",
    "metadata": {
      "section": null,
      "issue_number": 143344,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "topic: not user facing",
        "module: inductor",
        "module: dynamo",
        "ciflow/inductor",
        "ciflow/rocm",
        "module: flex attention"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "390731f32e7a31196502548d7d9a46d0",
    "source": "github_issue",
    "title": "Fix #149806 : Fix path lookup in _preload_cuda_deps",
    "text": "Question:\nFixes #149806\r\n\n\nAnswer:\nRecreated it because of some account issues... https://github.com/pytorch/pytorch/pull/149808",
    "url": "https://github.com/pytorch/pytorch/pull/149807",
    "metadata": {
      "section": null,
      "issue_number": 149807,
      "labels": [
        "open source"
      ],
      "answer_author": "Divain"
    }
  },
  {
    "doc_id": "5a36d43daba886bef049fdd4df0bb2d0",
    "source": "github_issue",
    "title": "Don't graph break on patched module methods or aliased methods",
    "text": "Question:\nSee added tests for the cases that were fixed.\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire\n\nAnswer:\nSplit into separate PRs https://github.com/pytorch/pytorch/pull/93309 and https://github.com/pytorch/pytorch/pull/93115.",
    "url": "https://github.com/pytorch/pytorch/pull/91018",
    "metadata": {
      "section": null,
      "issue_number": 91018,
      "labels": [
        "Merged",
        "Reverted",
        "ciflow/trunk",
        "module: dynamo",
        "ciflow/inductor",
        "release notes: dynamo"
      ],
      "answer_author": "williamwen42"
    }
  },
  {
    "doc_id": "4a770d17c60f02561199284c39726e72",
    "source": "github_issue",
    "title": "When nopython=True, Dynamo can't allow graph breaks.",
    "text": "Question:\nI count the number of sub-graphs (for tiny-GPT2 in huggingface) by\r\n```\r\n    class GraphCaptureCompiler:\r\n        def __init__(self):\r\n            self.captured_graphs = []\r\n        def compile(self, gm, example_inputs):\r\n            self.captured_graphs.append(gm)\r\n            return gm\r\n    compiler = GraphCaptureCompiler()\r\n    torch._dynamo.optimize(compiler, nopython=True)(Wrapper(fn))(*args)\r\n```\r\n\r\nAlthough `len(compiler.captured_graphs)` is 2, no error was thrown during the compilation. This observation conflicts with `nopython=True`. After some digging, I found a check is missed before making graph break. This PR adds it.\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @chunyuan-w @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/3952837408\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/90970",
    "metadata": {
      "section": null,
      "issue_number": 90970,
      "labels": [
        "triaged",
        "open source",
        "Merged",
        "Reverted",
        "ciflow/trunk",
        "module: dynamo",
        "ciflow/inductor"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "360f6c990a92780aa6bf4838ee5343cb",
    "source": "github_issue",
    "title": "Fix aten.copy device mismatch bug in FakeTensor",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #102664\r\n\r\nFixes `pytest ./generated/test_yizhou_wang_RODNet.py -k test_000` failure in https://github.com/pytorch/pytorch/issues/92670.\r\n\r\nFakeTensor would raise an error upon trying to run `aten.copy` with inputs with different devices, although this is allowed behavior.\r\n\r\nAlso fix `aten.slice_scatter`, since it also takes args with different devices.\r\n\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/5150048037\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/102664",
    "metadata": {
      "section": null,
      "issue_number": 102664,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "topic: not user facing",
        "module: fakeTensor",
        "ciflow/inductor"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "ab4e49a9cb1c32d3042be7b5bdadf3bb",
    "source": "github_issue",
    "title": "Fix flaky Dynamo export tests",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #96419\n* #96421\n* __->__ #96488\n\r\nPlanning to do a full writeup later. The short story is, sometimes the following chain of events happens:\r\n\r\n1. We turn on Dynamo's custom frame handler\r\n2. GC triggers (and all of the finalizers run under Dynamo)\r\n3. GC hits a GeneratorExit frame\r\n4. You end up in the custom frame handler with throw_flag == TRUE and PyErr_Occurred() != NULL\r\n\r\nIf this happens and we blindly call into other Python functions (like the Python callback), the executed Python code will immediately raise an exception (because there's already an ambient exception set.) This is very, very confusing. The fix is to defer to the regular handler when throw_flag is TRUE.\r\n\r\nI triggered this locally with\r\n\r\n```\r\nPYTHONUNBUFFERED=1 pytest test/dynamo/test_dynamic_shapes.py   -k 'Unspec and export and not dupes and not reorder' -v -x -s\r\n```\r\n\r\nBut I also have some tests which trigger the problem synthetically.\r\n\r\nFixes https://github.com/pytorch/pytorch/issues/93781\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\r\n\r\ncc @soumith @voznesenskym @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire\n\nAnswer:\n### Merge started\nYour change will be merged immediately since you used the force (-f) flag, **bypassing any CI checks** (ETA: 1-5 minutes).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/4388724002\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/96488",
    "metadata": {
      "section": null,
      "issue_number": 96488,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "topic: bug fixes",
        "module: dynamo",
        "release notes: dynamo"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "687a53d14a06c654ed5716c6d0398b4a",
    "source": "github_issue",
    "title": "Fixes nan with large bf16 values",
    "text": "Question:\nFixes #121558\r\n\r\nPerformance on main:\r\n``` Markdown\r\n+------------+-----------+-----------+------------+-----------+-----------+----------------+--------------------+--------------------+\r\n| batch_size | num_heads | q_seq_len | kv_seq_len | embed_dim | is_causal |     dtype      |    forward_time    |   backward_time    |\r\n+------------+-----------+-----------+------------+-----------+-----------+----------------+--------------------+--------------------+\r\n|     1      |    16     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 12.608132004970683 | 65.90210803551601  |\r\n|     1      |    16     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 11.75877740024589  | 64.83824399765581  |\r\n|     1      |    16     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 16.465420153690506 |  67.6770955324173  |\r\n|     1      |    16     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 17.398148600477725 | 68.19829455344006  |\r\n|     1      |    16     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 29.053532000398263 | 99.58901099162175  |\r\n|     1      |    16     |    512    |    512     |   2048    |   False   | torch.bfloat16 |  27.826815698063   | 98.05690299253911  |\r\n|     1      |    16     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 49.89655229728669  | 178.24282555375248 |\r\n|     1      |    16     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 48.840098950313404 | 174.5950729819015  |\r\n|     1      |    16     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 505.66218036692584 | 1865.9265094902366 |\r\n|     1      |    16     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 295.0534054543823  | 967.3831606050952  |\r\n|     1      |    32     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 11.496030446141958 | 55.11070846114308  |\r\n|     1      |    32     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 11.47399884648621  | 55.452342028729625 |\r\n|     1      |    32     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 13.216444296995178 | 55.14447903260589  |\r\n|     1      |    32     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 12.763233599252999 | 55.142355500720434 |\r\n|     1      |    32     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 19.409965351223946 |  74.9107634765096  |\r\n|     1      |    32     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 19.02470579952933  | 74.84168506925926  |\r\n|     1      |    32     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 46.37695319834165  | 172.19150450546294 |\r\n|     1      |    32     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 45.225963747361675 | 185.19691249821335 |\r\n|     1      |    32     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 634.3090848531574  | 2249.057865119539  |\r\n|     1      |    32     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 320.47313248040155 | 1053.0515247955916 |\r\n|     4      |    16     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 13.448987301671878 | 63.63581650657579  |\r\n|     4      |    16     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 12.509283400140703 | 63.059300999157124 |\r\n|     4      |    16     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 19.71098779467866  | 105.55780201684684 |\r\n|     4      |    16     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 18.264925852417946 | 105.12311349157244 |\r\n|     4      |    16     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 45.218703348655254 | 222.87272597895935 |\r\n|     4      |    16     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 43.55393464793451  | 230.63290398567915 |\r\n|     4      |    16     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 134.02968645095825 | 514.6893998607993  |\r\n|     4      |    16     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 157.13709802366793 | 624.5892751030624  |\r\n|     4      |    16     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 1776.7079547047617 | 6353.551096981391  |\r\n|     4      |    16     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 1143.6000745743513 | 3811.8767354171723 |\r\n|     4      |    32     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 11.717129248427227 | 55.35991647047922  |\r\n|     4      |    32     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 11.746983398916198 | 55.76716404175386  |\r\n|     4      |    32     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 17.255573300644752 | 106.47456656442955 |\r\n|     4      |    32     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 16.46409669774584  | 108.07770595420152 |\r\n|     4      |    32     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 46.63354124641045  | 213.74862996162847 |\r\n|     4      |    32     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 47.01801469782367  | 240.78139301855117 |\r\n|     4      |    32     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 127.76448752265424 | 508.08745552785695 |\r\n|     4      |    32     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 168.6308984644711  | 667.2996102133766  |\r\n|     4      |    32     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 2268.1598202325404 | 7727.2648515645415 |\r\n|     4      |    32     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 1242.8469699807465 | 4161.965740495361  |\r\n|     8      |    16     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 14.340955897932872 | 93.72280450770633  |\r\n|     8      |    16     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 13.25262250029482  |  93.2030284893699  |\r\n|     8      |    16     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 27.598425600444898 | 183.23776399483904 |\r\n|     8      |    16     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 26.362583553418514 | 183.51862096460536 |\r\n|     8      |    16     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 84.52303148806094  | 383.50319798337296 |\r\n|     8      |    16     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 89.41743348259479  | 432.5502900755964  |\r\n|     8      |    16     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 217.76640450116247 | 943.9354750793427  |\r\n|     8      |    16     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 303.0781910638325  | 1225.4394043702632 |\r\n|     8      |    16     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 3470.8542854059488 | 12194.579601055011 |\r\n|     8      |    16     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 2268.1174043100327 | 7608.0941944383085 |\r\n|     8      |    32     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 12.289720651460811 | 95.88620596332476  |\r\n|     8      |    32     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 11.618648946750909 | 95.56685149436818  |\r\n|     8      |    32     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 31.567946751601994 | 180.62468653079122 |\r\n|     8      |    32     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 28.611703700153157 | 189.4215695792809  |\r\n|     8      |    32     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 84.11306998459621  | 385.25596749968827 |\r\n|     8      |    32     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 93.82540901424363  | 455.77428903197875 |\r\n|     8      |    32     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 226.80530551588163 | 965.8026450779289  |\r\n|     8      |    32     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 327.4116570246406  | 1312.5067745568228 |\r\n|     8      |    32     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 4445.5064804060385 | 15020.768146496266 |\r\n|     8      |    32     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 2433.0302356975153 | 8300.016750581563  |\r\n+------------+-----------+-----------+------------+-----------+-----------+----------------+--------------------+--------------------+\r\n```\r\n\r\nPerformance on this branch:\r\n```Markdown\r\n+------------+-----------+-----------+------------+-----------+-----------+----------------+--------------------+--------------------+\r\n| batch_size | num_heads | q_seq_len | kv_seq_len | embed_dim | is_causal |     dtype      |    forward_time    |   backward_time    |\r\n+------------+-----------+-----------+------------+-----------+-----------+----------------+--------------------+--------------------+\r\n|     1      |    16     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 12.783618393586949 | 65.59692794689909  |\r\n|     1      |    16     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 12.064015300711617 | 56.99719698168337  |\r\n|     1      |    16     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 16.629025398287922 | 68.65267595276237  |\r\n|     1      |    16     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 17.462356004398313 | 68.35797848179936  |\r\n|     1      |    16     |    512    |    512     |   2048    |   True    | torch.bfloat16 |  29.5476081490051  | 101.22994752600789 |\r\n|     1      |    16     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 28.395320149138573 | 98.62275794148445  |\r\n|     1      |    16     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 50.50016101449728  | 181.4357690163888  |\r\n|     1      |    16     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 49.450615647947416 | 175.86063902126625 |\r\n|     1      |    16     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 506.06461532879626 | 1866.0613044630736 |\r\n|     1      |    16     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 299.9336270149797  | 976.4662646921353  |\r\n|     1      |    32     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 11.45752210286446  | 58.79682704107836  |\r\n|     1      |    32     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 11.407129396684468 | 58.14061599085107  |\r\n|     1      |    32     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 13.822759891627355 | 56.56979401828722  |\r\n|     1      |    32     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 13.39154909946956  |  56.7130644340068  |\r\n|     1      |    32     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 20.282494352431968 | 77.29688903782517  |\r\n|     1      |    32     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 19.899454596452415 |  75.4446149803698  |\r\n|     1      |    32     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 48.494275606935844 | 177.5322465109639  |\r\n|     1      |    32     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 46.84524350450374  | 189.1778860008344  |\r\n|     1      |    32     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 635.1026654010639  | 2248.0451600858937 |\r\n|     1      |    32     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 335.1591735263355  | 1080.4320796160027 |\r\n|     4      |    16     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 13.63953539985232  | 65.50709309522063  |\r\n|     4      |    16     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 12.858113402035087 | 63.021871959790595 |\r\n|     4      |    16     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 19.98318645055406  | 105.87883047992364 |\r\n|     4      |    16     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 18.619045056402683 | 104.90188701078296 |\r\n|     4      |    16     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 45.91175540117546  | 226.00732848513871 |\r\n|     4      |    16     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 44.39614630537107  | 232.39317198749632 |\r\n|     4      |    16     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 135.5409600073472  | 522.7949097752571  |\r\n|     4      |    16     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 158.79383607534692 | 628.5856699105352  |\r\n|     4      |    16     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 1775.9978299727663 | 6343.203847063706  |\r\n|     4      |    16     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 1160.680354805663  | 3842.235009651631  |\r\n|     4      |    32     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 11.553713708417488 | 65.50691701704638  |\r\n|     4      |    32     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 11.486379051348194 |  56.9980075233616  |\r\n|     4      |    32     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 17.56585600087419  | 107.89892700267956 |\r\n|     4      |    32     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 16.828144202008843 | 109.05519902007653 |\r\n|     4      |    32     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 48.23235589428805  | 217.8974545095116  |\r\n|     4      |    32     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 49.09284680034033  | 244.73925953498107 |\r\n|     4      |    32     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 134.77827049791813 | 522.7259948151186  |\r\n|     4      |    32     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 176.60772847011688 | 681.5171707421541  |\r\n|     4      |    32     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 2267.821540008299  | 7720.425300067291  |\r\n|     4      |    32     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 1295.3941145678982 | 4272.425139788538  |\r\n|     8      |    16     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 14.514714101096615 |  94.2192979855463  |\r\n|     8      |    16     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 13.553097198018804 |  93.244242540095   |\r\n|     8      |    16     |    256    |    256     |   2048    |   True    | torch.bfloat16 | 27.95821905019693  | 185.0469880155288  |\r\n|     8      |    16     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 26.709681446664035 | 184.22623950755226 |\r\n|     8      |    16     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 85.85420495364815  | 388.3417735341937  |\r\n|     8      |    16     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 89.97473795898259  | 434.4228169647977  |\r\n|     8      |    16     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 | 220.6919804448262  | 958.9654899900779  |\r\n|     8      |    16     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 306.55586952343583 | 1233.2170095760375 |\r\n|     8      |    16     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 3470.7326447824016 | 12183.611298678443 |\r\n|     8      |    16     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 2299.064100370742  | 7669.618452200666  |\r\n|     8      |    32     |    128    |    128     |   2048    |   True    | torch.bfloat16 | 12.427107692928985 | 96.96270158747211  |\r\n|     8      |    32     |    128    |    128     |   2048    |   False   | torch.bfloat16 | 11.856995843118057 | 96.38117247959599  |\r\n|     8      |    32     |    256    |    256     |   2048    |   True    | torch.bfloat16 |  32.9956392000895  | 182.52741603646427 |\r\n|     8      |    32     |    256    |    256     |   2048    |   False   | torch.bfloat16 | 29.397601098753512 | 191.0755339777097  |\r\n|     8      |    32     |    512    |    512     |   2048    |   True    | torch.bfloat16 | 89.06024845782667  | 392.2585004474967  |\r\n|     8      |    32     |    512    |    512     |   2048    |   False   | torch.bfloat16 | 97.78487798757851  | 462.07307645818213 |\r\n|     8      |    32     |   1024    |    1024    |   2048    |   True    | torch.bfloat16 |  240.521906001959  | 992.4693452194335  |\r\n|     8      |    32     |   1024    |    1024    |   2048    |   False   | torch.bfloat16 | 341.98952303268015 | 1339.2950996058062 |\r\n|     8      |    32     |   4096    |    2048    |   2048    |   True    | torch.bfloat16 | 4445.311005110853  | 15001.030603889374 |\r\n|     8      |    32     |   4096    |    2048    |   2048    |   False   | torch.bfloat16 | 2535.9767401823774 | 8528.990152990447  |\r\n+------------+-----------+-----------+------------+-----------+-----------+----------------+--------------------+--------------------+\r\n```\r\n\r\n```\r\n{'avg_forward_time_nan_fix': 399.7900972732653,\r\n 'avg_backward_time_nan_fix': 1409.652114014413,\r\n 'avg_forward_time_main_branch': 394.6807206988645,\r\n 'avg_backward_time_main_branch': 1399.4055472857629,\r\n 'geo_mean_nan_fix': 150.95049601244946,\r\n 'geo_mean_main_branch': 148.3381648508822}\r\n ```\r\n \r\nThe y axis is wrong and is micro seconds but the relative comparison still works\r\n<img width=\"790\" alt=\"Screenshot 2024-03-18 at 3 34 15\u202fPM\" src=\"https://github.com/pytorch/pytorch/assets/32754868/ca278c15-b815-4535-bdcd-07e522055466\">\r\n\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/8346690007\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/122135",
    "metadata": {
      "section": null,
      "issue_number": 122135,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "release notes: nn",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "8c14897471ad999134e3ba00e8793077",
    "source": "github_issue",
    "title": "Update skip message to reflect why test is being skipped",
    "text": "Question:\nSummary: Update skip message to reflect why test is being skipped\n\nTest Plan: github\n\nDifferential Revision: D43423288\n\n\n\nAnswer:\n### Merge started\nYour change will be merged immediately since you used the force (-f) flag, **bypassing any CI checks** (ETA: 1-5 minutes).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/4295758775\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/95127",
    "metadata": {
      "section": null,
      "issue_number": 95127,
      "labels": [
        "fb-exported",
        "Merged",
        "ciflow/trunk",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "8a941b9e106186e12b8c4ce04d776f52",
    "source": "github_issue",
    "title": "De-select odd numbered heads from nn.MHA fastpath",
    "text": "Question:\nSummary:\nhttps://github.com/pytorch/pytorch/issues/97128\n\n* Add test for mha num_heads %2 != 0\n* Fix test\n* Add test for bias false\n* show test passes\n\nTest Plan: sandcastle\n\nDifferential Revision: D45161767\n\n\n\ncc @jbschlosser @bhosmer @cpuhrsch @erichan1\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/4791722993\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/99672",
    "metadata": {
      "section": null,
      "issue_number": 99672,
      "labels": [
        "fb-exported",
        "Merged",
        "ciflow/trunk",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "fc9d92e07bf3ac434998ea8bb3036346",
    "source": "github_issue",
    "title": "MHA torch.jit.script fix for in_proj_weight = None",
    "text": "Question:\nSummary: MHA fix to support in_proj_weight being None\n\nTest Plan: sandcastle\n\nDifferential Revision: D43628206\n\n\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/4295604693\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/95653",
    "metadata": {
      "section": null,
      "issue_number": 95653,
      "labels": [
        "fb-exported",
        "Merged",
        "ciflow/trunk",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "82f6899e8ddcbed9dcec766cafad7655",
    "source": "github_issue",
    "title": "Add unit test for nested_tensor input to nn.TransformerEncoder",
    "text": "Question:\nSummary: Add unit test for nested_tensor input & fix\n\nTest Plan: sandcastle\n\nDifferential Revision: D45580393\n\n\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/4898280929\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/100650",
    "metadata": {
      "section": null,
      "issue_number": 100650,
      "labels": [
        "better-engineering",
        "fb-exported",
        "Merged",
        "ciflow/trunk",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "39101359f79e5b7d0664493e85f40a7a",
    "source": "github_issue",
    "title": "Fix a segfault in `new_empty_strided`",
    "text": "Question:\nFixes #82416.\r\n\n\nAnswer:\nHey @nkaretnikov.\nYou've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found [here](https://github.com/pytorch/pytorch/labels?q=release+notes) for the 'release notes: ...' and [here](https://github.com/pytorch/pytorch/labels?q=topic) for the 'topics: ...'.\nFor changes that are 'topic: not user facing' there is no need for a release notes label.",
    "url": "https://github.com/pytorch/pytorch/pull/82422",
    "metadata": {
      "section": null,
      "issue_number": 82422,
      "labels": [
        "module: crash",
        "open source",
        "Merged",
        "cla signed",
        "release notes: cpp",
        "topic: bug fixes",
        "module: edge cases"
      ],
      "answer_author": "github-actions[bot]"
    }
  },
  {
    "doc_id": "65688b8cf5aa83b831e464e3fbdf5b19",
    "source": "github_issue",
    "title": "Fix for `special.zeta` nan handling",
    "text": "Question:\nFixes #[138464](https://github.com/pytorch/pytorch/issues/138464)\r\n\r\nIt seems to me that `torch.special.zeta` is being tested using `scipy.special.zeta`, which also manifests the same counterintuitive behavior, i.e. by executing for example\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy.special import zeta\r\n\r\nx_nan = np.nan\r\nq_nan = np.nan\r\n\r\nresult1 = zeta(x_nan, 1)  \r\nresult2 = zeta(1, q_nan)  \r\n\r\nprint(result1)  # nan\r\nprint(result2)  # inf\r\n```\r\nTherefore, I propose to deviate for this specific edge case from the implementation of `scipy.special.zeta`, and return `NaN`-based values instead.\n\ncc @mruberry @kshitij12345\n\nAnswer:\n@vladimirrotariu Yes I agree this topic is not resolved--could you migrate some concluded details to a new issue linking to this PR instead? That may be the best way to maintain visibility on this issue.",
    "url": "https://github.com/pytorch/pytorch/pull/138653",
    "metadata": {
      "section": null,
      "issue_number": 138653,
      "labels": [
        "triaged",
        "module: NaNs and Infs",
        "open source",
        "module: special",
        "Stale",
        "module: edge cases"
      ],
      "answer_author": "janeyx99"
    }
  },
  {
    "doc_id": "a9d06c0c62424c510ac09523836b3954",
    "source": "github_issue",
    "title": "Fix TypeError in Conv2d by ensuring dilation is always a tuple",
    "text": "Question:\n**Description**:\r\nThis PR addresses an issue in the `Conv2d `class where dilation set as an integer (rather than a tuple) can lead to a `TypeError` when calling methods like `extra_repr` that expect dilation to be a tuple.\r\n\r\n**Problem**\r\nIn certain cases, if dilation is set to an integer, calling methods that access `len(self.dilation)` will raise the following error:\r\n```\r\nTypeError: object of type 'int' has no len()`\r\n```\r\nThis is because dilation can sometimes be stored as an integer, while some internal methods assume it's always a tuple. The [PyTorch Conv2d documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) specifies that dilation can be set as either an integer or a tuple.\r\n\r\n**Changes made**\r\n- Added `@property` and `@dilation.setter` in `Conv2d` to convert integer inputs to tuples using `_pair`.\r\n- Updated the internal `dilation_` usage in the constructor to use `self._dilation`, enforcing the correct format.\r\n- Applied `getattr` fallback logic for loading legacy models, setting dilation as `(1, 1)` if absent.\r\n\r\n**Tests**\r\nThe following code demonstrates the fix. Previously, setting `self.conv.dilation = 1` could raise a `TypeError` when calling `extra_repr`, but this fix resolves that issue:\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass SimpleModel(nn.Module):\r\n    def __init__(self):\r\n        super(SimpleModel, self).__init__()\r\n        self.conv = nn.Conv2d(1, 1, kernel_size=3)\r\n        self.conv.dilation = 1  # setting dilation as an integer should now be handled\r\n\r\n    def forward(self, x):\r\n        return self.conv(x)\r\n\r\nmodel = SimpleModel()\r\ninput_tensor = torch.randn(1, 1, 5, 5)\r\noutput = model(input_tensor)\r\nprint(model)\r\n\r\n```\r\n\n\ncc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki\n\nAnswer:\nLooks like this PR hasn't been updated in a while so we're going to go ahead and mark this as `Stale`. <br>Feel free to remove the `Stale` label if you feel this was a mistake. <br>If you are unable to remove the `Stale` label please contact a maintainer in order to do so. <br>If you want the bot to never mark this PR stale again, add the `no-stale` label.<br>`Stale` pull requests will automatically be closed after 30 days of inactivity.<br>",
    "url": "https://github.com/pytorch/pytorch/pull/140235",
    "metadata": {
      "section": null,
      "issue_number": 140235,
      "labels": [
        "module: nn",
        "module: convolution",
        "triaged",
        "open source",
        "Stale",
        "release notes: nn"
      ],
      "answer_author": "github-actions[bot]"
    }
  },
  {
    "doc_id": "a03593ab4391bcd124929476b68cc73c",
    "source": "github_issue",
    "title": "Update doc strings to make description of is_causal consistent for nn.Transformer and nn.MHA",
    "text": "Question:\nSummary: Update doc strings to make description of is_causal consistent for nn.Transformer and nn.MHA\r\n\r\nTest Plan: sandcastle & github CI/CD\r\n\r\nDifferential Revision: D45737197\r\n\r\n\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/4967686300\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/101089",
    "metadata": {
      "section": null,
      "issue_number": 101089,
      "labels": [
        "fb-exported",
        "Merged",
        "ciflow/trunk",
        "release notes: nn",
        "topic: not user facing",
        "suppress-bc-linter"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "34d9c6edb8c8fc0a7401e73e3deed5bd",
    "source": "github_issue",
    "title": "Fix atomic operation compatibility for ARMv8-A (Raspberry Pi 4) by adjusting compilation flags",
    "text": "Question:\n**Issue:**\r\n* The ldaddal instruction is an AArch64 atomic operation available from ARMv8.1-A onwards.\r\n* Raspberry Pi 4 (Cortex-A72) is ARMv8-A, which does not support ldaddal, leading to failures when running PyTorch built with march=armv8.2-a+sve\r\n* This led to an issue when running PyTorch on ARMv8-A (Raspberry Pi 4), as unsupported atomic operations were generated.\r\n\r\n**Fix:**\r\n* Updated the build flags to explicitly use **-march=armv8-a+sve**, ensuring GCC and clang promotes it correctly and resolves compatibility issues with armv8 and still work correctly for SVE like before.\r\n* This ensures that PyTorch builds correctly for ARMv8-A platforms (e.g., Raspberry Pi 4) while still enabling SVE for supported hardware.\r\n\r\n\r\nTest plan:\r\n - Allocate `a1.4xlarge` on AWS\r\n - Run following script using wheel produced by this PR\r\n ```python\r\nimport torch\r\ndef f(x):\r\n    return x.sin() + x.cos()\r\n\r\nprint(torch.__version__)\r\nf_c = torch.jit.script(f)\r\n```\r\n- Observe no crash\r\n```\r\n$ python3 foo.py \r\n2.7.0.dev20250313+cpu\r\n```\r\n- Observe crash with 2.6.0\r\n```\r\n$ python3 foo.py \r\n2.6.0+cpu\r\nIllegal instruction (core dumped)\r\n```\r\n\r\nFixes #146792\r\n\r\n\r\ncc @malfet @snadampal @milpuz01\n\nAnswer:\n### Cherry picking #148070\nThe cherry pick PR is at https://github.com/pytorch/pytorch/pull/149878 and it is recommended to link a critical cherry pick PR with an issue. The following tracker issues are updated:\n* https://github.com/pytorch/pytorch/issues/149044#issuecomment-2749311817\n\n\n<details><summary>Details for Dev Infra team</summary>\nRaised by <a href=\"https://github.com/pytorch/pytorch/actions/runs/14045108789\">workflow job</a>\n\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/148070",
    "metadata": {
      "section": null,
      "issue_number": 148070,
      "labels": [
        "triaged",
        "open source",
        "module: arm",
        "Merged",
        "ciflow/trunk",
        "release notes: build",
        "topic: bug fixes",
        "ciflow/binaries_wheel",
        "ciflow/linux-aarch64"
      ],
      "answer_author": "pytorchbot"
    }
  },
  {
    "doc_id": "c92a5d308fd96a58c6ac7178b22d6aac",
    "source": "github_issue",
    "title": "[FlexAttention] Fix multiple calls to flex bug",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #137452\n* __->__ #140761\n\r\n\r\n# Summary\r\nFixes long-standing bug we've had in the backward pass for flex attention. See https://github.com/pytorch/pytorch/issues/135161 for details\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang @aakhundov @Chillee @yanboliang @BoyuanFeng\n\nAnswer:\n### Merge started\nYour change will be merged immediately since you used the force (-f) flag, **bypassing any CI checks** (ETA: 1-5 minutes).  Please use `-f` as last resort and instead consider `-i/--ignore-current` to continue the merge ignoring current failures.  This will allow currently pending tests to finish and report signal before the merge.\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/11867356365\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/140761",
    "metadata": {
      "section": null,
      "issue_number": 140761,
      "labels": [
        "Merged",
        "Reverted",
        "ciflow/trunk",
        "topic: not user facing",
        "module: inductor",
        "ciflow/inductor",
        "ciflow/rocm",
        "ci-no-td",
        "module: flex attention"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "dbd63c2d54405576d16c3f2736e7bd5e",
    "source": "github_issue",
    "title": "[FlexAttention] Fix output layout",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* #135884\r\n* __->__ #135882\r\n\r\n\r\n\r\nWe previously only supported the same v_head dim and + qk_head dim. When allowed for different head-dims I accidently kept the same query strides for the output. This PR fixes this bug as well it ensures that we always produce output in the same stride order as the input query.\r\n\r\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang\n\nAnswer:\nConfirm that this is fixed in 2.5.0 by running `pytest -v test/inductor/test_flex_attention.py -k test_flex_attention_stride_ordering`",
    "url": "https://github.com/pytorch/pytorch/pull/135882",
    "metadata": {
      "section": null,
      "issue_number": 135882,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "topic: not user facing",
        "module: inductor",
        "ciflow/inductor"
      ],
      "answer_author": "huydhn"
    }
  },
  {
    "doc_id": "71c2877882078802c57205533359f58c",
    "source": "github_issue",
    "title": "Add missing format string",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #93866\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @desertfire\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/4067221834\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/93866",
    "metadata": {
      "section": null,
      "issue_number": 93866,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "topic: bug fixes",
        "topic: not user facing",
        "module: dynamo",
        "ciflow/inductor",
        "release notes: dynamo"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "bf8f80c45d818b0f383a53d3fba2c56c",
    "source": "github_issue",
    "title": "The static checks of the TransformerEncoder should consider ...",
    "text": "Question:\n**Transformer support num_encoder_layers=0 as PyTorch 1.6**\r\n\r\nThe static checks of the TransformerEncoder should consider num_layers to avoid IndexError.\r\n\r\n\r\nThe `Transformer` can be used to implement all the capabilities of the `TransformerEncoder` and `TransformerDecoder`. Therefore it is also possible to set the `Transformer` initialization parameter `num_encoder_layers` to `0`. In this case the current static check (`first_layer = self.layers[0]`) will throw an exception `IndexError: index 0 is out of range`. This pr is to fix this bug of out-of-bounds array access.\r\n\r\ncc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki @bhosmer @cpuhrsch @erichan1 @drisspg\n\nAnswer:\nDidn't find following labels among repository labels: topic: Transformer support num_encoder_layers=0 as PyTorch 1.6",
    "url": "https://github.com/pytorch/pytorch/pull/103335",
    "metadata": {
      "section": null,
      "issue_number": 103335,
      "labels": [
        "module: nn",
        "triaged",
        "open source",
        "module: edge cases"
      ],
      "answer_author": "pytorch-bot[bot]"
    }
  },
  {
    "doc_id": "36457c0592c56b809040036384eb92cf",
    "source": "github_issue",
    "title": "Remove branch from ReferenceAnalysis mod implementation",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #126170\r\n\r\nThis is still slightly wrong for floats, but it covers the most common\r\ncase.  A proper fix involves refactoring all of the modulus machinery to\r\ndistinguish between Python modulus and C modulus, and a way of\r\npreferentially converting things into C modulus as much as possible.\r\n\r\nBandaids over https://github.com/pytorch/pytorch/issues/125376\r\n\r\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\r\n\r\ncc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang\n\nAnswer:\n## Merge failed\n**Reason**: 2 mandatory check(s) failed.  The first few are:\n- [Lint / Test collect_env (with_torch)](https://github.com/pytorch/pytorch/actions/runs/9081196442/job/24954453415)\n- [Lint / Test collect_env (older_python_version)](https://github.com/pytorch/pytorch/actions/runs/9081196442/job/24954454430)\n\nDig deeper by [viewing the failures on hud](https://hud.pytorch.org/pytorch/pytorch/commit/8b7d9070376cbcf53f0bf35507e9ef7c34b5f484)\n\n<details><summary>Details for Dev Infra team</summary>\nRaised by <a href=\"https://github.com/pytorch/pytorch/actions/runs/9081226967\">workflow job</a>\n\nFailing merge rule: Core Maintainers\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/126170",
    "metadata": {
      "section": null,
      "issue_number": 126170,
      "labels": [
        "module: cpu",
        "ciflow/trunk",
        "release notes: composability",
        "topic: bug fixes",
        "module: dynamo",
        "ci-td-distributed"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "dc7294825f776f16d742f9105c26a75c",
    "source": "github_issue",
    "title": "Fix #149806 : Fix path lookup in _preload_cuda_deps",
    "text": "Question:\n@pytorchbot label \"bug\"\r\n\r\nFixes #149806\r\n\n\nAnswer:\n### Cherry picking #149808\nThe cherry pick PR is at https://github.com/pytorch/pytorch/pull/150068 and it is linked with issue preload cuda dependencies logic. The following tracker issues are updated:\n* https://github.com/pytorch/pytorch/issues/149044#issuecomment-2755994135\n\n\n<details><summary>Details for Dev Infra team</summary>\nRaised by <a href=\"https://github.com/pytorch/pytorch/actions/runs/14095387094\">workflow job</a>\n\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/149808",
    "metadata": {
      "section": null,
      "issue_number": 149808,
      "labels": [
        "triaged",
        "open source",
        "Merged",
        "ciflow/trunk",
        "topic: not user facing"
      ],
      "answer_author": "pytorchbot"
    }
  },
  {
    "doc_id": "63d43e1bd3d7d8c287ac9a754f640de3",
    "source": "github_issue",
    "title": "Add lowerings for all symbolic shape operators",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #94121\n\nIn particular, this fixes the missing negative problem.\n\nSigned-off-by: Edward Z. Yang <ezyang@meta.com>\n\ncc @mlazos @soumith @voznesenskym @yanboliang @penguinwu @anijain2305 @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @Xia-Weiwen @wenzhe-nrv @jiayisunx @peterbell10 @desertfire\n\nAnswer:\n### Merge started\nYour change will be merged immediately since you used the force (-f) flag, **bypassing any CI checks** (ETA: 1-5 minutes).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/4091675669\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/94121",
    "metadata": {
      "section": null,
      "issue_number": 94121,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "release notes: fx",
        "topic: bug fixes",
        "module: inductor",
        "ciflow/inductor",
        "release notes: inductor"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "8b8672fa76d7a03a558f2bc86cf0cdfb",
    "source": "github_issue",
    "title": "[primTorch] Fix off by 1 in `canonicalize_dim`",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack):\n* #82323\n* #82322\n* #83278\n* #82321\n* __->__ #83198\n\nAlso fix an issue in the `unsqueeze` ref due to this change.\n\nAnswer:\nHey @nkaretnikov.\nYou've committed this PR, but it does not have both a 'release notes: ...' and 'topics: ...' label. Please add one of each to the PR. The 'release notes: ...' label should represent the part of PyTorch that this PR changes (fx, autograd, distributed, etc) and the 'topics: ...' label should represent the kind of PR it is (not user facing, new feature, bug fix, perf improvement, etc). The list of valid labels can be found [here](https://github.com/pytorch/pytorch/labels?q=release+notes) for the 'release notes: ...' and [here](https://github.com/pytorch/pytorch/labels?q=topic) for the 'topics: ...'.\nFor changes that are 'topic: not user facing' there is no need for a release notes label.",
    "url": "https://github.com/pytorch/pytorch/pull/83198",
    "metadata": {
      "section": null,
      "issue_number": 83198,
      "labels": [
        "open source",
        "Merged",
        "cla signed",
        "topic: bug fixes",
        "module: primTorch"
      ],
      "answer_author": "github-actions[bot]"
    }
  },
  {
    "doc_id": "185e3525190a64b5df3016a591973adb",
    "source": "github_issue",
    "title": "correct BLAS input",
    "text": "Question:\nFixes #32407\r\n\r\nWith this little correction to Dependencies.cmake it is possible to build an MKL-free version of Pytorch up from version v2.0.0 by explicitly choosing another MKL-free BLAS.\r\n\r\nThis pullrequest fulfills the \"if not already present\" part of the original comment in  Dependencies.cmake:\r\n\"setting default preferred BLAS options if not already present.\" \r\n\r\nIt's tested with this Action-.yml:\r\n```\r\nname: Build PyTorch v2.0.0 without AVX\r\n\r\non:\r\n  push:\r\n    branches:\r\n      - v2.0.0\r\n  pull_request:\r\n    branches:\r\n      - v2.0.0\r\n\r\njobs:\r\n  build:\r\n    runs-on: ubuntu-20.04\r\n    defaults:\r\n      run:\r\n        shell: bash -el {0}\r\n    steps:\r\n\r\n    - name: Checkout repository\r\n      uses: actions/checkout@v4\r\n      with:\r\n        #repository: 'pytorch/pytorch'\r\n        #ref: 'v2.3.0'\r\n        submodules: 'recursive'\r\n\r\n    - uses: conda-incubator/setup-miniconda@v3\r\n      with:\r\n        auto-activate-base: true\r\n        activate-environment: true\r\n        python-version: 3.10.13\r\n\r\n    - name: Install Dependencies - Common - Linux 2\r\n      run: |\r\n        conda info\r\n        conda list\r\n        conda install nomkl\r\n        conda install astunparse numpy ninja pyyaml setuptools cmake cffi typing_extensions future six requests dataclasses\r\n        export PYTORCH_CPU_CAPABILITY=cpu\r\n        export ATEN_CPU_CAPABILITY_DEFAULT=cpu\r\n        export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\r\n        export ATEN_CPU_CAPABILITY=default\r\n        export USE_NNPACK=0\r\n        export MAX_JOBS=4\r\n        export USE_CUDA=0\r\n        export USE_ROCM=0\r\n        export BLAS=OpenBLAS\r\n        export CMAKE_ARGS=\"-D CMAKE_BUILD_TYPE=Release -D USE_AVX=OFF -D USE_NNPACK=OFF -D C_HAS_AVX_2=OFF -D C_HAS_AVX2_2=OFF -D CXX_HAS_AVX_2=OFF -D CXX_HAS_AVX2_2=OFF -D CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS=OFF -DPYTHON_INCLUDE_DIR=$(python -c \"import sysconfig; print(sysconfig.get_path('include'))\") -DPYTHON_LIBRARY=$(python -c \"import sysconfig; print(sysconfig.get_config_var('LIBDIR'))\") -DPYTHON_EXECUTABLE:FILEPATH=`which python`\"\r\n        pip install build wheel typing_extensions\r\n        python setup.py bdist_wheel\r\n    - name: Archive production artifacts\r\n      uses: actions/upload-artifact@v4\r\n      with:\r\n        name: dist-without-markdown\r\n        path: |\r\n          dist\r\n          !dist/**/*.md\r\n```\n\nAnswer:\n@malfet can you take a look at the errors in the faulty runs? I doubt I caused them.",
    "url": "https://github.com/pytorch/pytorch/pull/126200",
    "metadata": {
      "section": null,
      "issue_number": 126200,
      "labels": [
        "triaged",
        "open source",
        "Merged",
        "Stale",
        "ciflow/trunk",
        "topic: not user facing",
        "ciflow/binaries_wheel"
      ],
      "answer_author": "rmast"
    }
  },
  {
    "doc_id": "8d3e727a7722865d6fd11076ff036cd8",
    "source": "github_issue",
    "title": "Align mask formatting of both masks more closely",
    "text": "Question:\nSummary: Align mask formatting of both masks more closely\n\nTest Plan: sandcastle & github\n\nDifferential Revision: D43878634\n\n\n\nAnswer:\n## Merge failed\n**Reason**: 1 jobs have failed, first few of them are: [trunk / macos-12-py3-arm64-mps / test (default, 1, 1)](https://github.com/pytorch/pytorch/actions/runs/4379758261/jobs/7666340600)\n\n<details><summary>Details for Dev Infra team</summary>\nRaised by <a href=\"https://github.com/pytorch/pytorch/actions/runs/4379819009\">workflow job</a>\n\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/96286",
    "metadata": {
      "section": null,
      "issue_number": 96286,
      "labels": [
        "fb-exported",
        "Merged",
        "ciflow/trunk",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "d86e0e9fa5b91a05f4a59f95853c97b6",
    "source": "github_issue",
    "title": "Fix bug in mem_eff kernel with attention mask and MQA",
    "text": "Question:\n# Summary\r\n\r\nFound using the repros mentioned in this issue: #112577\r\n\r\nAfter many go rounds with compute-sanitizer and eventual printf debugging I feel pretty confident that this was the underlying issue \n\ncc @ptrblck\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/7293528484\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/116234",
    "metadata": {
      "section": null,
      "issue_number": 116234,
      "labels": [
        "module: cuda",
        "Merged",
        "ciflow/trunk",
        "release notes: nn"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "25906dbfd43e693d1111b2817afc3655",
    "source": "github_issue",
    "title": "Fix FakeTensorProp on Module with Parameters or Buffers",
    "text": "Question:\nIn `FakeTensorMode.__torch_dispatch__`, the output is now always computed by meta kernels in\r\n```python\r\n        try:\r\n            with in_kernel_invocation_manager(self):\r\n                r = func(*args, **kwargs)  # <----- \"r\" can be a real tensor.\r\n        except NotImplementedError as not_implemented_error:\r\n            # no meta kernel registered, fallback to kernel for the device\r\n            if not self.allow_fallback_kernels:\r\n                raise not_implemented_error\r\n            return run_fallback_kernel(self, func, args, kwargs, not_implemented_error)\r\n\r\n        return self.wrap_meta_outputs_with_default_device_logic(r, func, args, kwargs)\r\n```\r\nFor example, I observed a CPU tensor is generated when executing `aten.addmm` when running `FakeTensorProp`. Therefore, I'd like to allow `FakeTensorMode` to wrap real tensor as `FakeTensor` during the computation. Does this PR look a good direction to fix this problem? If yes, I can go ahead and add some tests.\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/3441698894\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/88700",
    "metadata": {
      "section": null,
      "issue_number": 88700,
      "labels": [
        "triaged",
        "open source",
        "Merged",
        "ciflow/trunk",
        "release notes: fx",
        "module: fakeTensor",
        "ciflow/inductor"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "2ad59bf5483cb116db391d5f5c34efde",
    "source": "github_issue",
    "title": "Suppress RecursionError in sympy; fix logging",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #90904\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/3709141282\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/90904",
    "metadata": {
      "section": null,
      "issue_number": 90904,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "release notes: composability",
        "release notes: fx",
        "topic: bug fixes"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "ffd440723bd00d176a53fb0f0de398dd",
    "source": "github_issue",
    "title": "torch/config: fix mock behaviour",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #140779\n\nMock only replaces the value that was removed, if after deletion, it\ndoes not see the attribute.\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/11924152093\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/140779",
    "metadata": {
      "section": null,
      "issue_number": 140779,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "release notes: inductor"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "8f032e6929f33cc2f4178a79032b70d5",
    "source": "github_issue",
    "title": "[FlexAttention]  Fix bug when checking whether to return LSE",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* #134511\n* #134507\n* __->__ #134495\n\n\n\ncc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @kadeng @muchulee8 @ColinPeppler @amjames @desertfire @chauhang\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/10566629462\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/134495",
    "metadata": {
      "section": null,
      "issue_number": 134495,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "topic: bug fixes",
        "topic: not user facing",
        "module: inductor"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "2f273b6d255ab1f5f2ebbc6728659f9e",
    "source": "github_issue",
    "title": "[FlexAttention] Fix IMA bug",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #147918\r\n\r\n# Summary\r\nFixes: https://github.com/pytorch/pytorch/issues/147268\r\n\r\nI got this right for the backwards and somehow forgot to do the flip in the forward, not sure how this wasnt found earlier..\r\n\r\nTesting IMAs is tuff in pytest so didnt add but verified on reproducer\r\n\r\n```py\r\n\u276f sanitize python flex/maurice_ima.py --setting 0\r\n========= COMPUTE-SANITIZER\r\npool: torch.Size([64, 8, 784, 64]) tensor(1.0078, device='cuda:0')\r\nFeat shape torch.Size([64, 8, 784, 64])\r\nFeat strides (401408, 50176, 64, 1)\r\nFeat is contig: True\r\nattn: torch.Size([64, 8, 784, 64]) tensor(1.7994, device='cuda:0')\r\n========= ERROR SUMMARY: 0 errors\r\n\u276f sanitize python flex/maurice_ima.py --setting 1\r\n========= COMPUTE-SANITIZER\r\npool: torch.Size([64, 8, 784, 64]) tensor(2.8297, device='cuda:0')\r\nFeat shape torch.Size([64, 8, 784, 64])\r\nFeat strides (401408, 50176, 64, 1)\r\nFeat is contig: True\r\nattn: torch.Size([64, 8, 784, 64]) tensor(1.9714, device='cuda:0')\r\n========= ERROR SUMMARY: 0 errors\r\n\u276f sanitize python flex/maurice_ima.py --setting 2\r\n========= COMPUTE-SANITIZER\r\npool: torch.Size([64, 8, 784, 64]) tensor(3.2232, device='cuda:0')\r\nFeat shape torch.Size([64, 8, 784, 64])\r\nFeat strides (401408, 50176, 64, 1)\r\nFeat is contig: True\r\nattn: torch.Size([64, 8, 784, 64]) tensor(2.2095, device='cuda:0')\r\n========= ERROR SUMMARY: 0 errors\r\n````\r\n\n\ncc @ezyang @gchanan @zou3519 @kadeng @msaroufim @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @yf225 @chenyang78 @muchulee8 @amjames @chauhang @aakhundov\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/13549928510\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/147918",
    "metadata": {
      "section": null,
      "issue_number": 147918,
      "labels": [
        "high priority",
        "module: nn",
        "Merged",
        "ciflow/trunk",
        "release notes: nn",
        "module: inductor",
        "ciflow/inductor"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "a9c21a8124829fe809047ebd08c5e0e7",
    "source": "github_issue",
    "title": "Handle trailing masked column behavior for nested tensor",
    "text": "Question:\nSummary:\r\nHandle trailing masked column behavior for nested tensor by padding during to_padded, to original tensor size\r\n\r\nhttps://github.com/pytorch/pytorch/issues/97111\r\n\r\nTest Plan: sandcastle & github\r\n\r\nDifferential Revision: D45167874\r\n\r\n\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/4866467963\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/100113",
    "metadata": {
      "section": null,
      "issue_number": 100113,
      "labels": [
        "fb-exported",
        "Merged",
        "ciflow/trunk",
        "topic: bug fixes",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "f94b38443714bcbfa1c64bb06bdf7f44",
    "source": "github_issue",
    "title": "is_causal transformer encoder",
    "text": "Question:\nSummary: is_causal transfromer encoder\n\nTest Plan: sandcastle / github\n\nDifferential Revision: D44152507\n\n\n\nAnswer:\nThis pull request was **exported** from Phabricator. Differential Revision: [D44152507](https://www.internalfb.com/diff/D44152507)",
    "url": "https://github.com/pytorch/pytorch/pull/97050",
    "metadata": {
      "section": null,
      "issue_number": 97050,
      "labels": [
        "fb-exported",
        "topic: not user facing"
      ],
      "answer_author": "facebook-github-bot"
    }
  },
  {
    "doc_id": "6da31defed5e6c8000f5c3efec251c58",
    "source": "github_issue",
    "title": "[cuDNN][cuDNN V8 API] Use suggest memory format for cuDNN V8 API",
    "text": "Question:\nFixes some failures we observed in `functorch` tests which seemed to stem from benchmark cache collisions on the same memory format. Changing the memory format to be dependent on both input and weight seems to resolve them.\r\n\r\nCC @crcrpar @ptrblck\n\ncc @csarofeen @ptrblck @xwang233\n\nAnswer:\n### Merge started\nYour change will be merged immediately since you used the force (-f) flag, **bypassing any CI checks** (ETA: 1-5 minutes).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/3325014076\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/87617",
    "metadata": {
      "section": null,
      "issue_number": 87617,
      "labels": [
        "module: cudnn",
        "open source",
        "Merged",
        "ciflow/trunk",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "0d2d514eef8ae77f61bfbe1666cfaa3f",
    "source": "github_issue",
    "title": "Use standard __func__ macro in symbolic shape.",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\r\n* __->__ #89264\r\n\r\nSummary:\r\nI saw the following issue only on Windows build in PR #88767:\r\n```\r\nRuntimeError: AttributeError: 'SymNode' object has no attribute 'torch::impl::PythonSymNodeImpl::ge'\r\n```\r\nIt's only on Windows because we get the attributes of SymNode in C++ with\r\n`__FUNCTION__` macro, which is not in C++ standard, therefore has platform specific behavior.\r\nIn this case, MSVC will include a function's namespace and class name, which is not intended here.\r\n\r\nInstead we should use `__func__`. see: https://en.cppreference.com/w/cpp/language/function#Function_definition\r\n\r\ngodbolt example to show the difference: https://godbolt.org/z/PGfvecxPx\r\n\r\nTest Plan:\r\nCI\r\n\r\nReviewers:\r\n\r\nSubscribers:\r\n\r\nTasks:\r\n\r\nTags:\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/3497393372\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/89264",
    "metadata": {
      "section": null,
      "issue_number": 89264,
      "labels": [
        "Merged",
        "ciflow/trunk",
        "release notes: composability",
        "topic: bug fixes",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "2a7e1470b31b0f80b2f8cd78fb34167b",
    "source": "github_issue",
    "title": "Fix Python-bound function signature (torch._C.Graph.addInput)",
    "text": "Question:\nIn pytorch/torch/_C/__init__.pyi, Graph.addInput has signature\r\n```python\r\n  def addInput(self, name: str) -> Value: ...\r\n```\r\nwhich doesn't match the corresponding function\r\n```cpp\r\n  Value* addInput(const std::string& name = \"\") {\r\n    return block_->addInput(name);\r\n  }\r\n\r\n```\r\n\r\nin python_ir.cpp. This PR aligns the bound function on both C++ and Python sides. Without this PR, mypy will compain whenever a change contains some calls to `addInput`; for example,\r\n![image](https://user-images.githubusercontent.com/3524474/200092086-429b8d63-9321-4d03-b0d6-f4c9bd361756.png)\r\n\r\n\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/3423484973\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/88528",
    "metadata": {
      "section": null,
      "issue_number": 88528,
      "labels": [
        "open source",
        "Merged",
        "ciflow/trunk",
        "release notes: jit"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "774b3374ce75233550a32f813cb0cc58",
    "source": "github_issue",
    "title": "[pthreadpool] Set max threadlimit to tsan limit",
    "text": "Question:\nSummary:\nThis will make sure we don't run into an internal assert for clang tsan which has a cap of 63 on concurrently held lock count.\nSeems like it is failing with 64 since the comparison is `<`, so setting it to 63 here.\n\n```\nllvm-project/compiler-rt/lib/sanitizer_common/sanitizer_deadlock_detector.h:67 \"((n_all_locks_)) < (((sizeof(all_locks_with_contexts_)/sizeof((all_locks_with_contexts_)[0]))))\"\n```\n\nCreated from CodeHub with https://fburl.com/edit-in-codehub\n\nTest Plan:\nCI\n\nSandcastle run\n\nReviewed By: kimishpatel, salilsdesai\n\nDifferential Revision: D41444710\n\n\n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/3644528686\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/89453",
    "metadata": {
      "section": null,
      "issue_number": 89453,
      "labels": [
        "caffe2",
        "fb-exported",
        "Merged",
        "ciflow/trunk",
        "topic: not user facing"
      ],
      "answer_author": "pytorchmergebot"
    }
  },
  {
    "doc_id": "e24a163e5a9d2a491798e05a3913ff0f",
    "source": "github_issue",
    "title": "Keep track of `ViewMeta` with symbolic inputs.",
    "text": "Question:\nStack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):\n* __->__ #125876\n\r\nFix: #125387\r\n\r\nThis PR helps keep track of whether an instantiated `ViewMeta` has symbolic values as\r\ninput or not. This is used for checking whether we use the AOTAutograd `ViewMeta`-replay\r\nexecution path, e.g. it doesn't support tensors that have `ViewMeta` with symbolic inputs.\r\n\r\nIn summary, the changes are:\r\n\r\n- Add the field `ViewMeta::has_symbolic_inputs` and make it a required constructor\r\nparameter\r\n- Add the field `FunctionalTensorWrapper::is_symbolic_` and the method\r\n`FunctionalTensorWrapper::maybe_mark_symbolic`\r\n    - Marks a `FunctionalTensorWrapper` as symbolic iff any of its `ViewMeta` have\r\n    symbolic inputs\r\n- Add the plumbing of `FunctionalTensorWrapper::is_symbolic` to the Python API\r\n- Codegen the computation of `ViewMeta::has_symbolic_inputs` for each view operation\r\n- Use the AOTAutograd `ViewMeta`-replay path if:\r\n    - `target_functional_tensor` is not `None`; and\r\n    - `target_functional_tensor` is not symbolic (instead of using a functorch config)\r\n\r\ncc @bdhirsh @miladm @lezcano \n\nAnswer:\n### Merge started\nYour change will be merged once all checks pass (ETA 0-4 Hours).\n\nLearn more about merging in the [wiki](https://github.com/pytorch/pytorch/wiki/Bot-commands).\n\nQuestions? Feedback? Please reach out to the [PyTorch DevX Team](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)<details><summary>Advanced Debugging</summary>\nCheck the merge workflow status \n<a href=\"https://github.com/pytorch/pytorch/actions/runs/9046724684\">here</a>\n</details>",
    "url": "https://github.com/pytorch/pytorch/pull/125876",
    "metadata": {
      "section": null,
      "issue_number": 125876,
      "labels": [
        "open source",
        "Merged",
        "ciflow/trunk",
        "topic: bug fixes",
        "ciflow/inductor",
        "release notes: dynamo"
      ],
      "answer_author": "pytorchmergebot"
    }
  }
]